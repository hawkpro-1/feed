{
  "version": "https://jsonfeed.org/version/1",
  "title": "Facebook",
  "home_page_url": "https://engineering.fb.com/rss",
  "items": [
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/09/13/core-data/superpack/",
      "title": "Superpack: Pushing the limits of compression in Facebook’s mobile apps",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eManaging app size at Facebook is a unique challenge: Every day, developers check in large volumes of code, and each line of code translates into additional bits in the apps that people ultimately download onto their phones. Left unchecked, this added code would make the app bigger and bigger until eventually the time it takes to download would become unacceptable. Compression is one of the methods we use to keep app size minimal. These compressed files take up less space, which means smaller apps that download faster and use less bandwidth for billions of users around the world. Such savings are especially important in regions where mobile bandwidth is limited, making it costly to download large apps. But compression alone isn’t enough to keep pace with all the updates we make and features we add to our apps. So we developed a technique called Superpack, which combines compiler analysis with data compression to uncover size optimizations beyond the capability of traditional compression tools. Superpack pushes the limits of compression to achieve significantly better compression ratios than existing compression tools.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOver the past two years, Superpack has been able to check developer-induced app size growth and keep our Android apps small. Superpack’s compression has helped reduce the size of our fleet of Android apps, which are substantially smaller in comparison to regular Android APK compression, with average savings of over 20 percent compared with Android’s default Zip compression. Some apps that use Superpack include Facebook, Instagram, WhatsApp, and Messenger. The reduction in the size of these apps thanks to Superpack is illustrated in the table below. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg\" alt=\"Table illustrating the reduction in the size of these apps thanks to Superpack\" width=\"1920\" height=\"1081\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=1024,577 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=1536,865 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg\" alt=\"Table showing percentage improvement in app size, thanks to Superpack \" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eSuperpack: Compilers meet data compression\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhile existing compression algorithms, such as Zip’s Deflate and Xz’s LZMA, work well with monolithic data, they weren’t enough to offset the pace of growth we were seeing in our apps, so we set out to develop our own solution. Compression is a mature field, and the techniques we’ve developed crosscut the entire compression spectrum, from data comprehension and Lempel-Ziv (LZ) parsing to statistical coding. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSuperpack’s strength lies in compressing code, such as machine code and bytecode, as well as other types of structured data. The approach underlying Superpack is based on an insight in \u003c/span\u003e\u003ca href=\"https://www.tandfonline.com/doi/abs/10.1080/00207166808803030\"\u003e\u003cspan\u003eKolmogorov’s algorithmic measure of complexity\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which defines the information content of a piece of data as the length of the shortest program that can generate that data. In other words, data can be compressed by representing it as a program that generates the data. When that data is code to begin with, then it can be transformed into one with a smaller compressed representation. A program that generates Fibonacci numbers, coupled with a list of their indices, is a highly compressed representation of a file containing such numbers. The idea of reducing Kolmogorov complexity in itself is not new to the domain of compression. Superpack’s novel approach involves combining compiler methods with modern compression techniques to achieve this goal.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThere is considerable benefit in formalizing compression as a generative process that produces small programs. It gives the data compression engineer access to a treasure trove of mature compiler tools and techniques that can be repurposed to the end of data compression. Superpack compression leverages common compiler techniques such as parsing and code generation, as well as more recent innovations such as \u003c/span\u003e\u003ca href=\"https://dl.acm.org/doi/10.1145/1995376.1995394\"\u003e\u003cspan\u003eSatisfiability modulo theories (SMT) solvers\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to find the smallest programs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOne important ingredient of Superpack’s effectiveness is its ability to marry these compiler techniques with those used in mainstream data compression. Semantic knowledge from the compiler half of Superpack leads to enhanced LZ parsing (the step in compression that eliminates redundancy), as well as improved entropy coding (the step that produces short codes for frequent pieces of information). \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eImproved LZ parsing\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eCompressors typically identify repeating sequences of bytes using an algorithm selected from the LZ family. Broadly, each such algorithm tries to substitute recurring sequences of data with pointers to their previous occurrences. The pointer consists of the distance in number of bytes to the previous occurrence, along with the length of the sequence. If the pointer can be represented in fewer bits than the actual data, then the substitution is a compressed-size win. Superpack improves the process of LZ parsing by enabling the discovery of longer repeating sequences while also reducing the number of bits to represent pointers.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn the programs being compressed, Superpack enables these improvements by grouping data based on its AST. For example, in the following sequence of instructions, the length of the longest repeating sequence is 2. However, when sorted into groups based on AST types, namely, the opcode and registers (Group 1 in the table below) and immediates (Group 2 in the table), the length increases to 4. In the raw parse of the original data, the distance between the repeated sequences is 2 instructions. But in the grouped version, the distance is 0. Smaller distances typically use fewer bits, and longer sequence matches save space by capturing more input data in a given pointer. Accordingly, the pointer that Superpack generates is smaller than the one computed naively.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg\" alt=\"In the programs being compressed, Superpack enables these improvements by grouping data based on its AST. \" width=\"1920\" height=\"882\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=916,421 916w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=768,353 768w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=1024,470 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=1536,706 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=96,44 96w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=192,88 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBut how do we decide when to split the code stream and when to leave it intact? Recent work in Superpack introduces hierarchical compression, which incorporates this decision into the optimizing component of LZ parsing, called the optimal parse. In the edited code below, it is best to leave the last segment of the snippet in its original form, and generate a single match with a pointer to the first five instructions, while splitting the rest of the snippet. In the split-out remainder, the sparseness of register combinations is exploited to generate longer matches. Grouping the code in this manner also further reduces distances by counting the number of logical units between repeating occurrences, as measured along the AST, instead of measuring the number of bytes.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg\" alt=\"Grouping the code in this manner also further reduces distances by counting the number of logical units between repeating occurrences, as measured along the AST, instead of measuring the number of bytes.\" width=\"1920\" height=\"902\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=916,430 916w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=768,361 768w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=1024,481 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=1536,722 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=192,90 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eImproved entropy coding\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eRepeating sequences of bytes are substituted efficiently with a pointer to the previous occurrence. But what does the compressor do for a nonrepeating sequence, or for short sequences that are cheaper to represent than a pointer? In such cases, compressors represent the data literally by coding the values in it\u003c/span\u003e\u003ci\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eThe number of bits used to represent a literal exploits the distribution of values that the literal can assume. Entropy coding is the process of representing a value using roughly as many bits as the entropy of the value in the data. Some well-known techniques that compressors use to this end include Huffman coding, arithmetic coding, range coding, and asymmetrical numeral systems (ANS).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSuperpack has a built-in ANS coder, but also features a pluggable architecture that supports multiple such coding back ends. Superpack improves entropy coding by identifying contexts in which the literals to be represented have lower entropy. Like in the case of LZ parsing, the contexts are derived from Superpack’s knowledge of the structure of the data extracted via compiler analysis. In the reduced sequence of instructions below, there are seven different addresses, each with the prefix 0x. In a large volume of different arrangements of this code, the number of bits used by a regular coder to represent the address field would approach 3\u003c/span\u003e\u003ci\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHowever, we notice that three out of the seven\u003c/span\u003e \u003cspan\u003eaddresses are paired with the BL opcode, while another\u003c/span\u003e \u003cspan\u003ethree are associated with B. Only one is coupled with both. If this pattern were to hold true in the entire body of code, then the opcode can be used as a coding context. With this context, the number of bits to represent these seven addresses approaches 2\u003c/span\u003e \u003cspan\u003einstead of 3. The table below shows the coding with and without the context. In the Superpack-compressed case in the third column, the opcode can be seen as predicting the missing bit. This simple example was contrived to illustrate how compiler contexts can be used to improve coding. In real data, the number of bits gained is usually fractional, and the mappings between contexts and data are seldom as direct as in this example.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg\" alt=\"n real data, the number of bits gained are usually fractional, and the mappings between contexts and data are seldom as direct as in this example.\" width=\"1920\" height=\"773\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=916,369 916w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=768,309 768w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=1024,412 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=1536,618 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=96,39 96w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=192,77 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePrograms as compressed representations\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe explained how Superpack improves LZ parsing and entropy coding when the data being compressed consists of code. But what happens when the data contains a stream of unstructured values? In such cases, Superpack tries to lend the values structure by transforming them into programs at compression time. Then, at decompression time, the programs are interpreted to recover the original data. An example of this technique is the compression of Dex references,\u003c/span\u003e \u003cspan\u003ewhich are labels for well-known values in Dex code. Dex references have a high degree of locality. To exploit this locality, we transform references into a language that stores recent values in a logical register, and issues forthcoming values as deltas from the values that were pinned down.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg\" alt=\"we transform references into a language that stores recent values in a logical register, and issues forthcoming values as deltas from the values that were pinned down.\" width=\"1920\" height=\"353\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=916,168 916w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=768,141 768w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=1024,188 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=1536,282 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=96,18 96w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=192,35 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWriting an efficient compressor for this representation reduces to the familiar register allocation problem in compilers, which decides when to evict values from registers to load new values. While this reduction is specific to reference bytecode,\u003c/span\u003e \u003cspan\u003ea general idea applies to any bytecode representation, namely, that the resulting code is amenable to the optimizations outlined in the previous two sections. In this example, LZ parsing is improved by cohorting the opcodes, MOV and PIN, in one group, collecting the deltas in a second group, and recent references in a third group.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eSuperpack on real data\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThere are three main payloads targeted by Superpack. The first is Dex bytecode, the format into which Java gets compiled in Android apps. The second is ARM machine code, which is code compiled for ARM processors. The third is Hermes bytecode, which is a specialized high performance bytecode representation of Javascript created at Facebook. All three representations use the full breadth of Superpack techniques, powered by compiler analysis based on a knowledge of the syntax and grammar of the code. In all three cases, there is one set of compression transforms that is applied to the stream of instructions and a different set that is applied to metadata. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe transforms applied to code are all alike. Metadata transforms have two parts. The first part leverages the structure of the data, by grouping items by type. The second part leverages organizing rules in the specification of the metadata, such as those that cause the data to be sorted or expose correlations between items that can be used to contextualize distances and literals.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe compression ratios yielded by Zip, Xz, and Superpack for these three formats are shown in the table below.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg\" alt=\"The compression ratios yielded by Zip, Xz, and Superpack for these three formats are shown in the table below.\" width=\"1920\" height=\"372\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=916,177 916w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=768,149 768w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=1024,198 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=1536,298 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=96,19 96w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=192,37 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eSuperpack architecture and implementation\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eSuperpack is a unique player in the compression space in that baked into it is knowledge of the types of data that it compresses. In order to scale the development and use of Superpack at Facebook, we developed a modular design with abstractions that could be reused across the different formats that we compress. Superpack is architected like an operating system, with a kernel that implements paged memory allocation, file and archive abstractions, abstractions for transforming and manipulating instructions, as well as interfaces to pluggable modules. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eCompiler-oriented mechanisms fall into a dedicated compiler layer. Each format is implemented as a pluggable\u003c/span\u003e \u003cspan\u003edriver. Drivers exploit properties of the data being compressed, and label correlations in the code, to eventually be leveraged by the compression layer. The machinery that parses the input code uses automated inference based on an SMT solver. How we use SMT solvers to aid compression is beyond the scope of this post but will make a fascinating topic for a future blog post.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe compression layer also consists of pluggable modules. One of these modules is Superpack’s own compressor, which includes a custom LZ engine and an entropy coding back end. While we were in the process of building this compressor, we plugged in modules that leveraged existing compression tools to do the compression work. In that setting, Superpack’s role is reduced to reorganizing the data into uncorrelated streams. A best effort compression by an existing tool follows, which is effective but limited in the granularity at which it can identify and use compiler information. Superpack’s custom compression back end solves this problem through a fine-grained view of the internal representation of the data, which enables it to exploit logical correlations at the fine granularity of a single bit. Abstracting out the mechanism used to do the compression work as a module gives us a selection of a number of trade-offs between compression ratio and decompression speed.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg\" alt=\"Abstracting out the mechanism used to do the compression work as a module gives us a selection of a number of tradeoffs between compression ratio and decompression speed.\" width=\"1920\" height=\"1081\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=1024,577 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=1536,865 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSuperpack’s implementation contains a mix of code written in the OCaml programming language and C code. OCaml is used on the compression side to manipulate complex compiler-oriented data structures and to interface with an SMT solver. C is a natural choice for decompression logic because it tends to be simple and at the same time is highly sensitive to the parameters of the processor on which the decompression code runs, such as the size of the L1 cache.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eLimitations and related work\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eSuperpack is an asymmetric compressor, which means that decompression is fast but compression is allowed to be slow. Streaming compression, in which data is compressed at the rate at which it is transmitted, has been a nongoal of Superpack. Superpack is unable to fit the constraints for this use case, as its present compression speed is not able to keep up with modern data transfer rates. Superpack has been applied to structured data, code, integer, and string data. It does not currently target image, video, or sound files.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOn the Android platform, there is a trade-off between using compression to reduce download time and a possible increase in disk footprint and update size. This trade-off is not a limitation of Superpack, rather that interoperability has not yet been established between the packaging tools used by Facebook and the distribution tools used on Android. For example, on Android, app updates are distributed as deltas between the contents of consecutive versions of an app. But such deltas can only be generated by tools that are able to decompress and recompress the app’s contents. Since the diffing process implemented in the current tooling is not able to interpret Superpack archives, the deltas come out to be larger for apps containing such archives. We believe that issues of this type could be addressed through finer-grained interfaces between Superpack and Android tools, increased customizability in Android’s distribution mechanisms, and a public documentation of Superpack’s file format and compression methods. Facebook’s apps are dominated by code of the type that Superpack excels at compressing, in a way that goes far beyond existing compression implemented as part of Google Play on Android. So, for the time being, our compression is beneficial to our users despite the trade-off.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSuperpack leverages Jarek Duda’s work on \u003c/span\u003e\u003ca href=\"https://arxiv.org/abs/0902.0271\"\u003e\u003cspan\u003easymmetrical numeral systems\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e as its entropy coding back end. Superpack draws on ideas in \u003c/span\u003e\u003ca href=\"https://dl.acm.org/doi/abs/10.1145/36177.36194\"\u003e\u003cspan\u003esuperoptimization\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, along with past work on \u003c/span\u003e\u003ca href=\"https://dl.acm.org/doi/abs/10.1145/258916.258947\"\u003e\u003cspan\u003ecode compression\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. It leverages the X\u003c/span\u003e\u003ca href=\"https://tukaani.org/xz/\"\u003e\u003cspan\u003ez\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, \u003c/span\u003e\u003ca href=\"https://facebook.github.io/zstd/\"\u003e\u003cspan\u003eZstd\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, and \u003c/span\u003e\u003ca href=\"https://github.com/google/brotli\"\u003e\u003cspan\u003eBrotli\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e compressors as optional back ends to do its compression work. Finally, Superpack uses Microsoft’s \u003c/span\u003e\u003ca href=\"https://github.com/Z3Prover/z3\"\u003e\u003cspan\u003eZ3 SMT solver\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to automatically parse and restructure a wide range of code formats.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhat’s next\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eSuperpack combines compiler and data compression techniques to increase the density of packed data in a way that is especially applicable to code such as Dex bytecode and ARM machine code. Superpack has substantially cut the size of our Android apps, and consequently saved billions of users around the world download time. We have described some of the core ideas underlying Superpack but have only scratched the surface of our work in asymmetric compression. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur journey has only just begun. Superpack continues to improve through enhancements to both its compiler and compression components. Superpack started out as a tool to cut mobile app size, but our success in improving the compression ratio of a variety of data types has led us to target other use cases of asymmetric compression. We are working on a new on-demand executable file format that saves disk space by keeping shared libraries compressed and decompressing them at load time. We are evaluating using Superpack for delta compression of code to reduce the size of software updates. We are also investigating using Superpack as a cold-storage compressor, to compress log data and files that are rarely used.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eUntil now, our mobile deployment has been limited to our Android apps. However, our work is equally applicable to other platforms, such as iOS, and we are looking into porting our implementation to those platforms. Presently, Superpack is available only to our engineers, but we aspire to bring the benefits of Superpack to everyone. To this end, we are exploring ways to improve the compatibility of our compression work with the Android ecosystem. This blog post is a step in this direction. We may someday consider open sourcing Superpack.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eManaging app size at Facebook is a unique challenge: Every day, developers check in large volumes of code, and each line of code translates into additional bits in the apps that people ultimately download onto their phones. Left unchecked, this added code would make the app bigger and bigger until eventually the time it takes [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/09/13/core-data/superpack/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/09/13/core-data/superpack/\"\u003eSuperpack: Pushing the limits of compression in Facebook’s mobile apps\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/09/Superpack_Hero_FINAL.jpg",
      "date_published": "2021-09-13T13:00:26Z",
      "author": {
        "name": "By Sapan Bhatia"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/09/10/security/whatsapp-e2ee-backups/",
      "title": "How WhatsApp is enabling end-to-end encrypted backups",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eFor years, in order to safeguard the privacy of people’s messages, WhatsApp has provided \u003ca href=\"https://engineering.fb.com/2021/04/16/security/dit/\"\u003eend-to-end encryption\u003c/a\u003e by default ​​so messages can be seen only by the sender and recipient, and no one in between. Now, we’re planning to give people the option to protect their WhatsApp backups using end-to-end encryption as well.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePeople can already back up their WhatsApp message history via cloud-based services like Google Drive and iCloud. WhatsApp does not have access to these backups, and they are secured by the individual cloud-based storage services. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBut now, if people choose to enable end-to-end encrypted (E2EE) backups once available, neither WhatsApp nor the backup service provider will be able to access their backup or their backup encryption key. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow E2EE backups work\u003c/span\u003e\u003c/h2\u003e\n\u003ch3\u003e\u003cspan\u003eGenerating encryption keys and passwords \u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eTo enable E2EE backups, we developed an entirely new system for encryption key storage that works with both iOS and Android. With E2EE backups enabled, backups will be encrypted with a unique, randomly generated encryption key. People can choose to secure the key manually or with a user password. When someone opts for a password, the key is stored in a Backup Key Vault that is built based on a component called a hardware security module (HSM) \u003c/span\u003e— \u003cspan\u003especialized, secure hardware that can be used to securely store encryption keys. When the account owner needs access to their backup, they can access it with their encryption key, or they can use their personal password to retrieve their encryption key from the HSM-based Backup Key Vault and decrypt their backup. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe HSM-based Backup Key Vault will be responsible for enforcing password verification attempts and rendering the key permanently inaccessible after a limited number of unsuccessful attempts to access it. These security measures provide protection against brute-force attempts to retrieve the key. WhatsApp will know only that a key exists in the HSM. It will not know the key itself.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eStoring keys in the Backup Key Vault\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWhatsApp’s front-end service, ChatD, handles client connections and client-server authentication, and will implement a protocol that sends the keys to the backups to and from WhatsApp’s servers. The client and HSM-based Backup Key Vault will exchange encrypted messages, the contents of which will not be accessible to ChatD itself.\u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe HSM-based Backup Key Vault will sit behind ChatD and provide highly available and secure storage for the encryption keys to the backups. The backups themselves will be generated as a continuous stream of data that is encrypted using symmetric encryption with the generated key. With E2EE backups enabled, upon being encrypted, a backup can then be stored off device (e.g., to iCloud or Google Drive). \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhatsApp serves over 2 billion people, and one of the core challenges of this product was to make sure the HSM-based Backup Key Vault operates reliably. To help ensure that the system is always available, the HSM-based Backup Key Vault service will be geographically distributed across multiple data centers to keep it up and running in case of a data center \u003ca href=\"https://engineering.fb.com/2021/06/02/data-center-engineering/how-facebook-deals-with-pcie-faults-to-keep-our-data-centers-running-reliably/\"\u003eoutage\u003c/a\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_18102\" aria-describedby=\"caption-attachment-18102\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?w=1024\" alt=\"WhatsApp end-to-end encrypted backups \" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg 3840w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-18102\"\u003eBackups can be end-to-end encrypted using a 64-digit encryption key.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cfigure id=\"attachment_18101\" aria-describedby=\"caption-attachment-18101\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?w=1024\" alt=\"WhatsApp end-to-end encrypted backups \" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg 3840w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-18101\"\u003eBackups can also be secured with a password, in which case the encryption key is saved to the HSM-based Backup Key Vault.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch3\u003e\u003cspan\u003eThe HSM-based Backup Key Vault and the encryption and decryption process\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003eWhen the account owner uses a personal password to protect their end-to-end encrypted backup, the HSM-based Backup Key Vault will store and safeguard it.\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhen someone wants to retrieve their backup:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003eThey enter their password, which is encrypted and then verified by the Backup Key Vault.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eOnce the password is verified, the Backup Key Vault will send the encryption key back to the WhatsApp client.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eWith the key in hand, the WhatsApp client can then decrypt the backups.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eAlternatively, if an account owner has chosen to use the 64-digit key alone, they will have to manually enter the key themselves to decrypt and access their backups. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eE2EE backups will be available on iOS and Android in the coming weeks. Check out the \u003ca href=\"https://www.whatsapp.com/security/WhatsApp_Security_Encrypted_Backups_Whitepaper.pdf\"\u003eend-to-end encrypted backups white paper\u003c/a\u003e to learn more about the technical details. \u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eFor years, in order to safeguard the privacy of people’s messages, WhatsApp has provided end-to-end encryption by default ​​so messages can be seen only by the sender and recipient, and no one in between. Now, we’re planning to give people the option to protect their WhatsApp backups using end-to-end encryption as well. People can already [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/09/10/security/whatsapp-e2ee-backups/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/09/10/security/whatsapp-e2ee-backups/\"\u003eHow WhatsApp is enabling end-to-end encrypted backups\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/09/Whatsapp_E2EE-Backups_EngBlog-1.png",
      "date_published": "2021-09-10T15:00:12Z",
      "author": {
        "name": "By Slavik Krassovsky, Gabriel Cadden"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/09/02/open-source/cachelib/",
      "title": "CacheLib, Facebook’s open source caching engine for web-scale services",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv id=\"single-wrapper\" tabindex=\"-1\"\u003e\n\t\t\u003cmain id=\"main\"\u003e\n\n\t\t\t\n\t\t\t\t\n\u003carticle id=\"post-18046\"\u003e\n\n\t\n\n\t\t\t\u003cfigure id=\"post-feat-image-container\"\u003e\n\t\t\t\t\u003cimg width=\"1920\" height=\"1080\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_hero.jpg\" alt=\"CacheLib, Facebook’s open source caching engine for web-scale services\" loading=\"lazy\" title=\"CacheLib_hero\"/\u003e\t\t\t\t\t\t\u003c/figure\u003e\n\t\t\n\t\n\n\t\n\t\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eCaching plays an important role in helping people access their information efficiently. For example, when an email app loads, it temporarily caches some messages, so the user can refresh the page without the app retrieving the same messages. However, large-scale caching has long been a complex engineering challenge. Companies must balance the fast experience people have come to expect from caching with keeping systems highly performant and cost-effective. Traditionally, each cache implementation is created and maintained independently by different engineering teams. This approach isn’t efficient, since it ignores different caching systems’ shared challenges, from deployment to maintenance. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs traditional dynamic random-access memory (DRAM) caches become more expensive and require more power to scale, companies like Facebook are exploring hardware choices such as non-volatile memory (NVM) drives to augment their caching systems. This DRAM and NVM hybrid model is a step forward, but innovative caching designs are needed to harness the full potential of the hybrid cache. This includes new caching heuristics research that must push the boundaries of traditional systems by identifying the relevant content to cache for the right duration. We have consolidated these innovations and taken them a step further through collaborations and open source work. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eToday, we’re announcing the release of \u003c/span\u003e\u003ca href=\"https://github.com/facebookincubator/CacheLib\"\u003e\u003cspan\u003eCacheLib\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, a pluggable in-process caching engine to \u003c/span\u003e\u003ca href=\"https://www.usenix.org/conference/osdi20/presentation/berg\"\u003e\u003cspan\u003ebuild and scale high-performance services\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e collaboratively. \u003c/span\u003e\u003cspan\u003eCacheLib’s C++ library\u003c/span\u003e \u003cspan\u003eenables developers to build and customize scalable and concurrent caches through its simple API. We are also open-sourcing \u003c/span\u003e\u003ca href=\"https://cachelib.org/docs/Cache_Library_User_Guides/Cachebench_Overview/\"\u003e\u003cspan\u003eCacheBench\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, a benchmarking tool for evaluating caching performance on diverse production workloads.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg\" alt=\"CacheLib’s C++ library enables developers to build and customize scalable and concurrent caches through its simple API.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eCacheLib is leveraged as an in-process cache in more than 70 large-scale systems at Facebook, including the social graph, content delivery network, storage, and \u003c/span\u003e\u003ca href=\"https://research.fb.com/publications/scaling-memcache-at-facebook/\"\u003e\u003cspan\u003elook-aside key-value caches\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. This existing scale and the potential for open source adoption make CacheLib an aggregation point for optimizations and CacheBench an effective benchmarking tool for evaluating new ideas across diverse caching applications.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eEnabling innovation through partnerships\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eAs an open source platform, CacheLib and CacheBench have the potential to become an industry standard for caching innovations and benchmarking. To date, our collaborations with research universities, hardware manufacturers, and software companies have yielded substantial results that show the value of this toolkit.  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOver the past two years, we have partnered with many well-known organizations to push the boundaries of caching innovation. Today, we are working with Twitter on integrating CacheLib into \u003c/span\u003e\u003ca href=\"https://github.com/twitter/pelikan\"\u003e\u003cspan\u003ePelikan.io\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to enable SSDs for caching objects within the Twitter infrastructure. Pinterest is evaluating the adoption of CacheLib within its machine learning infrastructure systems to improve prediction performance and system stability.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn academia, researchers at Carnegie Mellon University, Princeton University, and Yale University are using CacheLib and CacheBench to \u003c/span\u003e\u003ca href=\"https://www.pdl.cmu.edu/PDL-FTP/NVM/McAllister-SOSP21.shtml\"\u003e\u003cspan\u003eprototype research ideas\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. By evaluating their prototypes against industry caching workloads, these researchers can iterate on their projects much more quickly and accurately than before.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe have also collaborated with hardware industry partners like Intel, KIOXIA, Samsung, and Western \u003c/span\u003e\u003cspan\u003eDigital\u003c/span\u003e\u003cspan\u003e to standardize and enhance SSD technologies which enable improved caching solutions. This work is now part of the \u003c/span\u003e\u003ca href=\"https://www.opencompute.org/documents/nvme-cloud-ssd-specification-v1-0-3-pdf\"\u003e\u003cspan\u003eOpen Compute Project (OCP) NVMe Cloud SSD Specification\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which we discussed in \u003c/span\u003e\u003ca href=\"https://www.opencompute.org/events/past-events/webinar-data-center-nvme-ssd-and-edsff-presented-by-facebook-sk-hynix-kioxia-intel-snia\"\u003e\u003cspan\u003ethis webinar\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. This specification, along with CacheLib, will help adapt future NVM technologies for caching workloads across the industry.  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eCacheLib and CacheBench have enormous potential to shape the future of caching, thanks to its developer-friendly API, access to many benchmark workloads across the industry, and the collaborative nature of open source. We are thankful for our partners’ support and contributions in using the platform to drive innovation in such an important and complex area. We are open-sourcing this work in an effort to make building the future of caching a more collaborative and open space for sharing across the entire industry. \u003ca href=\"http://www.cachelib.org/\"\u003eRead more about the project at \u003c/a\u003e\u003cspan role=\"gridcell\"\u003eCachelib.org.\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\n\n\t\t\n\t\u003c/div\u003e\n\n\n\u003c/article\u003e\n\n\n\n\n\n\n\n\n\t\t\t\t\t\n\t \n\n\t\t\n\t\t\t\t\n\t\t\t\n\t\t\u003c/main\u003e\n\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eCaching plays an important role in helping people access their information efficiently. For example, when an email app loads, it temporarily caches some messages, so the user can refresh the page without the app retrieving the same messages. However, large-scale caching has long been a complex engineering challenge. Companies must balance the fast experience people [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/09/02/open-source/cachelib/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/09/02/open-source/cachelib/\"\u003eCacheLib, Facebook’s open source caching engine for web-scale services\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_hero.jpg",
      "date_published": "2021-09-02T16:00:27Z",
      "author": {
        "name": "By Sathya Gunasekar, Snehal Khandkar, Dmitry Vinnik, Michael Cheng"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/08/18/core-data/ramp-tao/",
      "title": "RAMP-TAO: Layering atomic transactions on Facebook’s online graph store",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle id=\"post-18025\"\u003e\n\n\t\n\n\t\t\t\u003cfigure id=\"post-feat-image-container\"\u003e\n\t\t\t\t\u003cimg width=\"824\" height=\"466\" src=\"https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg\" alt=\"HHVM Jump-Start: Boosting the performance of virtual machines\" loading=\"lazy\" title=\"RiB_LightNavy\"/\u003e\t\t\t\t\t\t\u003c/figure\u003e\n\t\t\n\t\n\n\t\n\t\u003cdiv\u003e\n\n\t\t\u003ch2\u003e\u003cspan\u003eWhat the research is: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eRAMP-TAO is a new protocol that improves the developer experience on \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2013/06/25/core-data/tao-the-power-of-the-graph/\"\u003e\u003cspan\u003eTAO\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, Facebook’s online social graph store, by providing stronger transactional guarantees. It is the first protocol to provide transactional semantics over an eventually consistent massive-scale data store while still preserving the system’s overall reliability and performance. RAMP-TAO enables an intuitive read transaction API, so developers do not have to investigate and handle rare anomalies.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur research has demonstrated that RAMP-TAO can be feasibly deployed in production with 0.42 percent memory overhead and enables the vast majority (over 99.9 percent) of reads to complete in one round-trip to the local cache with tail latency on par with existing TAO reads.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow it works:  \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eRAMP-TAO draws inspiration from the \u003c/span\u003e\u003ca href=\"https://people.eecs.berkeley.edu/~alig/papers/ramp.pdf\"\u003e\u003cspan\u003eRead Atomic Multi-Partition (RAMP) protocols\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which prevent fractured reads but impose unacceptably high performance and storage overheads for TAO.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe RAMP-TAO protocol provides stronger guarantees by layering them on top of TAO and is similar in spirit to the \u003c/span\u003e\u003ca href=\"https://people.eecs.berkeley.edu/~alig/papers/bolt-on-causal-consistency.pdf\"\u003e\u003cspan\u003e“bolt-on” approach\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. Since TAO, like most systems, ensures that all data is eventually consistent, we only need to guard against fractured reads for recent, transactionally updated data. We leverage this knowledge to minimize performance and storage overheads, especially for the existing TAO workloads that are not transactional.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?w=999\" alt=\"Ramp tao\" width=\"999\" height=\"314\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png 999w, https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?resize=916,288 916w, https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?resize=768,241 768w, https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?resize=96,30 96w, https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?resize=192,60 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhy it matters: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eTAO serves billions of reads and millions of writes per second and supports many of Facebook’s applications. It has traditionally prioritized availability and scalability to serve its large, read-dominant workloads. By layering stronger transactional guarantees on top of the existing system, we can provide more intuitive system behavior while retaining its reliability and efficiency. This strategy also enables RAMP-TAO to be cache-friendly, hotspot tolerant, and extensible to different data stores. Moreover, we incur overhead only for applications that opt in, rather than causing every application to take a performance hit.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAlthough we focus on TAO in this work, these properties are crucial to large-scale, read-optimized systems. Our layered approach can be a practical solution for other systems, many of which have sought to strengthen their guarantees. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eRead the full paper:\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://www.vldb.org/pvldb/vol14/p3014-cheng.pdf\"\u003e\u003cspan\u003eRAMP-TAO: Layering atomic transactions on Facebook’s online TAO data store\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\n\n\n\u003c/article\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWhat the research is:  RAMP-TAO is a new protocol that improves the developer experience on TAO, Facebook’s online social graph store, by providing stronger transactional guarantees. It is the first protocol to provide transactional semantics over an eventually consistent massive-scale data store while still preserving the system’s overall reliability and performance. RAMP-TAO enables an intuitive [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/08/18/core-data/ramp-tao/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/08/18/core-data/ramp-tao/\"\u003eRAMP-TAO: Layering atomic transactions on Facebook’s online graph store\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg",
      "date_published": "2021-08-18T16:00:37Z",
      "author": {
        "name": "By Audrey Cheng"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/08/15/connectivity/apricot-subsea-cable/",
      "title": "Apricot subsea cable will boost internet capacity, speeds in the Asia-Pacific region",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle id=\"post-18020\"\u003e\n\n\t\n\n\t\t\t\u003cfigure id=\"post-feat-image-container\"\u003e\n\t\t\t\t\u003cimg width=\"1920\" height=\"1280\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/Apricot.jpg\" alt=\"Apricot subsea cable will boost internet capacity, speeds in the Asia-Pacific region\" loading=\"lazy\"/\u003e\t\t\t\t\t\t\u003c/figure\u003e\n\t\t\n\t\n\n\t\n\t\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eWe are excited to announce our participation in the Apricot subsea cable system, together with leading regional and global partners. \u003c/span\u003e\u003cspan\u003eWhen completed, the project (which is still subject to regulatory approvals) will deliver much-needed internet capacity, redundancy, and reliability to expand connections in the Asia-Pacific region.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe 12,000-kilometer-long cable will connect Japan, Taiwan, Guam, the Philippines, Indonesia and Singapore. Apricot will feature a state of the art configuration allowing flexibility in trunk and branch capacity.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eExpected to launch in 2024, the Apricot cable system will have an initial design capacity of more than 190 terabits per second to meet rising data demands in the region and support existing cable systems, such as the recently announced \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2021/03/28/connectivity/echo-bifrost/\"\u003e\u003cspan\u003eEcho and Bifrost cable systems\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. The cable will help meet the growing demand for 4G, 5G, and broadband access in the region.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe Apricot cable is part of our ongoing effort to expand global network infrastructure and better serve the more than 3.5 billion people around the world who use our services every month. To accomplish this, we collaborate with partners all over the world to build subsea fiber-optic cables. Apricot is the latest example of our innovative partnership model, in which all parties  benefit from developing scale infrastructure and shared technology expertise.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\n\n\n\u003c/article\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWe are excited to announce our participation in the Apricot subsea cable system, together with leading regional and global partners. When completed, the project (which is still subject to regulatory approvals) will deliver much-needed internet capacity, redundancy, and reliability to expand connections in the Asia-Pacific region. The 12,000-kilometer-long cable will connect Japan, Taiwan, Guam, the [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/08/15/connectivity/apricot-subsea-cable/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/08/15/connectivity/apricot-subsea-cable/\"\u003eApricot subsea cable will boost internet capacity, speeds in the Asia-Pacific region\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/08/Apricot.jpg",
      "date_published": "2021-08-16T00:30:20Z",
      "author": {
        "name": "By Nico Roehrich"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/08/11/open-source/time-appliance/",
      "title": "Open-sourcing a more precise time appliance",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFacebook engineers have built and open-sourced an Open Compute Time Appliance, an important component of the modern timing infrastructure.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eTo make this possible, we came up with the Time Card — a PCI Express (PCIe) card that can turn almost any commodity server into a time appliance. \u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWith the help of the OCP community, we established the \u003c/span\u003e\u003ca href=\"http://ocptap.com\"\u003e\u003cspan\u003eOpen Compute Time Appliance Project\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and open-sourced every aspect of the \u003c/span\u003e\u003ca href=\"http://opentimeserver.com/\"\u003e\u003cspan\u003eOpen Time Server\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eIn March 2020, we announced that we were in the process of switching over the servers in our data centers (together with our consumer products) to \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/\"\u003e\u003cspan\u003ea new timekeeping service based on the Network Time Protocol (NTP)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. The new service, built in-house and later open-sourced, was more scalable and improved the accuracy of timekeeping in the Facebook infrastructure from 10 milliseconds to 100 microseconds. \u003c/span\u003e\u003cspan\u003eMore accurate time keeping enables more advanced infrastructure management across our data centers, as well as faster performance of distributed databases.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe new NTP-based time architecture uses a \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/\"\u003e\u003cspan\u003eStratum 1\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e — an important component that is directly linked to an authoritative source of time, such as a global navigation satellite system (GNSS) or a cesium clock.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg\" alt=\"Figure showing Stratums 0 through 3. Stratum 1 is an important component which is a device directly linked to an authoritative source of time, such as a Global Navigation Satellite System or a Cesium clock.\" width=\"1921\" height=\"1081\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eMany companies rely on public NTP pools such as time.facebook.com to act as their Stratum 1. However, this approach has its drawbacks. These pools add dependency on internet connectivity and can impact overall security and reliability of the system. For instance, if connectivity is lost or an external service is down, it can result in outages or drift in timing for the dependent system.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo remove these dependencies, we’ve built a new dedicated piece of hardware called Time Appliance, which consists of a GNSS receiver and a miniaturized atomic clock (MAC). Users of time appliances can keep accurate time, even in the event of GNSS connectivity loss. While building our Time Appliance, we also invented a Time Card, a PCIe card that can turn any commodity server into a time appliance.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhy do we need a new time device?\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eOff-the-shelf time appliances have their own benefits. They work right out of the box and because many of these devices have been on the market for decades, they are battle-tested and generally stable enough to work without supervision for a long time.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHowever, these solutions also come with trade-offs:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eIn most cases, they are outdated and often vulnerable to software security concerns. Feature requests and security fixes may take months or even years to implement.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThese devices come with closed source software, which makes configuring and monitoring them limited and challenging. While configuration is done manually via a proprietary CLI or Web UI, monitoring often uses SNMP, a protocol that was not designed for this purpose.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThey include proprietary hardware that is not user-serviceable. When a single component breaks, there is no easy way to replace it. You have to either ship it to the vendor for repair or buy an entire new appliance.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSince off-the-shelf devices are made in low quantities, they come with a higher markup and can become very costly to operate over time. The high cost associated with off-the-shelf devices create limitations for many in the industry. An open source version would open the door to broader applications.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eUntil now, companies have had to accept these trade-offs and work within the constraints described above. We\u003c/span\u003e \u003cspan\u003edecided it was time to try something different, so we took a serious look at what it would take to build our new Time Appliance — specifically, one using the x86 architecture.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003ePrototyping the Time Appliance \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eHere’s a block diagram of what we envisioned:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg\" alt=\"Block diagram of the time appliance prototype\" width=\"1921\" height=\"1081\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e \u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003cspan\u003eIt all starts from a GNSS receiver that provides the time of day (ToD) as well as the one pulse per second (PPS). When the receiver is backed by a high-stability oscillator (e.g., an atomic clock or an oven-controlled crystal oscillator), it can provide time that is nanosecond-accurate. The time is delivered across the network via an off-the-shelf network card which supports PPS in/out and hardware time stamping of packets, such as the NVIDIA Mellanox ConnectX-6 Dx used in our initial appliance.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe output of the GPS disciplined oscillator (GPSDO) was fed into the EXT time-stamping of the ConnectX-6 Dx network card. In addition, the GNSS receiver provides the ToD via a serial port and a popular GPS reporting protocol called NMEA. Using \u003c/span\u003e\u003cspan\u003ets2phc\u003c/span\u003e\u003cspan\u003e tool allowed us to synchronize the physical hardware clock of the network interface controller (NIC) down to a couple of tens of nanoseconds, as shown below:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg\" alt=\"block of code \" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur prototype gave us confidence that building such an appliance was possible. However, there was a lot of room for improvement.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo increase the reliability of the system, we divided it into two major parts: payload and delivery. The payload is the precision time that is essentially an interpolation system driven by a local oscillator to create nanoseconds of time measurement between consecutive PPS signals received by the GNSS receiver. We considered putting the GNSS receiver, the high-stability local oscillator, and the necessary processing logic into a PCIe form factor, and we called it the Time Card.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg\" alt=\"Block diagram of the time card functionality\" width=\"1921\" height=\"1081\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHere is the sketch of the Time Card we initially envisioned on a napkin:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg\" alt=\"Original design for the time appliance\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe used an onboard MAC, a multiband GNSS receiver, and a field-programmable gate array (FPGA) to implement the time engine. The time engine’s job is to interpolate in nanoseconds the granularity required between consecutive PPS signals. The GNSS receiver also provides a ToD in addition to a 1 PPS signal. In the event of the loss of GNSS reception, the time engine relies on the ongoing synchronization of the atomic clock based on an average ensemble of the consecutive PPS pulses. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe time engine consists of a set of processing blocks implemented on the FPGA of the Time Card. These processing blocks include various filtering, synchronization, error checking, time-stamping, and PCIe-related subsystems to allow the Time Card to perform as a system peripheral that provides precision time for the open time server.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIt should be noted that the accuracy of a GNSS receiver is within tens of nanoseconds, while the required ongoing synchronization (calibration) of the MAC is within 10 picoseconds (1,000 times more accurate). \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAt first, this sounds impossible. However, the GNSS system provides timing based on continuous communication with standard time. This ability allows the GNSS onboard clock to be constantly synchronized with a source of time provided to its constellation, giving it virtually no long-term drifting error. Therefore, the MAC’s calibration is performed via a comparison of a MAC-driven counter and the GNSS-provided PPS pulse. Taking more time for the comparison allows us to achieve a higher precision of calibration for the MAC. Of course, this is with the consideration that the MAC is a linear time invariant system.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg\" alt=\"In this block diagram, you can see a 10 MHz signal from the rubidium clock entering the time engine. The clock signal feeds into a digital clock module and a digital PLL, resulting in a 125 MHz frequency. The 125 MHz feeds into the ToD unit.\" width=\"1921\" height=\"1479\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=916,705 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=768,591 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=1024,788 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=1536,1183 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=96,74 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=192,148 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn this block diagram, you can see a 10 MHz signal from the rubidium clock entering the time engine. This clock signal can be replaced by a 10 MHz SMA input. The clock signal feeds into a digital clock module and a digital PLL (12.5x resulted from 25 up and divided by 2), resulting in a 125 MHz frequency. The 125 MHz (8-nanosecond periods) feeds into the ToD unit.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg\" alt=\"Block diagram showing the filtering process\" width=\"1921\" height=\"1101\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=916,525 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=768,440 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=1024,587 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=1536,880 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=96,55 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=192,110 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe ToD unit associates the 8-nanosecond increments in digital values of 0b000001 since the LSB (least significant bit) is associated to 250 picoseconds (driven from 32 bits of subsecond accuracy on the gPTP).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOn the other hand, the PPS signal coming filtered from the GNSS is used to snapshot the result of the increments. If the 125 MHz is accurate, the accumulated increments should result in exactly 1-second intervals. However, in reality, there is always a mismatch between the accumulated value and a theoretical 1-second interval.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg\" alt=\"\" width=\"1921\" height=\"850\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=916,405 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=768,340 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=1024,453 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=1536,680 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=96,42 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=192,85 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e \u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003cspan\u003eThe values can be adjusted using an internal PI (proportional and integral) control loop. The adjustment can be done by either altering the 0b000001 value by steps of 250 picoseconds or fine-tuning the 12.5x PPL. In addition, further (more finely tuned) adjustments can be applied by steering the rubidium oscillator. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe longer a GNSS isn’t available, the more time accuracy is lost. The rate of the time accuracy deterioration is called holdover. Usually, holdover is described as a timeframe for accuracy and how long it takes to exceed it. For example, the holdover of a MAC is within 1 microsecond for 24 hours. This means that after 24 hours, the time accuracy is nondeterministic but accurate within 1 microsecond.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs an alternative approach, we are counting on the new generation of chip-scale and miniaturized atomic clocks with their capability to receive PPS inputs. This allows the time engine of the Time Card to hand off the ultraprecision syntonization of the high-stability oscillator to the component rather than use digital resources to reach the target. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs a general principle, the more accurate the tuning, the better the holdover performance that can be achieved. In terms of delivery, using a NIC with precision timing ensures that network packets receive very accurate time stamps, which is critical for keeping the time precise as it is shared with other servers across the network. Such a NIC can also receive a PPS signal directly from the Time Card.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAfter conceptualizing the idea and various implementation iterations, we were able to put together a prototype. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg\" alt=\"Image of the time card prototype\" width=\"1921\" height=\"1430\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=916,682 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=768,572 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=1024,762 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=1536,1143 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=96,71 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=192,143 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe Time Appliance in action\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe Time Card allows any x86 machine with a NIC capable of hardware time-stamping to be turned into a time appliance. This system is agnostic to whether it runs for NTP, PTP, SyncE, or any other time synchronization protocol, since the accuracy and stability provided by the Time Card is sufficient for almost any system. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg\" alt=\"Image showing the components to build a time appliance\" width=\"1921\" height=\"835\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=916,398 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=768,334 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=1024,445 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=1536,668 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=96,42 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=192,83 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/span\u003e\u003cbr/\u003e\n\u003cspan\u003eThe beauty of using PCIe cards is that the setup can be assembled even on a home PC, as long as it has enough PCIe slots available.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe next step would be to install Linux. The Time Card driver is included in Linux kernel \u003c/span\u003e\u003ca href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=773bda96492153e11d21eb63ac814669b51fc701\"\u003e\u003cspan\u003e5.15\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e or newer. Or, it can be built from the\u003c/span\u003e\u003ca href=\"https://github.com/opencomputeproject/Time-Appliance-Project/tree/master/Time-Card/DRV\"\u003e \u003cspan\u003eOCP GitHub repository\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e on kernel 5.12 or newer.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe driver will expose several devices, including the PHC clock, GNSS, PPS, and atomic clock serial:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ls -l /sys/class/timecard/ocp0/\nlrwxrwxrwx. 1 root    0 Aug  3 19:49 device -\u0026gt; ../../../0000:04:00.0/\n-r--r--r--. 1 root 4096 Aug  3 19:49 gnss_sync\nlrwxrwxrwx. 1 root    0 Aug  3 19:49 i2c -\u0026gt; ../../xiic-i2c.1024/i2c-2/\nlrwxrwxrwx. 1 root    0 Aug  3 19:49 pps -\u0026gt; ../../../../../virtual/pps/pps1/\nlrwxrwxrwx. 1 root    0 Aug  3 19:49 ptp -\u0026gt; ../../ptp/ptp2/\nlrwxrwxrwx. 1 root    0 Aug  3 19:49 ttyGNSS -\u0026gt; ../../tty/ttyS7/\nlrwxrwxrwx. 1 root    0 Aug  3 19:49 ttyMAC -\u0026gt; ../../tty/ttyS8/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eThe driver also allows us to monitor the Time Card, the GNSS receiver, and the atomic clock status and flash a new FPGA bitstream using the \u003ccode\u003edevlink\u003c/code\u003e cli.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe only thing left to do is to configure the NTP and/or PTP server to use the Time Card as a reference clock. To configure chrony, one simply needs to specify \u003ccode\u003erefclock\u003c/code\u003e attribute:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ grep refclock /etc/chrony.conf\nrefclock PHC /dev/ptp2 tai poll 0 trust\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eAnd enjoy a very precise and stable NTP Stratum 1 server:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ chronyc sources\n210 Number of sources = 1\nMS Name/IP address         Stratum Poll Reach LastRx Last sample\n===============================================================================\n#* PHC0                          0   0   377     1     +4ns[   +4ns] +/-   \n36ns\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eFor the PTP server (for example, \u003c/span\u003e\u003ca href=\"https://github.com/facebookincubator/ptp\"\u003e\u003cspan\u003eptp4u\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e) one will first need to synchronize Time Card PHC with the NIC PHC. This can be easily done by using the \u003ccode\u003ephc2sys\u003c/code\u003e tool which will sync the clock values with the high precision usually staying within single digits of nanoseconds:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ phc2sys -s /dev/ptp2 -c eth0 -O 0 -m\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eFor greater precision, it’s recommended to connect the Time Card and the NIC to the same CPU PCIe lane. For greater precision, one can connect the PPS output of the Time Card to the PPS input of the NIC.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo validate and confirm the precision, we’ve used an external validation device called Calnex Sentinel connected to the same network via several switches and an independent GNSS antenna. It can perform PPS testing as well as NTP and/or PTP protocols: \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg\" alt=\"On this graph, the blue line represents NTP measurement results. The precision stays within ±40 microseconds throughout the 48-hour measurement interval. The orange line represents PTP measurement results. The offset is practically 0 ranging within nanoseconds range.\" width=\"1921\" height=\"1081\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe blue line represents NTP measurement results. The precision stays within ±40 microseconds throughout the 48-hour measurement interval.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe orange line represents PTP measurement results. The offset is practically 0 ranging within nanoseconds range.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIndeed, when we compare 1 PPS between Time Card output and the internal reference of the Calnex Sentinel, we see that the combined error ranges within ±200 nanoseconds:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg\" alt=\"when we compare 1 PPS between Time Card output and the internal reference of the Calnex Sentinel, we see that the combined error ranges within ±200 nanoseconds\" width=\"1921\" height=\"1081\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003eBut what’s even more important is that these measurements demonstrate stability of the Time Appliance outputs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn the event of the GNSS signal loss, we need to make sure the time drift (aka holdover) of the atomic-backed Time Card stays within 1 microsecond per 24 hours. Here is a graph showing the holdover of the atomic clock (SA.53s) over a 24-hour interval. As you can see, the PPS drift stays within 300 nanoseconds, which is within the atomic clock spec.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg\" alt=\"Here is a graph showing the holdover of the atomic clock (SA.53s) over a 24-hour interval. As you can see, the PPS drift stays within 300 nanoseconds, which is within the atomic clock spec.\" width=\"1921\" height=\"1081\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe modular design of the Time Card allows the swap of the atomic clock with an oven-controlled crystal oscillator (OCXO) or a temperature compensated crystal oscillator (TCXO) for a budget solution with the compromise on the holdover capabilities. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eOpen-sourcing the design of the Time Appliance\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eBuilding a device that is very precise, inexpensive, and free from vendor lock was an achievement on its own. But we wanted to have a greater impact on the industry. We wanted to truly set it free and make it open and affordable for everyone, from a research scientist to a large cloud data center. That’s why we engaged with the Open Compute Project (OCP) to create a brand-new \u003c/span\u003e\u003ca href=\"http://www.ocptap.com/\"\u003e\u003cspan\u003eTime Appliance Project\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e (TAP). Under the OCP umbrella, we open-sourced at the Time Appliance Project \u003c/span\u003e\u003ca href=\"https://github.com/opencomputeproject/Time-Appliance-Project\"\u003e\u003cspan\u003eGitHub repository\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, including the specs, schematics, mechanics, BOM, and the source code. Now, as long as printing the PCB and soldering tiny components does not sound scary, anyone can build their own Time Card for a fraction of the cost of a regular time appliance. We also worked with several vendors such as \u003c/span\u003e\u003ca href=\"https://www.orolia.com/about-the-atomic-reference-time-card-art-card/\"\u003e\u003cspan\u003eOrolia\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e who will be building and selling time cards, and NVIDIA who are selling the precision timing-capable ConnectX-6 Dx (and the precision timing-capable BlueField-2 DPU).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe published an Open Time Server spec at \u003c/span\u003e\u003ca href=\"http://www.opentimeserver.com\"\u003e\u003cspan\u003ewww.opentimeserver.com\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which explains in great detail how to combine the hardware (Time Card, Network Card, and a commodity server) and the software (OS driver, NTP, and/or PTP server) to build the Time Appliance. Building an appliance based on this spec will give full control to the engineers maintaining the device, improving monitoring, configuration, management, and security.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe Time Appliance is an important step in the journey to improve the timing infrastructure for everyone, but there is more to be done. We will continue to work on other elements, including improving the precision and accuracy of the synchronization of our own servers, and we intend to continue sharing this work with the Open Compute community.  \u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eFacebook engineers have built and open-sourced an Open Compute Time Appliance, an important component of the modern timing infrastructure. To make this possible, we came up with the Time Card — a PCI Express (PCIe) card that can turn almost any commodity server into a time appliance. With the help of the OCP community, we [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/08/11/open-source/time-appliance/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/08/11/open-source/time-appliance/\"\u003eOpen-sourcing a more precise time appliance\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-Hero-image-v2.jpg",
      "date_published": "2021-08-11T11:00:34Z",
      "author": {
        "name": "By Ahmad Byagowi, Oleg Obleukhov"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/08/09/connectivity/backbone-management/",
      "title": "Risk-driven backbone management during COVID-19 and beyond",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003ch2\u003e\u003cspan\u003eWhat the research is: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eA first-of-its-kind study detailing our backbone management strategy to ensure high service performance throughout the COVID-19 pandemic. The pandemic moved most social interactions online and caused an unprecedented stress test on our global network infrastructure with tens of data center regions. At this scale, failures such as fiber cuts, router misconfigurations, and power outages are a frequent occurrence.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe ran a simulation system that identifies possible failures and quantifies their potential severity with a set of metrics that measure network risk. The risk metrics, in turn, guided operational decisions for capacity deployment. Coupled with traffic priority management and proactive capacity enhancement, our backbone resiliently withstood the COVID-19 stress test while achieving high service availability and low latency, and efficiently handled traffic surges.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow it works: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eTo satisfy the network’s service-level objectives (SLO), we started by defining a set of risk metrics around demand loss, availability, and latency stretch. All these metrics are computed with respect to possible failure scenarios in the network, which can be enumerated by going through all the components making up the network. The goal of the failure modeling is to estimate the likelihood of a failure scenario as well as the duration of the failure event. Each component failure is characterized by its mean time between failures and mean time to repair. These statistics are estimated based on a combination of historical data and clustering followed by Bayesian regression modeling on common features such as vendor, ownership, and geographical region.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur risk simulation system periodically computes the aforementioned risk metrics. It works by taking a fresh snapshot of the network topology and demand and the set of failure scenarios to consider together with their failure characteristics as its input. Due to the high number of failure scenarios, each is sharded onto a number of worker jobs that run the same code as our \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2017/05/01/data-center-engineering/building-express-backbone-facebook-s-new-long-haul-network/\"\u003e\u003cspan\u003eSD-WAN controller\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to compute the traffic engineering decision for the given failure scenario. The decisions are aggregated to derive the risk metrics and then logged for continuous monitoring.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDuring the onset of COVID-19, the risk metrics reported a significant increase in demand loss (which captures the highest traffic loss across all simulated failure scenarios), a decrease in availability and increase in latency for all quality of service (QoS) classes. The risk metrics guided us to the possible failure scenarios that, were they to occur, would degrade the network operating conditions for certain regions. Capacity was proactively deployed to mitigate these risks. Another helpful technique was looking at the traffic flows from the regions at risk, differentiating the traffic by criticality and then downgrading the QoS to a lower priority. The QoS classes are, in order of importance, infrastructure control (class 1), user traffic (class 2), internal applications (class 3), and bulk data transfer (class 4). We downgraded a lot of latency-insensitive traffic from class 3 to class 4. Less capacity is thus needed to guarantee the same level of SLO.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhy it matters: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThere is a long lead time of months to years for building up capacity for backbone networks. As such, network operators typically procure capacity based on estimated traffic growth. When COVID-19 hit, there was a significant unplanned ramp-up in traffic within a short period of time, stressing backbone infrastructure all across the world. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFacebook was able to react swiftly thanks to its risk-driven backbone management strategy. Leveraging the risk metrics computed by our simulation systems, we quickly identified the operational pain points and prioritized capacity enhancements to bring the network back to normal. Our experience has shown that a metrics-centric approach to backbone management could adapt to rare adverse external shock. We hope our research can help operators looking to build a more resilient network. We would like to thank Ying Zhang, Guanqing Yan, Satyajeet Singh Ahuja, Alexander Nikolaidis, Soshant Bali, Bob Kamma and Gaya Nagarajan for their work on this project. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.usenix.org/conference/nsdi21/presentation/xia\"\u003eTo learn more, watch our presentation at NSDI 2021\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eRead the full paper:\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://research.fb.com/publications/a-social-network-under-social-distancing-risk-driven-backbone-management-during-covid-19-and-beyond/\"\u003e\u003cspan\u003eA social network under social distancing: Risk-driven network management during COVID-19 and beyond\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWhat the research is:  A first-of-its-kind study detailing our backbone management strategy to ensure high service performance throughout the COVID-19 pandemic. The pandemic moved most social interactions online and caused an unprecedented stress test on our global network infrastructure with tens of data center regions. At this scale, failures such as fiber cuts, router misconfigurations, [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/08/09/connectivity/backbone-management/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/08/09/connectivity/backbone-management/\"\u003eRisk-driven backbone management during COVID-19 and beyond\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/08/RiB_RichTeal.jpg",
      "date_published": "2021-08-09T16:00:55Z",
      "author": {
        "name": "By Chiun Lin Lim"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/08/06/core-data/zippydb/",
      "title": "How we built a general purpose key value store for Facebook with ZippyDB",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. Since we first deployed ZippyDB in 2012, this key-value store has expanded rapidly, and today, ZippyDB serves a number of use cases, ranging from metadata for a distributed filesystem, counting events for both internal and external purposes, to product data that’s used for various app features. ZippyDB offers a lot of flexibility to applications in terms of tunable durability, consistency, availability, and latency guarantees, which has made the service a popular choice within Facebook for storing both ephemeral and nonephemeral small key-value \u003c/span\u003e\u003cspan\u003edata. In this post, we are sharing for the first time the history and evolution of ZippyDB and some of the unique design choices and trade-offs made in building this service that addressed the majority of key-value store scenarios at Facebook.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHistory of ZippyDB\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB uses\u003c/span\u003e\u003ca href=\"https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-and-open-sourcing-rocksdb/10151822347683920/\"\u003e \u003cspan\u003eRocksDB\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e as the underlying storage engine. Before ZippyDB, various teams across Facebook used RocksDB directly to manage their data. This resulted, however, in a duplication of efforts in terms of each team solving similar challenges such as consistency, fault tolerance, failure recovery, replication, and capacity management. To address the needs of these various teams, we built ZippyDB to provide a highly durable and consistent key-value data store that allowed products to move a lot faster by offloading all the data and the challenges associated with managing this data at scale to ZippyDB.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOne of the significant design decisions we made early during the development of ZippyDB was to reuse as much of the existing infrastructure as possible. Consequently, most of our initial efforts were focused on building a reusable and flexible data replication library called Data Shuttle. We built a fully managed distributed key-value store by combining Data Shuttle with a preexisting and well-established storage engine (RocksDB) and layering this on top of our existing shard management (\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\"\u003e\u003cspan\u003eShard Manager\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003eand \u003c/span\u003e\u003cspan\u003edistributed \u003c/span\u003e\u003cspan\u003econfiguration service (built on \u003c/span\u003e\u003ca href=\"https://zookeeper.apache.org/\"\u003e\u003cspan\u003eZooKeeper\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e), that together solves load balancing, shard placement, failure detection, and service discovery.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eArchitecture\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB is deployed in units known as tiers. A tier consists of compute and storage resources spread across several geographic areas known as regions\u003c/span\u003e \u003cspan\u003eworldwide, which makes it resilient to failures. There are only a handful of ZippyDB tiers that exist today, including the default “wildcard” tier and specialized tiers for distributed filesystem metadata and other product groups within Facebook. Each tier hosts multiple use cases. Normally, use cases are created on the wildcard tier, which is our generic multitenant tier. This is the preferred tier because of its better utilization of hardware and lower operational overhead, but we occasionally bring up dedicated tiers if there is a need, usually due to stricter isolation requirements.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe data belonging to a use case on a tier is split into units known as shards,\u003c/span\u003e \u003cspan\u003ewhich are\u003c/span\u003e \u003cspan\u003ethe basic units of data management on the server side. Each shard is replicated across multiple regions (for fault tolerance) using Data Shuttle, which uses either \u003ca href=\"https://dl.acm.org/doi/10.1145/279227.279229\"\u003ePaxos\u003c/a\u003e or\u003c/span\u003e \u003cspan\u003easync replication to\u003c/span\u003e \u003cspan\u003ereplicate\u003c/span\u003e \u003cspan\u003edata, depending on the configuration. Within a shard, a subset of replicas are configured to be a part of the Paxos quorum group, also known as global scope,\u003c/span\u003e \u003cspan\u003ewhere data is synchronously replicated using Multi-Paxos to provide high durability and availability in case of failures. The remaining replicas, if any, are configured as followers.\u003c/span\u003e \u003cspan\u003eThese are similar to learners in Paxos terminology and receive data asynchronously. Followers allow applications to have many in-region replicas to support low-latency reads with relaxed consistency, while keeping the quorum size small for lower write latency. This flexibility in replica role configuration within a shard allows applications to strike a balance between durability, write performance, and read performance depending on their needs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn addition to the sync or async replication strategy, applications also have the option to provide “hints” to the service about the regions in which the replicas of a shard must be placed. These hints, also known as stickiness\u003c/span\u003e \u003cspan\u003econstraints, allow applications to have some control over the latency of reads and writes by having replicas built in regions from where they expect most of the access to come. ZippyDB also provides a caching layer and integrates with a pub-sub system allowing subscriptions to data mutations on shards, both of which are opt-ins depending on the requirements of the use case.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eData model\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB supports a simple key-value\u003c/span\u003e \u003cspan\u003edata model with APIs to get, put, and delete keys along with their batch variants. It supports iterating over key prefixes and deleting a range of keys. These APIs are very similar to the API exposed by the underlying RocksDB storage engine. In addition, we also support a test-and-set API for basic read-modify-write operations and transactions, conditional writes for more generic read-modify-write operations (more about this later). This minimal API set has proved to be sufficient for most use cases to manage their data on ZippyDB. For ephemeral data, ZippyDB has native TTL support where the client can optionally specify the expiry time for an object at the time of the write. We piggyback on RocksDB’s periodic compaction support to clean up all the expired keys efficiently while filtering out dead keys on the read side in between compaction runs. Many applications actually access data on ZippyDB through an ORM layer on top of ZippyDB, which translates these accesses into ZippyDB API. Among other things, this layer serves to abstract the details of the underlying storage service.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eShard is the unit of data management on the server side. The optimal assignment of shards to servers needs to take into account load, failure domains, user constraints, etc., and this is handled by ShardManager\u003c/span\u003e\u003ci\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eShardManager\u003c/span\u003e \u003cspan\u003eis responsible for monitoring servers for load imbalance, failures, and initiating shard movement between servers. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eShard, often referred to as physical shard or p-shard, is a server-side concept and isn’t exposed to applications directly. Instead, we allow use cases to partition their key space into smaller units of related data known as μshards (micro-shards)\u003c/span\u003e\u003ci\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eA typical\u003c/span\u003e \u003cspan\u003ephysical\u003c/span\u003e \u003cspan\u003eshard\u003c/span\u003e \u003cspan\u003ehas a size of 50–100 GB, hosting several tens of thousands of μshards. This additional layer of abstraction allows ZippyDB to reshard the data transparently without any changes on the client.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB supports two kinds of mappings from \u003c/span\u003e\u003ci\u003e\u003cspan\u003eμ\u003c/span\u003e\u003c/i\u003e\u003cspan\u003eshards to physical shards: compact mapping and Akkio mapping. Compact mapping\u003c/span\u003e \u003cspan\u003eis used\u003c/span\u003e \u003cspan\u003ewhen the assignment is fairly static and mapping is only changed when there is a need to split shards that have become too large or hot. In practice, this is a fairly infrequent operation when compared with Akkio mapping, where mapping of \u003c/span\u003e\u003ci\u003e\u003cspan\u003eμ\u003c/span\u003e\u003c/i\u003e\u003cspan\u003eshards is managed by a service known as\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2018/10/08/core-data/akkio/\"\u003e \u003cspan\u003eAkkio\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. Akkio splits use cases’ key space into μshards and places these μshards in regions where the information is typically accessed. Akkio helps reduce data set duplication and provides a significantly more efficient solution for low latency access than having to place data in every region.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs we mentioned earlier, Data Shuttle uses Multi-Paxos to synchronously replicate data to all replicas in the global scope\u003c/span\u003e\u003ci\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eConceptually, time is subdivided into units known as epochs. Each epoch has a unique leader, whose role is assigned using an external shard management service called ShardManager. Once a leader is assigned, it has a lease for the entire duration of the epoch. Periodic heartbeats used to keep a lease active until ShardManager bumps up the epoch on the shard (e.g., for failover, primary load balancing, etc.). When a failure occurs, ShardManager detects the failure, assigns a new leader with a higher epoch and restores write availability. Within each epoch, the leader generates a total ordering of all writes to the shard, by assigning each write a monotonically increasing sequence number. The writes are then written to a replicated durable log using Multi-Paxos\u003c/span\u003e \u003cspan\u003eto achieve consensus on the ordering. Once the writes have reached consensus, they are drained in-order across all replicas.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe chose to use an external service to detect failures and assign leaders to keep the design of the service simple in the initial implementation. However, in the future we plan to move towards detecting failures entirely within\u003c/span\u003e \u003cspan\u003eData Shuttle (“in-band”) and reelecting the leaders more proactively without having to wait for ShardManager and incurring delays.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eConsistency\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB provides configurable consistency and durability levels to applications, which can be specified as options in read and write APIs. This allows applications to make durability, consistency, and performance trade-offs dynamically on a per-request level.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBy default, a write involves persisting the data on a majority of replicas’ Paxos logs and writing the data to RocksDB on the primary before acknowledging the write to the client. With the default write mode, a read on primary will always see the most recent write. Some applications cannot tolerate cross-region latencies for every write, so ZippyDB supports a fast-acknowledge\u003c/span\u003e \u003cspan\u003emode, where writes are acknowledged as soon as they are enqueued on the primary for replication. The durability and consistency guarantees for this mode are obviously lower, which is the trade-off for higher performance.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOn the read side, the three most popular consistency levels are eventual, read-your-writes, and strong. The eventual consistency level supported by ZippyDB is actually a much stronger consistency level than the more well-known eventual consistency. ZippyDB provides total ordering for all writes within a shard and ensures that reads aren’t served by replicas that are lagging behind primary/quorum beyond a certain configurable threshold (heartbeats are used to detect lag), so eventual reads supported by ZippyDB are closer to bounded staleness consistency in literature.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor read-your-writes, the clients cache the latest sequence number returned by the server for writes and use the version to run at-or-later\u003c/span\u003e \u003cspan\u003equeries\u003c/span\u003e \u003cspan\u003ewhile reading. The cache of versions is within the same client process.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB also provides strong consistency or \u003c/span\u003e\u003ca href=\"https://dl.acm.org/doi/10.1145/78969.78972\"\u003e\u003cspan\u003elinearizability\u003c/span\u003e\u003c/a\u003e\u003ci\u003e\u003cspan\u003e, \u003c/span\u003e\u003c/i\u003e\u003cspan\u003ewhere\u003c/span\u003e \u003cspan\u003eclients can see the effects of the most recent writes regardless of where the writes or reads come from. Strong reads today are implemented by routing the reads to the primary in order to avoid the need to speak to a quorum, mostly for performance reasons. The primary relies on owning the lease to ensure that there is no other primary before serving reads. In certain outlier cases, where the primary hasn’t heard about the lease renewal, strong reads on primary turn into a quorum check and read.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eTransactions and conditional writes\u003c/span\u003e\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB4.Revised.jpg?w=16\" alt=\"\" width=\"16\" height=\"9\"/\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg 2304w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/Screen-Shot-2021-05-25-at-10.20.41-AM.jpg?w=16\" alt=\"\" width=\"16\" height=\"9\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB supports transactions and conditional writes for use cases that need atomic read-modify-write operations on a set of keys.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAll transactions are serializable by default on a shard, and we don’t support lower isolation levels. This simplifies the server-side implementation and the reasoning about correctness of concurrently executing transactions on the client side. Transactions use optimistic concurrency control to detect and resolve \u003c/span\u003e\u003ca href=\"https://dl.acm.org/doi/10.1145/568271.223787\"\u003e\u003cspan\u003econflicts\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which works as shown in the figure above. The clients typically read from a secondary all of the data from a snapshot\u003c/span\u003e \u003cspan\u003eof the DB, compose the write\u003c/span\u003e \u003cspan\u003eset, and send both the read and write sets to the primary to commit. Upon receiving the read and write sets and the snapshot against which reads were performed, the primary checks whether there were conflicting writes by other concurrently executing transactions that have already been admitted. The transaction is admitted only if there are no conflicts, after which the transaction is guaranteed to succeed, assuming no server failures. Conflict resolution on the primary relies on tracking all of the recent\u003c/span\u003e \u003cspan\u003ewrites performed by previously admitted transactions during the same epoch on the primary. Transactions spanning epochs are rejected, as this simplifies write set\u003c/span\u003e \u003cspan\u003etracking without requiring replication. The history of writes maintained on the primary is also periodically purged to keep the space usage low. Since the complete history isn’t maintained, the primary needs to maintain a minimum tracked version\u003c/span\u003e \u003cspan\u003eand reject all transactions that have reads against a snapshot with lower version to guarantee serializability. Read-only transactions work exactly similar to read-write transactions, except that the write set is empty.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eConditional write is implemented using “server-side transactions”. It provides a more user friendly client side API for use cases where clients want to atomically modify a set of keys based on some common preconditions such as key_present, key_not_present, and value_matches_or_key_not_present. When a primary receives a conditional write request it sets up a transaction context and converts the preconditions and write set to a transaction on the server, reusing all of the machinery for transactions. The conditional-write API can be more efficient than the transaction API in cases where clients can compute the precondition without requiring a read.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe future of ZippyDB\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eDistributed key-value stores have many applications, and the need for them often comes up while building a variety of systems, from products to storing metadata for various infrastructure services. Building a scalable, strongly consistent, and fault-tolerant key-value store can be very challenging and often requires thinking through many trade-offs to provide a curated combination of system capabilities and guarantees that works well in practice for a variety of workloads. This blog post introduced ZippyDB, Facebook’s biggest key-value store, which has been in production for more than six years serving a lot of different workloads. Since its inception, the service has seen very steep adoption, mostly due to the flexibility that it offers in terms of making efficiency, availability, and performance trade-offs. The service also enables us to use engineering resources effectively as a company and use our key-value store capacity efficiently as a single pool. ZippyDB is still evolving and currently undergoing significant architectural changes, such as storage-compute disaggregation, fundamental changes to membership management, failure detection and recovery, and distributed transactions, in order to adapt to the changing ecosystem and product requirements.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. Since we first deployed ZippyDB in 2012, this key-value store has expanded rapidly, and today, ZippyDB serves a number of use cases, ranging from metadata for a distributed filesystem, counting events for both internal and external purposes, to product data that’s used for [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/08/06/core-data/zippydb/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/08/06/core-data/zippydb/\"\u003eHow we built a general purpose key value store for Facebook with ZippyDB\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Hero_.Image_.jpg",
      "date_published": "2021-08-06T16:52:02Z",
      "author": {
        "name": "By Sarang Masti"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/08/04/open-source/winterfell/",
      "title": "Open sourcing Winterfell: A STARK prover and verifier",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe are releasing Winterfell, our implementation of a STARK prover/verifier to Crates.io \u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWinterfell is an easy to use open source implementation of STARKs for security and privacy applications.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eOne potential application for Winterfell’s zero-knowledge proofs is blockchain privacy and scalability. \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003e“Any sufficiently advanced technology is indistinguishable from magic.” —Clarke’s Third Law\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhat if the average developer could benefit from proofs of computational integrity (CI) that would normally require an in-depth knowledge of cryptography to implement? \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eCI proofs, of which zero-knowledge proofs (ZKPs) are a subset, are a cryptographic technology that let you do seemingly impossible things. For example, you can run a computation and get some result. You can then use a CI proof to convince anyone that you did the computation correctly without their having to rerun the computation themselves.\u003c/span\u003e \u003cspan\u003eAnd they can verify this correctness in just a few milliseconds, regardless of how complex or long-running the original computation was.\u003c/span\u003e \u003cspan\u003eTo bring the power of CI proofs to the masses, we’ve developed Winterfell, a general-purpose STARK (Scalable Transparent Arguments of Knowledge) prover and verifier. We are happy to be publishing the v0.1 version of the library to crates.io.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_FINAL.gif\" alt=\"To bring the power of CI proofs to the masses, we’ve developed Winterfell, a general-purpose STARK prover and verifier. \" width=\"1920\" height=\"1080\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAnother important property of these CI proofs is the ability to hide some (or all) of the inputs that were used to run the computation. This is the zero-knowledge aspect. For example, you could prove that a number is in a given range without revealing the exact value of the number (these types of proofs are usually called range proofs). Or, you could do something as complex as comparing two number sequences, one public and one private (known only to yourself), and prove to anyone beyond a doubt that there is or isn’t a match between them.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe magic of ZKPs\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe general ideas behind ZKPs were developed as early as the 1980s, but interest in this area of cryptography has \u003c/span\u003e\u003ca href=\"https://nakamoto.com/cambrian-explosion-of-crypto-proofs/\"\u003e\u003cspan\u003eexploded\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e recently, driven in part by emergent applications in the blockchain space. In the last few years, over a dozen new proving systems have appeared. Some of them have even been deployed in production where tens of billions of dollars depend on their security properties. However, ZKPs are far from mainstream, primarily for two reasons:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUntil recently, deploying ZKPs in applications required expert cryptographers with years of experience. The situation is somewhat better now, as there are plenty of relatively accessible materials available and more projects that try to make ZKPs accessible to the average developer. But even now, making sense of different proving systems and the trade-offs associated with them requires deep expertise and/or a steep learning curve, even for experienced software engineers.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWhile verifying a ZK proof is extremely fast and requires very few compute resources, generating a proof is a computationally intensive process. It may take seconds or even minutes (or many CPU cores) to generate proofs for even relatively simple computations. Only relatively recent advances in cryptography and implementation improvements have brought a large segment of computations to within practical feasibility for ZKPs. And there is a lot of ongoing work to expand the set of computations for which proof generation is practical.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eWe developed Winterfell to bridge these gaps and to bring ZKPs within reach of regular developers. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWinterfell is here\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWinterfell is a general purpose STARK prover and verifier written in \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2021/04/29/developer-tools/rust/\"\u003e\u003cspan\u003eRust\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e at \u003c/span\u003e\u003ca href=\"https://research.fb.com/category/blockchain-and-cryptoeconomics/\"\u003e\u003cspan\u003eNovi Research\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. \u003c/span\u003e\u003ci\u003e\u003cspan\u003eGeneral purpose\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e means that Winterfell can generate CI proofs for any computation. Basically, for any program that can be described with a Turing-complete language, we can generate a CI proof using Winterfell (though this would be much more straightforward for some programs than for others).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWinterfell uses STARKs, a proof-of-computation scheme developed by Eli Ben-Sasson, Michael Riabzev, et al. In comparison with many other CI proving systems, STARKs have a number of attractive properties, including:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSTARKs rely on very few cryptographic assumptions. In fact, the only cryptographic primitive we need for STARKs to work is a collision resistant hash function (e.g., SHA256). This also makes STARKs resistant to potential attacks from adversaries with quantum computers.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUnlike many other proving systems, STARKs are fully transparent. This means we don’t need to run complicated trusted setup ceremonies to start using STARKs. Trusted setups are a potential security weakness in other zero knowledge protocols, because a compromised trusted setup allows attackers to generate fake CI proofs. STARKs are immune to this.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eIn comparison with other systems, STARK proof generation is extremely fast when we deal with uniform computations, or computations with regular structures. Fortunately, the vast majority of programs people write do possess such regular structures. Moreover, pretty much every single step of the STARK proof generation process is massively parallelizable. Thus, we can frequently speed up proof generation by distributing it across more and more CPU cores.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eNone of the individual properties listed above are unique to STARKs. However, no other proving system combines lean cryptography, transparency, and performance to the extent STARKs do. Winterfell takes full advantage of these benefits while abstracting away most of the complexity. For example, proof generation can be distributed across multiple CPU cores to dramatically reduce proof generation time (see our benchmarks \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell#performance\"\u003e\u003cspan\u003ehere\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e). Moreover, we have plans to enable fully distributed proof generation across multiple machines and have already started work in this direction.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn addition to being performant, Winterfell is highly configurable. That is, you can dynamically tune almost all parameters of the STARK protocol to attain specific performance and security targets. We are able to achieve such high configurability without sacrificing performance or code clarity by relying on Rust’s zero-cost abstractions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eLast, and perhaps most important, you don’t need to be a cryptographer to use Winterfell. As mentioned previously, Winterfell abstracts away most of the complexity of the STARK protocol. The only thing the user is responsible for is describing their computation in a format that the STARK prover/verifier can understand. This format is called algebraic intermediate representation (AIR), and the step of translating a program into AIR is called arithmetization.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg\" alt=\"\" width=\"1920\" height=\"847\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=916,404 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=768,339 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=1024,452 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=1536,678 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=96,42 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=192,85 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eUsing Winterfell\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWinterfell exposes a relatively simple interface for describing AIR for any computation. However, the concept of arithmetization is not something most developers are familiar with, so there is going to be a learning curve. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo help you get started, we’ve put together an end-to-end \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell#usage\"\u003e\u003cspan\u003etutorial\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e on how to define AIR for a very simple computation. We also have examples of more interesting computations in the \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell/tree/main/examples\"\u003e\u003cspan\u003eexamples crate\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, ranging from something as simple as a Fibonacci sequence to something as sophisticated as aggregation of hash-based signatures. And if you would like to get a little bit deeper into theory, we recommend reading two excellent blog posts from StarkWare: \u003c/span\u003e\u003ca href=\"https://medium.com/starkware/arithmetization-i-15c046390862\"\u003e\u003cspan\u003eArithmetization I\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://medium.com/starkware/arithmetization-ii-403c3b3f4355\"\u003e\u003cspan\u003eArithmetization II\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOnce you are comfortable with writing AIRs, using Winterfell to generate STARK proofs becomes relatively easy. For example, AIR for a Fibonacci sequence requires less than 100 lines of code and can be put together in about 15 minutes. Even for the relatively complicated example of hash-based signature aggregation mentioned above, the AIR is described in about 600 lines of code (though it did take several days to put together).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAnother point worth mentioning: We wrote Winterfell as a set of modular crates, all of which are being published to \u003ca href=\"https://crates.io/users/irakliyk\"\u003eCrates.io\u003c/a\u003e today as well. While we use these crates to build a STARK proving system, many of them are general enough to be used as building blocks in other CI proving systems. For example, for low-degree testing, we use the FRI protocol implemented in the \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell/tree/main/fri\"\u003e\u003cspan\u003ewinter-fri\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e crate, which is also used as a building block for several other proof systems (e.g., \u003c/span\u003e\u003ca href=\"https://eprint.iacr.org/2019/1076.pdf\"\u003e\u003cspan\u003eFractal\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://eprint.iacr.org/2018/828.pdf\"\u003e\u003cspan\u003eAurora\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e) that aim to be transparent and post-quantum. Thus, we hope that our work will help implementers of these protocols get their job done more quickly and efficiently.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eApplications\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eRecent advancements in ZKPs are driven by emergent use cases in the blockchain space. Specifically, ZKPs offer attractive solutions to perhaps two of the most pressing blockchain challenges: privacy and scalability. However, ZKPs have numerous potential applications outside of the blockchain space as well.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile there still remain some technical challenges to overcome before proofs of computational integrity can be considered practical at a large scale, we believe that Winterfell represents an important stepping stone for bringing a well-studied subject in academic research into practical deployments. And we hope that the security and privacy community will also benefit from an easy to use open source implementation of STARKs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePlease check out the \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell\"\u003e\u003cspan\u003eWinterfell repository\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, and feel free to open issues for comment and leave feedback!\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWe are releasing Winterfell, our implementation of a STARK prover/verifier to Crates.io  Winterfell is an easy to use open source implementation of STARKs for security and privacy applications. One potential application for Winterfell’s zero-knowledge proofs is blockchain privacy and scalability.  “Any sufficiently advanced technology is indistinguishable from magic.” —Clarke’s Third Law What if the average [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/08/04/open-source/winterfell/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/08/04/open-source/winterfell/\"\u003eOpen sourcing Winterfell: A STARK prover and verifier\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/08/CD21_418_Open_Sourcing_Winterfell_HERO_FINAL.jpg",
      "date_published": "2021-08-04T16:00:43Z",
      "author": {
        "name": "By Irakliy Khaburzaniya, Kostas Chalkias, Kevin Lewi, Harjasleen Malvai"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/",
      "title": "A linear programming approach for optimizing features in ML models",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eWhether it’s iterating on \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/\"\u003e\u003cspan\u003eFacebook’s News Feed ranking algorithm\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e or delivering the most relevant ads to users, we are constantly exploring new features to help improve our machine learning (ML) models. Every time we add new features, we create a challenging data engineering problem that requires us to think strategically about the choices we make. More complex features and sophisticated techniques require additional storage space. Even at a company the size of Facebook, capacity isn’t infinite. If left unchecked, accepting all features would quickly overwhelm our capacity and slow down our iteration speed, decreasing the efficiency of running the models.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo better understand the relationship between these features and the capacity of the infrastructure that needs to support them, we can frame the system as a linear programming problem. By doing this, we can maximize a model’s performance, probe the sensitivity of its performance to different infrastructure constraints, and study the relationships between different services. \u003c/span\u003e\u003cspan\u003eThis work was done by data scientists embedded in our engineering team and demonstrates the value of analytics and data science in ML.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eSupporting feature development\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eIt’s important to continually introduce features that best leverage new data to maintain performant models. New features are responsible for the majority of incremental model improvements. These ML models are useful for our ad delivery system. They work together to predict a person’s likelihood of taking specific action on the ad. We work to continuously improve our models so our systems deliver only those ads that are relevant to a user.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs our techniques become more sophisticated, we develop more complex features that demand more of our infrastructure. A feature can leverage different services depending on its purpose. Some features have a higher memory cost, while others require extra CPU or take up more storage. It’s important to use our infrastructure wisely to maximize the performance of our models. We must be able to smartly allocate resources and be able to quantify the trade-offs of different scenarios.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo address these problems, we frame our system as a linear programming problem that maximizes our model’s metrics. We use this framework to better understand the interaction between our features and services. With this knowledge, we can automatically select the best features, identify infrastructure services to invest in, and maintain the health of both our models and services.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eFraming our problem\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eTo get a handle on our framework, we’ll first introduce a model problem. Say we have multiple features that all take up some amount of space (the height of the rectangles) and contribute some amount of gain to our models (the teal squares), and we are unable to accommodate them all in our limited capacity.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg\" alt=\"Say we have multiple features that all take up some amount of space (the height of the rectangles) and contribute some amount of gain to our models (the teal squares), and we are unable to accommodate them all in our limited capacity.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003eA naive solution would be to just start picking the features with the most gain (teal squares) until you are out of capacity. However, you might not be making the best use of your resources if you just prioritize the gain. For example, by taking in a big feature with a large gain, you could be taking up room that two smaller features with less gain could use instead. Together, those two smaller features would give you more bang for your buck than the big feature.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg\" alt=\"by taking in a big feature with a large gain, you could be taking up room that two smaller features with less gain could use instead.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIf we get a little less naive, we could instead look to pick features that give us the most bang per buck — features that have the most gain per storage. However, if we pick features only from that perspective, we could end up leaving out some less efficient features that we would still have room to accommodate.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg\" alt=\"we could instead look to pick features that give us the most bang per buck — features that have the most gain per storage\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe’ve been looking at a very simplified view of infrastructure, but the reality is a bit more complex. For example, features often don’t take up just one resource but need many — such as memory, CPU, or storage in other services. We can make our example slightly more sophisticated by adding in Service B, and saying that orange features take up space in both Service A and Service B.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg\" alt=\"We can make our example slightly more sophisticated by adding in Service B, and saying that orange features take up space in both Service A and Service B.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePicking which features we use is not the only way to control our infrastructure usage. We can also employ various techniques to increase the efficiency of our feature storage. This sometimes comes with a cost, either through the feature itself or capacity from a service. In this case, let’s say that we can halve the storage cost of some features (bordered in pink) but only at the cost of reducing the gain of the feature, and using some of the limited capacity in Service B.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg\" alt=\"let’s say that we can halve the storage cost of some features (bordered in pink), but only at the cost of reducing the gain of the feature, and using some of the limited capacity in Service B.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe’ll stop the example at this point, but this is enough for the general message to be clear — infrastructure can be a complicated interconnected system of different constraints. In reality, our capacity is not set in stone. We can move resources around if it is warranted. Features are also not the only thing we’re working on. There are plenty of other projects and workflows that compete for the same resources. We need to not only choose the features that maximize our gain but also be able to answer questions about how our system responds to changes:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWhich features do we select to optimize the gain?\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eIs feature compression worth it? More important, is it worth an engineer’s time to implement it?\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eHow does the gain change if we add more capacity to Service A?\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eHow do service dependencies interact? If we increase the capacity of Service B, can we use less of Service A?\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eScaling the problem\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eLet’s step back and review the conditions of our model problem:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe want to maximize our gain.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe are limited by the capacity of Service A.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe are also limited by the capacity of Service B, which only some features contribute to.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSome features may be compressed, but:\u003c/span\u003e\n\u003col\u003e\n\u003cli aria-level=\"2\"\u003e\u003cspan\u003eThey suffer a loss to their gain.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"2\"\u003e\u003cspan\u003eSome of Service B’s capacity must be used.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eWe can express all these constraints as a system of linear equations.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eLet 𝑥 be a vector that is 0 or 1 that signifies whether we select the feature, and let 𝑔 be a vector that stores the gain of the feature. The subscripts 𝑓 and 𝑐 denote whether we are specifying a full cost or compressed feature. For example, 𝑥\u003c/span\u003e\u003cspan\u003e𝑓\u003c/span\u003e\u003cspan\u003e denotes full, uncompressed features that we have selected to include, and 𝑔\u003c/span\u003e\u003cspan\u003e𝑐\u003c/span\u003e\u003cspan\u003e represents the cost of compressed features.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eGiven these definitions, our objective is to maximize:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg\" alt=\"linear programming equation\" width=\"1920\" height=\"244\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=916,116 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=768,98 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=1024,130 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=1536,195 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=96,12 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=192,24 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe can now add our constraints that model the limitations of our infrastructure:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003eFeatures will either be selected and compressed, selected but not compressed, or not selected. We should not select the compressed and uncompressed versions of the same feature.\u003cbr/\u003e\n\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg\" alt=\"linear programming equation\" width=\"1920\" height=\"894\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=916,427 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=768,358 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=1024,477 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=1536,715 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=192,89 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eLet 𝑠 be the storage cost of the feature and the subscripts 𝐴 and 𝐵 represent Service A and B, respectively. For example, 𝑠\u003c/span\u003e\u003cspan\u003e𝐴𝑐\u003c/span\u003e\u003cspan\u003e represents the storage cost of compressed features in Service A. We are constrained by the capacity of the two services.\u003cbr/\u003e\n\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg\" alt=\"\" width=\"1920\" height=\"594\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=916,283 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=768,238 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=1024,317 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=1536,475 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=96,30 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=192,59 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eSome of Service B must be utilized to enable compression. Let’s represent that as a few features that must be selected.\u003cbr/\u003e\n\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg\" alt=\"\" width=\"1920\" height=\"892\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=916,426 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=768,357 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=1024,476 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=1536,714 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=192,89 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eWith this, we have now completely specified our problem in a few equations and can solve them using linear programming techniques. Of course, as we are interested in automating and productionalizing this, it can easily be specified in code. For this example, we accomplish this in Python using the excellent \u003c/span\u003e\u003ca href=\"https://numpy.org/\"\u003e\u003cspan\u003eNumPy\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://www.cvxpy.org/\"\u003e\u003cspan\u003eCVXPY\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e packages.\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport cvxpy as cp\nimport numpy as np\nimport pandas as pd\n \n# Assuming data is a Pandas DataFrame that contains relevant feature data\ndata = pd.DataFrame(...)\n# These variables contain the maximum capacity of various services\nservice_a = ...\nservice_b = ...\n \nselected_full_features = cp.Variable(data.shape[0], boolean=True)\nselected_compressed_features = cp.Variable(data.shape[0], boolean=True)\n \n# Maximize the feature gain\nfeature_gain = (\n   data.uncompressed_feature_gain.to_numpy() @ selected_full_features\n   + data.compressed_feature_gain.to_numpy() @ selected_compressed_features\n)\n \nconstraints = [\n   # 1. We should not select the compressed and uncompressed version\n   #    of the same feature\n   selected_full_features + selected_compressed_features \u0026lt;= np.ones(data.shape[0]),\n   # 2. Features are restricted by the maximum capacity of the services\n   data.full_storage_cost.to_numpy() @ selected_full_features\n   + data.compressed_storage_cost.to_numpy() @ selected_full_features\n   \u0026lt;= service_a,\n   data.full_memory_cost.to_numpy() @ selected_full_features\n   + data.compressed_memory_cost.to_numpy() @ selected_compressed_features\n   \u0026lt;= service_b,\n   # 3. Some features must be selected to enable compression\n   selected_full_features \u0026gt;= data.special_features.to_numpy(),\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003cspan\u003eLeveraging the framework\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eNow we have a framework that we can use to express our questions and hypotheticals. If we want to find out how an increase in Service A translates to a feature gain, we can  run the optimization problem above at different values for Service A capacity and plot the gain. This way, we can directly quantify the return for each incremental increase in capacity. We can use this as a strong signal for what services we should invest in for the future and directly compare the return on investment on more feature memory, computing, or storage.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg\" alt=\"chart showing return on service A capacity\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSimilarly, we can look at the relationships between services. We simply vary the capacity of Services A and B while keeping the gain constant. We can see that as Service B’s capacity increases, less of Service A is needed to achieve the same gain. This can be leveraged if one service is overly stressed compared with another.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg\" alt=\"chart showing the relationship between Service A and service B capacity\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eLinear programming as a framework for automating decisions\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003ePreviously, feature approval was a manual process where teams would spend valuable time calculating how many features we could support and analyzing what the return on investment was for increasing the capacity of our services. In a company like Facebook — where we have multiple models being continuously iterated on — this approach does not scale. By framing our services as a system of linear equations, we take a complex interconnected system and simplify it into basic relationships that are easily communicated. By doing this, we can make smarter decisions about the features we deploy and the infrastructure we invest in.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWhether it’s iterating on Facebook’s News Feed ranking algorithm or delivering the most relevant ads to users, we are constantly exploring new features to help improve our machine learning (ML) models. Every time we add new features, we create a challenging data engineering problem that requires us to think strategically about the choices we make. [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/\"\u003eA linear programming approach for optimizing features in ML models\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/CD21-479_Framing-Feature-Resources_Hero-1920x1080-1.jpg",
      "date_published": "2021-07-29T15:57:28Z",
      "author": {
        "name": "By Paulo Silva Costa"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/",
      "title": "Migrating Facebook to MySQL 8.0",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003ca href=\"https://github.com/facebook/mysql-5.6\"\u003e\u003cspan\u003eMySQL\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, an open source database developed by Oracle, powers some of Facebook’s most important workloads. We actively develop new features in MySQL to support our evolving requirements. These features change many different areas of MySQL, including client connectors, storage engine, optimizer, and replication. Each new major version of MySQL requires significant time and effort to migrate our workloads. The challenges  include:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003ePorting our custom features to the new version\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eEnsuring replication is compatible between the major versions\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eMinimizing changes needed for existing application queries\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFixing performance regressions that prevent the server from supporting our workloads\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eOur last major version upgrade, to MySQL 5.6, took more than a year to roll out. When version 5.7 was released, we were still in the midst of developing our LSM-Tree storage engine, \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2016/08/31/core-data/myrocks-a-space-and-write-optimized-mysql-database/\"\u003e\u003cspan\u003eMyRocks\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, on version 5.6. Since upgrading to 5.7 while simultaneously building a new storage engine would have significantly slowed the progress on MyRocks, we opted to stay with 5.6 until MyRocks was complete. MySQL 8.0 was announced as we were finishing the rollout of MyRocks to our user database (UDB) service tier. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThat version included compelling features like writeset-based parallel replication and a transactional data dictionary that provided atomic DDL support. For us, moving to 8.0 would also bring in the 5.7 features we had missed, including Document Store. Version 5.6 was approaching end of life, and we wanted to stay active within the MySQL community, especially with our work on the MyRocks storage engine. Enhancements in 8.0, like instant DDL, could speed up MyRocks schema changes, but we needed to be on the 8.0 codebase to use it. Given the benefits of the code update, we decided to migrate to 8.0. We’re sharing how we tackled our 8.0 migration project — and some of the surprises we discovered in the process. When we initially scoped out the project, it was clear that moving to 8.0 would be even more difficult than migrating to 5.6 or MyRocks.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAt the time, our customized 5.6 branch had over 1,700 code patches to port to 8.0. As we were porting those changes, new Facebook MySQL features and fixes were added to the 5.6 codebase that moved the goalpost further away.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe have many MySQL servers running in production, serving a large number of disparate applications. We also have extensive software infrastructure for managing MySQL instances. These applications perform operations like gathering statistics and managing server backups.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUpgrading from 5.6 to 8.0 skipped over 5.7 entirely. APIs that were active in 5.6 would have been deprecated in 5.7 and possibly removed in 8.0, requiring us to update any application using the now-removed APIs.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA number of Facebook features were not forward-compatible with similar ones in 8.0 and required a deprecation and migration path forward.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eMyRocks enhancements were needed to run in 8.0, including native partitioning and crash recovery.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eCode patches\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe first set up the 8.0 branch for building and testing in our development environments. We then began the long journey to port the patches from our 5.6 branch. There were more than 1,700 patches when we started, but we were able to organize them into a few major categories. Most of our custom code had good comments and descriptions so we could easily determine whether it was still needed by the applications or if it could be dropped. Features that were enabled by special keywords or unique variable names also made it easy to determine relevance because we could search through our application codebases to find their use cases. A few patches were very obscure and required detective work — digging through old design documents, posts, and/or code review comments — to understand their history.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe sorted each patch into one of four buckets:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eDrop: Features that were no longer used, or had equivalent functionality in 8.0, did not need to be ported.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eBuild/Client: Non-server features that supported our build environment and modified MySQL tools like mysqlbinlog, or added functionality like the async client API, were ported.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eNon-MyRocks Server: Features in the mysqld server that were not related to our MyRocks storage engine were ported.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eMyRocks Server: Features that supported the MyRocks storage engine were ported.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eWe tracked the status and relevant historical information of each patch using spreadsheets, and recorded our reasoning when dropping a patch. Multiple patches that updated the same feature were grouped together for porting. Patches ported and committed to the 8.0 branch were annotated with the 5.6 commit information. Discrepancies on porting status would inevitably arise due to the large number of patches we needed to sift through and these notes helped us resolve them.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEach of the client and server categories naturally became a software release milestone. With all client-related changes ported, we were able to update our client tooling and connector code to 8.0. Once all of the non-MyRocks server features were ported, we were able to deploy 8.0 mysqld for InnoDB servers. Finishing up the MyRocks server features enabled us to update MyRocks installations.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSome of the most complex features required significant changes for 8.0, and a few areas had major compatibility problems. For example, upstream 8.0 binlog event formats were incompatible with some of our custom 5.6 modifications. Error codes used by Facebook 5.6 features conflicted with those assigned to new features by upstream 8.0. We ultimately needed to patch our 5.6 server to be forward-compatible with 8.0.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIt took a couple of years to complete porting all of these features. By the time we got to the end, we had evaluated more than 2,300 patches and ported 1,500 of those to 8.0.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe migration path\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe group together multiple mysqld instances into a single MySQL replica set. Each instance in a replica set contains the same data but is geographically distributed to a different data center to provide data availability and failover support. Each replica set has one primary instance. The remaining instances are all secondaries. The primary handles all write traffic and replicates the data asynchronously to all secondaries.\u003cbr/\u003e\n\u003c/span\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003eWe started with replica sets consisting of 5.6 primary/5.6 secondaries and the end goal was replica sets with 8.0 primary/8.0 secondaries. We followed a plan similar to the\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2017/09/25/core-data/migrating-a-database-from-innodb-to-myrocks/\"\u003e \u003cspan\u003eUDB MyRocks migration plan\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFor each replica set, create and add 8.0 secondaries via a logical copy using mysqldump. These secondaries do not serve any application read traffic.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eEnable read traffic on the 8.0 secondaries.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAllow the 8.0 instance to be promoted to primary.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eDisable the 5.6 instances for read traffic.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eRemove all the 5.6 instances.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eEach replica set could transition through each of the steps above independently and stay on a step as long as needed. We separated replica sets into much smaller groups, which we shepherded through each transition. If we found problems, we could rollback to the previous step. In some cases, replica sets were able to reach the last step before others started.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo automate the transition of a large number of replica sets, we needed to build new software infrastructure. We could group replica sets together and move them through each stage by simply changing a line in a configuration file. Any replica set that encountered problems could then be individually rolled back.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eRow-based replication\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eAs part of the 8.0 migration effort, we decided to standardize on using row-based replication (RBR). Some 8.0 features required RBR, and it simplified our MyRocks porting efforts. While most of our MySQL replica sets were already using RBR, those still running statement-based replication (SBR) could not be easily converted. These replica sets usually had tables without any high cardinality keys. Switching completely to RBR had been a goal, but the long tail of work needed to add primary keys was often prioritized lower than other projects.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHence, we made RBR a requirement for 8.0. After evaluating and adding primary keys to every table, we switched over the last SBR replica set this year. Using RBR also gave us an alternative solution for resolving an application issue that we encountered when we moved some replica sets to 8.0 primaries, which will be discussed later.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAutomation validation\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eMost of the 8.0 migration process involved testing and verifying the mysqld server with our automation infrastructure and application queries.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs our MySQL fleet grew, so did the automation infrastructure we use to manage the servers. In order to ensure all of our MySQL automation was compatible with the 8.0 version, we invested in building a test environment, which leveraged test replica sets with virtual machines to verify the behaviors. We wrote integration tests to canary each piece of automation to run on both the 5.6 version and the 8.0 version and verified their correctness. We found several bugs and behavior differences as we went through this exercise.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs each piece of MySQL infrastructure was validated against our 8.0 server, we found and fixed (or worked around) a number of interesting issues:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSoftware that parsed text output from error log, mysqldump output, or server show commands easily broke. Slight changes in the server output often revealed bugs in a tool’s parsing logic.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe 8.0’s default \u003c/span\u003e\u003cspan\u003eutf8mb4\u003c/span\u003e\u003cspan\u003e collation settings resulted in collation mismatches between our 5.6 and 8.0 instances. 8.0 tables may use the new \u003c/span\u003e\u003cspan\u003eutf8mb4_0900\u003c/span\u003e\u003cspan\u003e collations even for create statements generated by 5.6’s show create table because the 5.6 schemas using \u003c/span\u003e\u003cspan\u003eutf8mb4_general_ci\u003c/span\u003e\u003cspan\u003e do not explicitly specify collation. These table differences often caused problems with replication and schema verification tools.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe error codes for certain replication failures changed and we had to fix our automation to handle them correctly.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe 8.0 version’s data dictionary obsoleted table .frm files, but some of our automation used them to detect table schema modifications.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe had to update our automation to support the dynamic privs introduced in 8.0.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\u003cspan\u003eApplication validation\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe wanted the transition for applications to be as transparent as possible, but some application queries hit performance regressions or would fail on 8.0.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor the MyRocks migration, we built a MySQL shadow testing framework that captured production traffic and replayed them to test instances. For each application workload, we constructed test instances on 8.0 and replayed shadow traffic queries to them. We captured and logged the errors returning from the 8.0 server and found some interesting problems. Unfortunately, not all of these problems were found during testing. For example, the transaction deadlock was discovered by applications during the migration. We were able to roll back these applications to 5.6 temporarily while we researched different solutions.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eNew reserved keywords were introduced in 8.0 and a few, such as groups and rank, conflicted with popular table column names and aliases used in application queries. These queries did not escape the names via backquotes, leading to parsing errors. Applications using software libraries that automatically escaped the column names in queries did not hit these issues, but not all applications used them. Fixing the problem was simple, but it took time to track down application owners and codebases generating these queries.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA few REGEXP incompatibilities were also found between 5.6 and 8.0.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA few applications hit \u003c/span\u003e\u003ca href=\"https://bugs.mysql.com/bug.php?id=98324\"\u003e\u003cspan\u003erepeatable-read transaction deadlocks\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e involving \u003c/span\u003e\u003cspan\u003einsert … on duplicate key\u003c/span\u003e\u003cspan\u003e queries on InnoDB. 5.6 had a bug which was corrected in 8.0, but the fix increased the likelihood of transaction deadlocks. After analyzing our queries, we were able to resolve them by lowering the isolation level. This option was available to us since we had made the switch to row-based replication.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eOur custom 5.6 Document Store and JSON functions were not compatible with 8.0’s. Applications using Document Store needed to convert the document type to text for the migration. For the JSON functions, we added 5.6-compatible versions to the 8.0 server so that applications could migrate to the 8.0 API at a later time.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eOur query and performance testing of the 8.0 server uncovered a few problems that needed to be addressed almost immediately.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe found new mutex contention hotspots around the ACL cache. When a large number of connections were opened simultaneously, they could all block on checking ACLs.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSimilar contention was found with binlog index access when many binlog files are present and high binlog write rates rotate files frequently.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSeveral queries involving temp tables were broken. The queries would return unexpected errors or take so long to run that they would time out.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eMemory usage compared with 5.6 had increased, especially for our MyRocks instances, because InnoDB in 8.0 must be loaded. The default performance_schema settings enabled all instruments and consumed significant memory. We limited the memory usage by only enabling a small number of instruments and making code changes to disable tables that could not be manually turned off. However, not all the increased memory was being allocated by performance_schema. We needed to examine and modify various InnoDB internal data structures to reduce the memory footprint further. This effort brought 8.0’s memory usage down to acceptable levels. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhat’s next\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe 8.0 migration has taken a few years so far. We have converted many of our InnoDB replica sets to running entirely on 8.0. Most of the remaining ones are at various stages along the migration path. Now that most of our custom features have been ported to 8.0, updating to Oracle’s minor releases has been comparatively easier and we plan to keep pace with the latest versions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSkipping a major version like 5.7 introduced problems, which our migration needed to solve.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFirst, we could not upgrade servers in place and needed to use logical dump and restore to build a new server. However, for very large mysqld instances, this can take many days on a live production server and this fragile process will likely be interrupted before it can complete. For these large instances, we had to modify our backup and restore systems to handle the rebuild.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSecond, it is much harder to detect API changes because 5.7 could have provided deprecation warnings to our application clients to fix potential issues. Instead, we needed to run additional shadow tests to find failures before we could migrate the production workloads. Using mysql client software that automatically escaped schema object names helps reduce the number of compatibility issues.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSupporting two major versions within a replica set is hard. Once a replica set promotes its primary to be an 8.0 instance, it is best to disable and remove the 5.6 ones as soon as possible. Application users tend to discover new features that are supported only by 8.0, like \u003c/span\u003e\u003cspan\u003eutf8mb4_0900\u003c/span\u003e\u003cspan\u003e collations, and using these can break the replication stream between 8.0 and 5.6 instances.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDespite all the hurdles in our migration path, we have already seen the benefits of running 8.0. Some applications have opted for early conversion to 8.0 to utilize features like Document Store and improved datetime support. We have been considering how to support storage engine features like Instant DDL on MyRocks. Overall, the new version greatly expands on what we can do with MySQL @ Facebook.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eMySQL, an open source database developed by Oracle, powers some of Facebook’s most important workloads. We actively develop new features in MySQL to support our evolving requirements. These features change many different areas of MySQL, including client connectors, storage engine, optimizer, and replication. Each new major version of MySQL requires significant time and effort to [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/\"\u003eMigrating Facebook to MySQL 8.0\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/CD21_390_ENG_MySQL_HERO_FINAL_2x.jpg",
      "date_published": "2021-07-22T16:00:35Z",
      "author": {
        "name": "By Herman Lee, Pradeep Nayak"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/15/open-source/fsdp/",
      "title": "Fully Sharded Data Parallel: faster AI training with fewer GPUs",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eTraining AI models at a large scale isn’t easy. Aside from the need for large amounts of computing power and resources, there is also considerable engineering complexity behind training very large models. At Facebook AI Research (FAIR) Engineering, we have been working on building tools and infrastructure to make training large AI models easier. Our recent work in areas such as \u003c/span\u003e\u003ca href=\"https://github.com/pytorch/fairseq/blob/master/examples/megatron_11b/README.md\"\u003e\u003cspan\u003eintra-layer model parallelism\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, \u003c/span\u003e\u003ca href=\"https://fairscale.readthedocs.io/en/latest/deep_dive/pipeline_parallelism.html\"\u003e\u003cspan\u003epipeline model parallelism\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale#optimizer-state-sharding-zero\"\u003e\u003cspan\u003eoptimizer state+gradient sharding\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, and \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/moe_layer.py\"\u003e\u003cspan\u003emixture of experts\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e is just part of our work to make training advanced AI models for any number of tasks more efficient.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFully Sharded Data Parallel (FSDP) is the newest tool we’re introducing. It \u003ca href=\"https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\"\u003eshards\u003c/a\u003e an AI model’s parameters across data parallel workers and can optionally offload part of the training computation to the CPUs. As its name suggests, FSDP is a type of data-parallel training algorithm. Although the parameters are sharded to different \u003ca href=\"https://engineering.fb.com/2018/03/20/ml-applications/the-next-step-in-facebook-s-ai-hardware-infrastructure/\"\u003eGPUs\u003c/a\u003e, the computation for each microbatch of data is still local to each GPU worker. This conceptual simplicity makes FSDP easier to understand and more applicable to a wide range of usage scenarios (compared with intra-layer parallelism and pipeline parallelism). Compared with optimizer state+gradient sharding data parallel methods, FSDP shards parameters more uniformly and is capable of better performance via communication and computation overlapping during training.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith FSDP, it is now possible to more efficiently train models that are orders of magnitude larger using fewer GPUs. FSDP has been implemented in the \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale\"\u003e\u003cspan\u003eFairScale library\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and allows engineers and developers to scale and optimize the training of their models with simple APIs. At Facebook, FSDP has already been integrated and tested for training some of our \u003c/span\u003e\u003ca href=\"https://github.com/pytorch/fairseq\"\u003e\u003cspan\u003eNLP\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and\u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/vissl\"\u003e\u003cspan\u003e Vision\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e models.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe high computational cost of large-scale training\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2001.08361.pdf\"\u003e\u003cspan\u003eNLP research\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e is one particular area where we can see the importance of efficiently leveraging compute for training AI. Last year, OpenAI announced that they had trained \u003c/span\u003e\u003ca href=\"https://neurips.cc/virtual/2020/public/poster_1457c0d6bfcb4967418bfb8ac142f64a.html\"\u003e\u003cspan\u003eGPT-3\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, the largest-ever neural language model, with 175 billion parameters. It is \u003c/span\u003e\u003ca href=\"https://lambdalabs.com/blog/demystifying-gpt-3/\"\u003e\u003cspan\u003eestimated\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to have taken roughly 355 GPU years to train GPT-3, or the equivalent of 1,000 GPUs working continuously for more than four months.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBesides requiring a lot of compute and engineering resources, most approaches to scaling like this introduce additional communication costs and require engineers to carefully evaluate trade-offs between memory use and computational efficiency. For example, typical data parallel training requires maintaining redundant copies of the model on each GPU, and model parallel training introduces additional communication costs to move activations between workers (GPUs).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFSDP is relatively free of trade-offs in comparison. It improves memory efficiency by sharding model parameters, gradients, and optimizer states across GPUs, and improves computational efficiency by decomposing the communication and overlapping it with both the forward and backward passes. FSDP produces identical results as standard distributed data parallel (DDP) training and is available in an easy-to-use interface that’s a drop-in replacement for PyTorch’s DistributedDataParallel module. Our early testing has shown that FSDP can enable scaling to trillions of parameters.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow FSDP works\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eIn standard DDP training, every worker processes a separate batch and the gradients are summed across workers using an \u003c/span\u003e\u003ca href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce\"\u003e\u003cspan\u003eall-reduce operation\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. While DDP has become very popular, it takes more GPU memory than it needs because the model weights and optimizer states are replicated across all DDP workers.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOne method to reduce replications is to apply a process called full parameter sharding, where only a subset of the model parameters, gradients, and optimizers needed for a local computation is made available. An implementation of this method, ZeRO-3, has already been popularized by Microsoft. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe key insight to unlock full parameter sharding is that we can decompose the \u003c/span\u003e\u003ca href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce\"\u003e\u003cspan\u003eall-reduce\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e operations in DDP into separate \u003c/span\u003e\u003ca href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#reducescatter\"\u003e\u003cspan\u003ereduce-scatter\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003ca href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allgather\"\u003eall-gather\u003c/a\u003e operations:\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17828\" aria-describedby=\"caption-attachment-17828\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?w=1024\" alt=\"Full Sharded Data Parallel graph\" width=\"1024\" height=\"562\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png 1264w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=916,503 916w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=768,422 768w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=1024,562 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=96,53 96w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=192,105 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17828\"\u003eAll-reduce as a combination of reduce-scatter and all-gather. The standard all-reduce operation to aggregate gradients can be decomposed into two separate phases: reduce-scatter and all-gather. During the reduce-scatter phase, the gradients are summed in equal blocks among ranks on each GPU based on their rank index. During the all-gather phase, the sharded portion of aggregated gradients available on each GPU are made available to all GPUs (see here for details on those operators).\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eWe can then rearrange the reduce-scatter and all-gather so that each DDP worker needs to store only a single shard of parameters and optimizer states. The figure below illustrates standard DDP training (top) and FSDP training (bottom):\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17812\" aria-describedby=\"caption-attachment-17812\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?w=907\" alt=\"Full Sharded Data Parallel graph\" width=\"907\" height=\"1024\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png 1566w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=811,916 811w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=768,867 768w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=907,1024 907w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=1361,1536 1361w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=96,108 96w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=192,217 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17812\"\u003eA comparison of standard data parallel training and fully sharded data parallel training. In standard data parallel training methods, a copy of the model is present on each GPU and a sequence of forward and backward passes are evaluated on only a shard of the data. After these local computations, the parameters and optimizers for each local process are shared with the other GPUs in order to calculate the global weight update. In FSDP, only a shard of the model is present on a GPU. Then, locally, all weights are gathered from the other GPUs — by means of an all-gather step — to calculate the forward pass. This gathering of weights is then performed again before the backward pass. After that backward pass, the local gradients are averaged and sharded across the GPUs by means of a reduce-scatter step, which allows each GPU to update its local weight shard.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eTo maximize memory efficiency, we can discard the full weights after each layer’s forward pass, saving memory for subsequent layers. This can be implemented by applying the FSDP wrapper to every layer in the network (with \u003c/span\u003e\u003cspan\u003ereshard_after_forward=True\u003c/span\u003e\u003cspan\u003e). \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn pseudo-code:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003cspan\u003eFSDP forward pass:\u003c/span\u003e\n\u003cspan\u003e    for layer_i in layers:\u003c/span\u003e\n\u003cspan\u003e        all-gather full weights for layer_i\u003c/span\u003e\n\u003cspan\u003e        forward pass for layer_i\u003c/span\u003e\n\u003cspan\u003e        discard full weights for layer_i\u003c/span\u003e\n\n\u003cspan\u003eFSDP backward pass:\u003c/span\u003e\n\u003cspan\u003e    for layer_i in layers:\u003c/span\u003e\n\u003cspan\u003e        all-gather full weights for layer_i\u003c/span\u003e\n\u003cspan\u003e        backward pass for layer_i\u003c/span\u003e\n\u003cspan\u003e        discard full weights for layer_i\u003c/span\u003e\n\u003cspan\u003e        reduce-scatter gradients for layer_i\u003c/span\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003cspan\u003eHow to use FSDP\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThere are several ways to use FSDP in large-scale AI research.\u003c/span\u003e\u003cspan\u003e At this time, we offer four solutions to adapt to different needs.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003e1. Using FSDP in language models\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eFor language models, FSDP is supported in the \u003c/span\u003e\u003ca href=\"https://github.com/pytorch/fairseq\"\u003e\u003ci\u003e\u003cspan\u003efairseq\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e framework\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e via the following new arguments:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e–ddp-backend=fully_sharded\u003c/span\u003e\u003cspan\u003e: enables full sharding via FSDP\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e–cpu-offload\u003c/span\u003e\u003cspan\u003e: offloads the optimizer state and FP32 model copy to CPU (combine with\u003c/span\u003e\u003cspan\u003e–optimizer=cpu_adam\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e–no-reshard-after-forward\u003c/span\u003e\u003cspan\u003e: increases training speed for large models (1B+ params) and is similar to ZeRO stage 2\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003eOther popular options (\u003cspan\u003e–fp16\u003c/span\u003e\u003cspan\u003e, \u003c/span\u003e\u003cspan\u003e–update-freq\u003c/span\u003e\u003cspan\u003e, \u003c/span\u003e\u003cspan\u003e–checkpoint-activations\u003c/span\u003e\u003cspan\u003e, \u003c/span\u003e\u003cspan\u003e–offload-activations\u003c/span\u003e\u003cspan\u003e, etc.) continue to work as normal\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eSee the \u003c/span\u003e\u003ca href=\"https://github.com/pytorch/fairseq/tree/master/examples/fully_sharded_data_parallel\"\u003e\u003cspan\u003efairseq tutorial\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e for instructions on using FSDP to train a 13B-parameter model on eight GPUs or on a single GPU with FSDP + CPU offloading.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003e2. Using FSDP in computer vision models\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eFor computer vision models, FSDP is supported in \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/vissl\"\u003e\u003cspan\u003eVISSL\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and tested on RegNets architectures. Layers like BatchNorm and ReLU are seamlessly handled and tested for convergence.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eUse the following options to enable FSDP:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003econfig.MODEL.FSDP_CONFIG.AUTO_SETUP_FSDP=True\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003econfig.MODEL.SYNC_BN_CONFIG.SYNC_BN_TYPE=pytorch\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003econfig.MODEL.AMP_PARAMS.AMP_TYPE=pytorch\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eSee \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/vissl/blob/40441123a6f7098500676ca8800025c1f02e28b3/vissl/config/defaults.yaml#L498-L513\"\u003e\u003cspan\u003ethis section\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e of the yaml config for additional options to config FSDP within VISSL.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003e3. Using FSDP from PyTorch Lightning\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eFor easier integration with more general use cases, FSDP is supported as a beta feature by PyTorch Lightning. \u003c/span\u003e\u003ca href=\"https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced_gpu.html#fully-sharded-training\"\u003e\u003cspan\u003eThis tutorial\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e contains a detailed example on how to use the FSDP plugin with PyTorch Lightning. At a high level, adding \u003c/span\u003e\u003cspan\u003eplugins=’fsdp’\u003c/span\u003e\u003cspan\u003e below can activate it.\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003cspan\u003emodel = MyModel()\u003c/span\u003e\n\u003cspan\u003etrainer = Trainer(gpus=4, \u003c/span\u003e\u003cb\u003eplugins=\u0026#39;fsdp\u0026#39;\u003c/b\u003e\u003cspan\u003e, precision=16)\u003c/span\u003e\n\u003cspan\u003etrainer.fit(model)\n\u003c/span\u003e\u003cspan\u003e\ntrainer.test()\u003c/span\u003e\n\u003cspan\u003etrainer.predict()\u003c/span\u003e\u003c/pre\u003e\n\u003ch3\u003e\u003cspan\u003e4. Using the FSDP library directly from FairScale\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThe main library where FSDP has been developed, and where you can find the latest updates, is \u003c/span\u003e\u003ca href=\"https://fairscale.readthedocs.io/en/latest/deep_dive/oss_sdp_fsdp.html\"\u003e\u003cspan\u003eFairScale\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. You can directly use FSDP from FairScale with the below example by simply replacing the \u003c/span\u003e\u003cspan\u003eDDP(my_module)\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003cspan\u003efrom fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\u003c/span\u003e\n\u003cspan\u003e...\u003c/span\u003e\n\u003cspan\u003esharded_module = \u003c/span\u003e\u003cspan\u003e\u003cdel\u003eDDP(my_module)\u003c/del\u003e\u003c/span\u003e\u003cb\u003eFSDP(my_module)\u003c/b\u003e\n\u003cspan\u003eoptim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)\u003c/span\u003e\n\u003cspan\u003efor sample, label in dataload.next_batch:\u003c/span\u003e\n\u003cspan\u003e  out = sharded_module(x=sample, y=3, z=torch.Tensor([1]))\u003c/span\u003e\n\u003cspan\u003e  loss = criterion(out, label)\u003c/span\u003e\n\u003cspan\u003e  loss.backward()\u003c/span\u003e\n\u003cspan\u003e  optim.step()\u003c/span\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eThe FSDP library in FairScale exposes the low-level options for many important aspects of large-scale training. Here are some few important areas to consider when you apply FSDP with its full power.\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eModel wrapping: \u003c/b\u003e\u003cspan\u003eIn order to minimize the transient GPU memory needs, users need to wrap a model in a nested fashion. This introduces additional complexity. The \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/wrap/auto_wrap.py\"\u003e\u003cspan\u003eauto_wrap\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e utility is useful in annotating existing PyTorch model code for nested wrapping purposes.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eModel initialization:\u003c/b\u003e\u003cspan\u003e Unlike DDP, FSDP does \u003c/span\u003e\u003cb\u003enot\u003c/b\u003e\u003cspan\u003e automatically synchronize model weights between GPU workers. This means model initialization must be done carefully so that all GPU workers have the identical initial weights.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eOptimizer settings:\u003c/b\u003e\u003cspan\u003e Due to sharding and wrapping, only certain types of optimizer and optimizer settings are supported by FSDP. In particular, if a module is wrapped by FSDP and its parameters are flattened into a single tensor, users cannot use different hyperparameters for different parameter groups in such a module.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eMixed precision:\u003c/b\u003e\u003cspan\u003e FSDP supports advanced mixed precision training with FP16 master weights, as well as FP16 reduce and scatter on the gradients. Certain parts of a model may converge only if full precision is used. In those cases, additional wrapping is needed to selectively run parts of a model in full precision.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eState checkpointing and inference:\u003c/b\u003e\u003cspan\u003e When the model scale is large, saving and loading the model state can become challenging. FSDP supports several ways to make that task possible, but it is by no means trivial.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFinally, FSDP is often used together with \u003c/span\u003e\u003cb\u003eactivation checkpointing\u003c/b\u003e\u003cspan\u003e functions like \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/checkpoint/checkpoint_activations.py\"\u003e\u003cspan\u003echeckpoint_wrapper\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e from FairScale. Users may need to carefully tune the activation checkpointing strategy to fit a large model within limited GPU memory space.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eNext steps\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eFSDP is open source, and early users have tried it and contributed to it. We think it can benefit the entire research community, and we look forward to working with everyone in making it better. In particular, these are some of the important areas.\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eMaking FSDP more general.\u003c/b\u003e\u003cspan\u003e So far, FSDP has been used on both NLP and vision models with SGD and Adam optimizers. As newer models and optimizers emerge, FSDP needs to continue supporting them. Being a purely data-parallel training scheme, FSDP has the greatest potential to be general in supporting a wide range of AI algorithms.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eMaking FSDP auto-tune. \u003c/b\u003e\u003cspan\u003eThere are many knobs that users can tune today with FSDP for both scaling and performance. We look forward to developing algorithms for auto-tuning both GPU memory usage and training performance.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eIn addition to training, more \u003c/span\u003e\u003cb\u003escalable inference\u003c/b\u003e\u003cspan\u003e and model serving is an important use case that FSDP might need to support.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eLast but not least, refactoring and continuing to \u003c/span\u003e\u003cb\u003emodularize FSDP\u003c/b\u003e\u003cspan\u003e and its core components is equally important to newer and better features.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eTry it out and contribute!\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eFSDP is currently available directly from the \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale\"\u003e\u003cspan\u003eFairScale library\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThanks for sticking with us thus far. Please try FSDP in your research or production work. We would love to hear your feedback, and, as always, pull requests are welcome! \u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eTraining AI models at a large scale isn’t easy. Aside from the need for large amounts of computing power and resources, there is also considerable engineering complexity behind training very large models. At Facebook AI Research (FAIR) Engineering, we have been working on building tools and infrastructure to make training large AI models easier. Our [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\"\u003eFully Sharded Data Parallel: faster AI training with fewer GPUs\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Hero-FINAL-1.png",
      "date_published": "2021-07-15T16:00:47Z",
      "author": {
        "name": "By Myle Ott, Sam Shleifer, Min Xu, Priya Goyal, Quentin Duval, Vittorio Caggiano"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/",
      "title": "How WhatsApp enables multi-device capability",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eFor years, people have been asking us to create a true multi-device experience that allows people to use WhatsApp on other devices without requiring a smartphone connection.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eToday, we’re announcing the rollout of a limited public beta test for WhatsApp’s updated multi-device capability. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith this new capability, you can now use WhatsApp on your phone and up to four other nonphone devices simultaneously — even if your phone battery is dead. Each companion device will connect to your WhatsApp independently while maintaining the same level of privacy and security through end-to-end encryption that people who use WhatsApp have come to expect. Importantly, we have developed new technologies to maintain end-to-end encryption while still managing to sync your data — such as contact names, chat archives, starred messages, and more — across devices.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo achieve this, we had to rethink WhatsApp’s architecture and design new systems to enable a standalone multi-device experience while preserving \u003ca href=\"https://engineering.fb.com/2021/04/16/security/dit/\"\u003eprivacy and end-to-end encryption\u003c/a\u003e. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eTaking smartphones out of the equation\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003eThe current WhatsApp experience for companion devices on web, macOS, Windows, and Portal uses a smartphone app as the primary device, making the phone the source of truth for all user data and the only device capable of end-to-end encrypting messages for another user, initiating calls, etc. Companion devices maintain a persistent secure connection with the phone and simply mirror its contents on their own UI.\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis architecture makes it easy to deliver a seamlessly synchronized experience between a phone and companion device without compromising on security. However, it comes with some significant reliability trade-offs: By requiring the phone to perform all operations, companion devices are slower and frequently get disconnected — especially when the phone has a poor connection, its battery is running low, or the application process gets killed by the phone’s OS. It also allows for only a single companion device to be operative at a time, meaning people can’t be on a call in Portal while checking their messages on their PC, for example. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe new WhatsApp multi-device architecture removes these hurdles, no longer requiring a smartphone to be the source of truth while still keeping user data seamlessly and securely synchronized and private.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe challenge in accomplishing this was in maintaining the secure user experience across devices without having to store people’s private messages on our servers in new ways.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eMeeting the security challenges of multiple devices\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003ePrior to the introduction of multi-device, everyone on WhatsApp was identified by a single identity key from which all encrypted communication keys were derived. With multi-device, each device now has its own identity key.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe WhatsApp server maintains a mapping between each person’s account and all their device identities. When someone wants to send a message, they get their device list keys from the server.  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe have also addressed the challenge of preventing a malicious or compromised server from eavesdropping on someone’s communications by surreptitiously adding devices to someone’s account. We use a combination of technologies to solve this: First, we have extended security codes to now represent the combination of all of someone’s device identities so that anyone and their contact can always verify all the devices they are sending messages to. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSecond, in order to reduce the number of times that someone needs to perform identity verifications, we have developed and will roll out a technology called Automatic Device Verification. This system allows for devices to automatically establish trust between each other in a way that someone needs to compare another user’s security code only if that user reregisters their entire account, rather than each time they link a new device to their account. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFinally, we also give people additional control and protections over which devices are linked to their account. First, everyone will continue to be required to link new companion devices by scanning a QR code from their phone. This process now requires biometric authentication before linking where people have enabled this feature on compatible devices. Finally, people will be able to see all the companion devices linked to their account as well as when they were last used, and will be able to log out of them remotely if needed. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eMaintaining message privacy\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhen people message each other in a one-on-one chat, a pairwise encrypted session is established between each of the sender’s and recipient’s devices. WhatsApp multi-device uses a client-fanout approach\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ewhere the WhatsApp client sending the message encrypts and transmits it N number of times to N number of different devices \u003c/span\u003e\u003cspan\u003e— those in the sender and receiver’s device lists\u003c/span\u003e\u003cspan\u003e. Each message is individually encrypted using the established pairwise encryption session with each device. M\u003c/span\u003e\u003cspan\u003eessages are not stored on the server after they are delivered. For groups, we still use the same scalable Sender Key encryption scheme from the Signal Protocol.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?w=1024\" alt=\"WhatsApp Multi-device graphic\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17849\" aria-describedby=\"caption-attachment-17849\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?w=1024\" alt=\"WhatsApp Multi-device graphic\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17849\"\u003eWhatsApp’s legacy architecture used a smartphone as the source of truth. But with the new multi-device capability, up to four other nonphone companion devices can connect to WhatsApp independently while still maintaining the same level of privacy and security.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch2\u003e\u003cspan\u003eAdapting voice and video protocols for multi-device, end-to-end encryption  \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhen someone on WhatsApp makes a voice or video call:\u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe initiator generates a set of random 32-byte \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secrets for each of the recipient’s devices.\u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe initiator sends an incoming call message (using the client-fanout approach described above) to each of the devices of the recipient. Each recipient’s device receives this message, which contains the encrypted \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secret.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003eIf the responder answers the call from one of the devices, a \u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e encrypted call is started, protected by the \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secret generated for that device.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eThe \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secret persists in memory on the client device and is used only during the call. Our servers do not have access to the \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secrets.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor group calls, the server randomly selects a participant device that is in the call (either the initiator or a device on which a user has accepted the call) to generate the \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secret. That device generates the secret and sends it to other active participant devices through pairwise end-to-end encryption. This process is repeated, and the keys are reset whenever someone joins or leaves the call.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eKeeping message history and other application states in sync across devices\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe want to ensure that people have a consistent experience with WhatsApp no matter the device they are using. To achieve this, we synchronize message history as well as other application state data (such as contact names, whether a chat is archived, or if a message is starred) across devices. All of this data is synchronized and end-to-end encrypted between your devices.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor message history: When a companion device is linked, the primary device encrypts a bundle of the messages from recent chats and transfers them to the newly linked device. The key to this encrypted message history blob is delivered to the newly linked device via an end-to-end encrypted message. After the companion device downloads, decrypts, unpacks, and stores the messages securely, the keys are deleted. From that point forward, the companion device accesses the message history from its own local database.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOther application data requires more than an initial transfer from the phone. We also need an ongoing synchronization every time someone modifies their application state (e.g., when they add a new contact, mute a chat, or star a message).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo solve this, the WhatsApp server securely stores a copy of each application state that all of someone’s devices can access. To properly secure this, all the information, and even the metadata about the information (what kind of user data is stored or accessed), is end-to-end encrypted with constantly changing keys known only to that person’s devices. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow to try WhatsApp multi-device beta \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe plan to initially test the experience with a small group of users from our existing beta program. We will continue optimizing performance and adding a few additional features before slowly rolling it out more broadly. Those who opt in can always opt back out.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor more information about the beta and to sign up, visit the \u003ca href=\"https://faq.whatsapp.com/general/download-and-installation/about-multi-device-beta\"\u003eWhatsApp Help Center\u003c/a\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor more information about WhatsApp multi-device, read our updated \u003ca href=\"https://www.whatsapp.com/security/WhatsApp_Security_Whitepaper_v4_Preview.pdf\"\u003ewhitepaper\u003c/a\u003e.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eFor years, people have been asking us to create a true multi-device experience that allows people to use WhatsApp on other devices without requiring a smartphone connection. Today, we’re announcing the rollout of a limited public beta test for WhatsApp’s updated multi-device capability.  With this new capability, you can now use WhatsApp on your phone [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/\"\u003eHow WhatsApp enables multi-device capability\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Hero-Image_FINAL.jpg",
      "date_published": "2021-07-14T18:59:40Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/12/security/enforcing-encryption/",
      "title": "Enforcing encryption at scale",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eOur infrastructure supports thousands of services that handle billions of requests per second. We’ve previously discussed how we built our \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/security/service-encryption/\"\u003e\u003cspan\u003eservice encryption infrastructure\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to keep these globally distributed services operating securely and performantly. This post discusses the system we designed to enforce encryption policies within our network and shares some of the lessons we learned in the process. The goal of this enforcement is to catch any regression quickly and shut it off, keeping our internal traffic secure at the application level via TLS.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eOrganizational challenges\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eImplementing a transit encryption enforcement policy at Facebook scale requires careful planning and communication, in addition to the technical challenges we’ll discuss in a bit. We want the site to stay up and remain reliable so the people using our services will be unaffected by and unaware of any changes to the infrastructure.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eCommunicating the intent, specific timelines, and rollout strategy went a long way toward minimizing any potential disruptions for the thousands of teams that run services at Facebook. We use \u003c/span\u003e\u003ca href=\"https://www.facebook.com/workplace\"\u003e\u003cspan\u003eWorkplace\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e within Facebook, which enables us to easily distribute that information across a variety of groups with a single share button and consolidate feedback and concerns in a single place for all employees to see. We made sure to include the following:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA description of the impact of our enforcement mechanism and how it might appear at the application layer\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA dashboard for engineers to see whether their traffic would be affected\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe rollout and monitoring plan\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eDedicated points of contact and a Workplace group where users could ask questions about impact and troubleshoot any issues\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eThe post required multiple discussions within the team to come up with a rollout plan, dashboard requirements, and realistic timelines to meet the goals of the project. This level of communication proved to be useful as the team gathered important feedback early in the process. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eBuilding our SSLWall\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eHardware choke points are a natural approach to providing transparent enforcement. There are options, such as layer 7 firewalls, that let us do deep packet inspection, but executing fine-grained rollouts and the complexities of Facebook’s network would make implementing such a solution a nightmare. Additionally, working at a network firewall level would introduce a much larger blast radius of impacted traffic, and a single configuration issue could end up killing off traffic that we weren’t meant to touch.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur team decided to develop and deploy what is internally known as SSLWall, a system that cuts off non-SSL connections across various boundaries. Let’s dive a bit into the design decisions behind this solution.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eRequirements \u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe needed to be thorough when considering the requirements of a system that would potentially block traffic at such a large scale. The team came up with the following requirements for SSLWall, all of which had an impact on our design decisions:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eVisibility into what traffic is being blocked. Service owners needed a way to assess impacts, and our team needed to be proactive and reach out whenever we felt there was a problem brewing.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA passive monitoring mode in which we could turn a knob to flip to active enforcement. This helps us determine impacts early on and prepare teams.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA mechanism to allow certain use cases to bypass enforcement, such as BGP, SSH, and approved network diagnostic tools.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSupport for cases like HTTP CONNECT and STARTTLS. These are instances that do a little bit of work over plaintext before doing a TLS handshake. We have many use cases for these in our infrastructure, such as HTTP tunneling, MySQL security, and SMTP, so these must not break, especially since they eventually encrypt the data with TLS.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eExtensible configurability. We might have different requirements depending on the environment in which SSLWall operates. Additionally, having important knobs that can be tuned with little disruption means we can roll features forward or back at our own pace.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eTransparent to the application. Applications should not need to rebuild their code or incur any additional library dependencies for SSLWall to operate. The team needed the ability to iterate quickly and change configuration options independently. In addition, being transparent to the application means SSLWall needs to be performant and use minimal resources without having an impact on latencies.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eThese requirements all led us down the path of managing a host-level daemon that had a user space and kernel-level component. We needed a low-compute way to inspect all connections transparently and act on them.  \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eeBPF\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eSince we wanted to inspect every connection without needing any changes at the application level, we needed to do some work in the kernel context. We \u003c/span\u003e\u003ca href=\"https://ebpf.io/\"\u003e\u003cspan\u003euse eBPF\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e extensively, and it provides all of the capabilities needed for SSLWall to achieve its goals. We leveraged a number of technologies that eBPF provides:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003ca href=\"http://man7.org/linux/man-pages/man8/tc-bpf.8.html\"\u003e\u003cspan\u003etc-bpf\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e: We leveraged Linux’s \u003c/span\u003e\u003ca href=\"http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html\"\u003e\u003cspan\u003etraffic control\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e (TC) facility and implemented a filter using eBPF.  At this layer, we are able to do some computation on a per-packet basis for packets flowing in and out of the box. TC allows us to operate on a broader range of kernels within Facebook’s fleet. It wasn’t the perfect solution, but it worked for our needs at the time.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003ekprobes: eBPF allows us to attach programs to kprobes, so we can run some code within the kernel context whenever certain functions are called. We were interested in the \u003c/span\u003e\u003cspan\u003etcp_connect\u003c/span\u003e\u003cspan\u003e and \u003c/span\u003e\u003cspan\u003etcp_v6_destroy_sock\u003c/span\u003e\u003cspan\u003e functions. These functions are called when a tcp connection is established and torn down, respectively. Old kernels played a factor in our use of kprobes as well.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003emaps: eBPF provides access to a number of map types, including arrays, bounded LRU maps, and perf events\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg\" alt=\"\" width=\"1920\" height=\"950\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=916,453 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=768,380 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=1024,507 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=1536,760 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=96,48 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=192,95 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17786\" aria-describedby=\"caption-attachment-17786\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg\" alt=\"Diagrams showing how kprobes, the tc filter, and our maps interact with one another when determining whether a connection needs to be blocked.\" width=\"1920\" height=\"882\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=916,421 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=768,353 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=1024,470 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=1536,706 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=96,44 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=192,88 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17786\"\u003eDiagrams showing how kprobes, the tc filter, and our maps interact with one another when determining whether a connection needs to be blocked.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch3\u003e\u003cspan\u003eThe management daemon\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe built a daemon, which manages the eBPF programs we install and emits logs to \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/data-infrastructure/scribe/\"\u003e\u003cspan\u003eScribe\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e from our perf events. The daemon also provides the ability to update our TC filter, handles configuration changes (leveraging \u003c/span\u003e\u003ca href=\"https://research.fb.com/wp-content/uploads/2016/11/holistic-configuration-management-at-facebook.pdf\"\u003e\u003cspan\u003eFacebook’s Configerator\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e), and monitors health.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur eBPF programs are also bundled with this daemon. This makes management of releases easier to deal with, as we only have one software unit to monitor instead of needing to track a daemon and eBPF release. Additionally, we can modify the schema of our BPF tables, which both user space and kernel space consult, without compatibility concerns between releases.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eTechnical challenges\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eAs one would expect, we encountered a number of interesting technical challenges while rolling out SSLWall at Facebook’s scale. A few highlights include:\u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003ca href=\"https://en.wikipedia.org/wiki/TCP_Fast_Open\"\u003e\u003cspan\u003eTCP Fast Open (TFO)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e: We hit an interesting challenge around kprobe and TC filter execution order that was exposed by our use of TFO within the infra. In particular, we needed to move some of our flow tracking code to a kprobe prehandler.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eBPF Program Size Limit: All BPF programs are subject to size and complexity limits, which may vary based on the kernel version.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003ePerformance: We spent many engineering cycles optimizing our BPF programs, particularly the TC filter, so that SSLWall’s CPU impact on some of our critical high QPS services with high fanout remained trivial. Identifying early exit conditions and using BPF arrays over LRUs where possible proved effective.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eTransparentTLS and the long tail\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWith enforcement in place, we needed a way to address noncompliant services without significant engineering time. This included things like torrent clients, open source message queues, and some Java applications. While most applications use common internal libraries where we could bake this logic in, the ones that do not need a different solution.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEssentially, the team was left with the following requirements for what we refer to as Transparent TLS (or TTLS for short):\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eTransparently encrypt connections without the need for application changes.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAvoid double encryption for existing TLS connections.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003ePerformance can be suboptimal for this long tail.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eIt’s clear that a proxy solution would have helped here, but we needed to ensure that the application code didn’t need to change and that configuration would be minimal.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe settled on the following architecture: \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg\" alt=\"\" width=\"1920\" height=\"739\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=916,353 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=768,296 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=1024,394 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=1536,591 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=96,37 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=192,74 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe challenge with this approach is transparently redirecting application connections to the local proxy. Once again, we use BPF to solve this problem. Thanks to the cgroup/connect6 hook, we can intercept all \u003c/span\u003e\u003ca href=\"https://man7.org/linux/man-pages/man2/connect.2.html\"\u003e\u003cspan\u003econnect(2)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e calls made by the application and redirect them to the proxy as needed.\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17788\" aria-describedby=\"caption-attachment-17788\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg\" alt=\"Diagram showing application and proxy logic for transparent connect.\" width=\"1920\" height=\"1181\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=916,563 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=768,472 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=1024,630 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=1536,945 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=96,59 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=192,118 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17788\"\u003eDiagram showing application and proxy logic for transparent connect.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eAside from the application remaining unchanged, the BPF program makes policy decisions about routing through the proxy. For instance, we optimized this flow to bypass the proxy for all TLS connections created by the application to avoid double encryption.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis work on enforcement has brought us to a state where we can confidently say that our traffic is encrypted at our scale. However, our work is not yet complete. For instance, there are many new facilities that have come about in BPF that we intend to leverage as we remove old kernel support. We can also improve our transparent proxy solutions and leverage custom protocols to multiplex connections and improve performance.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eWe’d like to thank Takshak Chahande, Lingnan Gao, Andrey Ignatov, Petr Lapukhov, Puneet Mehra, Kyle Nekritz, Deepak Ravikumar, Paul Saab, and Michael Shao for their work on this project.\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eOur infrastructure supports thousands of services that handle billions of requests per second. We’ve previously discussed how we built our service encryption infrastructure to keep these globally distributed services operating securely and performantly. This post discusses the system we designed to enforce encryption policies within our network and shares some of the lessons we learned [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/12/security/enforcing-encryption/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/12/security/enforcing-encryption/\"\u003eEnforcing encryption at scale\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/encryption_hero.jpg",
      "date_published": "2021-07-12T16:00:56Z",
      "author": {
        "name": "By Neel Goyal, Ajanthan Asogamoorthy, Mingtao Yang"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/",
      "title": "Ribbon filter: Practically smaller than Bloom and Xor",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003ch2\u003e\u003cspan\u003eWhat the research is:\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe Ribbon filter is a new data structure that is more space-efficient than the popular Bloom filters that are widely used for optimizing data retrieval. One of the ways that Bloom, and now Ribbon, filters solve real engineering problems is by providing smooth configurability unmatched by other filters. Bloom filters work by overapproximating a set of keys associated with some data resource. With a Bloom filter, almost all negative queries to that resource can be skipped (filtered) because the Bloom filter rejects almost all query keys not associated with the resource.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith proper data layout and design, the Ribbon filter is the first Bloom alternative to match the near-continuous, hazard-free, accuracy-versus-space trade-off provided by Bloom filters.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHere, near-continuous means efficiently utilizing any amount of memory to represent any number of keys, so that wasted memory such as in internal fragmentation can be minimized to zero. The typical hazard to the accuracy-versus-space trade-off is bit alignment, where some data sizes (e.g., 4, 8, or 16 bits per key) are faster to access than others. Like Bloom, our data layout for Ribbon does not suffer this hazard in access times so it is more freely configurable. And Ribbon filters add new freedom of configurability to the space-versus-time trade-off.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBuilding on some prior lines of research, the Ribbon filter combines a simplified, faster, and more flexible construction algorithm; a data layout optimized for filter queries; and near-continuous configurability to make a practical alternative to static (immutable) Bloom filters.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile well-engineered Bloom filters are extremely fast, they use roughly 50 percent more space (overhead) than the information-theoretic lower bound for filters on arbitrary keys. When Bloom filters cannot meet an application’s space efficiency targets, Ribbon filter variants dominate in space-versus-time trade-offs with near continuous configurability and space overhead as low as 1 percent or less. Ribbon filters have O(1) query times and save roughly 1/3 of memory compared with Bloom filters.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow it works: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eLike some related immutable structures used for perfect hashing and maps, Ribbon filters are constructed by solving a linear system given by hash functions applied to a set of keys. Each row in the linear system expresses that querying as some key, which involves XOR-ing the values at some set of array indices, must yield a prescribed value to indicate it is “in” the set of keys. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDespite using Boolean — GF(2) — arithmetic, the approach to solving this logical system of equations is to use Gaussian elimination, which fundamentally means subtracting equations from one another until you can isolate variables (unknowns). If a solution exists, this approach will find it.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe name Ribbon has dual meanings. First, we use a linear system from Dietzfelbinger and Walzer whose sorted coefficient matrix resembles a physical ribbon, or a wavy approximation of a band matrix. Gaussian elimination is fundamentally more efficient on this system because it is already close to a reduced form.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eRibbon also stands for Rapid Incremental Boolean Banding ON the fly, which is the name of our fast and flexible new Gaussian solver. Through an approach resembling insertion into a linear-probed hash table, Ribbon does Gaussian elimination on the fly. This saves time and space in construction because row reductions can be done in registers rather than in memory, and because the reduced form of the ribbon coefficient matrix — a band matrix — is more space-efficient than explicitly representing the ribbon form. On-the-fly construction also unlocks a solution to the core challenge of the Ribbon approach: scaling its space efficiency to very large numbers of keys.  \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhy it matters: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eAt Facebook’s scale, we expect Ribbon filters to save several percent of RAM resources, with a tiny increase in CPU usage for some major storage systems. However, we do not implement efficiency gains at all engineering costs, so it’s also important to have a user-friendly data structure. This issue stalled implementation of other Bloom alternatives offering some space savings. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe Ribbon filter opens these new trade-offs without introducing notable discontinuities or hazards in the configuration space. In other words, there is some complexity to make Ribbon filters general and highly configurable, but these details can be hidden behind a relatively simple API. You have essentially free choice over any three of the four core performance dimensions — number of keys added to the set, memory usage, CPU efficiency, and accuracy — and the accuracy is automatically well optimized.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eRead the full paper: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2103.02515\"\u003e\u003cspan\u003eRibbon filter: Practically smaller than Bloom and Xor\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWhat the research is: The Ribbon filter is a new data structure that is more space-efficient than the popular Bloom filters that are widely used for optimizing data retrieval. One of the ways that Bloom, and now Ribbon, filters solve real engineering problems is by providing smooth configurability unmatched by other filters. Bloom filters work [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/\"\u003eRibbon filter: Practically smaller than Bloom and Xor\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg",
      "date_published": "2021-07-09T16:00:16Z",
      "author": {
        "name": "By Peter Dillinger"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/",
      "title": "Asicmon: A platform agnostic observability system for AI accelerators",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cb\u003e\u003ci\u003eWe will be hosting a talk about our work on, “\u003ca href=\"https://atscaleconference.com/events/systems-scale-summer-2021/\"\u003eA Platform Agnostic Observability System for AI Accelerators\u003c/a\u003e” during our virtual \u003ca href=\"https://atscaleconference.com/events/systems-scale-summer-2021/\"\u003eSystems @Scale\u003c/a\u003e event at 10:20 a.m. PT on Wednesday, June 30, followed by a live Q\u0026amp;A session. Please submit any questions to \u003ca href=\"mailto:systemsatscale@fb.com\"\u003esystemsatscale@fb.com\u003c/a\u003e before the event.\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAccelerators are special-purpose hardware devices optimized for specific applications, like AI prediction and video encoding. And Application-specific hardware platforms play an important role in meeting the growing latency and compute demands of workloads like deep learning, content understanding, and video encoding.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAt Facebook, the inevitable rise in use of accelerators in our data centers has led to better performance and energy efficiency. However, it is challenging to operate these heterogeneous platforms efficiently at scale. To ensure that these complex accelerators operate smoothly, we need an excellent observability system with monitoring and tracing capabilities so we can understand the performance and interactions between CPUs and accelerators.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo meet these challenges, we’ve introduced three new tools:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eASIC Monitoring (Asicmon)\u003c/b\u003e\u003cspan\u003e, a scalable observability framework. Asicmon’s library abstracts an accelerator’s custom interfaces and provides a standard interface to our internal tools. Asicmon has facilitated load balancing, performance monitoring, and automated health checks for hundreds of thousands of accelerators running in our data centers.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eAsimov\u003c/b\u003e\u003cspan\u003e, a custom specification language that makes developing and rapid prototyping new accelerators easier. It has shrunk our development time for onboarding a new accelerator from a month to under a week.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eAtrace\u003c/b\u003e\u003cspan\u003e, an accelerator tracing solution that collects traces remotely on production servers. It allows us to inspect accelerator systems in detail and provides actionable trace summaries and analyses. An initial version of Atrace allowed us to close a 10 percent performance gap between \u003c/span\u003e\u003ca href=\"https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/\"\u003e\u003cspan\u003eCaffe2 and PyTorch implementations\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e of a large AI model. \u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eBackground\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eFacebook’s cloud infrastructure handles about 150 trillion AI predictions per day for tasks ranging from feed recommendations to combating harmful content. Running these AI models comes with heavy infrastructure demands. And as these models improve, so do their \u003ca href=\"https://arxiv.org/pdf/2003.09518.pdf\"\u003ecomputational requirements\u003c/a\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe graph below of AI model adoption at Facebook illustrates this \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2003.09518.pdf\"\u003e\u003cspan\u003eunmistakable pattern\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?w=852\" alt=\"\" width=\"852\" height=\"916\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png 988w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=852,916 852w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=768,826 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=953,1024 953w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=96,103 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=192,206 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe need for accelerators\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eGood old general-purpose processors (CPUs) offer versatility and have grown exponentially faster over the decades. However, CPUs fail to meet the rising\u003c/span\u003e\u003ca href=\"https://openai.com/blog/ai-and-compute/\"\u003e \u003cspan\u003ecomputational demands of AI applications\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e today. They also tend to exhibit inefficiency in terms of energy used per AI prediction. As investigated by the OpenAI community, we’ve seen \u003c/span\u003e\u003ca href=\"https://openai.com/blog/ai-and-compute/\"\u003e\u003cspan\u003etwo distinct eras of compute in AI models\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. \u003c/span\u003e\u003cspan\u003eIn recent times, model complexity and compute requirements for AI have grown by roughly a factor of 10 each year. \u003c/span\u003e\u003cspan\u003eThis far outpaces improvements in CPU performance.   \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHow do we remedy this? By designing hardware that is customized to accelerate AI operations via application-specific integrated circuits (ASICs).  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSince 2019, Facebook has invested \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2019/03/14/data-center-engineering/accelerating-infrastructure/\"\u003e\u003cspan\u003eheavily in deploying accelerator-based servers\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to provide higher performance and energy efficiency. Today, our first-generation systems are 10-30x more performant on our largest AI models. They also delivered a 3-10x performance-per-watt improvement over a CPU.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe also invested in specialized hardware for \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2021/04/05/video-engineering/how-facebook-encodes-your-videos/\"\u003e\u003cspan\u003evideo encoding\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and decoding. This enables Facebook to process the nearly 250 million videos uploaded to our app each day. These videos are viewable on any device and with varying internet bandwidth. Our first-generation video accelerators delivered a 10x performance-per-watt improvement in processing 4K videos.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe figure below illustrates the design of our AI inference server. As you can see, it consists of two Twin Lake CPUs and multiple accelerators (M.2 modules) connected to them using a PCIE switch\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?w=1024\" alt=\"\" width=\"1024\" height=\"615\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png 2000w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=916,550 916w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=768,461 768w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=1024,615 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=1536,922 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=96,58 96w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=192,115 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe challenges of operating accelerators\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eIn your typical cloud server, the CPU represents the most complex component. We focus a lot on building software to efficiently operate the CPU and monitor its performance and availability. However, with an accelerator system, we can imagine the CPU now has a complicated and brawnier sibling! The accelerator, or ASIC, represents a complex hardware and software system in its own right.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo deliver an excellent user experience, the cloud infrastructure needs to keep hundreds of thousands of accelerators running reliably and efficiently. This is where observability systems come to our rescue. Observability allows us to understand what happens in the accelerator hardware and software when any issue arises. It is useful in multiple ways: \u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eHealth monitoring:\u003c/b\u003e\u003cspan\u003e Just like any other piece of hardware, accelerators can overheat or hit a faulty condition or a functional bug. We can track various health metrics for the ASICs and use them in automated systems. These systems can then (if needed) remediate the issue by rebooting the accelerator or moving it into a repair state.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003ePerformance monitoring:\u003c/b\u003e\u003cspan\u003e By monitoring the performance and system load on an accelerator, we can efficiently scale our AI jobs to meet variable demand throughout the day. It also enables us to detect regressions in performance with new software deployments.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003ePerformance profiling:\u003c/b\u003e\u003cspan\u003e When we encounter issues such as poor performance or time-outs, we need to look deeper into how the accelerator server is functioning. We also need to equip software developers with tools to understand the performance of their applications while they run on accelerators. \u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eThe accelerator zoo\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eSpecialization is both a boon and bane for accelerators. As a result, we end up running multiple types of accelerators in our data centers at any given point.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn 2020 we started\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2019/03/14/data-center-engineering/accelerating-infrastructure/\"\u003e \u003cspan\u003edeploying the first generation\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e of these accelerators. In the near future, we will be developing two to three new accelerators for the second generation. Each accelerator will have unique driver interfaces, making the task of operating them harder. But duplicating the observability software for each accelerator would not be feasible in the timeline we have set out. The observability framework must be easy to prototype and adapt to multiple types of accelerators in a short time. It also needs to be efficient to avoid interfering with the original application. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow we developed Asicmon and Asimov\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eOur first challenge involved finding a way to effectively monitor different types of accelerators without duplicating code (and developer time). As you may have guessed, we can leverage abstraction to achieve this. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor example, consider an abstract metric: \u003c/span\u003e\u003ci\u003e\u003cspan\u003edevice_utilization\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e — the measure of how busy an accelerator is — which becomes useful for balancing load across accelerators. To compute this metric, we may need to understand the internal architecture of the accelerator. With an abstract counter, however, engineers working on load balancing can more easily use the metric without being aware of finer details.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003edevice_utilization = max(compute_core_active_i) /  total_time \u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith the above in mind, we designed Asicmon with these design objectives: \u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eAbstraction:\u003c/b\u003e\u003cspan\u003e We needed a simple and uniform interface for all of our internal monitoring and operational tools to use. This enables infrastructure engineers and hardware teams to effectively operate multiple accelerators in a common way.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eDevelopment velocity:\u003c/b\u003e\u003cspan\u003e Accelerators are new. Interfaces can also change due to evolving requirements. The framework should be easy to learn and able to iterate quickly.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003ePerformance:\u003c/b\u003e\u003cspan\u003e Finally, any observability system should be lightweight in terms of resources. As a result, it diminishes interference with high-throughput video and AI applications.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eThe diagram below illustrates the overall software stack for monitoring accelerators. Asicmon acts as a bridge between individual accelerator drivers and the rest of the internal monitoring software. The left top illustrates automated health check tools that spot bad health signals and\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/12/09/data-center-engineering/how-facebook-keeps-its-large-scale-infrastructure-hardware-up-and-running/\"\u003e \u003cspan\u003eautomatically fix faulty ASICs\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. On the right, a telemetry daemon periodically publishes performance metrics for engineers to inspect the accelerators. Furthermore, automated load balancing and auto-scaling systems like\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\"\u003e \u003cspan\u003eShard Manager\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e utilize these counters. \u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eHow does Asicmon work?  \u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eUnder the hood, Asicmon creates an instance of a monitoring module per accelerator device. It maintains a cache of statistics that it updates periodically by probing the accelerator driver and computing-derived metrics. Queries to Asicmon’s standard interface for counters get implemented as a lookup into this cache. This shields the system against accidental overload of counter requests.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eEnter Asimov\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eAll great so far! We used abstraction to address the scalability aspect of observability software layers above Asicmon. However, the problem of building the glue code between the accelerator driver and these standard metrics still eluded us. This has to be done separately for each of the accelerators that have aggressive and overlapping timelines. So, we needed a method to develop on Asicmon that was quick to iterate and easy to ramp up on, while also being efficient. That’s where Asimov comes in. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png 1999w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAsimov is an expressive Python-like custom language to instrument the accelerator driver. It essentially allows developers to focus on how to probe the accelerator interfaces and express derived metrics using them. The Asimov compiler generates an efficient C++ implementation of the monitoring module. It also handles details like caching the metrics, periodically reading them, and providing thread safety.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe code snippets below show examples of Asimov being used to read system metrics using interfaces ranging from Linux sysfs files (a) to custom library C functions (b).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAsimov incorporates the same standard interface as Asicmon in its internal representation (the stats data structure, left hand side in the code). We can also invoke C-library functions provided by the device driver and express equations/conditions for derived metrics like any regular language.  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAsimov is built with the\u003c/span\u003e\u003ca href=\"https://www.antlr.org/\"\u003e \u003cspan\u003eANTLR\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e compiler framework under the hood to provide the lexer/parser logic for the language. We then emit C++ code using templates that manage all the essential parts, like initialization, thread safety, etc., so someone using Asimov doesn’t need to worry about it.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAsicmon in action\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eLet’s look at a few illustrative examples of how Asimov and Asicmon are beneficial for operating accelerators at scale.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor AI inference applications, we use a system called\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\"\u003e \u003cspan\u003eShard Manager\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to automatically scale the inference service instances. A shard is essentially a copy of the AI model that can serve inferences. Asicmon measures the load on the device using an abstract metric — accelerator\u003c/span\u003e \u003cspan\u003edevice\u003c/span\u003e \u003cspan\u003eutilization. This helps Shard Manager effectively balance the load among servers and automatically scale up or down the number of shards. The diagram below explains how the number of shards gets scaled automatically during model update rollouts and increases in traffic.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe figure below illustrates the advantages of building observability early on in a project’s development cycle. In our test deployment for video accelerators, we detected a memory leak using an Asicmon counter for available device memory. It took multiple fixes to the driver to finally resolve the issue, well in time before its debut in production.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFinally, let’s take a look at the ease of prototyping with Asimov. While we certainly took longer to build the first version of Asimov alongside the first video accelerator, supporting the second one (the AI inference accelerator) went incredibly fast. Bootstrapping basic metrics for the AI inference accelerator took less than a week. Since implementing Asicmon we’ve been able to increase our AI accelerator metrics support from ~30 percent to ~75 percent\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAtrace: Accelerator tracing at scale\u003c/span\u003e\u003c/h2\u003e\n\u003ch3\u003e\u003cspan\u003eWhy tracing?\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eNow that we can monitor the performance of accelerators in our data centers, the next step involves addressing why performance metrics like the latency and throughput change over time. The tried-and-tested method for CPUs involves leveraging a stack-based profiler to sample the running function call stack at periodic intervals. However, for inference accelerators, tracing is the best form of profiling. Why? Because accelerators use special hardware units and thus do not have an equivalent notion of a function stack on a core. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs shown in the figure below, a trace essentially consists of a time series of events occurring on different parts in a system. Events in a trace can represent, among many things, functions, execution of AI operators, or data transfers. Traces offer deeper insights into the operation of the system, including understanding the latency and scheduling of operators and how the CPU and accelerator interact with each other. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eDesigning the tracing system\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWhile AI inference accelerator vendors do provide tools and APIs to collect traces from the device. These tools are designed to work on a single server and are often hard to use. In order to profile production systems better, we set out building a layer on top of this native capability. This better scales out the collection, processing, and analysis of traces themselves. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe kept two target use cases in mind while developing Atrace: \u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eModel development:\u003c/b\u003e\u003cspan\u003e Model developers would typically be attempting to target their AI models to new inference hardware. They can run the tracing tool locally. But by integrating it with internal visualization and summarization tools, we can provide quicker feedback to engineers to iteratively tune their model.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eProduction:\u003c/b\u003e\u003cspan\u003e Debugging performance issues in production is an important use case for tracing. For instance, say a continuous integration (CI) test detects a regression in performance. By collecting traces remotely and on the fly, production engineers can quickly diagnose the problem.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eTo develop a scalable and ubiquitous tracing solution, we built a set of components that remotely trigger and collect traces. We save each trace to a shared storage and post process and summarize it. The diagram below outlines this, starting on the left with the trace being triggered, to the trace collection and post processing on the right.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eInsights from accelerator traces\u003c/span\u003e\u003c/h2\u003e\n\u003ch3\u003e\u003cspan\u003eTrace profiles and summaries\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eTraces themselves can be enormous and overwhelming to dive into directly. However, we can learn a great deal about an AI program by summarizing the trace at a high level. To achieve this, we built a summary of trace statistics grouped by various AI operator types, as shown below.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis operator breakdown shows our engineers which operators consume the most execution time and merit optimization. It also allows for comparisons and debugging of performance regressions between two software versions.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eTrace critical path analysis\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eFor advanced users, who might want to delve deeper into the traces, we added visualization support for both the open source\u003c/span\u003e\u003ca href=\"https://www.chromium.org/developers/how-tos/trace-event-profiling-tool\"\u003e \u003cspan\u003eChrome trace viewer\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and an internal trace visualization tool from Facebook. It works all from a single click. We can also run automated analysis on the trace to infer the critical path of operators. This uses the dependency graph of the AI model and trace statistics.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis analysis lets us optimize the latency of the AI prediction. It can also highlight issues like an imbalance in operators. Doing so closed a 10 percent latency gap between the Caffe2 and PyTorch versions of one of our AI models.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eTrace correlation\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eLastly, it is also noteworthy that several software layers exist to handle the processing of an inference request. These include the application layer, PyTorch framework, and\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2018/09/13/ml-applications/glow-a-community-driven-approach-to-ai-infrastructure/\"\u003e \u003cspan\u003eGlow\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, an open source graph lowering compiler for accelerators.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor more complex models involving video understanding or natural language processing, we learned that the model may be run partially on a CPU and partially on an accelerator. Thus, tracing the operations across multiple layers on the CPU and correlating them with the accelerator becomes a necessity.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe developed a\u003c/span\u003e\u003ca href=\"https://github.com/pytorch/glow/pull/5568\"\u003e \u003cspan\u003eprototype of trace correlation\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e into Glow and PyTorch. This allowed us to connect operations on the CPU in the Glow runtime, to the accelerator. Trace correlation is important for examining the complex software stack used for AI inference.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eNext steps\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eIn addition to continuing to support next-generation AI and video accelerators using Asimov and the Asicmon we are also exploring:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eOpen source specifications:\u003c/b\u003e\u003cspan\u003e There are multitudes of companies building accelerator chips today. But the monitoring interfaces for accelerators lack standardization. We are collaborating with the\u003c/span\u003e\u003ca href=\"https://www.opencompute.org/wiki/Server/ODSA\"\u003e \u003cspan\u003eOpen Domain-Specific Accelerators (ODSA)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e project so the industry as whole can benefit from a common specification.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eTrace visualization and analysis:\u003c/b\u003e\u003cspan\u003e We are investigating ways to automatically generate optimization recommendations from the trace and support better visualizations, such as integrating with TensorBoard.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eDistributed tracing:\u003c/b\u003e\u003cspan\u003e Since microservices do not run in isolation, we plan on exploring how to correlate distributed traces collected by the Canopy distributed tracing tool with system-level accelerator traces. This would allow us to debug the end-to-end latency of microservices that use AI accelerators.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eThanks\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e\u003cspan\u003eWe would like to thank our many collaborators at Facebook, including Jerry Liu, Thiara Ortiz, Jeremy Yang, Ashwin Poojary, Deng Pan, Craig Ross, Ashwin Narasimha, Gisle Dankel, Michael Anderson, Allan Di Wu, Yinghai Lu, Satish Nadathur, Garret Catron, and Jack Montgomery for supporting us in creating this framework.\u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWe will be hosting a talk about our work on, “A Platform Agnostic Observability System for AI Accelerators” during our virtual Systems @Scale event at 10:20 a.m. PT on Wednesday, June 30, followed by a live Q\u0026#38;A session. Please submit any questions to systemsatscale@fb.com before the event. Accelerators are special-purpose hardware devices optimized for specific [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/\"\u003eAsicmon: A platform agnostic observability system for AI accelerators\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png",
      "date_published": "2021-06-28T16:00:24Z",
      "author": {
        "name": "By Brian Coutinho, Hao Wang, David Carrillo-Cisneros, Cynthia Liu, Parth Malani"
      }
    }
  ]
}
