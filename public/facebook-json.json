{
  "version": "https://jsonfeed.org/version/1",
  "title": "Facebook",
  "home_page_url": "https://engineering.fb.com/feed",
  "items": [
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/08/06/core-data/zippydb/",
      "title": "How we built a general purpose key value store for Facebook with ZippyDB",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. Since we first deployed ZippyDB in 2012, this key-value store has expanded rapidly, and today, ZippyDB serves a number of use cases, ranging from metadata for a distributed filesystem, counting events for both internal and external purposes, to product data that’s used for various app features. ZippyDB offers a lot of flexibility to applications in terms of tunable durability, consistency, availability, and latency guarantees, which has made the service a popular choice within Facebook for storing both ephemeral and nonephemeral small key-value \u003c/span\u003e\u003cspan\u003edata. In this post, we are sharing for the first time the history and evolution of ZippyDB and some of the unique design choices and trade-offs made in building this service that addressed the majority of key-value store scenarios at Facebook.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHistory of ZippyDB\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB uses\u003c/span\u003e\u003ca href=\"https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-and-open-sourcing-rocksdb/10151822347683920/\"\u003e \u003cspan\u003eRocksDB\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e as the underlying storage engine. Before ZippyDB, various teams across Facebook used RocksDB directly to manage their data. This resulted, however, in a duplication of efforts in terms of each team solving similar challenges such as consistency, fault tolerance, failure recovery, replication, and capacity management. To address the needs of these various teams, we built ZippyDB to provide a highly durable and consistent key-value data store that allowed products to move a lot faster by offloading all the data and the challenges associated with managing this data at scale to ZippyDB.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOne of the significant design decisions we made early during the development of ZippyDB was to reuse as much of the existing infrastructure as possible. Consequently, most of our initial efforts were focused on building a reusable and flexible data replication library called Data Shuttle. We built a fully managed distributed key-value store by combining Data Shuttle with a preexisting and well-established storage engine (RocksDB) and layering this on top of our existing shard management (\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\"\u003e\u003cspan\u003eShard Manager\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e)\u003c/span\u003e \u003cspan\u003eand \u003c/span\u003e\u003cspan\u003edistributed \u003c/span\u003e\u003cspan\u003econfiguration service (built on \u003c/span\u003e\u003ca href=\"https://zookeeper.apache.org/\"\u003e\u003cspan\u003eZooKeeper\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e), that together solves load balancing, shard placement, failure detection, and service discovery.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eArchitecture\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB is deployed in units known as tiers. A tier consists of compute and storage resources spread across several geographic areas known as regions\u003c/span\u003e \u003cspan\u003eworldwide, which makes it resilient to failures. There are only a handful of ZippyDB tiers that exist today, including the default “wildcard” tier and specialized tiers for distributed filesystem metadata and other product groups within Facebook. Each tier hosts multiple use cases. Normally, use cases are created on the wildcard tier, which is our generic multitenant tier. This is the preferred tier because of its better utilization of hardware and lower operational overhead, but we occasionally bring up dedicated tiers if there is a need, usually due to stricter isolation requirements.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe data belonging to a use case on a tier is split into units known as shards,\u003c/span\u003e \u003cspan\u003ewhich are\u003c/span\u003e \u003cspan\u003ethe basic units of data management on the server side. Each shard is replicated across multiple regions (for fault tolerance) using Data Shuttle, which uses either \u003ca href=\"https://dl.acm.org/doi/10.1145/279227.279229\"\u003ePaxos\u003c/a\u003e or\u003c/span\u003e \u003cspan\u003easync replication to\u003c/span\u003e \u003cspan\u003ereplicate\u003c/span\u003e \u003cspan\u003edata, depending on the configuration. Within a shard, a subset of replicas are configured to be a part of the Paxos quorum group, also known as global scope,\u003c/span\u003e \u003cspan\u003ewhere data is synchronously replicated using Multi-Paxos to provide high durability and availability in case of failures. The remaining replicas, if any, are configured as followers.\u003c/span\u003e \u003cspan\u003eThese are similar to learners in Paxos terminology and receive data asynchronously. Followers allow applications to have many in-region replicas to support low-latency reads with relaxed consistency, while keeping the quorum size small for lower write latency. This flexibility in replica role configuration within a shard allows applications to strike a balance between durability, write performance, and read performance depending on their needs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn addition to the sync or async replication strategy, applications also have the option to provide “hints” to the service about the regions in which the replicas of a shard must be placed. These hints, also known as stickiness\u003c/span\u003e \u003cspan\u003econstraints, allow applications to have some control over the latency of reads and writes by having replicas built in regions from where they expect most of the access to come. ZippyDB also provides a caching layer and integrates with a pub-sub system allowing subscriptions to data mutations on shards, both of which are opt-ins depending on the requirements of the use case.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eData model\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB supports a simple key-value\u003c/span\u003e \u003cspan\u003edata model with APIs to get, put, and delete keys along with their batch variants. It supports iterating over key prefixes and deleting a range of keys. These APIs are very similar to the API exposed by the underlying RocksDB storage engine. In addition, we also support a test-and-set API for basic read-modify-write operations and transactions, conditional writes for more generic read-modify-write operations (more about this later). This minimal API set has proved to be sufficient for most use cases to manage their data on ZippyDB. For ephemeral data, ZippyDB has native TTL support where the client can optionally specify the expiry time for an object at the time of the write. We piggyback on RocksDB’s periodic compaction support to clean up all the expired keys efficiently while filtering out dead keys on the read side in between compaction runs. Many applications actually access data on ZippyDB through an ORM layer on top of ZippyDB, which translates these accesses into ZippyDB API. Among other things, this layer serves to abstract the details of the underlying storage service.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eShard is the unit of data management on the server side. The optimal assignment of shards to servers needs to take into account load, failure domains, user constraints, etc., and this is handled by ShardManager\u003c/span\u003e\u003ci\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eShardManager\u003c/span\u003e \u003cspan\u003eis responsible for monitoring servers for load imbalance, failures, and initiating shard movement between servers. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eShard, often referred to as physical shard or p-shard, is a server-side concept and isn’t exposed to applications directly. Instead, we allow use cases to partition their key space into smaller units of related data known as μshards (micro-shards)\u003c/span\u003e\u003ci\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eA typical\u003c/span\u003e \u003cspan\u003ephysical\u003c/span\u003e \u003cspan\u003eshard\u003c/span\u003e \u003cspan\u003ehas a size of 50–100 GB, hosting several tens of thousands of μshards. This additional layer of abstraction allows ZippyDB to reshard the data transparently without any changes on the client.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB supports two kinds of mappings from \u003c/span\u003e\u003ci\u003e\u003cspan\u003eμ\u003c/span\u003e\u003c/i\u003e\u003cspan\u003eshards to physical shards: compact mapping and Akkio mapping. Compact mapping\u003c/span\u003e \u003cspan\u003eis used\u003c/span\u003e \u003cspan\u003ewhen the assignment is fairly static and mapping is only changed when there is a need to split shards that have become too large or hot. In practice, this is a fairly infrequent operation when compared with Akkio mapping, where mapping of \u003c/span\u003e\u003ci\u003e\u003cspan\u003eμ\u003c/span\u003e\u003c/i\u003e\u003cspan\u003eshards is managed by a service known as\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2018/10/08/core-data/akkio/\"\u003e \u003cspan\u003eAkkio\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. Akkio splits use cases’ key space into μshards and places these μshards in regions where the information is typically accessed. Akkio helps reduce data set duplication and provides a significantly more efficient solution for low latency access than having to place data in every region.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs we mentioned earlier, Data Shuttle uses Multi-Paxos to synchronously replicate data to all replicas in the global scope\u003c/span\u003e\u003ci\u003e\u003cspan\u003e. \u003c/span\u003e\u003c/i\u003e\u003cspan\u003eConceptually, time is subdivided into units known as epochs. Each epoch has a unique leader, whose role is assigned using an external shard management service called ShardManager. Once a leader is assigned, it has a lease for the entire duration of the epoch. Periodic heartbeats used to keep a lease active until ShardManager bumps up the epoch on the shard (e.g., for failover, primary load balancing, etc.). When a failure occurs, ShardManager detects the failure, assigns a new leader with a higher epoch and restores write availability. Within each epoch, the leader generates a total ordering of all writes to the shard, by assigning each write a monotonically increasing sequence number. The writes are then written to a replicated durable log using Multi-Paxos\u003c/span\u003e \u003cspan\u003eto achieve consensus on the ordering. Once the writes have reached consensus, they are drained in-order across all replicas.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe chose to use an external service to detect failures and assign leaders to keep the design of the service simple in the initial implementation. However, in the future we plan to move towards detecting failures entirely within\u003c/span\u003e \u003cspan\u003eData Shuttle (“in-band”) and reelecting the leaders more proactively without having to wait for ShardManager and incurring delays.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eConsistency\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB provides configurable consistency and durability levels to applications, which can be specified as options in read and write APIs. This allows applications to make durability, consistency, and performance trade-offs dynamically on a per-request level.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBy default, a write involves persisting the data on a majority of replicas’ Paxos logs and writing the data to RocksDB on the primary before acknowledging the write to the client. With the default write mode, a read on primary will always see the most recent write. Some applications cannot tolerate cross-region latencies for every write, so ZippyDB supports a fast-acknowledge\u003c/span\u003e \u003cspan\u003emode, where writes are acknowledged as soon as they are enqueued on the primary for replication. The durability and consistency guarantees for this mode are obviously lower, which is the trade-off for higher performance.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOn the read side, the three most popular consistency levels are eventual, read-your-writes, and strong. The eventual consistency level supported by ZippyDB is actually a much stronger consistency level than the more well-known eventual consistency. ZippyDB provides total ordering for all writes within a shard and ensures that reads aren’t served by replicas that are lagging behind primary/quorum beyond a certain configurable threshold (heartbeats are used to detect lag), so eventual reads supported by ZippyDB are closer to bounded staleness consistency in literature.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor read-your-writes, the clients cache the latest sequence number returned by the server for writes and use the version to run at-or-later\u003c/span\u003e \u003cspan\u003equeries\u003c/span\u003e \u003cspan\u003ewhile reading. The cache of versions is within the same client process.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB also provides strong consistency or \u003c/span\u003e\u003ca href=\"https://dl.acm.org/doi/10.1145/78969.78972\"\u003e\u003cspan\u003elinearizability\u003c/span\u003e\u003c/a\u003e\u003ci\u003e\u003cspan\u003e, \u003c/span\u003e\u003c/i\u003e\u003cspan\u003ewhere\u003c/span\u003e \u003cspan\u003eclients can see the effects of the most recent writes regardless of where the writes or reads come from. Strong reads today are implemented by routing the reads to the primary in order to avoid the need to speak to a quorum, mostly for performance reasons. The primary relies on owning the lease to ensure that there is no other primary before serving reads. In certain outlier cases, where the primary hasn’t heard about the lease renewal, strong reads on primary turn into a quorum check and read.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eTransactions and conditional writes\u003c/span\u003e\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB4.Revised.jpg?w=16\" alt=\"\" width=\"16\" height=\"9\"/\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?w=916\" alt=\"\" width=\"916\" height=\"515\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg 2304w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/Screen-Shot-2021-05-25-at-10.20.41-AM.jpg?w=16\" alt=\"\" width=\"16\" height=\"9\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eZippyDB supports transactions and conditional writes for use cases that need atomic read-modify-write operations on a set of keys.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAll transactions are serializable by default on a shard, and we don’t support lower isolation levels. This simplifies the server-side implementation and the reasoning about correctness of concurrently executing transactions on the client side. Transactions use optimistic concurrency control to detect and resolve \u003c/span\u003e\u003ca href=\"https://dl.acm.org/doi/10.1145/568271.223787\"\u003e\u003cspan\u003econflicts\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, which works as shown in the figure above. The clients typically read from a secondary all of the data from a snapshot\u003c/span\u003e \u003cspan\u003eof the DB, compose the write\u003c/span\u003e \u003cspan\u003eset, and send both the read and write sets to the primary to commit. Upon receiving the read and write sets and the snapshot against which reads were performed, the primary checks whether there were conflicting writes by other concurrently executing transactions that have already been admitted. The transaction is admitted only if there are no conflicts, after which the transaction is guaranteed to succeed, assuming no server failures. Conflict resolution on the primary relies on tracking all of the recent\u003c/span\u003e \u003cspan\u003ewrites performed by previously admitted transactions during the same epoch on the primary. Transactions spanning epochs are rejected, as this simplifies write set\u003c/span\u003e \u003cspan\u003etracking without requiring replication. The history of writes maintained on the primary is also periodically purged to keep the space usage low. Since the complete history isn’t maintained, the primary needs to maintain a minimum tracked version\u003c/span\u003e \u003cspan\u003eand reject all transactions that have reads against a snapshot with lower version to guarantee serializability. Read-only transactions work exactly similar to read-write transactions, except that the write set is empty.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eConditional write is implemented using “server-side transactions”. It provides a more user friendly client side API for use cases where clients want to atomically modify a set of keys based on some common preconditions such as key_present, key_not_present, and value_matches_or_key_not_present. When a primary receives a conditional write request it sets up a transaction context and converts the preconditions and write set to a transaction on the server, reusing all of the machinery for transactions. The conditional-write API can be more efficient than the transaction API in cases where clients can compute the precondition without requiring a read.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe future of ZippyDB\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eDistributed key-value stores have many applications, and the need for them often comes up while building a variety of systems, from products to storing metadata for various infrastructure services. Building a scalable, strongly consistent, and fault-tolerant key-value store can be very challenging and often requires thinking through many trade-offs to provide a curated combination of system capabilities and guarantees that works well in practice for a variety of workloads. This blog post introduced ZippyDB, Facebook’s biggest key-value store, which has been in production for more than six years serving a lot of different workloads. Since its inception, the service has seen very steep adoption, mostly due to the flexibility that it offers in terms of making efficiency, availability, and performance trade-offs. The service also enables us to use engineering resources effectively as a company and use our key-value store capacity efficiently as a single pool. ZippyDB is still evolving and currently undergoing significant architectural changes, such as storage-compute disaggregation, fundamental changes to membership management, failure detection and recovery, and distributed transactions, in order to adapt to the changing ecosystem and product requirements.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. Since we first deployed ZippyDB in 2012, this key-value store has expanded rapidly, and today, ZippyDB serves a number of use cases, ranging from metadata for a distributed filesystem, counting events for both internal and external purposes, to product data that’s used for [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/08/06/core-data/zippydb/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/08/06/core-data/zippydb/\"\u003eHow we built a general purpose key value store for Facebook with ZippyDB\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Hero_.Image_.jpg",
      "date_published": "2021-08-06T16:52:02Z",
      "author": {
        "name": "By Sarang Masti"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/08/04/open-source/winterfell/",
      "title": "Open sourcing Winterfell: A STARK prover and verifier",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe are releasing Winterfell, our implementation of a STARK prover/verifier to Crates.io \u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWinterfell is an easy to use open source implementation of STARKs for security and privacy applications.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eOne potential application for Winterfell’s zero-knowledge proofs is blockchain privacy and scalability. \u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003e“Any sufficiently advanced technology is indistinguishable from magic.” —Clarke’s Third Law\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhat if the average developer could benefit from proofs of computational integrity (CI) that would normally require an in-depth knowledge of cryptography to implement? \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eCI proofs, of which zero-knowledge proofs (ZKPs) are a subset, are a cryptographic technology that let you do seemingly impossible things. For example, you can run a computation and get some result. You can then use a CI proof to convince anyone that you did the computation correctly without their having to rerun the computation themselves.\u003c/span\u003e \u003cspan\u003eAnd they can verify this correctness in just a few milliseconds, regardless of how complex or long-running the original computation was.\u003c/span\u003e \u003cspan\u003eTo bring the power of CI proofs to the masses, we’ve developed Winterfell, a general-purpose STARK (Scalable Transparent Arguments of Knowledge) prover and verifier. We are happy to be publishing the v0.1 version of the library to crates.io.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_FINAL.gif\" alt=\"To bring the power of CI proofs to the masses, we’ve developed Winterfell, a general-purpose STARK prover and verifier. \" width=\"1920\" height=\"1080\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAnother important property of these CI proofs is the ability to hide some (or all) of the inputs that were used to run the computation. This is the zero-knowledge aspect. For example, you could prove that a number is in a given range without revealing the exact value of the number (these types of proofs are usually called range proofs). Or, you could do something as complex as comparing two number sequences, one public and one private (known only to yourself), and prove to anyone beyond a doubt that there is or isn’t a match between them.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe magic of ZKPs\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe general ideas behind ZKPs were developed as early as the 1980s, but interest in this area of cryptography has \u003c/span\u003e\u003ca href=\"https://nakamoto.com/cambrian-explosion-of-crypto-proofs/\"\u003e\u003cspan\u003eexploded\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e recently, driven in part by emergent applications in the blockchain space. In the last few years, over a dozen new proving systems have appeared. Some of them have even been deployed in production where tens of billions of dollars depend on their security properties. However, ZKPs are far from mainstream, primarily for two reasons:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUntil recently, deploying ZKPs in applications required expert cryptographers with years of experience. The situation is somewhat better now, as there are plenty of relatively accessible materials available and more projects that try to make ZKPs accessible to the average developer. But even now, making sense of different proving systems and the trade-offs associated with them requires deep expertise and/or a steep learning curve, even for experienced software engineers.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWhile verifying a ZK proof is extremely fast and requires very few compute resources, generating a proof is a computationally intensive process. It may take seconds or even minutes (or many CPU cores) to generate proofs for even relatively simple computations. Only relatively recent advances in cryptography and implementation improvements have brought a large segment of computations to within practical feasibility for ZKPs. And there is a lot of ongoing work to expand the set of computations for which proof generation is practical.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eWe developed Winterfell to bridge these gaps and to bring ZKPs within reach of regular developers. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWinterfell is here\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWinterfell is a general purpose STARK prover and verifier written in \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2021/04/29/developer-tools/rust/\"\u003e\u003cspan\u003eRust\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e at \u003c/span\u003e\u003ca href=\"https://research.fb.com/category/blockchain-and-cryptoeconomics/\"\u003e\u003cspan\u003eNovi Research\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. \u003c/span\u003e\u003ci\u003e\u003cspan\u003eGeneral purpose\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e means that Winterfell can generate CI proofs for any computation. Basically, for any program that can be described with a Turing-complete language, we can generate a CI proof using Winterfell (though this would be much more straightforward for some programs than for others).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWinterfell uses STARKs, a proof-of-computation scheme developed by Eli Ben-Sasson, Michael Riabzev, et al. In comparison with many other CI proving systems, STARKs have a number of attractive properties, including:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSTARKs rely on very few cryptographic assumptions. In fact, the only cryptographic primitive we need for STARKs to work is a collision resistant hash function (e.g., SHA256). This also makes STARKs resistant to potential attacks from adversaries with quantum computers.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUnlike many other proving systems, STARKs are fully transparent. This means we don’t need to run complicated trusted setup ceremonies to start using STARKs. Trusted setups are a potential security weakness in other zero knowledge protocols, because a compromised trusted setup allows attackers to generate fake CI proofs. STARKs are immune to this.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eIn comparison with other systems, STARK proof generation is extremely fast when we deal with uniform computations, or computations with regular structures. Fortunately, the vast majority of programs people write do possess such regular structures. Moreover, pretty much every single step of the STARK proof generation process is massively parallelizable. Thus, we can frequently speed up proof generation by distributing it across more and more CPU cores.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eNone of the individual properties listed above are unique to STARKs. However, no other proving system combines lean cryptography, transparency, and performance to the extent STARKs do. Winterfell takes full advantage of these benefits while abstracting away most of the complexity. For example, proof generation can be distributed across multiple CPU cores to dramatically reduce proof generation time (see our benchmarks \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell#performance\"\u003e\u003cspan\u003ehere\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e). Moreover, we have plans to enable fully distributed proof generation across multiple machines and have already started work in this direction.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn addition to being performant, Winterfell is highly configurable. That is, you can dynamically tune almost all parameters of the STARK protocol to attain specific performance and security targets. We are able to achieve such high configurability without sacrificing performance or code clarity by relying on Rust’s zero-cost abstractions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eLast, and perhaps most important, you don’t need to be a cryptographer to use Winterfell. As mentioned previously, Winterfell abstracts away most of the complexity of the STARK protocol. The only thing the user is responsible for is describing their computation in a format that the STARK prover/verifier can understand. This format is called algebraic intermediate representation (AIR), and the step of translating a program into AIR is called arithmetization.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg\" alt=\"\" width=\"1920\" height=\"847\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=916,404 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=768,339 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=1024,452 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=1536,678 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=96,42 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=192,85 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eUsing Winterfell\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWinterfell exposes a relatively simple interface for describing AIR for any computation. However, the concept of arithmetization is not something most developers are familiar with, so there is going to be a learning curve. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo help you get started, we’ve put together an end-to-end \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell#usage\"\u003e\u003cspan\u003etutorial\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e on how to define AIR for a very simple computation. We also have examples of more interesting computations in the \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell/tree/main/examples\"\u003e\u003cspan\u003eexamples crate\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, ranging from something as simple as a Fibonacci sequence to something as sophisticated as aggregation of hash-based signatures. And if you would like to get a little bit deeper into theory, we recommend reading two excellent blog posts from StarkWare: \u003c/span\u003e\u003ca href=\"https://medium.com/starkware/arithmetization-i-15c046390862\"\u003e\u003cspan\u003eArithmetization I\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://medium.com/starkware/arithmetization-ii-403c3b3f4355\"\u003e\u003cspan\u003eArithmetization II\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOnce you are comfortable with writing AIRs, using Winterfell to generate STARK proofs becomes relatively easy. For example, AIR for a Fibonacci sequence requires less than 100 lines of code and can be put together in about 15 minutes. Even for the relatively complicated example of hash-based signature aggregation mentioned above, the AIR is described in about 600 lines of code (though it did take several days to put together).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAnother point worth mentioning: We wrote Winterfell as a set of modular crates, all of which are being published to \u003ca href=\"https://crates.io/users/irakliyk\"\u003eCrates.io\u003c/a\u003e today as well. While we use these crates to build a STARK proving system, many of them are general enough to be used as building blocks in other CI proving systems. For example, for low-degree testing, we use the FRI protocol implemented in the \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell/tree/main/fri\"\u003e\u003cspan\u003ewinter-fri\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e crate, which is also used as a building block for several other proof systems (e.g., \u003c/span\u003e\u003ca href=\"https://eprint.iacr.org/2019/1076.pdf\"\u003e\u003cspan\u003eFractal\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://eprint.iacr.org/2018/828.pdf\"\u003e\u003cspan\u003eAurora\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e) that aim to be transparent and post-quantum. Thus, we hope that our work will help implementers of these protocols get their job done more quickly and efficiently.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eApplications\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eRecent advancements in ZKPs are driven by emergent use cases in the blockchain space. Specifically, ZKPs offer attractive solutions to perhaps two of the most pressing blockchain challenges: privacy and scalability. However, ZKPs have numerous potential applications outside of the blockchain space as well.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile there still remain some technical challenges to overcome before proofs of computational integrity can be considered practical at a large scale, we believe that Winterfell represents an important stepping stone for bringing a well-studied subject in academic research into practical deployments. And we hope that the security and privacy community will also benefit from an easy to use open source implementation of STARKs.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePlease check out the \u003c/span\u003e\u003ca href=\"https://github.com/novifinancial/winterfell\"\u003e\u003cspan\u003eWinterfell repository\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, and feel free to open issues for comment and leave feedback!\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWe are releasing Winterfell, our implementation of a STARK prover/verifier to Crates.io  Winterfell is an easy to use open source implementation of STARKs for security and privacy applications. One potential application for Winterfell’s zero-knowledge proofs is blockchain privacy and scalability.  “Any sufficiently advanced technology is indistinguishable from magic.” —Clarke’s Third Law What if the average [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/08/04/open-source/winterfell/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/08/04/open-source/winterfell/\"\u003eOpen sourcing Winterfell: A STARK prover and verifier\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/08/CD21_418_Open_Sourcing_Winterfell_HERO_FINAL.jpg",
      "date_published": "2021-08-04T16:00:43Z",
      "author": {
        "name": "By Irakliy Khaburzaniya, Kostas Chalkias, Kevin Lewi, Harjasleen Malvai"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/",
      "title": "A linear programming approach for optimizing features in ML models",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eWhether it’s iterating on \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/\"\u003e\u003cspan\u003eFacebook’s News Feed ranking algorithm\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e or delivering the most relevant ads to users, we are constantly exploring new features to help improve our machine learning (ML) models. Every time we add new features, we create a challenging data engineering problem that requires us to think strategically about the choices we make. More complex features and sophisticated techniques require additional storage space. Even at a company the size of Facebook, capacity isn’t infinite. If left unchecked, accepting all features would quickly overwhelm our capacity and slow down our iteration speed, decreasing the efficiency of running the models.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo better understand the relationship between these features and the capacity of the infrastructure that needs to support them, we can frame the system as a linear programming problem. By doing this, we can maximize a model’s performance, probe the sensitivity of its performance to different infrastructure constraints, and study the relationships between different services. \u003c/span\u003e\u003cspan\u003eThis work was done by data scientists embedded in our engineering team and demonstrates the value of analytics and data science in ML.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eSupporting feature development\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eIt’s important to continually introduce features that best leverage new data to maintain performant models. New features are responsible for the majority of incremental model improvements. These ML models are useful for our ad delivery system. They work together to predict a person’s likelihood of taking specific action on the ad. We work to continuously improve our models so our systems deliver only those ads that are relevant to a user.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs our techniques become more sophisticated, we develop more complex features that demand more of our infrastructure. A feature can leverage different services depending on its purpose. Some features have a higher memory cost, while others require extra CPU or take up more storage. It’s important to use our infrastructure wisely to maximize the performance of our models. We must be able to smartly allocate resources and be able to quantify the trade-offs of different scenarios.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo address these problems, we frame our system as a linear programming problem that maximizes our model’s metrics. We use this framework to better understand the interaction between our features and services. With this knowledge, we can automatically select the best features, identify infrastructure services to invest in, and maintain the health of both our models and services.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eFraming our problem\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eTo get a handle on our framework, we’ll first introduce a model problem. Say we have multiple features that all take up some amount of space (the height of the rectangles) and contribute some amount of gain to our models (the teal squares), and we are unable to accommodate them all in our limited capacity.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg\" alt=\"Say we have multiple features that all take up some amount of space (the height of the rectangles) and contribute some amount of gain to our models (the teal squares), and we are unable to accommodate them all in our limited capacity.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003eA naive solution would be to just start picking the features with the most gain (teal squares) until you are out of capacity. However, you might not be making the best use of your resources if you just prioritize the gain. For example, by taking in a big feature with a large gain, you could be taking up room that two smaller features with less gain could use instead. Together, those two smaller features would give you more bang for your buck than the big feature.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg\" alt=\"by taking in a big feature with a large gain, you could be taking up room that two smaller features with less gain could use instead.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIf we get a little less naive, we could instead look to pick features that give us the most bang per buck — features that have the most gain per storage. However, if we pick features only from that perspective, we could end up leaving out some less efficient features that we would still have room to accommodate.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg\" alt=\"we could instead look to pick features that give us the most bang per buck — features that have the most gain per storage\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe’ve been looking at a very simplified view of infrastructure, but the reality is a bit more complex. For example, features often don’t take up just one resource but need many — such as memory, CPU, or storage in other services. We can make our example slightly more sophisticated by adding in Service B, and saying that orange features take up space in both Service A and Service B.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg\" alt=\"We can make our example slightly more sophisticated by adding in Service B, and saying that orange features take up space in both Service A and Service B.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003ePicking which features we use is not the only way to control our infrastructure usage. We can also employ various techniques to increase the efficiency of our feature storage. This sometimes comes with a cost, either through the feature itself or capacity from a service. In this case, let’s say that we can halve the storage cost of some features (bordered in pink) but only at the cost of reducing the gain of the feature, and using some of the limited capacity in Service B.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg\" alt=\"let’s say that we can halve the storage cost of some features (bordered in pink), but only at the cost of reducing the gain of the feature, and using some of the limited capacity in Service B.\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe’ll stop the example at this point, but this is enough for the general message to be clear — infrastructure can be a complicated interconnected system of different constraints. In reality, our capacity is not set in stone. We can move resources around if it is warranted. Features are also not the only thing we’re working on. There are plenty of other projects and workflows that compete for the same resources. We need to not only choose the features that maximize our gain but also be able to answer questions about how our system responds to changes:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWhich features do we select to optimize the gain?\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eIs feature compression worth it? More important, is it worth an engineer’s time to implement it?\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eHow does the gain change if we add more capacity to Service A?\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eHow do service dependencies interact? If we increase the capacity of Service B, can we use less of Service A?\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eScaling the problem\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eLet’s step back and review the conditions of our model problem:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe want to maximize our gain.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe are limited by the capacity of Service A.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe are also limited by the capacity of Service B, which only some features contribute to.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSome features may be compressed, but:\u003c/span\u003e\n\u003col\u003e\n\u003cli aria-level=\"2\"\u003e\u003cspan\u003eThey suffer a loss to their gain.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"2\"\u003e\u003cspan\u003eSome of Service B’s capacity must be used.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eWe can express all these constraints as a system of linear equations.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eLet 𝑥 be a vector that is 0 or 1 that signifies whether we select the feature, and let 𝑔 be a vector that stores the gain of the feature. The subscripts 𝑓 and 𝑐 denote whether we are specifying a full cost or compressed feature. For example, 𝑥\u003c/span\u003e\u003cspan\u003e𝑓\u003c/span\u003e\u003cspan\u003e denotes full, uncompressed features that we have selected to include, and 𝑔\u003c/span\u003e\u003cspan\u003e𝑐\u003c/span\u003e\u003cspan\u003e represents the cost of compressed features.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eGiven these definitions, our objective is to maximize:\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg\" alt=\"linear programming equation\" width=\"1920\" height=\"244\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=916,116 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=768,98 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=1024,130 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=1536,195 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=96,12 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=192,24 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe can now add our constraints that model the limitations of our infrastructure:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan\u003eFeatures will either be selected and compressed, selected but not compressed, or not selected. We should not select the compressed and uncompressed versions of the same feature.\u003cbr/\u003e\n\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg\" alt=\"linear programming equation\" width=\"1920\" height=\"894\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=916,427 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=768,358 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=1024,477 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=1536,715 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=192,89 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eLet 𝑠 be the storage cost of the feature and the subscripts 𝐴 and 𝐵 represent Service A and B, respectively. For example, 𝑠\u003c/span\u003e\u003cspan\u003e𝐴𝑐\u003c/span\u003e\u003cspan\u003e represents the storage cost of compressed features in Service A. We are constrained by the capacity of the two services.\u003cbr/\u003e\n\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg\" alt=\"\" width=\"1920\" height=\"594\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=916,283 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=768,238 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=1024,317 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=1536,475 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=96,30 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=192,59 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan\u003eSome of Service B must be utilized to enable compression. Let’s represent that as a few features that must be selected.\u003cbr/\u003e\n\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg\" alt=\"\" width=\"1920\" height=\"892\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=916,426 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=768,357 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=1024,476 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=1536,714 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=192,89 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eWith this, we have now completely specified our problem in a few equations and can solve them using linear programming techniques. Of course, as we are interested in automating and productionalizing this, it can easily be specified in code. For this example, we accomplish this in Python using the excellent \u003c/span\u003e\u003ca href=\"https://numpy.org/\"\u003e\u003cspan\u003eNumPy\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003c/span\u003e\u003ca href=\"https://www.cvxpy.org/\"\u003e\u003cspan\u003eCVXPY\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e packages.\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport cvxpy as cp\nimport numpy as np\nimport pandas as pd\n \n# Assuming data is a Pandas DataFrame that contains relevant feature data\ndata = pd.DataFrame(...)\n# These variables contain the maximum capacity of various services\nservice_a = ...\nservice_b = ...\n \nselected_full_features = cp.Variable(data.shape[0], boolean=True)\nselected_compressed_features = cp.Variable(data.shape[0], boolean=True)\n \n# Maximize the feature gain\nfeature_gain = (\n   data.uncompressed_feature_gain.to_numpy() @ selected_full_features\n   + data.compressed_feature_gain.to_numpy() @ selected_compressed_features\n)\n \nconstraints = [\n   # 1. We should not select the compressed and uncompressed version\n   #    of the same feature\n   selected_full_features + selected_compressed_features \u0026lt;= np.ones(data.shape[0]),\n   # 2. Features are restricted by the maximum capacity of the services\n   data.full_storage_cost.to_numpy() @ selected_full_features\n   + data.compressed_storage_cost.to_numpy() @ selected_full_features\n   \u0026lt;= service_a,\n   data.full_memory_cost.to_numpy() @ selected_full_features\n   + data.compressed_memory_cost.to_numpy() @ selected_compressed_features\n   \u0026lt;= service_b,\n   # 3. Some features must be selected to enable compression\n   selected_full_features \u0026gt;= data.special_features.to_numpy(),\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003cspan\u003eLeveraging the framework\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eNow we have a framework that we can use to express our questions and hypotheticals. If we want to find out how an increase in Service A translates to a feature gain, we can  run the optimization problem above at different values for Service A capacity and plot the gain. This way, we can directly quantify the return for each incremental increase in capacity. We can use this as a strong signal for what services we should invest in for the future and directly compare the return on investment on more feature memory, computing, or storage.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg\" alt=\"chart showing return on service A capacity\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSimilarly, we can look at the relationships between services. We simply vary the capacity of Services A and B while keeping the gain constant. We can see that as Service B’s capacity increases, less of Service A is needed to achieve the same gain. This can be leveraged if one service is overly stressed compared with another.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg\" alt=\"chart showing the relationship between Service A and service B capacity\" width=\"1920\" height=\"1080\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eLinear programming as a framework for automating decisions\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003ePreviously, feature approval was a manual process where teams would spend valuable time calculating how many features we could support and analyzing what the return on investment was for increasing the capacity of our services. In a company like Facebook — where we have multiple models being continuously iterated on — this approach does not scale. By framing our services as a system of linear equations, we take a complex interconnected system and simplify it into basic relationships that are easily communicated. By doing this, we can make smarter decisions about the features we deploy and the infrastructure we invest in.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWhether it’s iterating on Facebook’s News Feed ranking algorithm or delivering the most relevant ads to users, we are constantly exploring new features to help improve our machine learning (ML) models. Every time we add new features, we create a challenging data engineering problem that requires us to think strategically about the choices we make. [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/\"\u003eA linear programming approach for optimizing features in ML models\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/CD21-479_Framing-Feature-Resources_Hero-1920x1080-1.jpg",
      "date_published": "2021-07-29T15:57:28Z",
      "author": {
        "name": "By Paulo Silva Costa"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/",
      "title": "Migrating Facebook to MySQL 8.0",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003ca href=\"https://github.com/facebook/mysql-5.6\"\u003e\u003cspan\u003eMySQL\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, an open source database developed by Oracle, powers some of Facebook’s most important workloads. We actively develop new features in MySQL to support our evolving requirements. These features change many different areas of MySQL, including client connectors, storage engine, optimizer, and replication. Each new major version of MySQL requires significant time and effort to migrate our workloads. The challenges  include:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003ePorting our custom features to the new version\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eEnsuring replication is compatible between the major versions\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eMinimizing changes needed for existing application queries\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFixing performance regressions that prevent the server from supporting our workloads\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eOur last major version upgrade, to MySQL 5.6, took more than a year to roll out. When version 5.7 was released, we were still in the midst of developing our LSM-Tree storage engine, \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2016/08/31/core-data/myrocks-a-space-and-write-optimized-mysql-database/\"\u003e\u003cspan\u003eMyRocks\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, on version 5.6. Since upgrading to 5.7 while simultaneously building a new storage engine would have significantly slowed the progress on MyRocks, we opted to stay with 5.6 until MyRocks was complete. MySQL 8.0 was announced as we were finishing the rollout of MyRocks to our user database (UDB) service tier. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThat version included compelling features like writeset-based parallel replication and a transactional data dictionary that provided atomic DDL support. For us, moving to 8.0 would also bring in the 5.7 features we had missed, including Document Store. Version 5.6 was approaching end of life, and we wanted to stay active within the MySQL community, especially with our work on the MyRocks storage engine. Enhancements in 8.0, like instant DDL, could speed up MyRocks schema changes, but we needed to be on the 8.0 codebase to use it. Given the benefits of the code update, we decided to migrate to 8.0. We’re sharing how we tackled our 8.0 migration project — and some of the surprises we discovered in the process. When we initially scoped out the project, it was clear that moving to 8.0 would be even more difficult than migrating to 5.6 or MyRocks.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAt the time, our customized 5.6 branch had over 1,700 code patches to port to 8.0. As we were porting those changes, new Facebook MySQL features and fixes were added to the 5.6 codebase that moved the goalpost further away.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe have many MySQL servers running in production, serving a large number of disparate applications. We also have extensive software infrastructure for managing MySQL instances. These applications perform operations like gathering statistics and managing server backups.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eUpgrading from 5.6 to 8.0 skipped over 5.7 entirely. APIs that were active in 5.6 would have been deprecated in 5.7 and possibly removed in 8.0, requiring us to update any application using the now-removed APIs.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA number of Facebook features were not forward-compatible with similar ones in 8.0 and required a deprecation and migration path forward.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eMyRocks enhancements were needed to run in 8.0, including native partitioning and crash recovery.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eCode patches\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe first set up the 8.0 branch for building and testing in our development environments. We then began the long journey to port the patches from our 5.6 branch. There were more than 1,700 patches when we started, but we were able to organize them into a few major categories. Most of our custom code had good comments and descriptions so we could easily determine whether it was still needed by the applications or if it could be dropped. Features that were enabled by special keywords or unique variable names also made it easy to determine relevance because we could search through our application codebases to find their use cases. A few patches were very obscure and required detective work — digging through old design documents, posts, and/or code review comments — to understand their history.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe sorted each patch into one of four buckets:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eDrop: Features that were no longer used, or had equivalent functionality in 8.0, did not need to be ported.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eBuild/Client: Non-server features that supported our build environment and modified MySQL tools like mysqlbinlog, or added functionality like the async client API, were ported.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eNon-MyRocks Server: Features in the mysqld server that were not related to our MyRocks storage engine were ported.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eMyRocks Server: Features that supported the MyRocks storage engine were ported.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eWe tracked the status and relevant historical information of each patch using spreadsheets, and recorded our reasoning when dropping a patch. Multiple patches that updated the same feature were grouped together for porting. Patches ported and committed to the 8.0 branch were annotated with the 5.6 commit information. Discrepancies on porting status would inevitably arise due to the large number of patches we needed to sift through and these notes helped us resolve them.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEach of the client and server categories naturally became a software release milestone. With all client-related changes ported, we were able to update our client tooling and connector code to 8.0. Once all of the non-MyRocks server features were ported, we were able to deploy 8.0 mysqld for InnoDB servers. Finishing up the MyRocks server features enabled us to update MyRocks installations.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSome of the most complex features required significant changes for 8.0, and a few areas had major compatibility problems. For example, upstream 8.0 binlog event formats were incompatible with some of our custom 5.6 modifications. Error codes used by Facebook 5.6 features conflicted with those assigned to new features by upstream 8.0. We ultimately needed to patch our 5.6 server to be forward-compatible with 8.0.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIt took a couple of years to complete porting all of these features. By the time we got to the end, we had evaluated more than 2,300 patches and ported 1,500 of those to 8.0.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe migration path\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe group together multiple mysqld instances into a single MySQL replica set. Each instance in a replica set contains the same data but is geographically distributed to a different data center to provide data availability and failover support. Each replica set has one primary instance. The remaining instances are all secondaries. The primary handles all write traffic and replicates the data asynchronously to all secondaries.\u003cbr/\u003e\n\u003c/span\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cspan\u003eWe started with replica sets consisting of 5.6 primary/5.6 secondaries and the end goal was replica sets with 8.0 primary/8.0 secondaries. We followed a plan similar to the\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2017/09/25/core-data/migrating-a-database-from-innodb-to-myrocks/\"\u003e \u003cspan\u003eUDB MyRocks migration plan\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFor each replica set, create and add 8.0 secondaries via a logical copy using mysqldump. These secondaries do not serve any application read traffic.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eEnable read traffic on the 8.0 secondaries.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAllow the 8.0 instance to be promoted to primary.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eDisable the 5.6 instances for read traffic.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eRemove all the 5.6 instances.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eEach replica set could transition through each of the steps above independently and stay on a step as long as needed. We separated replica sets into much smaller groups, which we shepherded through each transition. If we found problems, we could rollback to the previous step. In some cases, replica sets were able to reach the last step before others started.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo automate the transition of a large number of replica sets, we needed to build new software infrastructure. We could group replica sets together and move them through each stage by simply changing a line in a configuration file. Any replica set that encountered problems could then be individually rolled back.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eRow-based replication\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eAs part of the 8.0 migration effort, we decided to standardize on using row-based replication (RBR). Some 8.0 features required RBR, and it simplified our MyRocks porting efforts. While most of our MySQL replica sets were already using RBR, those still running statement-based replication (SBR) could not be easily converted. These replica sets usually had tables without any high cardinality keys. Switching completely to RBR had been a goal, but the long tail of work needed to add primary keys was often prioritized lower than other projects.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHence, we made RBR a requirement for 8.0. After evaluating and adding primary keys to every table, we switched over the last SBR replica set this year. Using RBR also gave us an alternative solution for resolving an application issue that we encountered when we moved some replica sets to 8.0 primaries, which will be discussed later.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAutomation validation\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eMost of the 8.0 migration process involved testing and verifying the mysqld server with our automation infrastructure and application queries.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs our MySQL fleet grew, so did the automation infrastructure we use to manage the servers. In order to ensure all of our MySQL automation was compatible with the 8.0 version, we invested in building a test environment, which leveraged test replica sets with virtual machines to verify the behaviors. We wrote integration tests to canary each piece of automation to run on both the 5.6 version and the 8.0 version and verified their correctness. We found several bugs and behavior differences as we went through this exercise.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs each piece of MySQL infrastructure was validated against our 8.0 server, we found and fixed (or worked around) a number of interesting issues:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSoftware that parsed text output from error log, mysqldump output, or server show commands easily broke. Slight changes in the server output often revealed bugs in a tool’s parsing logic.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe 8.0’s default \u003c/span\u003e\u003cspan\u003eutf8mb4\u003c/span\u003e\u003cspan\u003e collation settings resulted in collation mismatches between our 5.6 and 8.0 instances. 8.0 tables may use the new \u003c/span\u003e\u003cspan\u003eutf8mb4_0900\u003c/span\u003e\u003cspan\u003e collations even for create statements generated by 5.6’s show create table because the 5.6 schemas using \u003c/span\u003e\u003cspan\u003eutf8mb4_general_ci\u003c/span\u003e\u003cspan\u003e do not explicitly specify collation. These table differences often caused problems with replication and schema verification tools.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe error codes for certain replication failures changed and we had to fix our automation to handle them correctly.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe 8.0 version’s data dictionary obsoleted table .frm files, but some of our automation used them to detect table schema modifications.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe had to update our automation to support the dynamic privs introduced in 8.0.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\u003cspan\u003eApplication validation\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe wanted the transition for applications to be as transparent as possible, but some application queries hit performance regressions or would fail on 8.0.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor the MyRocks migration, we built a MySQL shadow testing framework that captured production traffic and replayed them to test instances. For each application workload, we constructed test instances on 8.0 and replayed shadow traffic queries to them. We captured and logged the errors returning from the 8.0 server and found some interesting problems. Unfortunately, not all of these problems were found during testing. For example, the transaction deadlock was discovered by applications during the migration. We were able to roll back these applications to 5.6 temporarily while we researched different solutions.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eNew reserved keywords were introduced in 8.0 and a few, such as groups and rank, conflicted with popular table column names and aliases used in application queries. These queries did not escape the names via backquotes, leading to parsing errors. Applications using software libraries that automatically escaped the column names in queries did not hit these issues, but not all applications used them. Fixing the problem was simple, but it took time to track down application owners and codebases generating these queries.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA few REGEXP incompatibilities were also found between 5.6 and 8.0.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA few applications hit \u003c/span\u003e\u003ca href=\"https://bugs.mysql.com/bug.php?id=98324\"\u003e\u003cspan\u003erepeatable-read transaction deadlocks\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e involving \u003c/span\u003e\u003cspan\u003einsert … on duplicate key\u003c/span\u003e\u003cspan\u003e queries on InnoDB. 5.6 had a bug which was corrected in 8.0, but the fix increased the likelihood of transaction deadlocks. After analyzing our queries, we were able to resolve them by lowering the isolation level. This option was available to us since we had made the switch to row-based replication.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eOur custom 5.6 Document Store and JSON functions were not compatible with 8.0’s. Applications using Document Store needed to convert the document type to text for the migration. For the JSON functions, we added 5.6-compatible versions to the 8.0 server so that applications could migrate to the 8.0 API at a later time.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eOur query and performance testing of the 8.0 server uncovered a few problems that needed to be addressed almost immediately.\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eWe found new mutex contention hotspots around the ACL cache. When a large number of connections were opened simultaneously, they could all block on checking ACLs.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSimilar contention was found with binlog index access when many binlog files are present and high binlog write rates rotate files frequently.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSeveral queries involving temp tables were broken. The queries would return unexpected errors or take so long to run that they would time out.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eMemory usage compared with 5.6 had increased, especially for our MyRocks instances, because InnoDB in 8.0 must be loaded. The default performance_schema settings enabled all instruments and consumed significant memory. We limited the memory usage by only enabling a small number of instruments and making code changes to disable tables that could not be manually turned off. However, not all the increased memory was being allocated by performance_schema. We needed to examine and modify various InnoDB internal data structures to reduce the memory footprint further. This effort brought 8.0’s memory usage down to acceptable levels. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhat’s next\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe 8.0 migration has taken a few years so far. We have converted many of our InnoDB replica sets to running entirely on 8.0. Most of the remaining ones are at various stages along the migration path. Now that most of our custom features have been ported to 8.0, updating to Oracle’s minor releases has been comparatively easier and we plan to keep pace with the latest versions.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSkipping a major version like 5.7 introduced problems, which our migration needed to solve.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFirst, we could not upgrade servers in place and needed to use logical dump and restore to build a new server. However, for very large mysqld instances, this can take many days on a live production server and this fragile process will likely be interrupted before it can complete. For these large instances, we had to modify our backup and restore systems to handle the rebuild.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSecond, it is much harder to detect API changes because 5.7 could have provided deprecation warnings to our application clients to fix potential issues. Instead, we needed to run additional shadow tests to find failures before we could migrate the production workloads. Using mysql client software that automatically escaped schema object names helps reduce the number of compatibility issues.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSupporting two major versions within a replica set is hard. Once a replica set promotes its primary to be an 8.0 instance, it is best to disable and remove the 5.6 ones as soon as possible. Application users tend to discover new features that are supported only by 8.0, like \u003c/span\u003e\u003cspan\u003eutf8mb4_0900\u003c/span\u003e\u003cspan\u003e collations, and using these can break the replication stream between 8.0 and 5.6 instances.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDespite all the hurdles in our migration path, we have already seen the benefits of running 8.0. Some applications have opted for early conversion to 8.0 to utilize features like Document Store and improved datetime support. We have been considering how to support storage engine features like Instant DDL on MyRocks. Overall, the new version greatly expands on what we can do with MySQL @ Facebook.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eMySQL, an open source database developed by Oracle, powers some of Facebook’s most important workloads. We actively develop new features in MySQL to support our evolving requirements. These features change many different areas of MySQL, including client connectors, storage engine, optimizer, and replication. Each new major version of MySQL requires significant time and effort to [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/\"\u003eMigrating Facebook to MySQL 8.0\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/CD21_390_ENG_MySQL_HERO_FINAL_2x.jpg",
      "date_published": "2021-07-22T16:00:35Z",
      "author": {
        "name": "By Herman Lee, Pradeep Nayak"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/15/open-source/fsdp/",
      "title": "Fully Sharded Data Parallel: faster AI training with fewer GPUs",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eTraining AI models at a large scale isn’t easy. Aside from the need for large amounts of computing power and resources, there is also considerable engineering complexity behind training very large models. At Facebook AI Research (FAIR) Engineering, we have been working on building tools and infrastructure to make training large AI models easier. Our recent work in areas such as \u003c/span\u003e\u003ca href=\"https://github.com/pytorch/fairseq/blob/master/examples/megatron_11b/README.md\"\u003e\u003cspan\u003eintra-layer model parallelism\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, \u003c/span\u003e\u003ca href=\"https://fairscale.readthedocs.io/en/latest/deep_dive/pipeline_parallelism.html\"\u003e\u003cspan\u003epipeline model parallelism\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale#optimizer-state-sharding-zero\"\u003e\u003cspan\u003eoptimizer state+gradient sharding\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, and \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/moe_layer.py\"\u003e\u003cspan\u003emixture of experts\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e is just part of our work to make training advanced AI models for any number of tasks more efficient.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFully Sharded Data Parallel (FSDP) is the newest tool we’re introducing. It \u003ca href=\"https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\"\u003eshards\u003c/a\u003e an AI model’s parameters across data parallel workers and can optionally offload part of the training computation to the CPUs. As its name suggests, FSDP is a type of data-parallel training algorithm. Although the parameters are sharded to different \u003ca href=\"https://engineering.fb.com/2018/03/20/ml-applications/the-next-step-in-facebook-s-ai-hardware-infrastructure/\"\u003eGPUs\u003c/a\u003e, the computation for each microbatch of data is still local to each GPU worker. This conceptual simplicity makes FSDP easier to understand and more applicable to a wide range of usage scenarios (compared with intra-layer parallelism and pipeline parallelism). Compared with optimizer state+gradient sharding data parallel methods, FSDP shards parameters more uniformly and is capable of better performance via communication and computation overlapping during training.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith FSDP, it is now possible to more efficiently train models that are orders of magnitude larger using fewer GPUs. FSDP has been implemented in the \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale\"\u003e\u003cspan\u003eFairScale library\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and allows engineers and developers to scale and optimize the training of their models with simple APIs. At Facebook, FSDP has already been integrated and tested for training some of our \u003c/span\u003e\u003ca href=\"https://github.com/pytorch/fairseq\"\u003e\u003cspan\u003eNLP\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and\u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/vissl\"\u003e\u003cspan\u003e Vision\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e models.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe high computational cost of large-scale training\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/pdf/2001.08361.pdf\"\u003e\u003cspan\u003eNLP research\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e is one particular area where we can see the importance of efficiently leveraging compute for training AI. Last year, OpenAI announced that they had trained \u003c/span\u003e\u003ca href=\"https://neurips.cc/virtual/2020/public/poster_1457c0d6bfcb4967418bfb8ac142f64a.html\"\u003e\u003cspan\u003eGPT-3\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, the largest-ever neural language model, with 175 billion parameters. It is \u003c/span\u003e\u003ca href=\"https://lambdalabs.com/blog/demystifying-gpt-3/\"\u003e\u003cspan\u003eestimated\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to have taken roughly 355 GPU years to train GPT-3, or the equivalent of 1,000 GPUs working continuously for more than four months.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBesides requiring a lot of compute and engineering resources, most approaches to scaling like this introduce additional communication costs and require engineers to carefully evaluate trade-offs between memory use and computational efficiency. For example, typical data parallel training requires maintaining redundant copies of the model on each GPU, and model parallel training introduces additional communication costs to move activations between workers (GPUs).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFSDP is relatively free of trade-offs in comparison. It improves memory efficiency by sharding model parameters, gradients, and optimizer states across GPUs, and improves computational efficiency by decomposing the communication and overlapping it with both the forward and backward passes. FSDP produces identical results as standard distributed data parallel (DDP) training and is available in an easy-to-use interface that’s a drop-in replacement for PyTorch’s DistributedDataParallel module. Our early testing has shown that FSDP can enable scaling to trillions of parameters.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow FSDP works\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eIn standard DDP training, every worker processes a separate batch and the gradients are summed across workers using an \u003c/span\u003e\u003ca href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce\"\u003e\u003cspan\u003eall-reduce operation\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. While DDP has become very popular, it takes more GPU memory than it needs because the model weights and optimizer states are replicated across all DDP workers.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOne method to reduce replications is to apply a process called full parameter sharding, where only a subset of the model parameters, gradients, and optimizers needed for a local computation is made available. An implementation of this method, ZeRO-3, has already been popularized by Microsoft. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe key insight to unlock full parameter sharding is that we can decompose the \u003c/span\u003e\u003ca href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce\"\u003e\u003cspan\u003eall-reduce\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e operations in DDP into separate \u003c/span\u003e\u003ca href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#reducescatter\"\u003e\u003cspan\u003ereduce-scatter\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and \u003ca href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allgather\"\u003eall-gather\u003c/a\u003e operations:\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17828\" aria-describedby=\"caption-attachment-17828\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?w=1024\" alt=\"Full Sharded Data Parallel graph\" width=\"1024\" height=\"562\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png 1264w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=916,503 916w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=768,422 768w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=1024,562 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=96,53 96w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=192,105 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17828\"\u003eAll-reduce as a combination of reduce-scatter and all-gather. The standard all-reduce operation to aggregate gradients can be decomposed into two separate phases: reduce-scatter and all-gather. During the reduce-scatter phase, the gradients are summed in equal blocks among ranks on each GPU based on their rank index. During the all-gather phase, the sharded portion of aggregated gradients available on each GPU are made available to all GPUs (see here for details on those operators).\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eWe can then rearrange the reduce-scatter and all-gather so that each DDP worker needs to store only a single shard of parameters and optimizer states. The figure below illustrates standard DDP training (top) and FSDP training (bottom):\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17812\" aria-describedby=\"caption-attachment-17812\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?w=907\" alt=\"Full Sharded Data Parallel graph\" width=\"907\" height=\"1024\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png 1566w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=811,916 811w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=768,867 768w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=907,1024 907w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=1361,1536 1361w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=96,108 96w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=192,217 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17812\"\u003eA comparison of standard data parallel training and fully sharded data parallel training. In standard data parallel training methods, a copy of the model is present on each GPU and a sequence of forward and backward passes are evaluated on only a shard of the data. After these local computations, the parameters and optimizers for each local process are shared with the other GPUs in order to calculate the global weight update. In FSDP, only a shard of the model is present on a GPU. Then, locally, all weights are gathered from the other GPUs — by means of an all-gather step — to calculate the forward pass. This gathering of weights is then performed again before the backward pass. After that backward pass, the local gradients are averaged and sharded across the GPUs by means of a reduce-scatter step, which allows each GPU to update its local weight shard.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eTo maximize memory efficiency, we can discard the full weights after each layer’s forward pass, saving memory for subsequent layers. This can be implemented by applying the FSDP wrapper to every layer in the network (with \u003c/span\u003e\u003cspan\u003ereshard_after_forward=True\u003c/span\u003e\u003cspan\u003e). \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn pseudo-code:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003cspan\u003eFSDP forward pass:\u003c/span\u003e\n\u003cspan\u003e    for layer_i in layers:\u003c/span\u003e\n\u003cspan\u003e        all-gather full weights for layer_i\u003c/span\u003e\n\u003cspan\u003e        forward pass for layer_i\u003c/span\u003e\n\u003cspan\u003e        discard full weights for layer_i\u003c/span\u003e\n\n\u003cspan\u003eFSDP backward pass:\u003c/span\u003e\n\u003cspan\u003e    for layer_i in layers:\u003c/span\u003e\n\u003cspan\u003e        all-gather full weights for layer_i\u003c/span\u003e\n\u003cspan\u003e        backward pass for layer_i\u003c/span\u003e\n\u003cspan\u003e        discard full weights for layer_i\u003c/span\u003e\n\u003cspan\u003e        reduce-scatter gradients for layer_i\u003c/span\u003e\u003c/pre\u003e\n\u003ch2\u003e\u003cspan\u003eHow to use FSDP\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThere are several ways to use FSDP in large-scale AI research.\u003c/span\u003e\u003cspan\u003e At this time, we offer four solutions to adapt to different needs.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003e1. Using FSDP in language models\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eFor language models, FSDP is supported in the \u003c/span\u003e\u003ca href=\"https://github.com/pytorch/fairseq\"\u003e\u003ci\u003e\u003cspan\u003efairseq\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e framework\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e via the following new arguments:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e–ddp-backend=fully_sharded\u003c/span\u003e\u003cspan\u003e: enables full sharding via FSDP\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e–cpu-offload\u003c/span\u003e\u003cspan\u003e: offloads the optimizer state and FP32 model copy to CPU (combine with\u003c/span\u003e\u003cspan\u003e–optimizer=cpu_adam\u003c/span\u003e\u003cspan\u003e)\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003e–no-reshard-after-forward\u003c/span\u003e\u003cspan\u003e: increases training speed for large models (1B+ params) and is similar to ZeRO stage 2\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003eOther popular options (\u003cspan\u003e–fp16\u003c/span\u003e\u003cspan\u003e, \u003c/span\u003e\u003cspan\u003e–update-freq\u003c/span\u003e\u003cspan\u003e, \u003c/span\u003e\u003cspan\u003e–checkpoint-activations\u003c/span\u003e\u003cspan\u003e, \u003c/span\u003e\u003cspan\u003e–offload-activations\u003c/span\u003e\u003cspan\u003e, etc.) continue to work as normal\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eSee the \u003c/span\u003e\u003ca href=\"https://github.com/pytorch/fairseq/tree/master/examples/fully_sharded_data_parallel\"\u003e\u003cspan\u003efairseq tutorial\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e for instructions on using FSDP to train a 13B-parameter model on eight GPUs or on a single GPU with FSDP + CPU offloading.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003e2. Using FSDP in computer vision models\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eFor computer vision models, FSDP is supported in \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/vissl\"\u003e\u003cspan\u003eVISSL\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and tested on RegNets architectures. Layers like BatchNorm and ReLU are seamlessly handled and tested for convergence.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eUse the following options to enable FSDP:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003econfig.MODEL.FSDP_CONFIG.AUTO_SETUP_FSDP=True\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003econfig.MODEL.SYNC_BN_CONFIG.SYNC_BN_TYPE=pytorch\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003econfig.MODEL.AMP_PARAMS.AMP_TYPE=pytorch\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eSee \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/vissl/blob/40441123a6f7098500676ca8800025c1f02e28b3/vissl/config/defaults.yaml#L498-L513\"\u003e\u003cspan\u003ethis section\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e of the yaml config for additional options to config FSDP within VISSL.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003e3. Using FSDP from PyTorch Lightning\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eFor easier integration with more general use cases, FSDP is supported as a beta feature by PyTorch Lightning. \u003c/span\u003e\u003ca href=\"https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced_gpu.html#fully-sharded-training\"\u003e\u003cspan\u003eThis tutorial\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e contains a detailed example on how to use the FSDP plugin with PyTorch Lightning. At a high level, adding \u003c/span\u003e\u003cspan\u003eplugins=’fsdp’\u003c/span\u003e\u003cspan\u003e below can activate it.\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003cspan\u003emodel = MyModel()\u003c/span\u003e\n\u003cspan\u003etrainer = Trainer(gpus=4, \u003c/span\u003e\u003cb\u003eplugins=\u0026#39;fsdp\u0026#39;\u003c/b\u003e\u003cspan\u003e, precision=16)\u003c/span\u003e\n\u003cspan\u003etrainer.fit(model)\n\u003c/span\u003e\u003cspan\u003e\ntrainer.test()\u003c/span\u003e\n\u003cspan\u003etrainer.predict()\u003c/span\u003e\u003c/pre\u003e\n\u003ch3\u003e\u003cspan\u003e4. Using the FSDP library directly from FairScale\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eThe main library where FSDP has been developed, and where you can find the latest updates, is \u003c/span\u003e\u003ca href=\"https://fairscale.readthedocs.io/en/latest/deep_dive/oss_sdp_fsdp.html\"\u003e\u003cspan\u003eFairScale\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. You can directly use FSDP from FairScale with the below example by simply replacing the \u003c/span\u003e\u003cspan\u003eDDP(my_module)\u003c/span\u003e\u003cspan\u003e:\u003c/span\u003e\u003c/p\u003e\n\u003cpre\u003e\u003cspan\u003efrom fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\u003c/span\u003e\n\u003cspan\u003e...\u003c/span\u003e\n\u003cspan\u003esharded_module = \u003c/span\u003e\u003cspan\u003e\u003cdel\u003eDDP(my_module)\u003c/del\u003e\u003c/span\u003e\u003cb\u003eFSDP(my_module)\u003c/b\u003e\n\u003cspan\u003eoptim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)\u003c/span\u003e\n\u003cspan\u003efor sample, label in dataload.next_batch:\u003c/span\u003e\n\u003cspan\u003e  out = sharded_module(x=sample, y=3, z=torch.Tensor([1]))\u003c/span\u003e\n\u003cspan\u003e  loss = criterion(out, label)\u003c/span\u003e\n\u003cspan\u003e  loss.backward()\u003c/span\u003e\n\u003cspan\u003e  optim.step()\u003c/span\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cspan\u003eThe FSDP library in FairScale exposes the low-level options for many important aspects of large-scale training. Here are some few important areas to consider when you apply FSDP with its full power.\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eModel wrapping: \u003c/b\u003e\u003cspan\u003eIn order to minimize the transient GPU memory needs, users need to wrap a model in a nested fashion. This introduces additional complexity. The \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/wrap/auto_wrap.py\"\u003e\u003cspan\u003eauto_wrap\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e utility is useful in annotating existing PyTorch model code for nested wrapping purposes.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eModel initialization:\u003c/b\u003e\u003cspan\u003e Unlike DDP, FSDP does \u003c/span\u003e\u003cb\u003enot\u003c/b\u003e\u003cspan\u003e automatically synchronize model weights between GPU workers. This means model initialization must be done carefully so that all GPU workers have the identical initial weights.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eOptimizer settings:\u003c/b\u003e\u003cspan\u003e Due to sharding and wrapping, only certain types of optimizer and optimizer settings are supported by FSDP. In particular, if a module is wrapped by FSDP and its parameters are flattened into a single tensor, users cannot use different hyperparameters for different parameter groups in such a module.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eMixed precision:\u003c/b\u003e\u003cspan\u003e FSDP supports advanced mixed precision training with FP16 master weights, as well as FP16 reduce and scatter on the gradients. Certain parts of a model may converge only if full precision is used. In those cases, additional wrapping is needed to selectively run parts of a model in full precision.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eState checkpointing and inference:\u003c/b\u003e\u003cspan\u003e When the model scale is large, saving and loading the model state can become challenging. FSDP supports several ways to make that task possible, but it is by no means trivial.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eFinally, FSDP is often used together with \u003c/span\u003e\u003cb\u003eactivation checkpointing\u003c/b\u003e\u003cspan\u003e functions like \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/checkpoint/checkpoint_activations.py\"\u003e\u003cspan\u003echeckpoint_wrapper\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e from FairScale. Users may need to carefully tune the activation checkpointing strategy to fit a large model within limited GPU memory space.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eNext steps\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eFSDP is open source, and early users have tried it and contributed to it. We think it can benefit the entire research community, and we look forward to working with everyone in making it better. In particular, these are some of the important areas.\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eMaking FSDP more general.\u003c/b\u003e\u003cspan\u003e So far, FSDP has been used on both NLP and vision models with SGD and Adam optimizers. As newer models and optimizers emerge, FSDP needs to continue supporting them. Being a purely data-parallel training scheme, FSDP has the greatest potential to be general in supporting a wide range of AI algorithms.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cb\u003eMaking FSDP auto-tune. \u003c/b\u003e\u003cspan\u003eThere are many knobs that users can tune today with FSDP for both scaling and performance. We look forward to developing algorithms for auto-tuning both GPU memory usage and training performance.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eIn addition to training, more \u003c/span\u003e\u003cb\u003escalable inference\u003c/b\u003e\u003cspan\u003e and model serving is an important use case that FSDP might need to support.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eLast but not least, refactoring and continuing to \u003c/span\u003e\u003cb\u003emodularize FSDP\u003c/b\u003e\u003cspan\u003e and its core components is equally important to newer and better features.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eTry it out and contribute!\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eFSDP is currently available directly from the \u003c/span\u003e\u003ca href=\"https://github.com/facebookresearch/fairscale\"\u003e\u003cspan\u003eFairScale library\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThanks for sticking with us thus far. Please try FSDP in your research or production work. We would love to hear your feedback, and, as always, pull requests are welcome! \u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eTraining AI models at a large scale isn’t easy. Aside from the need for large amounts of computing power and resources, there is also considerable engineering complexity behind training very large models. At Facebook AI Research (FAIR) Engineering, we have been working on building tools and infrastructure to make training large AI models easier. Our [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/15/open-source/fsdp/\"\u003eFully Sharded Data Parallel: faster AI training with fewer GPUs\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Hero-FINAL-1.png",
      "date_published": "2021-07-15T16:00:47Z",
      "author": {
        "name": "By Myle Ott, Sam Shleifer, Min Xu, Priya Goyal, Quentin Duval, Vittorio Caggiano"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/",
      "title": "How WhatsApp enables multi-device capability",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eFor years, people have been asking us to create a true multi-device experience that allows people to use WhatsApp on other devices without requiring a smartphone connection.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eToday, we’re announcing the rollout of a limited public beta test for WhatsApp’s updated multi-device capability. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith this new capability, you can now use WhatsApp on your phone and up to four other nonphone devices simultaneously — even if your phone battery is dead. Each companion device will connect to your WhatsApp independently while maintaining the same level of privacy and security through end-to-end encryption that people who use WhatsApp have come to expect. Importantly, we have developed new technologies to maintain end-to-end encryption while still managing to sync your data — such as contact names, chat archives, starred messages, and more — across devices.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo achieve this, we had to rethink WhatsApp’s architecture and design new systems to enable a standalone multi-device experience while preserving \u003ca href=\"https://engineering.fb.com/2021/04/16/security/dit/\"\u003eprivacy and end-to-end encryption\u003c/a\u003e. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eTaking smartphones out of the equation\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003eThe current WhatsApp experience for companion devices on web, macOS, Windows, and Portal uses a smartphone app as the primary device, making the phone the source of truth for all user data and the only device capable of end-to-end encrypting messages for another user, initiating calls, etc. Companion devices maintain a persistent secure connection with the phone and simply mirror its contents on their own UI.\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis architecture makes it easy to deliver a seamlessly synchronized experience between a phone and companion device without compromising on security. However, it comes with some significant reliability trade-offs: By requiring the phone to perform all operations, companion devices are slower and frequently get disconnected — especially when the phone has a poor connection, its battery is running low, or the application process gets killed by the phone’s OS. It also allows for only a single companion device to be operative at a time, meaning people can’t be on a call in Portal while checking their messages on their PC, for example. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe new WhatsApp multi-device architecture removes these hurdles, no longer requiring a smartphone to be the source of truth while still keeping user data seamlessly and securely synchronized and private.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe challenge in accomplishing this was in maintaining the secure user experience across devices without having to store people’s private messages on our servers in new ways.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eMeeting the security challenges of multiple devices\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003ePrior to the introduction of multi-device, everyone on WhatsApp was identified by a single identity key from which all encrypted communication keys were derived. With multi-device, each device now has its own identity key.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe WhatsApp server maintains a mapping between each person’s account and all their device identities. When someone wants to send a message, they get their device list keys from the server.  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe have also addressed the challenge of preventing a malicious or compromised server from eavesdropping on someone’s communications by surreptitiously adding devices to someone’s account. We use a combination of technologies to solve this: First, we have extended security codes to now represent the combination of all of someone’s device identities so that anyone and their contact can always verify all the devices they are sending messages to. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSecond, in order to reduce the number of times that someone needs to perform identity verifications, we have developed and will roll out a technology called Automatic Device Verification. This system allows for devices to automatically establish trust between each other in a way that someone needs to compare another user’s security code only if that user reregisters their entire account, rather than each time they link a new device to their account. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFinally, we also give people additional control and protections over which devices are linked to their account. First, everyone will continue to be required to link new companion devices by scanning a QR code from their phone. This process now requires biometric authentication before linking where people have enabled this feature on compatible devices. Finally, people will be able to see all the companion devices linked to their account as well as when they were last used, and will be able to log out of them remotely if needed. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eMaintaining message privacy\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhen people message each other in a one-on-one chat, a pairwise encrypted session is established between each of the sender’s and recipient’s devices. WhatsApp multi-device uses a client-fanout approach\u003c/span\u003e\u003cspan\u003e,\u003c/span\u003e \u003cspan\u003ewhere the WhatsApp client sending the message encrypts and transmits it N number of times to N number of different devices \u003c/span\u003e\u003cspan\u003e— those in the sender and receiver’s device lists\u003c/span\u003e\u003cspan\u003e. Each message is individually encrypted using the established pairwise encryption session with each device. M\u003c/span\u003e\u003cspan\u003eessages are not stored on the server after they are delivered. For groups, we still use the same scalable Sender Key encryption scheme from the Signal Protocol.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?w=1024\" alt=\"WhatsApp Multi-device graphic\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17849\" aria-describedby=\"caption-attachment-17849\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?w=1024\" alt=\"WhatsApp Multi-device graphic\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17849\"\u003eWhatsApp’s legacy architecture used a smartphone as the source of truth. But with the new multi-device capability, up to four other nonphone companion devices can connect to WhatsApp independently while still maintaining the same level of privacy and security.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch2\u003e\u003cspan\u003eAdapting voice and video protocols for multi-device, end-to-end encryption  \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWhen someone on WhatsApp makes a voice or video call:\u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe initiator generates a set of random 32-byte \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secrets for each of the recipient’s devices.\u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe initiator sends an incoming call message (using the client-fanout approach described above) to each of the devices of the recipient. Each recipient’s device receives this message, which contains the encrypted \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secret.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003eIf the responder answers the call from one of the devices, a \u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e encrypted call is started, protected by the \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secret generated for that device.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eThe \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secret persists in memory on the client device and is used only during the call. Our servers do not have access to the \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secrets.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor group calls, the server randomly selects a participant device that is in the call (either the initiator or a device on which a user has accepted the call) to generate the \u003c/span\u003e\u003cspan\u003eSRTP\u003c/span\u003e\u003cspan\u003e master secret. That device generates the secret and sends it to other active participant devices through pairwise end-to-end encryption. This process is repeated, and the keys are reset whenever someone joins or leaves the call.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eKeeping message history and other application states in sync across devices\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe want to ensure that people have a consistent experience with WhatsApp no matter the device they are using. To achieve this, we synchronize message history as well as other application state data (such as contact names, whether a chat is archived, or if a message is starred) across devices. All of this data is synchronized and end-to-end encrypted between your devices.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor message history: When a companion device is linked, the primary device encrypts a bundle of the messages from recent chats and transfers them to the newly linked device. The key to this encrypted message history blob is delivered to the newly linked device via an end-to-end encrypted message. After the companion device downloads, decrypts, unpacks, and stores the messages securely, the keys are deleted. From that point forward, the companion device accesses the message history from its own local database.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOther application data requires more than an initial transfer from the phone. We also need an ongoing synchronization every time someone modifies their application state (e.g., when they add a new contact, mute a chat, or star a message).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo solve this, the WhatsApp server securely stores a copy of each application state that all of someone’s devices can access. To properly secure this, all the information, and even the metadata about the information (what kind of user data is stored or accessed), is end-to-end encrypted with constantly changing keys known only to that person’s devices. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow to try WhatsApp multi-device beta \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWe plan to initially test the experience with a small group of users from our existing beta program. We will continue optimizing performance and adding a few additional features before slowly rolling it out more broadly. Those who opt in can always opt back out.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor more information about the beta and to sign up, visit the \u003ca href=\"https://faq.whatsapp.com/general/download-and-installation/about-multi-device-beta\"\u003eWhatsApp Help Center\u003c/a\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor more information about WhatsApp multi-device, read our updated \u003ca href=\"https://www.whatsapp.com/security/WhatsApp_Security_Whitepaper_v4_Preview.pdf\"\u003ewhitepaper\u003c/a\u003e.\u003c/span\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eFor years, people have been asking us to create a true multi-device experience that allows people to use WhatsApp on other devices without requiring a smartphone connection. Today, we’re announcing the rollout of a limited public beta test for WhatsApp’s updated multi-device capability.  With this new capability, you can now use WhatsApp on your phone [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/\"\u003eHow WhatsApp enables multi-device capability\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Hero-Image_FINAL.jpg",
      "date_published": "2021-07-14T18:59:40Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/12/security/enforcing-encryption/",
      "title": "Enforcing encryption at scale",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cspan\u003eOur infrastructure supports thousands of services that handle billions of requests per second. We’ve previously discussed how we built our \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/security/service-encryption/\"\u003e\u003cspan\u003eservice encryption infrastructure\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to keep these globally distributed services operating securely and performantly. This post discusses the system we designed to enforce encryption policies within our network and shares some of the lessons we learned in the process. The goal of this enforcement is to catch any regression quickly and shut it off, keeping our internal traffic secure at the application level via TLS.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eOrganizational challenges\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eImplementing a transit encryption enforcement policy at Facebook scale requires careful planning and communication, in addition to the technical challenges we’ll discuss in a bit. We want the site to stay up and remain reliable so the people using our services will be unaffected by and unaware of any changes to the infrastructure.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eCommunicating the intent, specific timelines, and rollout strategy went a long way toward minimizing any potential disruptions for the thousands of teams that run services at Facebook. We use \u003c/span\u003e\u003ca href=\"https://www.facebook.com/workplace\"\u003e\u003cspan\u003eWorkplace\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e within Facebook, which enables us to easily distribute that information across a variety of groups with a single share button and consolidate feedback and concerns in a single place for all employees to see. We made sure to include the following:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA description of the impact of our enforcement mechanism and how it might appear at the application layer\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA dashboard for engineers to see whether their traffic would be affected\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eThe rollout and monitoring plan\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eDedicated points of contact and a Workplace group where users could ask questions about impact and troubleshoot any issues\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eThe post required multiple discussions within the team to come up with a rollout plan, dashboard requirements, and realistic timelines to meet the goals of the project. This level of communication proved to be useful as the team gathered important feedback early in the process. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eBuilding our SSLWall\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eHardware choke points are a natural approach to providing transparent enforcement. There are options, such as layer 7 firewalls, that let us do deep packet inspection, but executing fine-grained rollouts and the complexities of Facebook’s network would make implementing such a solution a nightmare. Additionally, working at a network firewall level would introduce a much larger blast radius of impacted traffic, and a single configuration issue could end up killing off traffic that we weren’t meant to touch.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur team decided to develop and deploy what is internally known as SSLWall, a system that cuts off non-SSL connections across various boundaries. Let’s dive a bit into the design decisions behind this solution.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eRequirements \u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe needed to be thorough when considering the requirements of a system that would potentially block traffic at such a large scale. The team came up with the following requirements for SSLWall, all of which had an impact on our design decisions:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eVisibility into what traffic is being blocked. Service owners needed a way to assess impacts, and our team needed to be proactive and reach out whenever we felt there was a problem brewing.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA passive monitoring mode in which we could turn a knob to flip to active enforcement. This helps us determine impacts early on and prepare teams.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eA mechanism to allow certain use cases to bypass enforcement, such as BGP, SSH, and approved network diagnostic tools.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eSupport for cases like HTTP CONNECT and STARTTLS. These are instances that do a little bit of work over plaintext before doing a TLS handshake. We have many use cases for these in our infrastructure, such as HTTP tunneling, MySQL security, and SMTP, so these must not break, especially since they eventually encrypt the data with TLS.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eExtensible configurability. We might have different requirements depending on the environment in which SSLWall operates. Additionally, having important knobs that can be tuned with little disruption means we can roll features forward or back at our own pace.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eTransparent to the application. Applications should not need to rebuild their code or incur any additional library dependencies for SSLWall to operate. The team needed the ability to iterate quickly and change configuration options independently. In addition, being transparent to the application means SSLWall needs to be performant and use minimal resources without having an impact on latencies.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eThese requirements all led us down the path of managing a host-level daemon that had a user space and kernel-level component. We needed a low-compute way to inspect all connections transparently and act on them.  \u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eeBPF\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eSince we wanted to inspect every connection without needing any changes at the application level, we needed to do some work in the kernel context. We \u003c/span\u003e\u003ca href=\"https://ebpf.io/\"\u003e\u003cspan\u003euse eBPF\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e extensively, and it provides all of the capabilities needed for SSLWall to achieve its goals. We leveraged a number of technologies that eBPF provides:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003ca href=\"http://man7.org/linux/man-pages/man8/tc-bpf.8.html\"\u003e\u003cspan\u003etc-bpf\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e: We leveraged Linux’s \u003c/span\u003e\u003ca href=\"http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html\"\u003e\u003cspan\u003etraffic control\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e (TC) facility and implemented a filter using eBPF.  At this layer, we are able to do some computation on a per-packet basis for packets flowing in and out of the box. TC allows us to operate on a broader range of kernels within Facebook’s fleet. It wasn’t the perfect solution, but it worked for our needs at the time.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003ekprobes: eBPF allows us to attach programs to kprobes, so we can run some code within the kernel context whenever certain functions are called. We were interested in the \u003c/span\u003e\u003cspan\u003etcp_connect\u003c/span\u003e\u003cspan\u003e and \u003c/span\u003e\u003cspan\u003etcp_v6_destroy_sock\u003c/span\u003e\u003cspan\u003e functions. These functions are called when a tcp connection is established and torn down, respectively. Old kernels played a factor in our use of kprobes as well.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003emaps: eBPF provides access to a number of map types, including arrays, bounded LRU maps, and perf events\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg\" alt=\"\" width=\"1920\" height=\"950\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=916,453 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=768,380 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=1024,507 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=1536,760 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=96,48 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=192,95 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17786\" aria-describedby=\"caption-attachment-17786\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg\" alt=\"Diagrams showing how kprobes, the tc filter, and our maps interact with one another when determining whether a connection needs to be blocked.\" width=\"1920\" height=\"882\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=916,421 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=768,353 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=1024,470 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=1536,706 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=96,44 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=192,88 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17786\"\u003eDiagrams showing how kprobes, the tc filter, and our maps interact with one another when determining whether a connection needs to be blocked.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003ch3\u003e\u003cspan\u003eThe management daemon\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWe built a daemon, which manages the eBPF programs we install and emits logs to \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/data-infrastructure/scribe/\"\u003e\u003cspan\u003eScribe\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e from our perf events. The daemon also provides the ability to update our TC filter, handles configuration changes (leveraging \u003c/span\u003e\u003ca href=\"https://research.fb.com/wp-content/uploads/2016/11/holistic-configuration-management-at-facebook.pdf\"\u003e\u003cspan\u003eFacebook’s Configerator\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e), and monitors health.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eOur eBPF programs are also bundled with this daemon. This makes management of releases easier to deal with, as we only have one software unit to monitor instead of needing to track a daemon and eBPF release. Additionally, we can modify the schema of our BPF tables, which both user space and kernel space consult, without compatibility concerns between releases.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eTechnical challenges\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eAs one would expect, we encountered a number of interesting technical challenges while rolling out SSLWall at Facebook’s scale. A few highlights include:\u003c/span\u003e\u003cspan\u003e\u003cbr/\u003e\n\u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003ca href=\"https://en.wikipedia.org/wiki/TCP_Fast_Open\"\u003e\u003cspan\u003eTCP Fast Open (TFO)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e: We hit an interesting challenge around kprobe and TC filter execution order that was exposed by our use of TFO within the infra. In particular, we needed to move some of our flow tracking code to a kprobe prehandler.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eBPF Program Size Limit: All BPF programs are subject to size and complexity limits, which may vary based on the kernel version.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003ePerformance: We spent many engineering cycles optimizing our BPF programs, particularly the TC filter, so that SSLWall’s CPU impact on some of our critical high QPS services with high fanout remained trivial. Identifying early exit conditions and using BPF arrays over LRUs where possible proved effective.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\u003cspan\u003eTransparentTLS and the long tail\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eWith enforcement in place, we needed a way to address noncompliant services without significant engineering time. This included things like torrent clients, open source message queues, and some Java applications. While most applications use common internal libraries where we could bake this logic in, the ones that do not need a different solution.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eEssentially, the team was left with the following requirements for what we refer to as Transparent TLS (or TTLS for short):\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eTransparently encrypt connections without the need for application changes.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003eAvoid double encryption for existing TLS connections.\u003c/span\u003e\u003c/li\u003e\n\u003cli aria-level=\"1\"\u003e\u003cspan\u003ePerformance can be suboptimal for this long tail.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan\u003eIt’s clear that a proxy solution would have helped here, but we needed to ensure that the application code didn’t need to change and that configuration would be minimal.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe settled on the following architecture: \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg\" alt=\"\" width=\"1920\" height=\"739\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=916,353 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=768,296 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=1024,394 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=1536,591 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=96,37 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=192,74 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe challenge with this approach is transparently redirecting application connections to the local proxy. Once again, we use BPF to solve this problem. Thanks to the cgroup/connect6 hook, we can intercept all \u003c/span\u003e\u003ca href=\"https://man7.org/linux/man-pages/man2/connect.2.html\"\u003e\u003cspan\u003econnect(2)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e calls made by the application and redirect them to the proxy as needed.\u003c/span\u003e\u003c/p\u003e\n\u003cfigure id=\"attachment_17788\" aria-describedby=\"caption-attachment-17788\"\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg\" alt=\"Diagram showing application and proxy logic for transparent connect.\" width=\"1920\" height=\"1181\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=916,563 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=768,472 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=1024,630 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=1536,945 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=96,59 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=192,118 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003cfigcaption id=\"caption-attachment-17788\"\u003eDiagram showing application and proxy logic for transparent connect.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003cp\u003e\u003cspan\u003eAside from the application remaining unchanged, the BPF program makes policy decisions about routing through the proxy. For instance, we optimized this flow to bypass the proxy for all TLS connections created by the application to avoid double encryption.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis work on enforcement has brought us to a state where we can confidently say that our traffic is encrypted at our scale. However, our work is not yet complete. For instance, there are many new facilities that have come about in BPF that we intend to leverage as we remove old kernel support. We can also improve our transparent proxy solutions and leverage custom protocols to multiplex connections and improve performance.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003eWe’d like to thank Takshak Chahande, Lingnan Gao, Andrey Ignatov, Petr Lapukhov, Puneet Mehra, Kyle Nekritz, Deepak Ravikumar, Paul Saab, and Michael Shao for their work on this project.\u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eOur infrastructure supports thousands of services that handle billions of requests per second. We’ve previously discussed how we built our service encryption infrastructure to keep these globally distributed services operating securely and performantly. This post discusses the system we designed to enforce encryption policies within our network and shares some of the lessons we learned [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/12/security/enforcing-encryption/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/12/security/enforcing-encryption/\"\u003eEnforcing encryption at scale\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/07/encryption_hero.jpg",
      "date_published": "2021-07-12T16:00:56Z",
      "author": {
        "name": "By Neel Goyal, Ajanthan Asogamoorthy, Mingtao Yang"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/",
      "title": "Ribbon filter: Practically smaller than Bloom and Xor",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003ch2\u003e\u003cspan\u003eWhat the research is:\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eThe Ribbon filter is a new data structure that is more space-efficient than the popular Bloom filters that are widely used for optimizing data retrieval. One of the ways that Bloom, and now Ribbon, filters solve real engineering problems is by providing smooth configurability unmatched by other filters. Bloom filters work by overapproximating a set of keys associated with some data resource. With a Bloom filter, almost all negative queries to that resource can be skipped (filtered) because the Bloom filter rejects almost all query keys not associated with the resource.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith proper data layout and design, the Ribbon filter is the first Bloom alternative to match the near-continuous, hazard-free, accuracy-versus-space trade-off provided by Bloom filters.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHere, near-continuous means efficiently utilizing any amount of memory to represent any number of keys, so that wasted memory such as in internal fragmentation can be minimized to zero. The typical hazard to the accuracy-versus-space trade-off is bit alignment, where some data sizes (e.g., 4, 8, or 16 bits per key) are faster to access than others. Like Bloom, our data layout for Ribbon does not suffer this hazard in access times so it is more freely configurable. And Ribbon filters add new freedom of configurability to the space-versus-time trade-off.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eBuilding on some prior lines of research, the Ribbon filter combines a simplified, faster, and more flexible construction algorithm; a data layout optimized for filter queries; and near-continuous configurability to make a practical alternative to static (immutable) Bloom filters.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWhile well-engineered Bloom filters are extremely fast, they use roughly 50 percent more space (overhead) than the information-theoretic lower bound for filters on arbitrary keys. When Bloom filters cannot meet an application’s space efficiency targets, Ribbon filter variants dominate in space-versus-time trade-offs with near continuous configurability and space overhead as low as 1 percent or less. Ribbon filters have O(1) query times and save roughly 1/3 of memory compared with Bloom filters.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow it works: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eLike some related immutable structures used for perfect hashing and maps, Ribbon filters are constructed by solving a linear system given by hash functions applied to a set of keys. Each row in the linear system expresses that querying as some key, which involves XOR-ing the values at some set of array indices, must yield a prescribed value to indicate it is “in” the set of keys. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eDespite using Boolean — GF(2) — arithmetic, the approach to solving this logical system of equations is to use Gaussian elimination, which fundamentally means subtracting equations from one another until you can isolate variables (unknowns). If a solution exists, this approach will find it.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe name Ribbon has dual meanings. First, we use a linear system from Dietzfelbinger and Walzer whose sorted coefficient matrix resembles a physical ribbon, or a wavy approximation of a band matrix. Gaussian elimination is fundamentally more efficient on this system because it is already close to a reduced form.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eRibbon also stands for Rapid Incremental Boolean Banding ON the fly, which is the name of our fast and flexible new Gaussian solver. Through an approach resembling insertion into a linear-probed hash table, Ribbon does Gaussian elimination on the fly. This saves time and space in construction because row reductions can be done in registers rather than in memory, and because the reduced form of the ribbon coefficient matrix — a band matrix — is more space-efficient than explicitly representing the ribbon form. On-the-fly construction also unlocks a solution to the core challenge of the Ribbon approach: scaling its space efficiency to very large numbers of keys.  \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eWhy it matters: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eAt Facebook’s scale, we expect Ribbon filters to save several percent of RAM resources, with a tiny increase in CPU usage for some major storage systems. However, we do not implement efficiency gains at all engineering costs, so it’s also important to have a user-friendly data structure. This issue stalled implementation of other Bloom alternatives offering some space savings. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe Ribbon filter opens these new trade-offs without introducing notable discontinuities or hazards in the configuration space. In other words, there is some complexity to make Ribbon filters general and highly configurable, but these details can be hidden behind a relatively simple API. You have essentially free choice over any three of the four core performance dimensions — number of keys added to the set, memory usage, CPU efficiency, and accuracy — and the accuracy is automatically well optimized.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eRead the full paper: \u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2103.02515\"\u003e\u003cspan\u003eRibbon filter: Practically smaller than Bloom and Xor\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWhat the research is: The Ribbon filter is a new data structure that is more space-efficient than the popular Bloom filters that are widely used for optimizing data retrieval. One of the ways that Bloom, and now Ribbon, filters solve real engineering problems is by providing smooth configurability unmatched by other filters. Bloom filters work [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/\"\u003eRibbon filter: Practically smaller than Bloom and Xor\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg",
      "date_published": "2021-07-09T16:00:16Z",
      "author": {
        "name": "By Peter Dillinger"
      }
    },
    {
      "id": "",
      "url": "https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/",
      "title": "Asicmon: A platform agnostic observability system for AI accelerators",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n\n\t\t\u003cp\u003e\u003cb\u003e\u003ci\u003eWe will be hosting a talk about our work on, “\u003ca href=\"https://atscaleconference.com/events/systems-scale-summer-2021/\"\u003eA Platform Agnostic Observability System for AI Accelerators\u003c/a\u003e” during our virtual \u003ca href=\"https://atscaleconference.com/events/systems-scale-summer-2021/\"\u003eSystems @Scale\u003c/a\u003e event at 10:20 a.m. PT on Wednesday, June 30, followed by a live Q\u0026amp;A session. Please submit any questions to \u003ca href=\"mailto:systemsatscale@fb.com\"\u003esystemsatscale@fb.com\u003c/a\u003e before the event.\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAccelerators are special-purpose hardware devices optimized for specific applications, like AI prediction and video encoding. And Application-specific hardware platforms play an important role in meeting the growing latency and compute demands of workloads like deep learning, content understanding, and video encoding.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAt Facebook, the inevitable rise in use of accelerators in our data centers has led to better performance and energy efficiency. However, it is challenging to operate these heterogeneous platforms efficiently at scale. To ensure that these complex accelerators operate smoothly, we need an excellent observability system with monitoring and tracing capabilities so we can understand the performance and interactions between CPUs and accelerators.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo meet these challenges, we’ve introduced three new tools:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eASIC Monitoring (Asicmon)\u003c/b\u003e\u003cspan\u003e, a scalable observability framework. Asicmon’s library abstracts an accelerator’s custom interfaces and provides a standard interface to our internal tools. Asicmon has facilitated load balancing, performance monitoring, and automated health checks for hundreds of thousands of accelerators running in our data centers.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eAsimov\u003c/b\u003e\u003cspan\u003e, a custom specification language that makes developing and rapid prototyping new accelerators easier. It has shrunk our development time for onboarding a new accelerator from a month to under a week.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eAtrace\u003c/b\u003e\u003cspan\u003e, an accelerator tracing solution that collects traces remotely on production servers. It allows us to inspect accelerator systems in detail and provides actionable trace summaries and analyses. An initial version of Atrace allowed us to close a 10 percent performance gap between \u003c/span\u003e\u003ca href=\"https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/\"\u003e\u003cspan\u003eCaffe2 and PyTorch implementations\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e of a large AI model. \u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eBackground\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eFacebook’s cloud infrastructure handles about 150 trillion AI predictions per day for tasks ranging from feed recommendations to combating harmful content. Running these AI models comes with heavy infrastructure demands. And as these models improve, so do their \u003ca href=\"https://arxiv.org/pdf/2003.09518.pdf\"\u003ecomputational requirements\u003c/a\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe graph below of AI model adoption at Facebook illustrates this \u003c/span\u003e\u003ca href=\"https://arxiv.org/pdf/2003.09518.pdf\"\u003e\u003cspan\u003eunmistakable pattern\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?w=852\" alt=\"\" width=\"852\" height=\"916\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png 988w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=852,916 852w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=768,826 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=953,1024 953w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=96,103 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=192,206 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe need for accelerators\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eGood old general-purpose processors (CPUs) offer versatility and have grown exponentially faster over the decades. However, CPUs fail to meet the rising\u003c/span\u003e\u003ca href=\"https://openai.com/blog/ai-and-compute/\"\u003e \u003cspan\u003ecomputational demands of AI applications\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e today. They also tend to exhibit inefficiency in terms of energy used per AI prediction. As investigated by the OpenAI community, we’ve seen \u003c/span\u003e\u003ca href=\"https://openai.com/blog/ai-and-compute/\"\u003e\u003cspan\u003etwo distinct eras of compute in AI models\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. \u003c/span\u003e\u003cspan\u003eIn recent times, model complexity and compute requirements for AI have grown by roughly a factor of 10 each year. \u003c/span\u003e\u003cspan\u003eThis far outpaces improvements in CPU performance.   \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eHow do we remedy this? By designing hardware that is customized to accelerate AI operations via application-specific integrated circuits (ASICs).  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eSince 2019, Facebook has invested \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2019/03/14/data-center-engineering/accelerating-infrastructure/\"\u003e\u003cspan\u003eheavily in deploying accelerator-based servers\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to provide higher performance and energy efficiency. Today, our first-generation systems are 10-30x more performant on our largest AI models. They also delivered a 3-10x performance-per-watt improvement over a CPU.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe also invested in specialized hardware for \u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2021/04/05/video-engineering/how-facebook-encodes-your-videos/\"\u003e\u003cspan\u003evideo encoding\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and decoding. This enables Facebook to process the nearly 250 million videos uploaded to our app each day. These videos are viewable on any device and with varying internet bandwidth. Our first-generation video accelerators delivered a 10x performance-per-watt improvement in processing 4K videos.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe figure below illustrates the design of our AI inference server. As you can see, it consists of two Twin Lake CPUs and multiple accelerators (M.2 modules) connected to them using a PCIE switch\u003c/span\u003e\u003cspan\u003e.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?w=1024\" alt=\"\" width=\"1024\" height=\"615\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png 2000w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=916,550 916w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=768,461 768w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=1024,615 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=1536,922 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=96,58 96w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=192,115 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eThe challenges of operating accelerators\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eIn your typical cloud server, the CPU represents the most complex component. We focus a lot on building software to efficiently operate the CPU and monitor its performance and availability. However, with an accelerator system, we can imagine the CPU now has a complicated and brawnier sibling! The accelerator, or ASIC, represents a complex hardware and software system in its own right.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eTo deliver an excellent user experience, the cloud infrastructure needs to keep hundreds of thousands of accelerators running reliably and efficiently. This is where observability systems come to our rescue. Observability allows us to understand what happens in the accelerator hardware and software when any issue arises. It is useful in multiple ways: \u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eHealth monitoring:\u003c/b\u003e\u003cspan\u003e Just like any other piece of hardware, accelerators can overheat or hit a faulty condition or a functional bug. We can track various health metrics for the ASICs and use them in automated systems. These systems can then (if needed) remediate the issue by rebooting the accelerator or moving it into a repair state.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003ePerformance monitoring:\u003c/b\u003e\u003cspan\u003e By monitoring the performance and system load on an accelerator, we can efficiently scale our AI jobs to meet variable demand throughout the day. It also enables us to detect regressions in performance with new software deployments.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003ePerformance profiling:\u003c/b\u003e\u003cspan\u003e When we encounter issues such as poor performance or time-outs, we need to look deeper into how the accelerator server is functioning. We also need to equip software developers with tools to understand the performance of their applications while they run on accelerators. \u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eThe accelerator zoo\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eSpecialization is both a boon and bane for accelerators. As a result, we end up running multiple types of accelerators in our data centers at any given point.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eIn 2020 we started\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2019/03/14/data-center-engineering/accelerating-infrastructure/\"\u003e \u003cspan\u003edeploying the first generation\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e of these accelerators. In the near future, we will be developing two to three new accelerators for the second generation. Each accelerator will have unique driver interfaces, making the task of operating them harder. But duplicating the observability software for each accelerator would not be feasible in the timeline we have set out. The observability framework must be easy to prototype and adapt to multiple types of accelerators in a short time. It also needs to be efficient to avoid interfering with the original application. \u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eHow we developed Asicmon and Asimov\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eOur first challenge involved finding a way to effectively monitor different types of accelerators without duplicating code (and developer time). As you may have guessed, we can leverage abstraction to achieve this. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor example, consider an abstract metric: \u003c/span\u003e\u003ci\u003e\u003cspan\u003edevice_utilization\u003c/span\u003e\u003c/i\u003e\u003cspan\u003e — the measure of how busy an accelerator is — which becomes useful for balancing load across accelerators. To compute this metric, we may need to understand the internal architecture of the accelerator. With an abstract counter, however, engineers working on load balancing can more easily use the metric without being aware of finer details.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ci\u003e\u003cspan\u003edevice_utilization = max(compute_core_active_i) /  total_time \u003c/span\u003e\u003c/i\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWith the above in mind, we designed Asicmon with these design objectives: \u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eAbstraction:\u003c/b\u003e\u003cspan\u003e We needed a simple and uniform interface for all of our internal monitoring and operational tools to use. This enables infrastructure engineers and hardware teams to effectively operate multiple accelerators in a common way.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eDevelopment velocity:\u003c/b\u003e\u003cspan\u003e Accelerators are new. Interfaces can also change due to evolving requirements. The framework should be easy to learn and able to iterate quickly.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003ePerformance:\u003c/b\u003e\u003cspan\u003e Finally, any observability system should be lightweight in terms of resources. As a result, it diminishes interference with high-throughput video and AI applications.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eThe diagram below illustrates the overall software stack for monitoring accelerators. Asicmon acts as a bridge between individual accelerator drivers and the rest of the internal monitoring software. The left top illustrates automated health check tools that spot bad health signals and\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/12/09/data-center-engineering/how-facebook-keeps-its-large-scale-infrastructure-hardware-up-and-running/\"\u003e \u003cspan\u003eautomatically fix faulty ASICs\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e. On the right, a telemetry daemon periodically publishes performance metrics for engineers to inspect the accelerators. Furthermore, automated load balancing and auto-scaling systems like\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\"\u003e \u003cspan\u003eShard Manager\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e utilize these counters. \u003c/span\u003e\u003cspan\u003e \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eHow does Asicmon work?  \u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eUnder the hood, Asicmon creates an instance of a monitoring module per accelerator device. It maintains a cache of statistics that it updates periodically by probing the accelerator driver and computing-derived metrics. Queries to Asicmon’s standard interface for counters get implemented as a lookup into this cache. This shields the system against accidental overload of counter requests.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eEnter Asimov\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eAll great so far! We used abstraction to address the scalability aspect of observability software layers above Asicmon. However, the problem of building the glue code between the accelerator driver and these standard metrics still eluded us. This has to be done separately for each of the accelerators that have aggressive and overlapping timelines. So, we needed a method to develop on Asicmon that was quick to iterate and easy to ramp up on, while also being efficient. That’s where Asimov comes in. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png 1999w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAsimov is an expressive Python-like custom language to instrument the accelerator driver. It essentially allows developers to focus on how to probe the accelerator interfaces and express derived metrics using them. The Asimov compiler generates an efficient C++ implementation of the monitoring module. It also handles details like caching the metrics, periodically reading them, and providing thread safety.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe code snippets below show examples of Asimov being used to read system metrics using interfaces ranging from Linux sysfs files (a) to custom library C functions (b).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAsimov incorporates the same standard interface as Asicmon in its internal representation (the stats data structure, left hand side in the code). We can also invoke C-library functions provided by the device driver and express equations/conditions for derived metrics like any regular language.  \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAsimov is built with the\u003c/span\u003e\u003ca href=\"https://www.antlr.org/\"\u003e \u003cspan\u003eANTLR\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e compiler framework under the hood to provide the lexer/parser logic for the language. We then emit C++ code using templates that manage all the essential parts, like initialization, thread safety, etc., so someone using Asimov doesn’t need to worry about it.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAsicmon in action\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eLet’s look at a few illustrative examples of how Asimov and Asicmon are beneficial for operating accelerators at scale.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor AI inference applications, we use a system called\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\"\u003e \u003cspan\u003eShard Manager\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e to automatically scale the inference service instances. A shard is essentially a copy of the AI model that can serve inferences. Asicmon measures the load on the device using an abstract metric — accelerator\u003c/span\u003e \u003cspan\u003edevice\u003c/span\u003e \u003cspan\u003eutilization. This helps Shard Manager effectively balance the load among servers and automatically scale up or down the number of shards. The diagram below explains how the number of shards gets scaled automatically during model update rollouts and increases in traffic.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThe figure below illustrates the advantages of building observability early on in a project’s development cycle. In our test deployment for video accelerators, we detected a memory leak using an Asicmon counter for available device memory. It took multiple fixes to the driver to finally resolve the issue, well in time before its debut in production.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFinally, let’s take a look at the ease of prototyping with Asimov. While we certainly took longer to build the first version of Asimov alongside the first video accelerator, supporting the second one (the AI inference accelerator) went incredibly fast. Bootstrapping basic metrics for the AI inference accelerator took less than a week. Since implementing Asicmon we’ve been able to increase our AI accelerator metrics support from ~30 percent to ~75 percent\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eAtrace: Accelerator tracing at scale\u003c/span\u003e\u003c/h2\u003e\n\u003ch3\u003e\u003cspan\u003eWhy tracing?\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eNow that we can monitor the performance of accelerators in our data centers, the next step involves addressing why performance metrics like the latency and throughput change over time. The tried-and-tested method for CPUs involves leveraging a stack-based profiler to sample the running function call stack at periodic intervals. However, for inference accelerators, tracing is the best form of profiling. Why? Because accelerators use special hardware units and thus do not have an equivalent notion of a function stack on a core. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eAs shown in the figure below, a trace essentially consists of a time series of events occurring on different parts in a system. Events in a trace can represent, among many things, functions, execution of AI operators, or data transfers. Traces offer deeper insights into the operation of the system, including understanding the latency and scheduling of operators and how the CPU and accelerator interact with each other. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eDesigning the tracing system\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eWhile AI inference accelerator vendors do provide tools and APIs to collect traces from the device. These tools are designed to work on a single server and are often hard to use. In order to profile production systems better, we set out building a layer on top of this native capability. This better scales out the collection, processing, and analysis of traces themselves. \u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe kept two target use cases in mind while developing Atrace: \u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eModel development:\u003c/b\u003e\u003cspan\u003e Model developers would typically be attempting to target their AI models to new inference hardware. They can run the tracing tool locally. But by integrating it with internal visualization and summarization tools, we can provide quicker feedback to engineers to iteratively tune their model.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eProduction:\u003c/b\u003e\u003cspan\u003e Debugging performance issues in production is an important use case for tracing. For instance, say a continuous integration (CI) test detects a regression in performance. By collecting traces remotely and on the fly, production engineers can quickly diagnose the problem.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cspan\u003eTo develop a scalable and ubiquitous tracing solution, we built a set of components that remotely trigger and collect traces. We save each trace to a shared storage and post process and summarize it. The diagram below outlines this, starting on the left with the trace being triggered, to the trace collection and post processing on the right.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eInsights from accelerator traces\u003c/span\u003e\u003c/h2\u003e\n\u003ch3\u003e\u003cspan\u003eTrace profiles and summaries\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eTraces themselves can be enormous and overwhelming to dive into directly. However, we can learn a great deal about an AI program by summarizing the trace at a high level. To achieve this, we built a summary of trace statistics grouped by various AI operator types, as shown below.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?w=1024\" alt=\"\" width=\"1024\" height=\"576\" srcset=\"https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=192,108 192w\" sizes=\"(max-width: 992px) 100vw, 62vw\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis operator breakdown shows our engineers which operators consume the most execution time and merit optimization. It also allows for comparisons and debugging of performance regressions between two software versions.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eTrace critical path analysis\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eFor advanced users, who might want to delve deeper into the traces, we added visualization support for both the open source\u003c/span\u003e\u003ca href=\"https://www.chromium.org/developers/how-tos/trace-event-profiling-tool\"\u003e \u003cspan\u003eChrome trace viewer\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e and an internal trace visualization tool from Facebook. It works all from a single click. We can also run automated analysis on the trace to infer the critical path of operators. This uses the dependency graph of the AI model and trace statistics.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eThis analysis lets us optimize the latency of the AI prediction. It can also highlight issues like an imbalance in operators. Doing so closed a 10 percent latency gap between the Caffe2 and PyTorch versions of one of our AI models.\u003c/span\u003e\u003c/p\u003e\n\u003ch3\u003e\u003cspan\u003eTrace correlation\u003c/span\u003e\u003c/h3\u003e\n\u003cp\u003e\u003cspan\u003eLastly, it is also noteworthy that several software layers exist to handle the processing of an inference request. These include the application layer, PyTorch framework, and\u003c/span\u003e\u003ca href=\"https://engineering.fb.com/2018/09/13/ml-applications/glow-a-community-driven-approach-to-ai-infrastructure/\"\u003e \u003cspan\u003eGlow\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e, an open source graph lowering compiler for accelerators.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eFor more complex models involving video understanding or natural language processing, we learned that the model may be run partially on a CPU and partially on an accelerator. Thus, tracing the operations across multiple layers on the CPU and correlating them with the accelerator becomes a necessity.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan\u003eWe developed a\u003c/span\u003e\u003ca href=\"https://github.com/pytorch/glow/pull/5568\"\u003e \u003cspan\u003eprototype of trace correlation\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e into Glow and PyTorch. This allowed us to connect operations on the CPU in the Glow runtime, to the accelerator. Trace correlation is important for examining the complex software stack used for AI inference.\u003c/span\u003e\u003c/p\u003e\n\u003ch2\u003e\u003cspan\u003eNext steps\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cspan\u003eIn addition to continuing to support next-generation AI and video accelerators using Asimov and the Asicmon we are also exploring:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cb\u003eOpen source specifications:\u003c/b\u003e\u003cspan\u003e There are multitudes of companies building accelerator chips today. But the monitoring interfaces for accelerators lack standardization. We are collaborating with the\u003c/span\u003e\u003ca href=\"https://www.opencompute.org/wiki/Server/ODSA\"\u003e \u003cspan\u003eOpen Domain-Specific Accelerators (ODSA)\u003c/span\u003e\u003c/a\u003e\u003cspan\u003e project so the industry as whole can benefit from a common specification.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eTrace visualization and analysis:\u003c/b\u003e\u003cspan\u003e We are investigating ways to automatically generate optimization recommendations from the trace and support better visualizations, such as integrating with TensorBoard.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cb\u003eDistributed tracing:\u003c/b\u003e\u003cspan\u003e Since microservices do not run in isolation, we plan on exploring how to correlate distributed traces collected by the Canopy distributed tracing tool with system-level accelerator traces. This would allow us to debug the end-to-end latency of microservices that use AI accelerators.\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\u003cspan\u003eThanks\u003c/span\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e\u003cspan\u003eWe would like to thank our many collaborators at Facebook, including Jerry Liu, Thiara Ortiz, Jeremy Yang, Ashwin Poojary, Deng Pan, Craig Ross, Ashwin Narasimha, Gisle Dankel, Michael Anderson, Allan Di Wu, Yinghai Lu, Satish Nadathur, Garret Catron, and Jack Montgomery for supporting us in creating this framework.\u003c/span\u003e\u003c/em\u003e\u003c/p\u003e\n\n\t\t\n\t\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cp\u003eWe will be hosting a talk about our work on, “A Platform Agnostic Observability System for AI Accelerators” during our virtual Systems @Scale event at 10:20 a.m. PT on Wednesday, June 30, followed by a live Q\u0026#38;A session. Please submit any questions to systemsatscale@fb.com before the event. Accelerators are special-purpose hardware devices optimized for specific [...]\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"btn btn-secondary understrap-read-more-link\" href=\"https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/\"\u003eRead More...\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe post \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/\"\u003eAsicmon: A platform agnostic observability system for AI accelerators\u003c/a\u003e appeared first on \u003ca rel=\"nofollow\" href=\"https://engineering.fb.com\"\u003eFacebook Engineering\u003c/a\u003e.\u003c/p\u003e\n",
      "image": "https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png",
      "date_published": "2021-06-28T16:00:24Z",
      "author": {
        "name": "By Brian Coutinho, Hao Wang, David Carrillo-Cisneros, Cynthia Liu, Parth Malani"
      }
    }
  ]
}
