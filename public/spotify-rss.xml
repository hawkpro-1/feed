<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Spotify</title>
    <link>https://engineering.atspotify.com/</link>
    <description></description>
    <item>
      <title>&#xA;                                            Spotify’s Player API: Your Toolkit for Controlling Spotify Programmatically&#xA;                                        </title>
      <link>https://engineering.atspotify.com/2022/04/spotifys-player-api/</link>
      <description>In 2017, we launched the Spotify Connect Web API, a set of tools that developers could use to programmatically start, stop, and manage Spotify audio playback from the web. This post presents an overview of what you can do with the API, now called the Player API, and some background information about</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 14, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/04/spotifys-player-api/" title="Spotify’s Player API: Your Toolkit for Controlling Spotify Programmatically">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/04/Spotify-Player-API_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/04/Spotify-Player-API_Header.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/04/Spotify-Player-API_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/04/Spotify-Player-API_Header-700x344.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/04/Spotify-Player-API_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/04/Spotify-Player-API_Header-120x59.png 120w" sizes="(max-width: 1200px) 100vw, 1200px"/>                    </a>
                        
        </p>

        

        
<p>In 2017, we launched the <a href="https://developer.spotify.com/documentation/web-api/reference/#/operations/get-information-about-the-users-current-playback" target="_blank" rel="noreferrer noopener">Spotify Connect Web API</a>, a set of tools that developers could use to programmatically start, stop, and manage Spotify audio playback from the web. This post presents an overview of what you can do with the API, now called the Player API, and some background information about how it came to exist.</p>



<p>Prior to shipping the Connect Web API, Spotify offered a public interface that allowed developers to connect to Spotify’s Mac desktop application from another app running on Mac OS X. Through AppleScript, third-party developers could start or stop the music that was playing from Spotify’s Mac app and observe the title of the track that was playing. Developers used this opportunity to extend the Spotify experience into a variety of creative apps for their own personal use, for friends, and for other Spotify users.</p>



<p>The Spotify Connect Web API extended these innovations to the web, and it now allows developers to control Spotify playback from any internet-connected device or app. The Player API has been used by thousands of developers to make third-party apps for home automation, social listening, music discovery, and much more.</p>



<p>In 2018, we enhanced the Player API further with the launch of our <a href="https://developer.spotify.com/documentation/web-playback-sdk/" target="_blank" rel="noreferrer noopener">Web Playback SDK</a>: a JavaScript SDK that enables developers to create and control a fully functional player in the browser. When paired with the Player API, our Web Playback SDK can be used to build a frontend application that contains a fully functional web player for Spotify Premium users.</p>



<p><br/>In 2020, the Player API was further improved by the addition of an add-to-queue API — a new endpoint that developers can use to add tracks to the list of songs or podcast episodes that are playing next. We’re excited about what’s in store for the future of the Player API, and about the apps that developers are making with its functionality.</p>



<h2>What can I do with it?</h2>



<h3>Issue commands</h3>



<p>At its core, the Player API empowers your web application to tell Spotify what audio to play and where and how to play it. In order to make commands, your app will need an OAuth access token authorized with the <a href="https://developer.spotify.com/documentation/general/guides/authorization/scopes/#user-modify-playback-state">user-modify-playback-state OAuth scope</a>. After your app has been granted a token with this permission by a user, your app can use that token to send commands to the Spotify Web API from any device connected to the internet. Note that most commands will only work for users who have a Spotify Premium subscription.</p>



<h3>Starting playback</h3>



<p><code>PUT https://api.spotify.com/v1/me/player/play</code></p>



<p>The namesake feature of our Player API is the <code>/v1/me/player/play</code> endpoint — your tool for beginning or continuing the playback of music or podcasts on Spotify. </p>



<p>It’s more powerful than the play button that you see in Spotify’s mobile app. When you make a request to the /v1/me/player/play  endpoint, your third-party app can specify tracks by ID, or pass a <strong>context</strong> that you’d like to play. Contexts can be a playlist, an album, or a collection, like your Liked Songs.</p>



<p>You can also tell Spotify which <strong>device</strong> you’d like to listen to by providing Spotify with a device identifier. The device can refer to a Web Playback SDK app, a running instance of a Spotify app, or one of your Spotify Connect-enabled speakers. You can find the device identifiers that are available to control through our Devices API. Note that some devices are not supported.</p>



<p>When you use the play endpoint, you can also tell Spotify how loud the music should be — a score from 0 to 100 — and you can also ask Spotify to offset playback by one or more playlist items. This could be useful if you want to start playback from the third song in a playlist, for example.</p>



<h3>Stopping playback</h3>



<p><code>PUT https://api.spotify.com/v1/me/player/pause</code></p>



<p>This endpoint is simple: it stops the audio that’s currently playing. After pausing playback with the pause endpoint, you can start it again by sending a request to <code>/v1/me/player/play</code> without any options.</p>



<h3>Turning up (or down) the volume</h3>



<p><code>PUT https://api.spotify.com/v1/me/player/volume</code></p>



<p>This command sets the volume between 0 and 100. Some apps use it to add a volume slider or switch into their user interface.</p>



<h3>Adding songs to the queue</h3>



<p><code>PUT https://api.spotify.com/v1/me/player/queue</code></p>



<p>You can use the Player API to add songs or podcast episodes to the “Up next” queue in your Spotify app. Use this endpoint carefully — there is no API yet for removing tracks from the queue, or for reading the contents of the queue.</p>



<h3>Audio seeking</h3>



<p><code>PUT https://api.spotify.com/v1/me/player/seek</code></p>



<p>Your app can use the seek endpoint to move the audio cursor to a given millisecond of the track or podcast episode that is currently playing. After the command is executed, Spotify will begin playback from that point in the song or episode. Some apps — like those using the Web Playback SDK — might use this endpoint to build an audio scrubbing interface.</p>



<h2>Observe state</h2>



<p>Issuing playback commands to Spotify would be hard without the ability to observe the state of the player and Connect environment. Luckily, the Player API includes methods you can use to do just that. You can find some of the more popular observation endpoints below. Calling these endpoints requires a token that’s been authorized with the <a href="https://developer.spotify.com/documentation/general/guides/authorization/scopes/#user-read-playback-state" target="_blank" rel="noreferrer noopener">user-read-playback-state OAuth scope. </a></p>



<h3>List devices</h3>



<p><code>GET https://api.spotify.com/v1/me/player/devices</code></p>



<p>You can use the devices API endpoint to fetch a list of active devices that you can target with playback commands. The list will include any open Web Playback SDK apps, many online Spotify Connect speakers, and Spotify-built apps that you may have open, like the <a href="https://open.spotify.com">web player</a>. </p>



<h3>See what’s playing</h3>



<p><code>GET https://api.spotify.com/v1/me/player</code></p>



<p>Making a GET request to the player endpoint will provide you with basic information about the currently playing context, the device that’s streaming, and other information about the playback state like whether or not shuffle is enabled and if the songs will repeat or not.</p>



<h2>What you can build with the Player API</h2>



<h3>Party apps and social listening experiences</h3>



<p>Music fans commonly use Spotify Connect on social occasions to listen to music with friends and family. You can use the Player APIs to build an app that helps with your next house party or backyard barbeque. </p>



<p>For example, the Player API allows you to fetch information about the currently playing track. Your app can combine this information with the<a href="https://developer.spotify.com/documentation/web-api/reference/#/operations/get-audio-features" target="_blank" rel="noreferrer noopener"> Audio Features API</a> to learn about the estimated danceability or tempo of the currently playing song, and integrate this information with a programmable lighting system to match the mood.</p>



<p>Developers can also combine the Player APIs with our <a href="https://developer.spotify.com/documentation/web-playback-sdk/" target="_blank" rel="noreferrer noopener">Web Playback SDK</a> to make a virtual “listening party” experience. By tracking the current point in the song and using the seek API endpoint when necessary, your app can allow groups of Spotify users to log into a web app, listen to music at the same time and share control over which songs will be played next. </p>



<h3>Home automation integrations</h3>



<p>Many home automation systems are extensible or open source. Developers can use the Player APIs to connect Spotify with smart home systems and make audio a part of their routine. For example, you could use the <code>/v1/me/player/play</code> endpoint to begin streaming your favorite morning podcast at the same time as a programmable coffee machine starts to brew coffee.</p>



<h2>Getting started</h2>



<p>With some programming knowledge, any Spotify Premium user can set up a Player API app or develop one for their own use. You can connect with other developers who are using the API in the <a href="https://community.spotify.com/t5/Spotify-for-Developers/bd-p/Spotify_Developer" target="_blank" rel="noreferrer noopener">Spotify for Developers forum</a>, or find examples of third-party apps in the <a href="https://developer.spotify.com/community/showcase/" target="_blank" rel="noreferrer noopener">Developer Showcase</a>.</p>



<p>We hope you get creative with it and make inventions that add to the Spotify experience for yourself or for your friends. To get started, visit <a href="https://developer.spotify.com/" target="_blank" rel="noreferrer noopener">Spotify for Developers</a> and log in with your Spotify account to register your first app. Happy coding! </p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Josh Brown, Senior Developer Advocate</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/04/Spotify-Player-API_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 14 Apr 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Comparing quantiles at scale in online A/B-testing&#xA;</title>
      <link>https://engineering.atspotify.com/2022/03/comparing-quantiles-at-scale-in-online-a-b-testing/</link>
      <description>TL;DR: Using the properties of the Poisson bootstrap algorithm and quantile estimators, we have been able to reduce the computational complexity of Poisson bootstrap difference-in-quantiles confidence intervals enough to unlock bootstrap inference for almost arbitrary large samples. At Spotify, we c</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 23, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/03/comparing-quantiles-at-scale-in-online-a-b-testing/" title="Comparing quantiles at scale in online A/B-testing">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-700x348.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-768x382.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-1536x765.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR:</strong> Using the properties of the Poisson bootstrap algorithm and quantile estimators, we have been able to reduce the computational complexity of Poisson bootstrap difference-in-quantiles confidence intervals enough to unlock bootstrap inference for almost arbitrary large samples. At Spotify, we can now easily calculate bootstrap confidence intervals for difference-in-quantiles in A/B tests with hundreds of millions of observations. </p>



<h2>Why are quantiles relevant for product development?</h2>



<p>In product development, the most common impact analysis of product changes is often summarized by the change in the average of some metric of interest. This is a natural measurement, since changes in an average, in many contexts, map more or less directly to changes in business value. In addition, averages have convenient mathematical properties that make it straightforward to quantify uncertainty in over-served changes. </p>



<p>For a product development team to understand the impact of their work and be able to make strategic decisions, averages are not always sufficient. It is often of interest to understand how users with different behaviors are reacting to product changes. For example, a change in app load time often has the biggest impact on users with the slowest devices. Another example is app engagement — if our goal is to increase app engagement, it is often of interest to see if the engagement of the least engaged users increased, or if it was only users that were already highly engaged that increased their engagement more. For this purpose, quantiles and changes in quantiles, i.e., difference-in-quantiles due to product changes, is a powerful complement to the average. However, quantiles have less neat mathematical properties, which makes it less straightforward to quantify the uncertainty in observed changes in quantiles analytically. One robust method for quantile inference is bootstrap.     </p>



<h2>What is (Poisson) bootstrap inference?</h2>



<p><a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" target="_blank" rel="noreferrer noopener">Bootstrap inference</a> is an ingenious resampling-based strategy for non-parametrically estimating the sampling distribution of any statistic using only the observed sample at hand. Bootstrap was originally proposed by the statistical legend Bradley Efron (Efron, 1979). Since the original paper, many extensions of the original idea have been developed, and their properties studied under various conditions for various test statistics. </p>



<p>Standard statistical inference describes how an estimator, such as the sample mean, would behave if the experiment that produced the observed data were to be repeated over and over again. For many simple estimators, like the sample mean, what the behavior is like when the experiment is repeated can be described mathematically, and inference can easily be made. However, for some statistics like quantiles or ratios of means, it is often not possible to describe the sampling distribution analytically. In these situations, the bootstrap is a powerful tool. The idea of the bootstrap is almost painfully simple as it uses the same idea of repeating the experiment, but without having to mathematically derive what will happen. Simply put, instead of viewing the observed data as a sample from a population, bootstrap views the sample as the population. The ingenuity of this framing is that in order to describe the behavior of an estimator, we just need to take samples from our (pseudo)population — our experimental sample. Bootstrapping thus simply amounts to resampling observations from the observed sample with replacement, recording the estimate, and repeating this procedure. The collection of estimates describes the sampling distribution of the estimator from which inference can be made.</p>



<p>To fully appreciate the idea, let’s assume we have observed a sample consisting of five observations: 4, 8, 11, 13, 15. To use bootstrap inference, we randomly draw five observations from our sample with replacement. Table 1 shows the results from 10 bootstrap replications. In the first resample, we obtain the observations 11, 11, 13, 13, 15. The sample median in this bootstrap sample is 13, which we record. Repeating this resampling step a large number of times provides us with a characterization of the sample median’s sampling distribution through the array of bootstrap sample medians found at the bottom of Table 1. The table shows results from 10 resamples, but in practice the number of resamples is usually in the thousands. A (1-alpha)100% level confidence interval can be computed by finding the alpha/2 and (1-alpha/2) quantiles of the bootstrap sample medians. </p>



<p>Table 1: Example of bootstrap distribution of the sample median for a sample of 5 units over 10 bootstrap samples. </p>



<div><figure><img loading="lazy" width="627" height="248" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Table-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Table-1.png 627w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Table-1-250x99.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Table-1-120x47.png 120w" sizes="(max-width: 627px) 100vw, 627px"/></figure></div>



<p>The validity of bootstrap is well studied for a large class of statistics. See, for example, Ghosh et al. (1984) and Falk and Reiss (1989) for details regarding quantiles.</p>



<h3>Computational challenge</h3>



<p>The computationally intensive nature of bootstrap caused by repeated resampling has made the primary use case small-sample experiments. For big data experiments, like A/B tests in the tech industry with millions and even hundreds of millions of users, bootstrap inference was for many years intractable unless only a fraction of the total number of observations was considered. Recently, Poisson bootstrap (Hanley and MacGibbon 2006, Chamandy et al. 2012) has gained traction as a computationally efficient way of approximating the original bootstrap’s sampling with replacement with big data, at least for linear estimators such as means. </p>



<p>The computational challenge of bootstrap inference is obvious. The Poisson bootstrap was proposed not because it is less computationally complex but because it is easier to implement in a scalable fashion for some estimators. For example, Chamandy et al. showed how Poisson bootstrap inference can be implemented with map-reduce (Dean and Ghemawat, 2008). Another approach that enables fast implementations of quantile bootstrap is the so-called “Little bag of bootstrap” (Kleiner et al., 2014). This approach also utilizes Poisson bootstrap, but the computational challenge is handled by splitting the full sample problem into smaller samples, performing calculations and then combining the results to yield the full sample results. Most tech companies have avoided bootstrap inference for quantiles altogether, and are instead relying on parametric approximations with analytical solutions. An exception includes bootstrapping based on compressed data, as described for example in the Netflix blog posts on <a href="https://netflixtechblog.com/streaming-video-experimentation-at-netflix-visualizing-practical-and-statistical-significance-7117420f4e9a" target="_blank" rel="noreferrer noopener">practical significance</a> and <a href="https://netflixtechblog.com/data-compression-for-large-scale-streaming-experimentation-c20bfab8b9ce" target="_blank" rel="noreferrer noopener">data compression</a>. Most approximations are based on the normal distribution, see for example <a href="https://towardsdatascience.com/how-wish-a-b-tests-percentiles-35ee3e4589e7" target="_blank" rel="noreferrer noopener">this blog post</a> that uses results from Liu et al. (2019). However, as for any parametric test, the robustness of such approximations is highly dependent on the data-generating process.  </p>



<p>Interestingly, as we will discuss in the next section, there are other properties of the Poisson sampling and the quantile estimators themselves that can be used to reduce the complexity of the bootstrap algorithm to the extent that it will almost always be possible to perform bootstrap inference for quantiles and difference-in-quantiles without complex implementations. </p>



<h2>Our new algorithm: Bootstrapping without resampling</h2>



<p>To understand our proposed algorithm, one must understand the class of quantile estimators we are studying. There are many ways to <a href="https://en.wikipedia.org/wiki/Quantile#Estimating_quantiles_from_a_sample" target="_blank" rel="noreferrer noopener">estimate a sample quantile</a>; most include some form of weighting of the closest order statistics. We propose to use a slightly more crude estimator. Let the quantile of interest be q and the sample size be N. If (N+1)q is an integer, the corresponding order statistic is our estimator. If (N+1)q is not an integer, we randomly select between the two neighboring integers. For example, assume we are interested in the median, and N=101. Then the 51st observation in the ordered list of observations is our median estimate. If N=100, then (N+1)q=50.5, and either the 50th or the 51st observation is, with equal probability, our median estimate.</p>



<p>Our big realization was that to enable a faster algorithm, the only thing we need in order to estimate the bootstrap sample quantile is to know what index from the original sample becomes the bootstrap sample quantile in each Poisson-generated sample. The naive (and computationally complex) way of doing this is to repeat each observation in the original sample Poi(1) number of times, and check which observation had the ordered index that corresponded to the quantile. Our proposal is to instead study the distribution of the index in the original sample that is recorded as the quantile of interest in the bootstrapped sample. If you look at Table 1 again, you can see that the indexes of the ordered observations that wound up as the bootstrap-sample medians are 4,3,2,3,1,5,3,2,2,4. That is, the value 4 (ordered index 1), was only median in one bootstrap sample. The value 8 (ordered index 2) was median in three bootstrap samples, the value 11 (ordered index 3) was median in 3 samples, etc. Naturally, the closer the index is to that of the original sample median, the more frequently the value becomes the bootstrap sample median. </p>



<p>Let’s look closer at the distribution of ordered indexes selected across repeated bootstrap samples. We consider a sample of size N=200, and suppose that we are interested in the 20th percentile, that is q=0.2. Figure 1 displays the frequencies at which indexes of observations in the ordered original sample were recorded as the 20th percentile in 1M Poisson bootstrap samples. </p>



<p>Figure 1: Frequencies at which indexes of observations in the ordered original sample were recorded as the 20th percentile in 1M Poisson bootstrap samples.</p>



<div><figure><img loading="lazy" width="700" height="438" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Bootstrapped-Distributon-of-Quantile-Indexes-700x438.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Bootstrapped-Distributon-of-Quantile-Indexes-700x438.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Bootstrapped-Distributon-of-Quantile-Indexes-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Bootstrapped-Distributon-of-Quantile-Indexes-768x480.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Bootstrapped-Distributon-of-Quantile-Indexes-1536x960.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Bootstrapped-Distributon-of-Quantile-Indexes-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Bootstrapped-Distributon-of-Quantile-Indexes.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>There are two important learnings from this figure. First, most ordered indexes between 1 and 200 are never found to be the bootstrap sample quantile. This is of course expected, but it means that we are generating Poisson values (resampling) for many order statistics that will never have a chance to be the sample quantile. The second learning is far more important — the distribution highly resembles a binomial distribution. It turns out that the distribution of ordered indexes in the original sample of size N that are recorded as the qth sample quantile in Poisson-generated bootstrap samples is very well approximated by a Bin(N+1,q) distribution for large N.</p>



<p>For the difference-in-quantile confidence intervals, the implications from this result are profound. Computationally, it means that for a difference-in-quantiles bootstrap confidence interval we can sort the outcomes once, and then sample indexes and use the corresponding ordered observations as bootstrap estimates for the quantile. If you are interested in the details regarding the reduction in complexity, see our paper <a href="https://arxiv.org/pdf/2202.10992.pdf" target="_blank" rel="noreferrer noopener">Schultzberg and Ankargren (2022)</a>. </p>



<p>Interestingly, for one-sample quantile bootstrap confidence intervals, our result is rediscovering the well known order-statistic based confidence intervals for quantiles, see for example Gibbons and Chakraborti (2010), but derived from the bootstrap approach. The important difference is that we are not only interested in the confidence intervals themselves, but the distribution of indexes. Ultimately, this is what enables us to do two-sample comparisons.</p>



<p>The Python code block below illustrates how simple it is to calculate a bootstrap confidence interval for a sample quantile estimator using the binomial approximation of the index distribution.  </p>



<pre><code>import numpy as np
from scipy.stats import binom

alpha=.05
quantile_of_interest=0.5
sample_size=10000
number_of_bootstrap_samples=1000000
outcome_sorted = np.sort(np.random.normal(1,1,sample_size))

ci_indexes = binom.ppf([alpha/2,1-alpha/2],sample_size+1, quantile_of_interest)
bootstrap_confidence_interval = outcome_sorted[[int(np.floor(ci_indexes[0])), int(np.ceil(ci_indexes[1]))]]

f&#34;The sample median is {np.quantile(outcome_sorted, quantile_of_interest)}, the {(1-alpha)*100}%\
confidence interval is given by ({bootstrap_confidence_interval}).&#34;</code></pre>



<p>For the one-sample case, the complexity consists of a single sort of the original outcome data (actually this can be simplified even further). This follows since we already know which order statistics will be boundaries for the alpha/2 and 1-alpha/2 tails of the binomial distribution of indexes that will become the quantile of interest in the distribution of bootstrap estimates. </p>



<p>For the two-sample case, which is our main contribution, we still need to calculate the difference between the two sample quantiles in each bootstrap iteration. The Python code block below illustrates how simple it is to calculate a bootstrap confidence interval for a difference-in-quantiles estimator using the binomial approximation.</p>



<div><p><code>import numpy as np<br/>from numpy.random import normal, binomial<p>alpha=.05<br/>quantile_of_interest=0.5<br/>sample_size=10000<br/>number_of_bootstrap_samples=1000000<br/>outcome_control_sorted = np.sort(normal(1,1,sample_size))<br/>outcome_treatment_sorted = np.sort(normal(1.2,1,sample_size))</p><p>bootstrap_difference_distribution = outcome_treatment_sorted[binomial(sample_size+1, quantile_of_interest,<br/>number_of_bootstrap_samples)] - outcome_control_sorted[binomial(sample_size+1,<br/>                        quantile_of_interest, number_of_bootstrap_samples)]<br/>bootstrap_confidence_interval = np.quantile(bootstrap_difference_distribution,</p></code> <br/><code>[alpha/2 , 1-alpha/2]) </code></p><p><code>f&#34;The sample difference-in-medians is \</code><br/><code>{np.quantile(outcome_treatment_sorted, quantile_of_interest)-np.quantile(outcome_control_sorted, quantile_of_interest)},\</code><br/><code>the {(1-alpha)*100}% confidence interval for the difference-in-medians is given by ({bootstrap_confidence_interval}).&#34;</code></p></div>



<p>It is easy to verify that these binomial-approximated confidence intervals give similar results as the usual bootstrap confidence intervals, and that they are statistically valid. For example, the code below shows a Monte Carlo simulation of the false positive rate for a one-sided bootstrap confidence interval for the sample median. </p>



<div><p><code>import numpy as np</code><br/><code>from scipy.stats import binom</code></p><p><code>alpha=.05</code><br/><code>quantile_of_interest=0.5</code><br/><code>sample_size=10000</code><br/><code>number_of_bootstrap_samples=1000000</code><br/><code>replications = 10000</code></p><p><code>bootstrap_confidence_intervals = []</code><br/><code>ci_index = int(np.floor(binom.ppf(alpha,sample_size+1,</code><br/><code>quantile_of_interest)))</code><br/><code>for i in range(replications):</code><br/><code>    outcome_sorted = np.sort(np.random.normal(0,1,sample_size))</code><br/><code>    bootstrap_confidence_intervals.append(outcome_sorted[ci_index])</code></p><p><code>f&#34;The empirical false positive rate of the test using the bootstrap confidence interval is {np.mean([1 if i&gt;0  else 0 for i in bootstrap_confidence_intervals])*100}%, the intended false positive rate is {alpha*100}%&#34;</code></p></div>



<p>A faster Julia script to run comparison simulations for the difference-in-quantiles case can be found in <a href="https://github.com/MSchultzberg/fast_quantile_bootstrap/blob/main/speed_and_memory_comparisons.jl" target="_blank" rel="noreferrer noopener">this GitHub repository</a>. </p>



<p>Just to give a feeling for the impact of this reduction, we ran some benchmarks of our implementation in Julia. For a two-sample median confidence interval, where the total sample size is 2,000 and we draw 10,000 bootstrap samples, the standard Poisson bootstrap implementation (realizing each bootstrap sample) has a median runtime of 1,821 milliseconds, using 2.4 GB of memory. The corresponding binomial-based confidence interval function takes 2.2 milliseconds, using 407 KiB of memory. </p>



<h3>SQL implementation</h3>



<p>A simple and powerful SQL+Python implementation of our proposed binomial-based quantile bootstrap confidence interval is the following: </p>



<p><strong>Step 1:</strong> [In Python] Calculate the indexes that correspond to the alpha/2 upper and lower tail on the Binom(N+1,q) distribution. </p>



<p><strong>Step 2:</strong><em> </em>[In SQL] Return the order statistics from the outcome according to the indexes from Step 1 as the confidence interval for the quantile. </p>



<h2>Conclusion</h2>



<p>Having robust and assumption-free inference methods for various types of estimators is critical for proper evaluation of A/B tests and risk management for product decisions. Using our new index-based bootstrap algorithm, we have simplified the computation of bootstrap confidence interval estimates for difference-in-quantiles sufficiently to be plausible for all sizes of data. This unlocks an important inference tool for A/B tests and dashboards at Spotify.</p>



<h2>References:</h2>



<p>Chamandy, N., Omkar Muralidharan, Amir Najmi, and Siddartha Naidu.“Estimating uncertainty for massive data streams.” <em>Technical report, Google, </em>(2012) <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43157.pdf" target="_blank" rel="noreferrer noopener">https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43157.pdf</a></p>



<p>Dean, J. and Sanjay Ghemawat. “MapReduce: Simplified Data Processing on Large Clusters.” <em>Communications of the ACM</em>, 51(1):107–113 (2008). <a href="https://doi.org/10.1145/1327452.1327492" target="_blank" rel="noreferrer noopener">https://doi.org/10.1145/1327452.1327492</a>.</p>



<p>Efron, Bradley. “Bootstrap Methods: Another Look at the Jackknife.” <em>The Annals of Statistics</em>, 7 (1) 1 – 26, January, 1979. <a href="https://doi.org/10.1214/aos/1176344552" target="_blank" rel="noreferrer noopener">https://doi.org/10.1214/aos/1176344552</a></p>



<p>Falk, M. and R.-D. Reiss. “Weak Convergence of Smoothed and Nonsmoothed Bootstrap Quantile Estimates.” <em>The Annals of Probability</em>, 17(1):362–371 (1989). <a href="https://doi.org/10.1214/aop/1176991515" target="_blank" rel="noreferrer noopener">https://doi.org/10.1214/aop/1176991515</a></p>



<p>Ghosh, M., William C. Parr, Kesar Singh, and G. Jogesh Babu. “A Note on Bootstrapping the Sample Median.” <em>The Annals of Statistics</em>, 12(3):1130–1135 (1984).  <a href="https://doi.org/10.1214/aos/1176346731" target="_blank" rel="noreferrer noopener">https://doi.org/10.1214/aos/1176346731</a>.</p>



<p>Gibbons, J.D. and Subhabrata Chakraborti. <em>Nonparametric Statistical Inference </em>(5th ed.). Chapman and Hall/CRC, (2010). <a href="https://doi.org/10.1201/9781439896129" target="_blank" rel="noreferrer noopener">https://doi.org/10.1201/9781439896129</a></p>



<p>Hanley, J. A. and Brenda MacGibbon. “Creating non-parametric bootstrap samples using Poisson frequencies.” <em>Computer Methods and Programs in Biomedicine</em>, 83:57–62 (2006). <a href="http://www.med.mcgill.ca/epidemiology/Hanley/Reprints/bootstrap-hanley-macgibbon2006.pdf" target="_blank" rel="noreferrer noopener">http://www.med.mcgill.ca/epidemiology/Hanley/Reprints/bootstrap-hanley-macgibbon2006.pdf</a></p>



<p>Kleiner, A., Ameet Talwalkar, Purnamrita Sarkar, and Michael I. Jordan. “A scalable bootstrap for massive data.” <em>Journal of the Royal Statistical Society</em>, Series B (Statistical Methodology), 76(4):795–816 (2014). <a rel="noreferrer noopener" href="https://doi.org/10.1111/rssb.12050" target="_blank">https://doi.org/10.1111/rssb.12050</a></p>



<p>Liu, M., Xiaohui Sun, Maneesh Varshney and Ya Xu. “Large-Scale Online Experimentation with Quantile Metrics.” arXiv e-prints, art. arXiv:1903.08762 (2019). <a href="https://arxiv.org/abs/1903.08762" target="_blank" rel="noreferrer noopener">https://arxiv.org/abs/1903.08762</a> </p>



<p>Schultzberg, M. and Sebastian Ankargren. “Resampling-free bootstrap inference for quantiles.” arXiv e-prints, art. arXiv:2202.10992, (2022). <a href="https://arxiv.org/abs/2202.10992" target="_blank" rel="noreferrer noopener">https://arxiv.org/abs/2202.10992</a>. Accepted for Publication in the <em>Proceedings of the Future Technologies Conference 2022.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 23 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Zela Taino: iOS Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2022/03/zela-taino-ios-engineer/</link>
      <description>Tell us more about working on Spotify Wrapped… As Tech Lead for Wrapped, I was in charge of laying out the road map for the mobile and backend engineers – as well as working with the brand, design, creative and localization teams to make sure everything went smoothly from an engineering point of</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-5229">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover-Zela.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover-Zela.png 1000w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover-Zela-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover-Zela-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover-Zela-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover-Zela-120x59.png 120w" sizes="(max-width: 1000px) 100vw, 1000px"/>
                                  
             </p>
             <div>
             
                 <p><b>Zela has worked on Spotify Wrapped for the past two years – most recently, taking on the role of Tech Lead… </b></p>
             </div>
         </div>

         


         

         
<p><strong>Tell us more about working on Spotify Wrapped… </strong></p>



<p>As Tech Lead for Wrapped, I was in charge of laying out the road map for the mobile and backend engineers – as well as working with the brand, design, creative and localization teams to make sure everything went smoothly from an engineering point of view. With 15+ engineers working on the project, it was a big job and stressful at times. But the energy was amazing and I really had fun.  </p>



<p><strong>What were the biggest challenges? </strong></p>



<p>One of the most challenging things was being an engineer in a room full of people from different disciplines – I had to manage stakeholders’ expectations, express my perspective and be confident enough to say what was and wasn’t possible. </p>



<p>For instance, there were these curves – or ribbons, as we called them – that were a signature of the Wrapped campaign design. At first, we were trying to animate text within them, but it was all distorting. And so, it was my job to say “this isn’t working, this isn’t technically feasible” and help come up with a cool alternative solution. </p>



<p><strong>What was the most inspiring moment? </strong></p>



<p>It was inspiring to see my friends – and even my parents! – enjoy something that I’d had a hand in. We created this big dashboard that showed when we were trending on Twitter – I loved knowing we were shaping internet culture and having the opportunity to learn from it.</p>



<p>And then, of course, there were the numbers! The number of people who engaged with the experience gave me chills – we really blew our goal out of the water.  </p>



<p><strong>What made it a fun project to be part of? </strong></p>



<p>Working on Wrapped was intense, but in the most fun way possible! I loved all the exploration and discovery involved – for instance, the audio auras in the user experience were new for this year, so I got to spend a few days playing around with code, experimenting and figuring out how they could work. There was a real start-up energy that’s unusual in such a large company and everything felt very dynamic and fast-paced. </p>



<p>I also enjoyed working with our embeds from across the country and getting to know them better. Connecting virtually with so many Spotifiers across the world was a real treat. </p>



<p><strong>Any special shout-outs? </strong></p>



<p>It’s hard to narrow it down, because everyone on the Wrapped team is a rockstar and I was honored to work with them all. But special thanks to my engineering manager, Udaya Uma Pillalamarri, who supported me brilliantly and gave me the courage to take on every challenge. </p><p>

         Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a>, <a href="https://engineering.atspotify.com/tag/wrapped21/" rel="tag">Wrapped21</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover-Zela.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 18 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing Natural Language Search for Podcast Episodes&#xA;</title>
      <link>https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/</link>
      <description>Beyond term-based Search Until recently, Search at Spotify relied mostly on term matching. For example, if you type the query &#34;electric cars climate impact&#34;, Elasticsearch will return search results that contain everything that has each of those query words in its indexed metadata (like in the ti</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 17, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/" title="Introducing Natural Language Search for Podcast Episodes">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search_Header-1536x758.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search_Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<h2>Beyond term-based Search</h2>



<p>Until recently, Search at Spotify relied mostly on term matching. For example, if you type the query “electric cars climate impact”, Elasticsearch will return search results that contain everything that has each of those query words in its indexed metadata (like in the title of a podcast episode).</p>



<p>However, we know users don’t always type the exact words for what they want to listen to, and we have to use fuzzy matching, normalization, and even manual aliases to make up for it. While these techniques are very helpful for the user, they have limitations, as they cannot capture all variations of expressing yourself in natural language, especially when using natural language sentences.</p>



<p>Going back to the query “electric cars climate impact”, our Elasticsearch cluster did not actually retrieve anything for it… but does this mean that we don’t have any relevant content to show to the user for this query? </p>



<p>Enter <em>Natural Language Search</em>.</p>



<h2>Natural Language Search</h2>



<p>To enable users to find more relevant content with less effort, we started investigating a technique called Natural Language Search, also known as<em> </em>Semantic Search in the literature. In a nutshell, Natural Language Search matches a query and a textual document that are semantically correlated instead of needing exact word matches. It matches synonyms, paraphrases, etc., and any variation of natural language that express the same meaning.</p>



<p>As a first step, we decided to apply Natural Language Search to podcast episode retrieval, as we thought that semantic matching would be most useful when searching for podcasts. Our solution is now deployed for most Spotify users<em>.</em> Thanks to this technique, one can retrieve relevant content for our example query:</p>



<div><figure><img loading="lazy" width="700" height="532" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search-Language-for-Podcast-Episodes-1-700x532.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search-Language-for-Podcast-Episodes-1-700x532.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search-Language-for-Podcast-Episodes-1-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search-Language-for-Podcast-Episodes-1-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search-Language-for-Podcast-Episodes-1-1536x1168.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search-Language-for-Podcast-Episodes-1-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search-Language-for-Podcast-Episodes-1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Example of Natural Language Search for podcast episodes.</figcaption></figure></div>



<p>It is noticeable that none of the retrieved episodes contains all of the query words in their title (that’s why Elasticsearch was not picking them up). However, those episodes seem quite relevant to the user’s query.</p>



<p>To achieve this result, we leveraged recent advances in Deep Learning / Natural Language Processing (NLP) like <a href="https://en.wikipedia.org/wiki/Self-supervised_learning" target="_blank" rel="noreferrer noopener">Self-supervised learning</a> and <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" target="_blank" rel="noreferrer noopener">Transformer neural networks</a>. We also took advantage of vector search techniques like <a href="https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6" target="_blank" rel="noreferrer noopener">Approximate Nearest Neighbor (ANN)</a> for fast online serving. The rest of this post will go into further detail about our architecture.</p>



<h2>Technical solution</h2>



<p>We are using a machine learning technique called Dense Retrieval, which consists of training a model that produces query and episode vectors in a shared embedding space. A vector is simply an array of float values. The objective is that the vectors of a search query and a relevant episode would be close together in the embedding space. For queries, we use the query text as input to the model, and for episodes, we use a concatenation of textual metadata fields of the episode such as its title, description, its parent podcast show’s title and description, and so on.</p>



<p>During live Search traffic, we can then employ sophisticated vector search techniques to efficiently retrieve the episodes whose vectors are the closest to the query vector.</p>



<div><figure><img loading="lazy" width="700" height="532" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Queries-and-Podcast-Episodes-700x532.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Queries-and-Podcast-Episodes-700x532.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Queries-and-Podcast-Episodes-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Queries-and-Podcast-Episodes-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Queries-and-Podcast-Episodes-1536x1167.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Queries-and-Podcast-Episodes-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Queries-and-Podcast-Episodes.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Shared embedding space for queries and podcast episodes.</figcaption></figure></div>



<h2>Picking the right pre-trained Transformer model for our task</h2>



<p>Transformer models like <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noreferrer noopener">BERT</a> are currently state-of-the-art on the vast majority of NLP tasks. The power of BERT mainly comes from two aspects:</p>



<ul><li>Self-supervised pre-training on a big corpus of text — the model is pre-trained on the Wikipedia and BookCorpus datasets with the main objective of predicting randomly masked words in sentences.</li><li>Bidirectional self-attention mechanism, which allows the model to produce high-quality contextual word embeddings.</li></ul>



<p>However, the vanilla BERT model is not best suited for our use case:</p>



<ul><li>Its self-supervised pre-training strategy is mostly focused on producing high-quality contextual <em>word </em>embedding, but off-the-shelf <em>sentence</em> representations are not very good, as shown in this paper on <a href="https://arxiv.org/pdf/1908.10084.pdf" target="_blank" rel="noreferrer noopener">SBERT, “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks”. </a></li><li>It was pre-trained on English text only, whereas we wanted to support multilingual queries and episodes.<br/></li></ul>



<p>Therefore, after a few experiments, we chose as our base model the Universal Sentence Encoder CMLM model, as detailed in the paper <a href="https://arxiv.org/pdf/2012.14388.pdf" target="_blank" rel="noreferrer noopener">“Universal Sentence Representation Learning with Conditional Masked Language Model</a>”. It has the following advantages for us:</p>



<ul><li>The newly introduced self-supervised objective Conditional Masked Language Modeling (CMLM) is designed to produce high-quality <em>sentence</em> embeddings directly.</li><li>The model is pre-trained on a huge multilingual corpora of more than 100 languages, and is publicly available on <a href="https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base-br/1" target="_blank" rel="noreferrer noopener">TFHub</a>.</li></ul>



<h2>Preparing the data</h2>



<p>Once we have this powerful pre-trained Transformer model, we need to fine-tune it on our target task of performing Natural Language Search on Spotify’s podcast episodes. To that end, we preprocess different types of data:</p>



<ul><li>From our past search logs, we take successful podcast searches and create (query, episode) pairs. Those successes mostly come from previous queries and their returned results through Elasticsearch.</li><li>Also from our past search logs, we examined user sessions to find successful attempts at search queries after an initial search came up unsuccessful. We used those query reformulations to create (query_prior_to_successful_reformulation, episode) pairs.<br/>With this data source, we hope to capture in our training set more “semantic” (query, episode) pairs with no exact word matching between the query and the episode metadata.</li><li>To further extend and diversify our training set, we generate synthetic queries from popular episode titles and descriptions, inspired by the paper <a rel="noreferrer noopener" href="https://arxiv.org/pdf/2009.10270.pdf" target="_blank">“Embedding-based Zero-shot Retrieval through Query Generation”</a>. More specifically, we fine-tuned a <a rel="noreferrer noopener" href="https://arxiv.org/pdf/1910.13461.pdf" target="_blank">BART</a> transformer model on the <a rel="noreferrer noopener" href="https://microsoft.github.io/msmarco/" target="_blank">MS MARCO dataset</a>, and used it to generate (synthetic_query, episode)<em> </em>pairs. </li><li>Finally, a small curated set of “semantic” queries was manually written for popular episodes.<br/></li></ul>



<p>All types of preprocessed data are used for both training and evaluation, except for the curated dataset that is used for evaluation only. Moreover, when splitting the data, we make sure that the episodes in the evaluation set are not present in the training set (to be able to verify the generalization ability of our model to new content).</p>



<h2>Training</h2>



<p>We have our model and our data, so the next step is the training. We use the pre-trained Universal Sentence Encoder CMLM model as a starting point for both the query encoder and the episode encoder. After a few experiments, we chose a siamese network setting where the weights are shared between the two encoders. Cosine similarity is used as the similarity measure between a query vector and an episode vector. </p>



<div><figure><img loading="lazy" width="700" height="532" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Model-Architecture-700x532.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Model-Architecture-700x532.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Model-Architecture-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Model-Architecture-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Model-Architecture-1536x1167.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Model-Architecture-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Model-Architecture.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Model architecture: bi-encoder Transformer in a siamese setting.</figcaption></figure></div>



<p>In order to effectively train our model, we need both positive (query, episode) pairs and negative ones. However, we only mined positive pairs in our training data. How then can we generate negative pairs to teach the model when <em>not</em> to retrieve an episode given a query?</p>



<p>Inspired by multiple papers in the literature like <a rel="noreferrer noopener" href="https://arxiv.org/abs/2004.04906" target="_blank">“Dense Passage Retrieval for Open-Domain Question Answering (DPR)”</a> and <a rel="noreferrer noopener" href="https://research.facebook.com/publications/que2search-fast-and-accurate-query-and-document-understanding-for-search-at-facebook/" target="_blank">“Que2Search: Fast and Accurate Query and Document Understanding for Search at Facebook”</a>, we used a technique called <em>in-batch negatives </em>to efficiently generate random negative pairs during training: for each (query, episode) positive pair within a training batch of size <em>B</em>, we take the episodes from the other pairs in the batch as negatives for the query of the positive pair. As a result, we’ll have <em>B</em> positive pairs and <em>B</em><sup>2</sup> <em>–</em> <em>B</em> negative pairs per batch. For computational efficiency, we encode the query and episode vectors for each training data positive pair once, and then compute the in-batch cosine similarity matrix from those vectors:</p>



<div><figure><img loading="lazy" width="700" height="396" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Cosine-Similarity-Matrix-700x396.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Cosine-Similarity-Matrix-700x396.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Cosine-Similarity-Matrix-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Cosine-Similarity-Matrix-768x435.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Cosine-Similarity-Matrix-1536x869.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Cosine-Similarity-Matrix-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Cosine-Similarity-Matrix.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Cosine similarity matrix: q<sub>i</sub> (respectively e<sub>i</sub>) represents the embedding of the i-th query (respectively episode) in a batch of size <em>B</em>.</figcaption></figure></div>



<p>In this matrix, the diagonal part corresponds to the cosine similarity of positive pairs, while the other values are cosine similarities of the negative pairs. From there, we can apply several losses to the matrix, like a Mean Squared Error (MSE) loss using the identity matrix as the label. In our latest iteration, we took a step further and used techniques like <a href="https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/loss/HardNegativeMining" target="_blank" rel="noreferrer noopener">in-batch hard negative mining</a> and <a href="https://gombru.github.io/2019/04/03/ranking_loss/" target="_blank" rel="noreferrer noopener">margin loss</a> to substantially improve our offline metrics.</p>



<h2>Offline evaluation</h2>



<p>In order to evaluate our models, we use two kinds of metrics:</p>



<ul><li><strong>In-batch metrics:</strong> Using in-batch negatives, we can efficiently compute metrics like Recall@1 and Mean Reciprocal Rank (MRR) at the batch level.</li><li><strong>Full-retrieval setting metrics:</strong> In order to also evaluate our model in a more realistic setting with more candidates than in a batch, during training, we periodically compute the vectors of all episodes in our eval set and compute metrics like Recall@30 and MRR@30 using the queries from the same eval set. We also compute retrieval metrics on our curated dataset.</li></ul>



<h2>Integration with production</h2>



<p>Now that we have our fine-tuned query encoder and episode encoder, we need to integrate them into our Search production workflow. </p>



<h3>Offline indexing of episode vectors</h3>



<p>Episode vectors are pre-computed for a large set of episodes using our episode encoder in an offline pipeline. Those vectors are then indexed in the <a href="https://vespa.ai/" target="_blank" rel="noreferrer noopener">Vespa</a> search engine, leveraging its native support for ANN Search. ANN allows retrieval latency on tens of millions of indexed episodes to be acceptable while having a minimal impact on retrieval metrics.</p>



<p>Moreover, Vespa allows us to define a first-phase ranking function (that will be executed on each Vespa content node) to efficiently re-rank the top episodes retrieved by ANN using other features like episode popularity.</p>



<h3>Online query encoding and retrieval</h3>



<p>When a user types a query, the query vector is computed on the fly by leveraging <a href="https://cloud.google.com/vertex-ai" target="_blank" rel="noreferrer noopener">Google Cloud Vertex AI</a> where the query encoder is deployed. Our main reason for using Vertex AI is its support for GPU inference, as large Transformer models like ours are usually more cost-effective when running on GPU even for inference (this was confirmed by our load test experiments with a 6x cost reduction factor between T4 GPU and CPU). Once the query vector is computed, it is used to retrieve from Vespa the top 30 “semantic podcast episodes” (using our previously described ANN Search). A vector cache is also used to avoid computing the same query vectors too often.</p>



<h3>There is no silver bullet in retrieval</h3>



<p>Although Dense Retrieval / Natural Language Search has very interesting properties, it often fails to perform as well as traditional IR methods on exact term matching (and is also more expensive to run on all queries). That’s why we decided to make our Natural Language Search an additional source rather than just replace our other retrieval sources (including our Elasticsearch cluster).</p>



<p>In Search at Spotify, we have a final-stage reranking model that takes the top candidates from each retrieval source and performs the final ranking to be shown to the user. To allow this model to better rank semantic candidates, we added the (query, episode) cosine similarity value to its input features.</p>



<div><figure><img loading="lazy" width="700" height="478" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Multisource-Retrieval-and-Ranking-700x478.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Multisource-Retrieval-and-Ranking-700x478.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Multisource-Retrieval-and-Ranking-250x171.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Multisource-Retrieval-and-Ranking-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Multisource-Retrieval-and-Ranking-1536x1048.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Multisource-Retrieval-and-Ranking-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Multisource-Retrieval-and-Ranking.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Multi-source retrieval and ranking.</figcaption></figure></div>



<h2>Conclusion and future works</h2>



<p>Using this multi-source retrieval and ranking architecture with Natural Language Search, we successfully launched an A/B test that resulted in a significant increase in podcast engagement. Our initial version of Natural Language Search has now been rolled out to most of our users, and new model improvements are already on their way.</p>



<p>Our team is excited about this successful first step, and we already have several ideas for future works ranging from model architecture improvements, better blending of dense and sparse retrieval, and increasing coverage. Stay tuned!</p>



<p><em>TensorFlow, the TensorFlow logo and any related marks are trademarks of Google Inc.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Alexandre Tamborrino, Machine Learning Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Natural-Search_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Why We Switched Our Data Orchestration Service￼&#xA;</title>
      <link>https://engineering.atspotify.com/2022/03/why-we-switched-our-data-orchestration-service/</link>
      <description>TL;DR Within Spotify, we run 20,000 batch data pipelines defined in 1,000+ repositories, owned by 300+ teams — daily. The majority of our pipelines rely on two tools: Luigi (for the Python folks) and Flo (for the Java folks). In 2019, the data orchestration team at Spotify decided to move away from</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-5189">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 14, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/03/why-we-switched-our-data-orchestration-service/" title="Why We Switched Our Data Orchestration Service￼">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Why-We-Switched-Data-Orchestration-Service_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Why-We-Switched-Data-Orchestration-Service_Header.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Why-We-Switched-Data-Orchestration-Service_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Why-We-Switched-Data-Orchestration-Service_Header-700x344.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Why-We-Switched-Data-Orchestration-Service_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Why-We-Switched-Data-Orchestration-Service_Header-120x59.png 120w" sizes="(max-width: 1200px) 100vw, 1200px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Within Spotify, we run 20,000 batch data pipelines defined in 1,000+ repositories, owned by 300+ teams — daily. The majority of our pipelines rely on two tools: <a rel="noreferrer noopener" href="https://github.com/spotify/luigi" target="_blank">Luigi</a> (for the Python folks) and <a rel="noreferrer noopener" href="https://github.com/spotify/flo" target="_blank">Flo</a> (for the Java folks). In 2019, the data orchestration team at Spotify decided to move away from these tools. In this post, the team details why the decision was made, and the journey they took to make the transition.</p>



<h2>Our current orchestration stack</h2>



<div><figure><img loading="lazy" width="700" height="341" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Current-Orchestration-Stack-700x341.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Current-Orchestration-Stack-700x341.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Current-Orchestration-Stack-250x122.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Current-Orchestration-Stack-768x374.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Current-Orchestration-Stack-120x58.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Current-Orchestration-Stack.png 956w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption><em>How Luigi works at Spotify</em>.</figcaption></figure></div>



<p>With Luigi and Flo, we define workflows as recursively dependent tasks. All different tasks, libraries, and logic gets packaged into a Docker image, which is deployed as is to <a href="https://github.com/spotify/styx" target="_blank" rel="noreferrer noopener">Styx</a> with an entry point to the root task to run. Styx handles scheduling and execution, so when it is time, it will start the Docker image in our GKE cluster. </p>



<p>At runtime, the graph of dependencies is discovered and tasks are run in the defined order. In this stage, the workflow also sends lineage data if it is instrumented to do so. </p>



<p>The tasks might start a dataflow or BigQuery job and publish data to different locations.</p>



<h2>Problems we needed to solve</h2>



<p>Luigi and Flo are shipped as libraries that the users then consume. There are a few issues with this approach:</p>



<ul><li><strong>Maintenance burden:</strong> We need to implement, maintain, and update features on two different libraries offering the same capabilities.</li><li><strong>Support burden:</strong> We need to push updates to all users, but users don’t always upgrade to the latest version. This makes it hard for us, as a platform team, to roll out fixes and new instrumentation to the entire fleet of 1,000+, and growing, repositories. Monitoring change adoption and managing the fragmented versioning impacts platform and productivity of data engineers.</li><li><strong>Lack of insights:</strong> The workflow container is a black box to the platform, and it is only possible to collect insights on task graphs, blocking upstreams only after an execution has taken place. This leads to considerable overhead wherein a portion of GKE cluster time is spent polling for missing dependencies.</li></ul>



<p>In 2019, starting this journey, it was evident that there were a number of areas where we could bring (potentially significant) improvements by investing in our scheduling/orchestration offering. We believed, though, that not all of them would have the same impact on our users and our ability to operate in an impactful way. </p>



<p>That said, our prioritized goals for the next-generation orchestration were, and still are, to:</p>



<ul><li><strong>Make upgrades as frictionless as possible. </strong>By pushing as much functionality as possible down to the platform teams, it allows us platform teams to own more of the foundational parts. This makes it possible to add features and fixes automatically to all. In the platform mission, we are strong believers that we add value by making things easier. We want to limit the amount of time spent by users on the maintenance of our tool. This allows them to focus and excel in their core domain of expertise.</li><li><strong>Enable the development of platform automation functionality. </strong>A few examples of automation requested by our users are: opt-in for downstream backfill triggering, various event-triggered actions, canceling workflow execution after a user-defined alert, notify downstream consumers in case of workflow failure/delay. To enable those features we need to have visibility on the tasks and workflows, which we currently don’t have with the Luigi/Flo setup running everything in a Docker image.</li></ul>



<p>Our vision on how to solve this has been to move away from the library/tool paradigm to build the orchestration logic into a service. More specifically, we proposed a new orchestration paradigm where:</p>



<ul><li>Orchestration of tasks within a workflow is performed by a managed service.</li><li>Business logic (task code) is run in an environment controlled by the platform.</li><li>Users define workflows and tasks using SDKs distributed by the platform teams.</li></ul>



<h3>How did we reach our preferred solution?</h3>



<p>In our elimination process, we took into account the following factors:</p>



<ul><li>Effort needed to migrate (or not) existing workflows</li><li>Development effort and time to market (a new in-house development vs. a ready-made product that we adapt and integrate into our ecosystem)</li><li>Maintenance burden (e.g., Google-managed vs. self-managed)</li><li>Extensibility of the solution</li><li>Ability to inspect what is done within a workflow (explicit definition vs. Docker image)</li><li>Multiple language support</li></ul>



<h2>The solution we found: Flyte</h2>



<p>There are a number of options to perform workflow orchestration. We have done extensive comparisons of a few other systems to get a feeling of what the state of the art is in the industry. </p>



<p>We came to the conclusion that <a href="https://flyte.org/" target="_blank" rel="noreferrer noopener">Flyte</a> was the best option because of its extensibility to integrate Spotify tooling, support for multiple languages, and scalability. </p>



<p>The product was actively being developed with a lot of support from the Flyte team, and we’ve been very happy with its progress thus far. The Flyte team had also shared their plans to release Flyte to the open source community, making it a great opportunity for Spotify to be a contributing partner and to give back to the community — something we’re <a href="https://engineering.atspotify.com/opensource/" target="_blank" rel="noreferrer noopener">passionate about</a>.</p>



<p>Some of the reasons why Flyte is the best option for Spotify are that it:</p>



<ul><li>Has a similar entity model and nomenclature as Luigi and Flo, making the user experience and migration easier.</li><li>Uses Task as a first-class citizen, making it easy for engineers to share and reuse tasks/workflows.</li><li>Has a thin client SDK by moving more to the backend: this makes the maintenance of the overall platform much simpler than our existing offering with two libraries (Python, Java) both holding the logic.</li><li>Is decoupled from the rest of our ecosystem, enabling mixing and matching in the service layer according to our strategy.</li><li>Is extensible to provide more SDKs for orchestration.</li><li>Has an extensible backend to support different types of tasks.</li><li>Is an existing tool with a platform approach, so that we do not have to develop the service in-house.</li><li>Is scalable and battle hardened, as it was already running with similar constraints that we have at Spotify.</li><li>Is an open source project actively developed and maintained: Spotify can get a lot of support to get started, and can become a contributing partner.</li></ul>



<p>This is how Flyte integrates into the Spotify ecosystem. </p>



<div><figure><img loading="lazy" width="700" height="210" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Future-Orchestration-Stack-700x210.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Future-Orchestration-Stack-700x210.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Future-Orchestration-Stack-250x75.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Future-Orchestration-Stack-768x231.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Future-Orchestration-Stack-120x36.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Future-Orchestration-Stack.png 919w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Defining a workflow with Flyte.</figcaption></figure></div>



<p>The actions described in workflows are visible by the platform team, and we can register workflows in the Flyte system (Flyte Admin) and in our own scheduler (Styx), making the integration smooth.</p>



<div><figure><img loading="lazy" width="494" height="105" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Styx-Execution.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Styx-Execution.png 494w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Styx-Execution-250x53.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Styx-Execution-120x26.png 120w" sizes="(max-width: 494px) 100vw, 494px"/><figcaption><em>Execution still triggered by Styx</em>.</figcaption></figure></div>



<h2>What’s next ? </h2>



<p>The Flyte journey at Spotify is still ongoing — we’re happy to be running critical pipelines, but we still need to migrate all the existing ones from Luigi to Flyte, which will be an interesting challenge to detail in a follow-up blog post.</p>



<p>If you want to join us to work on products to help Spotifiers build reliable data pipelines, take a look at our <a href="https://www.lifeatspotify.com/jobs?c=backend&amp;c=data" target="_blank" rel="noreferrer noopener">career website</a>. We’re looking for talented engineers to help us make a change in that space!</p>



<h2>Acknowledgments</h2>



<p>The Flyte journey would never have started without the Union team building Flyte and passionate engineers within Platform — Hongxin Liang, Nelson Arapé, Gleb Kanterov, Sonja Ericsson, Babis Kiosidis, Lucía Pasarin, Nian Tang, Julien Bisconti. I want to extend this thank you to early users who helped us build and iterate on our implementation — the financial engineers and platform teams. We are grateful for your help and energy.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Guillaume Perchais, Sr. Product Manager</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Why-We-Switched-Data-Orchestration-Service_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 14 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Incident Report: Spotify Outage on March 8&#xA;</title>
      <link>https://engineering.atspotify.com/2022/03/incident-report-spotify-outage-on-march-8/</link>
      <description>On March 8, we experienced a global outage triggered by issues in a cloud-hosted service discovery system used at Spotify. We were made aware of issues with login at 18:12 UTC / 13:12 ET and started implementing fixes to critical systems at 18:39 UTC / 13:39 ET. This outage affected our users and we</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-5183">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 11, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/03/incident-report-spotify-outage-on-march-8/" title="Incident Report: Spotify Outage on March 8">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header-700x344.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header-120x59.png 120w" sizes="(max-width: 1200px) 100vw, 1200px"/>                    </a>
                        
        </p>

        

        
<p>On March 8, we experienced a global outage triggered by issues in a cloud-hosted service discovery system used at Spotify. We were made aware of issues with login at 18:12 UTC / 13:12 ET and started implementing fixes to critical systems at 18:39 UTC / 13:39 ET. This outage affected our users and we apologize for the inconvenience it may have caused. Our service has now fully recovered.</p>



<h2>What happened?</h2>



<p>The Spotify backend consists of multiple microservices that communicate with each other. For microservices to be able to find each other, we utilize multiple service discovery technologies. Most of our services are using a DNS based service discovery system; however, some of our services use an xDS based traffic control plane and discovery system called <a href="https://cloud.google.com/traffic-director" target="_blank" rel="noreferrer noopener">Traffic Director</a>. </p>



<p>On March 8, <a href="https://status.cloud.google.com/incidents/LuGcJVjNTeC5Sb9pSJ9o" target="_blank" rel="noreferrer noopener">Google Cloud Traffic Director experienced an outage</a>. This in coordination with <a href="https://github.com/grpc/grpc-java/issues/8950" target="_blank" rel="noreferrer noopener">a bug in a client (gRPC) library</a> caused the Spotify outage that affected many of our users: if you were logged out of a Spotify app, you were unable to log back in.</p>



<p>As soon as the problem was discovered, we rolled out configuration changes to revert our affected systems back to use our DNS-based service discovery and saw it recover gradually. See the timeline below for more details.</p>



<h3>Timeline</h3>



<p>18:12 UTC / 13:12 ET  – Reports of users being logged out of the different client apps start to surface.</p>



<p>18:39 UTC / 13:39 ET – Remediations were being put in place to restore the affected systems</p>



<p>20:35 UTC / 15:35 ET – Incident fully mitigated at Spotify</p>



<h2>Where do we go from here?</h2>



<p>In the short term:</p>



<ul><li>We are working with Google Cloud to better understand how issues with Traffic Director resulted in a large outage affecting Spotify’s users.</li><li>We will add additional monitoring and alerting to ensure that we would catch similar service discovery related problems earlier.</li></ul>



<p>We will continue to invest in resiliency by identifying and implementing additional safety nets in terms of monitoring, automatic error detection, and self-recovery.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Chantal Delfeld: Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2022/03/chantal-delfeld-engineer/</link>
      <description>Chantal is a Software Engineer and part of the Spotify team in New York. But she lives thousands of miles away in Austin, Texas – with her husband, mother and two young children.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-5158">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld-700x438.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld-768x480.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld-120x75.png 120w" sizes="(max-width: 1200px) 100vw, 1200px"/>
                                  
             </p>
             <div>
             
                 <p><b>Chantal is a Software Engineer and part of the Spotify team in New York. But she lives thousands of miles away in Austin, Texas – with her husband, mother and two young children. </b></p>
             </div>
         </div>

         


         

         
<blockquote><p>7:00am</p></blockquote>



<p>My early mornings are spent running about after my children – getting them dressed, giving them breakfast and putting on their sunscreen, so they’re all ready for the day ahead. At 8am, I drive my daughter to preschool, while my mom looks after my one-year-old son – she’s been living with us since the start of the pandemic and has been a huge help when it comes to family life and childcare.  </p>



<blockquote><p>8:30am</p></blockquote>



<p>Back home after drop-off, I make myself a nice coffee, head to the home office we’ve built in our backyard and get started on my working day. I’m a full-stack engineer, focussing mostly on the backend. But I’m becoming ‘T-shaped’ – which means gaining a broad knowledge of certain areas and doing some frontend work too – and I really like the variety that brings.  </p>



<p>I’ve been at Spotify six months now, working as part of a big team that deals with commerce-related projects. Our main product is a wallet that allows creators to add credit cards as payment. And right now, we’re improving our operations ahead of opening up in new markets – getting ourselves ready for the increased traffic and making sure we’ve got all the right learnings and monitoring in place for this exciting new phase. </p>



<p>We’re also in the process of developing a middle layer between our frontend and backend services – a sort of gateway service that means frontend won’t ever have to deal with backend directly. We have a lot of dependencies with other services and this new gateway will help us integrate with them more easily. It’ll also make things more secure, so it’s a really great project to be part of. </p>



<blockquote><p>12:00pm</p></blockquote>



<p>I take a half-hour break around noon and grab a few minutes with my mom and son while heating up some food for lunch. I usually eat at my desk though – I’m new to the world of commerce and take every chance to do additional reading or watch something that’s related to the field. </p>



<blockquote><p>12:30pm</p></blockquote>



<p>In the afternoon, I tend to have a few more meetings, or I often pair with someone in our team and work through a certain task or issue together. It’s nice to have someone to communicate with in this way, since I work remotely and haven’t actually met any of my team in person! </p>



<p>I can’t see myself moving to New York anytime soon, but what’s really great is that Spotify is running lots of workshops to help with remote working – I’m doing one called ‘Rethinking Communication in a Distributed World’ and another all about asynchronous communication. I’m really keen to make sure that my team know I’m there for them whenever they need me – despite the distance and the time difference. That’s super-important to me and I’m doing everything I can to make sure things run smoothly.</p>



<blockquote><p>3:30pm</p></blockquote>



<p>Time to pick up my daughter from preschool and have a quick play with both children, before wrapping up a few last bits and bobs for work. When that’s all done, I make dinner for the kids and go through the usual bedtime madness – then, I sit down with my husband for dinner, get a little grown-up time and chat about the ups and downs of our days.</p>











<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="595" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-700x595.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-700x595.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-250x213.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-768x653.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-1536x1306.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-120x102.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown.png 1584w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p><em>You can hear more about Chantal’s life as a Spotify engineer on our new podcast <a href="https://open.spotify.com/episode/43cbJh4ccRD7lzM2730YK3">NerdOut@Spotify</a>.</em></p><p>

         Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 09 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Jordan Loeser: Web Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2022/03/jordan-loeser-web-engineer/</link>
      <description>Tell us more about working on Spotify Wrapped… My main focus on Wrapped was the social media share cards – the static images that summarize the information from someone’s data stories and can be shared on platforms like Instagram, TikTok and Snapchat. Since these cards must accommodate a variety</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-5150">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan.png 1000w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan-120x59.png 120w" sizes="(max-width: 1000px) 100vw, 1000px"/>
                                  
             </p>
             <div>
             
                 <p><b>Jordan works at Spotify in New York and has been part of Spotify Wrapped for the past two years…</b></p>
             </div>
         </div>

         


         

         
<p><strong>Tell us more about working on Spotify Wrapped… </strong></p>



<p>My main focus on Wrapped was the social media share cards – the static images that summarize the information from someone’s data stories and can be shared on platforms like Instagram, TikTok and Snapchat. Since these cards must accommodate a variety of languages and dynamic data within a fixed space, we needed to work really closely with our designers and localization team right from the start and there were plenty of interesting challenges to work through together. </p>



<p><strong>What were the biggest challenges? </strong></p>



<p>By far the most challenging – and exciting – share card to implement was the <a href="https://engineering.atspotify.com/2021/12/the-audio-aura-story-mystical-to-mathematical/" target="_blank" rel="noreferrer noopener">Audio Aura</a> story. This was our first time leveraging this kind of advanced web technology to generate graphics in real-time, so there was lots of trial and error. But after experimenting and collaborating closely with our designers, we were able to get things just right. And with nearly 100 different aura combinations presented in 30 different languages, this share card really got people talking. </p>



<p><strong>What was the most inspiring moment? </strong></p>



<p>One moment that stands out to me vividly is the first time we were able to run through the full, end-to-end Wrapped experience using our own listening data. That test session was the first time we could see all the brand kit, data stories and share cards come together and really enjoy the fruits of our collaboration in a tangible way. Seeing our own favorite artists and hearing our own top tracks was a great reminder of why the experience resonates with listeners and what makes Wrapped so magical in the first place.</p>



<p><strong>What made it a fun project to be part of? </strong></p>



<p>It was a joy to collaborate with such an incredible team of bright, talented colleagues. And I love working on something so visual and user-facing – it’s amazing to release Wrapped into the wild and see how users engage and make it their own after launch. Working on a project that has such a presence on social media and generates so many memes and comments never fails to blow me away – it’s the part of the campaign that I truly cherish every year.</p>



<p><strong>Any special shout-outs? </strong></p>



<p>It really does take a village! Share cards wouldn’t have been possible without the help of so many brilliant Spotifiers: </p>



<ul><li>Samantha Whitt, my collaborator and share card builder extraordinaire</li><li>Melia Wagner, expert in all things Localization</li><li>Our exceptional designers, Angeline Toh, Cait Charniga and Sona Dolasia</li><li>My backend heroes, Mike Smith and Patty Santa Cruz</li><li>Genius problem-solver, Itay Yahimovitz</li><li>Our fearless tech lead and my mentor, Zela Taino</li><li>The glue of it all, our road manager, Udaya Uma Pillalamarri</li><li>Last but not least, my incredible manager and advocate, Ashley Casey.</li></ul><p>

         Tags: <a href="https://engineering.atspotify.com/tag/web/" rel="tag">web</a>, <a href="https://engineering.atspotify.com/tag/wrapped21/" rel="tag">Wrapped21</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 04 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing NerdOut@Spotify: A New Podcast for Developers&#xA;</title>
      <link>https://engineering.atspotify.com/2022/03/introducing-nerdout-at-spotify-tech-podcast/</link>
      <description>TL;DR For years, Spotify’s official engineering blog has been giving you a peek behind the curtain at Spotify R&amp;D. Today, we’re announcing NerdOut@Spotify, our new R&amp;D podcast that gives you another view into our tech world. In each episode I’ll talk with Spotify developers about challenging</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-5125">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 1, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/03/introducing-nerdout-at-spotify-tech-podcast/" title="Introducing NerdOut@Spotify: A New Podcast for Developers">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify.png 1000w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify-120x59.png 120w" sizes="(max-width: 1000px) 100vw, 1000px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR</strong> For years, Spotify’s official engineering blog has been giving you a peek behind the curtain at Spotify R&amp;D. Today, we’re announcing <a href="https://open.spotify.com/show/5eXZwvvxt3K2dxha3BSaAe?si=a6db69f76efe4459" target="_blank" rel="noreferrer noopener">NerdOut@Spotify</a>, our new R&amp;D podcast that gives you another view into our tech world. In each episode I’ll talk with Spotify developers about challenging tech problems and give you a firsthand look into what we’re doing, what we’re building, and what we’re nerding out about at Spotify every day.</p>



<p>As a Spotify engineer, I’ve seen this company transition from a music-only service that was fighting to keep up with the speed of our user growth to becoming an audio streaming platform operating at global scale. I’m excited to see what happens next in our tech journey, and this podcast gives us a way to share that journey with you, while introducing you to the amazing people who work here. </p>



<h2>Come learn with us</h2>



<p>Software development has always been a very rapidly changing field, and every company does it a little bit differently. Both writing software and owning and operating high-scale systems have drastically changed over the years, and will continue to rapidly change every day. This brings new challenges, new technologies, new languages, but also bigger opportunities and all new ways to make an impact. </p>



<p>As developers in this rapidly changing industry, we’re constantly learning; finding new things that we didn’t know about every day. If you’d like to join us on our journey of constantly experimenting, inventing, building, testing, and debugging, and just dig much deeper into all of the nerdy things that we’re up to at Spotify, NerdOut@Spotify is the podcast for you.</p>



<h2>Tackling tech issues from the inside out</h2>



<p>Join us on a behind-the-scenes tour of how we innovate at Spotify. We’re tackling interesting challenges and exploring the possibilities that new technology and trends bring to our products. Find out what we’re currently learning about — new cloud services, open source software like our own Backstage, infrastructure trends like Kubernetes, applying new innovations in machine learning, and our constantly increasing scale and the challenges that that brings. </p>



<p>We believe that these are the same problems that many of our peer companies are facing. So, we are bringing these conversations from the hallways of conferences to your podcast player. We’ll bring these topics in a way that only Spotify can, and highlight the ways in which we are approaching these challenges that are unique to our aligned and autonomous squad culture. Of course, we’ll also touch on the playful sides of our culture and show you some of the nerdy ways in which we have fun.</p>



<h2>Meet the people behind the tech</h2>



<p>In every episode, you’ll hear from nerds just like you digging into topics that they love. We’ll introduce you to people from all over Spotify and all around the world — developers, engineering managers, data scientists, designers, PMs, and just about any other R&amp;D role — from all experience levels and backgrounds. Hear their individual stories and how they got to where they are now. Share in their passions, and learn from their failures and successes. </p>



<p>We’ll cover plenty of big topics, but won’t be afraid to go a little too deep into some small obsessions, covering everything from the world of DevOps and some language wars, to Lego, gymnastics, and Kubernetes clusters running the lights in our homes.</p>



<h2>Listen and subscribe now</h2>



<p>Our first few episodes are live now and dive into the deep end: We explore open source, the problem of developer experience, and <a href="https://backstage.io/docs/overview/what-is-backstage" target="_blank" rel="noreferrer noopener">Backstage</a>, the open platform for building developer portals that was created at Spotify, donated to the <a href="https://www.cncf.io/" target="_blank" rel="noreferrer noopener">Cloud Native Computing Foundation (CNCF)</a>, and is now maintained by a worldwide community of contributors. </p>



<div><figure><img loading="lazy" width="700" height="343" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-700x343.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-120x59.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage.png 1200w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Come and <a href="https://open.spotify.com/show/5eXZwvvxt3K2dxha3BSaAe?si=a6db69f76efe4459" target="_blank" rel="noreferrer noopener">listen on Spotify</a> or wherever you get your podcasts, including:</p>



<ul><li><a href="https://podcasts.apple.com/us/podcast/nerdout-spotify/id1610028432" target="_blank" rel="noreferrer noopener">Apple Podcasts</a></li><li><a href="https://podcasts.google.com/feed/aHR0cHM6Ly9hbmNob3IuZm0vcy84MDU2ODJiMC9wb2RjYXN0L3Jzcw" target="_blank" rel="noreferrer noopener">Google Podcasts</a></li><li><a href="https://overcast.fm/itunes1610028432/nerdout-spotify" target="_blank" rel="noreferrer noopener">Overcast</a></li><li><a href="https://pca.st/1t2eonyr" target="_blank" rel="noreferrer noopener">Pocket Casts</a></li></ul>



<p>Be sure to follow <a href="https://open.spotify.com/show/5eXZwvvxt3K2dxha3BSaAe?si=a6db69f76efe4459" target="_blank" rel="noreferrer noopener">the show</a> so you’re in the loop when new episodes drop. I hope that you’ll listen and enjoy the show.</p>



<p><em>NerdOut@Spotify is produced by Spotify’s Ted Vergakis and our friends at </em><a href="https://seaplanearmada.com/"><em>Seaplane Armada</em></a><em>.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-culture/" rel="tag">engineering culture</a>, <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Dave Zolotusky, Principal Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Search Journey Towards Better Experimentation Practices &#xA;</title>
      <link>https://engineering.atspotify.com/2022/02/search-journey-towards-better-experimentation-practices/</link>
      <description>At Spotify, we aim to build and improve our product in a data-informed way. To do that, teams are encouraged to generate and test hypotheses by running experiments and gathering evidence for what works and what doesn’t. In the Search team, in our journey towards this goal, we have learned that,</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>February 28, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/02/search-journey-towards-better-experimentation-practices/" title="Search Journey Towards Better Experimentation Practices ">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header.png 2106w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-120x60.png 120w" sizes="(max-width: 2106px) 100vw, 2106px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, we aim to build and improve our product in a data-informed way. To do that, teams are encouraged to generate and test hypotheses by running experiments and gathering evidence for what works and what doesn’t. </p>



<p>In the Search team, in our journey towards this goal, we have learned that, besides having the ambition, we need at least two more things:</p>



<ol><li>An experimentation platform that allows us to run experiments at scale and generate accurate results</li><li>A product development culture with evidence-based hypothesis testing at its core</li></ol>



<h2><strong>Background</strong></h2>



<p>Over the last two years, Spotify has invested in building a new Experimentation Platform (see <a href="https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/" target="_blank" rel="noreferrer noopener">Spotify’s New Experimentation Platform (Part 1)</a> and <a href="https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/" target="_blank" rel="noreferrer noopener">Spotify’s New Experimentation Platform (Part 2)</a>) which, to a large extent, solves the first point. The experimentation platform is already mature and offers most of the functionality and flexibility required — and the pace at which the experimentation platform improves is often faster than the rate of adopting new functionality by individual teams. In addition, a lot of documentation and best practices have been established for the platform’s functionalities, which help with the process of setting up tests. </p>



<p>But the effort to adopt a truly data-informed product development process doesn’t stop there — it requires continuous effort and push. </p>



<p>Experimentation practices and data-driven product development is often considered a data science topic — and that is true to some extent. Data scientists usually have a deeper understanding of statistics and experimentation, so they tend to be the most excited about adopting a data-driven product development culture. The Search team at Spotify consists of several engineering teams, an array of product managers, a product insights team with a number of data scientists, and a product area leads group. You’d probably guess that engineering teams have to be pretty independent from data scientists to avoid creating bottlenecks, and you’d be right. </p>



<p>Defining best practices for experimentation is made relatively easily thanks to hundreds of articles that explain how to do things like perform a power analysis, choose metrics, interpret a p-value, etc. It is, however, less easy to integrate those practices day to day and turn them into habits. This requires creating an environment where adapting ways of working to improve our experimentation culture feels empowering, rather than uncomfortable, in a diverse team with different disciplines and varying levels of knowledge. </p>



<p>Read on to find out more about the two key drivers of our success and the concrete actions we took over the last year.</p>



<h2><strong>Key driver 1 – Roadmap</strong></h2>



<p>When it comes to adopting a truly data-informed product development process, the road to the end goal can feel overwhelming both for the engineering team and for data scientists. It is crucial that the plan towards that goal takes into account the maturity, ability, and bandwidth of the teams. In large organizations, these aspects often vary a lot from team to team and across parts of the organization. Too much too soon will lead to people feeling overwhelmed and giving up; too little too seldom and the momentum and excitement will be lost. To make this balancing easier, we created a roadmap with three simple steps:</p>



<ol><li>Individual experiment quality — Adopt quality standards for each experiment to meet</li><li>Cross-experiments quality — Coordinate individual experiments</li><li>Measure total business impact — Estimate the cumulative impact of all the features released in a quarter</li></ol>



<h3>Step 1 — Individual experiment quality </h3>



<p>For an experiment to bring the promised gold-standard value to a product decision, many things have to be in place: proper hypothesis, well-defined metrics to evaluate the hypothesis, decision rules set before the experiment starts, etc. These individual practices can be introduced to the teams gradually, making each one easy to integrate into the workflow. Combined, they raise each experiment to the gold-standard level. The main goal of Step 1 is to ensure that each experiment leads to a highly reliable data-informed product decision. Once the quality, and thereby the value, of each experiment starts to improve, the number of experiments will often also increase. </p>



<p>At Spotify Search, we continue to scale the speed of iteration in each team using experiments, and thus need to make each decision transparent and self-explanatory for anyone, e.g., new team members or those external to the team. For this reason, it is important to guide individual teams during Step 1 into using shared practices. Examples of such shared practices are using common conventions for naming experiments, writing hypotheses, and logging decisions after the experiment ends. We have applied this by having our Search data science team provide and advocate for simple templates for test specifications and experiment setups. These templates help remind each team member of important steps and considerations for each test while also streamlining the practices across teams. </p>



<h3>Step 2 — Cross-experiments quality</h3>



<p>Another important aspect of experimentation at Spotify is coordination. Most of the time, teams have to run several experiments on top of each other, and some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time.  The Search team is no exception and coordinating experiments is a big part of our effort.</p>



<p><a href="https://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/" target="_blank" rel="noreferrer noopener">Spotify’s experimentation platform</a> has flexible coordination capabilities, so coordinating experiments at scale is possible, but it requires cross-team collaboration and alignment. The Search data science team helps experimenting teams find the right setup for the right experiment, and provides guidelines to avoid running out of users that are available to try our new search features. This effort is again helped by the Step 1 alignment, where template experiments have pre-selected coordination behaviors that nudge team members to select the appropriate setup.</p>



<h3>Step 3 — Measure total business impact</h3>



<p>The final step of our plan to adopt a truly data-informed product development process is to measure the total impact of a product initiative over some relevant reporting time period, e.g., a quarter. The goal is to answer the question “What is the combined causal effect of all our product changes on our key metrics?” This can be done in several ways. A simple and naive total impact analysis is to sum the individual estimated causal effects from the experiments of all shipped product changes. At Spotify Search we use quarterly holdbacks to estimate the total impact of the product development program. This is achieved by holding a set of users back from all product changes during the quarter. At the end of the quarter, we run one experiment on the users in the holdback, where one group is given no product change (control group), and one group is given all the shipped product changes (treatment group). This yields an unbiased estimator of the total causal effect of all product changes and directly answers the question posed above. </p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-700x352.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-120x60.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter.png 1011w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: During the quarter</figcaption></figure></div>



<div><figure><img loading="lazy" width="700" height="307" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-700x307.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-700x307.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-250x110.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-768x337.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-120x53.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter.png 1011w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: After the quarter</figcaption></figure></div>



<p>An important aspect of these steps is that they grow from within one team and spread across teams and within each individual experiment to across several experiments within the same initiative.</p>



<p>It is not uncommon that experimentation initiatives in larger organizations are driven by the middle-level decision makers’ need for better total business impact estimates to guide future priorities. However, it is important to acknowledge that measuring the impact of a full product initiative with decent precision is not a trivial thing to achieve. This is especially challenging when product evaluation is not already a natural part of the product development culture. Our belief and experience is that the quickest way to achieve proper overall product evaluation is to build from the bottom up with solid product evaluation practices in each team. Moreover, we have found that each of these steps has brought a lot of value to our product development cycle beyond measuring impact, e.g., product quality awareness, better practices around product evaluation in general, knowledge sharing, and alignment across product teams. </p>



<h2><strong>Key driver 2 – Constant injection of energy</strong></h2>



<h3><strong>Starting the fire</strong></h3>



<p>Building great experimentation practices is like starting a fire with wet wood. It won’t start just because you provide a spark, but if you give it care and provide it with the right resources, the fire will eventually start and burn by itself. It’s the same for experimentation practices. You will need to start with a source of energy, pulling and pushing in the right direction, and you will need to continuously inject energy, for a long time.</p>



<p>Adopting new practices that sometimes conflict with existing habits is a big challenge — it will take time and effort to convince teams of the value that experimentation brings to the product development process.</p>



<p>Our learnings reveal that at the beginning of the journey, a lot of things have to be boosted and done <em>for</em> the engineering teams rather than <em>by</em> them. By actually setting up and reviewing tests <em>for</em> the teams, it allows them to see for themselves that the amount of work required on a day-to-day basis is manageable and, most importantly, the value those efforts bring to the team and product. Doing certain tasks for the teams will also deepen the trust between the experimentation advocate and the teams, reassuring engineers that they will get support along the way.</p>



<p>In the early experimenting stage of any team, it is critical to accept that some tests will not be perfect and some product decisions, therefore, not fully data informed. However, it is of the utmost importance that the teams get sufficient support to succeed with an experiment by continuously injecting support and energy, starting the fire and keeping it burning. In our experience, the key to the success of starting experimentation is endurance and continuity. The smallest lighter can make any wet wood burn with sufficient time.</p>



<h3><strong>Maintaining the fire</strong></h3>



<p>Just as we had to scale our practices between teams, we had to scale the injection of energy. The solution to scale presented itself in the form of individuals across Search teams who developed an interest and expertise in experimentation and became new precious sources of energy. As we progressed as a team towards more data-driven product development, the expertise and support gradually transferred from the advocating team to individual team members who naturally started to inject energy. </p>



<p>In big and agile organizations, there are often organizational changes made to fit new needs, as well as new people joining, which makes it harder to keep practices aligned. So even if the fire burns by itself at some point, it is still crucial to have people dedicated to keep injecting energy, restarting and boosting the fire continuously. </p>



<h2><strong>Spotify Search in 2021</strong></h2>



<p>Now that we have covered the major steps and principles that led Spotify Search on the journey to data-driven product development, we will look in more detail at the specific actions that were taken in Search last year.</p>



<div><figure><img loading="lazy" width="700" height="177" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-700x177.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-700x177.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-250x63.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-768x195.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-120x30.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline.png 1404w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<ol><li><strong>December 2020:</strong> Unlock the use of all features from our experimentation platform (like sample size calculation or result page) by integrating our Search-specific metric into the system. </li><li><strong>January 2021: </strong>Creation of a dedicated weekly forum and communication channel to make information flow better between the search engineering teams and data scientists.</li><li><strong>March 2021:</strong> Standardization of template for the test setup and test specification with pre-filled parameters. That unlocked the possibility for anyone to create and launch an experiment with basic search-specific parameters without prior knowledge.</li><li><strong>June 2021:</strong> End-of-quarter cumulative experiment. At this point, most of the new features we developed for users were tested with an experiment, so we had all the ingredients to start measuring the cumulative impact of all the features we launch in Search over a quarter. </li><li><strong>August 2021:</strong> Experimentation Champion program. Scale knowledge-sharing when new members join by establishing one person from each team as a dedicated experimentation champion. The role of the champion is to help spread knowledge between the Search data science team and their own.</li><li><strong>September 2021:</strong> Naming convention for experiments. As part of our effort to scale and make each test more transparent, we created a naming convention to be able to determine from the name of the experiment which quarter and initiative this experiment was part of.</li><li><strong>September 2021:</strong> The team committed to shipping all product changes gradually using monitoring. This was a great milestone for us! It showed that the whole team was feeling confident enough in our practices to commit to this objective.</li><li><strong>October 2021:</strong> Decision-making process improvement. To ensure that we made good decisions for each experiment we ran, the experiment design needed to contain three elements: Hypothesis, Metrics, and Decision rules.</li></ol>



<p>The key to the success of this roadmap was that the team only needed to focus on one small step at a time. Implementing these small steps over the course of just one year led to an impressive change in Search experimentation practices. </p>



<h2><strong>In 2022: keep the fire burning!</strong></h2>



<p>In 2021 we made great progress improving each experiment’s quality, improving coordination between experiments, and measuring global impact. This year in Search, we will take one more step toward truly building and improving our product in a data-informed way by integrating experimentation into a more global evaluation process for each initiative of our product development. If you’re interested in joining the team, take a look at our <a href="https://www.lifeatspotify.com/jobs/data-scientist-search-insights" target="_blank" rel="noreferrer noopener">open jobs</a>!</p>



<h2><strong>Conclusion</strong></h2>



<p>Approaching this problem, I thought that changing product development culture could only come from a change in expectations and incentives from leaders higher up in the organization. However, during the last year, I have seen many people from several disciplines stepping in and taking responsibility for the progress of our experimentation practices. It is now common to see engineers discussing if they should stop a product change from being launched to prevent a negative user experience. We have PMs on our team driving the education of PMs at Spotify globally on the topic of experimentation. </p>



<p>To me this evolution of behavior is proof that it is possible to improve our product evaluation process substantially by taking three simple measures: providing a safe environment for teams to evolve their practices, continuously injecting energy and support, and ensuring that the teams can see, with their own eyes, the benefits of the changes they’ve made.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Claire Detilleux, Sr. Data Scientist</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 28 Feb 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Mesfin Mekonnen: Senior Engineer &#xA;</title>
      <link>https://engineering.atspotify.com/2022/02/mesfin-mekonnen-senior-engineer/</link>
      <description>Mesfin is part of our Spotify New York team and jumped at the chance to work as an embed on 2021 Wrapped.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-5103">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin.png 1000w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin-120x59.png 120w" sizes="(max-width: 1000px) 100vw, 1000px"/>
                                  
             </p>
             <div>
             
                 <p><b>Mesfin is part of our Spotify New York team and jumped at the chance to work as an embed on 2021 Wrapped. </b></p>
             </div>
         </div>

         


         

         
<p><strong>Tell us more about working on Spotify Wrapped… </strong></p>



<p>I specialize in iOS Engineering and was one of a few iOS embeds working on 2021 Wrapped. We divided up the various Wrapped stories amongst ourselves — my focus was on Top Five Artists, Top Five Songs, Top Five Podcasts, Top Genres and the <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/12/17/the-audio-aura-story-mystical-to-mathematical/" target="_blank">Audio Aura</a>, which was brand new for this year and really fun to be part of. </p>



<p><strong>What were the biggest challenges? </strong></p>



<p>We all thought the Audio Aura was going to be the toughest story to build — it certainly took the most time to implement, because it had a lot of different components and we were starting from scratch, rather than building on executions from previous years. </p>



<p>However, I actually found the Top Genres story to be the most technically challenging — we needed to use a special type of software for the bar charts, programmatically drawing the text into a canvas and rendering the context as an image. I’d never done that before and learned a lot from the deeper knowledge of the other engineers. </p>



<p><strong>What made it a fun project to be part of? </strong></p>



<p>The best part of working on Wrapped was that it was a truly collaborative effort. We had a tight schedule and weekly deliverables for all the stories, which could be stressful at times. But whenever blockers came up, we could always rely on our teammates to pitch in and help out — it was a really safe, supportive atmosphere and we were constantly learning from one another. </p>



<p>All the stories had iOS and Android counterparts, so we communicated a lot with our mobile teammates to share our approaches and flag up any issues we encountered along the way. Basically, it was just a really fun team to be part of — there were so many genuinely great people involved. </p>



<p><strong>Any special shout-outs? </strong></p>



<p>So many! The whole core team was amazing – they gave me and the other embeds such a warm welcome and I really enjoyed the opportunity to work with them. </p>



<p>In particular, I’d like to shout out: </p>



<ul><li>Cait Charniga and Sona Dolasia for their incredibly detailed designs and awesome motion specs</li><li>Udaya Pillalamarri, the engineering manager, for caring so much about our wellbeing and fostering a working environment where we could be our true selves — it showed that she cared about us personally beyond delivering the project</li><li>Zela Taino, the tech lead, for her impressive project and time management skills — managing the Wrapped experience, as well as implementing some of the Wrapped stories</li><li>Kylan McBride for his deep technical iOS expertise and being someone I could always turn to with my toughest challenges.</li></ul>



<p><strong>What were the most inspiring moments? </strong></p>



<p>Having always admired the Wrapped experience from afar, I knew it would take a lot to deliver such a personalized project, with such massive scale and reach. However, I didn’t truly appreciate the amount of people and work involved — it requires literally hundreds of Spotifiers. </p>



<p>Throughout the project, I was very inspired by the attention to detail shown by our localization teams — we had weekly testing sessions, where the localization manager would identify any issues related to translations and ensure we had a quality experience in all languages. Their input pushed us to work harder on fixing issues that came up with non-Latin characters. And this kind of dedication — along with a real focus on accessibility — is something I’m taking back to my home squad and my day-to-day work at Spotify.</p><p>

         Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Fred Wang: Senior Backend Engineer￼&#xA;</title>
      <link>https://engineering.atspotify.com/2022/02/fred-wang-senior-backend-engineer/</link>
      <description>As a Senior Backend Engineer at Spotify New York, Fred’s role on 2021 Wrapped involved serving data stories content for iOS and Android. Here, he shares some of the most memorable moments…</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-5047">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred.png 600w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred-250x168.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred-120x81.png 120w" sizes="(max-width: 600px) 100vw, 600px"/>
                                  
             </p>
             <div>
             
                 <p>As a Senior Backend Engineer at Spotify New York, Fred’s role on 2021 Wrapped involved serving data stories content for iOS and Android. Here, he shares some of the most memorable moments…</p>
             </div>
         </div>

         


         

         




<h3 id="tell-us-more-about-working-on-spotify-wrapped"><strong>Tell us more about working on Spotify Wrapped… </strong></h3>



<p>I’m part of the Mambas team at Spotify, which is responsible for all the animation, presentation and personalized data in the Wrapped interactive mobile app. My job is to take the raw data from the backend and enrich it, translate it and format it for different regions before it goes to the frontend mobile engineers. That includes deciding what music to play during data stories and what images and localised text to display – all the things we serve from the backend to help iOS and Android show the data stories correctly. </p>



<h3 id="what-were-the-biggest-challenges"><strong>What were the biggest challenges? </strong></h3>



<p>Load-testing was definitely the most challenging aspect of working on this project. In the first few hours after launch, Wrapped gets a high volume of unique visitors – all with their own unique data stories, all served to different types of mobile devices. </p>



<p>So we can’t just roll it out and hope for the best – we have to simulate that load and make sure we can withstand all the requests we’re going to get at peak usage. </p>



<p>During load test sessions, we process a high volume of requests and try to uncover as many issues across our dependencies as possible. It’s a big job, but really helps discover all weak links in the chain</p>



<h3 id="what-were-the-best-moments"><strong>What were the best moments? </strong></h3>



<p>In the first two hours after the launch of Spotify Wrapped, we saw unprecedented traffic from around the world – yet we scaled without any major issues. It was nerve-wracking, but also extremely exciting to see all our hard work pay off like that.</p>



<h3 id="what-made-it-a-fun-project-to-be-part-of"><strong>What made it a fun project to be part of? </strong></h3>



<p>I really enjoyed collaborating with all the great engineers and localization teams to make the data stories work cleanly across every region, language and format. And I loved the moment when the data pipelines started working – we all turned on live data for our endpoints and got to see our own Wrapped stories for the very first time. We were all so giddy and excited – all we could do was play around and compare notes for a good few hours afterwards! </p><p>

         Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a>, <a href="https://engineering.atspotify.com/tag/wrapped21/" rel="tag">Wrapped21</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing Ruler: Our Tool for Measuring Android App Size&#xA;</title>
      <link>https://engineering.atspotify.com/2022/02/introducing-ruler-our-tool-for-measuring-android-app-size/</link>
      <description>At Spotify, we strive to make our apps available to as many people as possible. As mobile developers, that means we want everybody to be able to download our app without hiccups or constraints. One important metric related to this goal is the size of the Spotify app — if it’s too big, users with</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>February 14, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/02/introducing-ruler-our-tool-for-measuring-android-app-size/" title="Introducing Ruler: Our Tool for Measuring Android App Size">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1.png 1011w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1-120x60.png 120w" sizes="(max-width: 1011px) 100vw, 1011px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, we strive to make our apps available to as many people as possible. As mobile developers, that means we want everybody to be able to download our app without hiccups or constraints.</p>



<p>One important metric related to this goal is the size of the Spotify app — if it’s too big, users with poor network connectivity or little device storage might not be able to download it. And since shrinking download size <a href="https://medium.com/googleplaydev/shrinking-apks-growing-installs-5d3fcba23ce2" target="_blank" rel="noreferrer noopener">has been shown</a> to improve install conversion rate, we aim to keep the app as lean as possible.</p>



<p>But working with app size is not always easy, particularly for large applications with numerous contributors adding cool new features. That’s why we built and open sourced <a href="https://github.com/spotify/ruler" target="_blank" rel="noreferrer noopener">Ruler</a> — a tool to measure and analyze the size of your Android apps, built with automation in mind.</p>



<h2 id="where-we-started">Where we started</h2>



<p>We set out to see how we could decrease app size, and started our investigations by using existing tools like <a href="https://github.com/jakewharton/diffuse" target="_blank" rel="noreferrer noopener">Diffuse</a> and Android Studio. Those work great if you want to get a high-level overview of your app, but when we wanted to dig deeper, we quickly arrived at another question — how much are certain modules and dependencies contributing to the overall app size?</p>



<p>The codebase of the main Android Spotify app consists of over 1,000 Gradle modules and hundreds of third-party dependencies. All of these modules and dependencies are merged and packaged into a single app, without a clear way to determine where things came from. This can make it hard to analyze app size and determine where optimization opportunities lie.</p>



<p>Ruler is a Gradle plugin that solves this exact problem. It allows you to analyze your app and gives you detailed insights into the origin and size of certain files, modules, and third-party dependencies.</p>



<h2 id="how-does-ruler-work">How does Ruler work?</h2>



<p>Android apps are typically packaged and uploaded to the Play Store as <a href="https://developer.android.com/guide/app-bundle" target="_blank" rel="noreferrer noopener">App Bundles</a>. The Play Store uses these bundles to generate an optimized <a href="https://en.wikipedia.org/wiki/Android_application_package" target="_blank" rel="noreferrer noopener">Android application package (APK)</a> for every device. Ruler replicates this mechanism (using <a href="https://github.com/google/bundletool" target="_blank" rel="noreferrer noopener">Google’s Bundletool</a>) to generate an APK for a given device configuration. We do that to accurately measure what ends up on the devices of our users.</p>



<p>Next, we analyze this APK to see which files actually end up in the app and how much space those files take up, leveraging <a href="https://developer.android.com/studio/command-line/apkanalyzer" target="_blank" rel="noreferrer noopener">apkanalyzer</a> to ensure the numbers reported by Ruler stay consistent with those analyzed by Android Studio and guaranteeing we measure app size after any optimizations done by another process (e.g. R8). </p>



<p>For each file, Ruler captures two measurements:</p>



<ol><li><strong>Download size: </strong>Bytes transferred over the network when a user downloads the app</li><li><strong>Install size:</strong> Bytes a file takes up on the device once the app has been installed</li></ol>



<h3 id="determining-the-origin-of-files">Determining the origin of files</h3>



<p>After analyzing the APK, we end up with a list of all files and their respective sizes. But how do we determine where they came from? To do that, Ruler scans through all Gradle modules and dependencies included in the build and analyzes which files they contain. The result of this is a second list of all components and their contents. Based on this second list, we can now group all files of the app by their source and therefore determine how much each module and dependency contributes to the overall app size.</p>



<p>Ruler adds a simple analyzeReleaseBundle task to your project, which you can use to execute all this logic. This task will generate two outputs — a JSON report you can use for further processing and an HTML report you can use to analyze and dig into the data yourself.</p>







<h3 id="class-size">Class size</h3>



<p>On Android, all classes are compressed into one or more DEX files. Because we want to be able to see the impact of individual classes, Ruler parses those DEX files and treats every class like a separate file.</p>



<p>When classes are compressed, some information is shared between class entries inside the DEX file. Because of that, it’s not possible to 100% accurately measure how much a single class contributes to the overall app size. Ruler solves this problem by approximating the size of each class by setting the raw class size in proportion to the size of the DEX file.</p>



<h3 id="ownership">Ownership</h3>



<p>Knowing which components contribute to the size of an app is great, but knowing who owns these components is even better. Because of this, Ruler supports gathering and analyzing app size ownership data.</p>



<p>If you provide a list of component owners to Ruler, it can analyze the contributions of certain teams to the size of the overall app. This can be helpful to determine who to talk to if questions about certain parts of the app arise.</p>



<div><figure><img loading="lazy" width="700" height="569" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-700x569.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-700x569.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-250x203.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-768x625.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-1536x1249.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-120x98.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figures above are for illustrative purposes only.</figcaption></figure></div>



<h2 id="ruler-at-spotify">Ruler at Spotify</h2>



<p>We’ve been using Ruler at Spotify for over half a year now and have seen great success. It has allowed us to identify many improvement opportunities and we have been able to reduce our app size by a little over 9% so far.</p>



<p>We export the app size data once a day, using the latest main build. This data is used to track historical trends, both of the app as a whole and of individual modules and third-party dependencies. Additionally, we analyze the app size impact of every pull request, so we can give early feedback to developers and prevent regressions from being merged in the first place.</p>



<h2 id="start-using-ruler-today">Start using Ruler today</h2>



<p>If you are curious about Ruler, you can try it out today. All you need to do is apply the plugin to your Android project and run a single Gradle task. Check out <a href="https://github.com/spotify/ruler" target="_blank" rel="noreferrer noopener">GitHub</a> for an always up-to-date guide on how you can integrate Ruler.</p>



<h2 id="contributing-to-ruler">Contributing to Ruler</h2>



<p>Ruler is fully written in Kotlin and leverages exciting technologies like Kotlin Multiplatform. Ruler continues to be actively developed, and we already have many exciting ideas about our new tool. At Spotify, we benefit immensely from open source software, so we decided to open source Ruler and give back to the community.</p>



<p>We’re always looking for input — both raising issues and opening pull requests are very welcome and appreciated. We believe that, together, we can move this project further and help make Android apps accessible to more people.</p>



<p>And if you want to work full time on tools like Ruler, please check out our <a href="https://www.lifeatspotify.com/jobs?c=mobile" target="_blank" rel="noreferrer noopener">open job positions</a> — we’re always looking for great minds to join the band.</p>




        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Simon Schiller, Android Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning&#xA;</title>
      <link>https://engineering.atspotify.com/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/</link>
      <description>Introduction Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built ML Home, the internal user interface for Spotify’s Ma</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>January 19, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/" title="Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<h2>Introduction </h2>



<p>Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built <strong>ML Home</strong>, the internal user interface for Spotify’s Machine Learning Platform, and the product lessons we learned along the way in our quest to entrench it in Spotify’s ML ecosystem.</p>



<p>It’s a massive understatement to say that machine learning is at the heart of Spotify’s success story. Spotify has delivered beloved audio experiences such as Discover Weekly, Daily Mix, and <a href="https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/" target="_blank" rel="noreferrer noopener">Wrapped</a> on the strength of ML-powered personalized recommendations. Today, almost every part of Spotify has some applied ML systems, and a significant and growing portion of our R&amp;D teams consist of ML engineers and data scientists. In order to support ML systems at the scale and speed that our business requires, and to apply ML responsibly for our listeners, we have platformized sizable parts of the most common ML infrastructure within our Machine Learning Platform. </p>



<h2>Overview of Spotify’s ML Platform<br/></h2>



<p>Since the beginning, our ambition for <a href="https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/" target="_blank" rel="noreferrer noopener">Spotify’s ML Platform</a> was to connect the end-to-end user journey for ML practitioners. We subscribe to the “walking skeleton” model of product development, focusing from the start on end-to-end workflow and subsequently fleshing out functionality once we’ve proven value.</p>



<div><figure><img loading="lazy" width="700" height="154" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-250x55.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-768x169.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow.png 1448w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>In late 2019 / early 2020, our ML Platform consisted of a few components that covered the (supervised) machine learning workflow experience for Spotify’s ML practitioners: </p>



<ul><li>Spotify Kubeflow<strong>, </strong>which is our version of the open source Kubeflow Pipelines platform that helped us standardize ML workflows on the TensorFlow Extended (TFX) ecosystem  </li><li>Jukebox,<strong> </strong>which is based on TensorFlow Transform and powers our feature engineering and management workflows </li><li>Salem, which is based on TensorFlow Serving and helps us standardize model serving and production workflows, and </li><li><a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/" target="_blank" rel="noreferrer noopener">Klio</a>, which is our open source solution for audio processing with Apache Beam and Dataflow</li></ul>



<div><figure><img loading="lazy" width="700" height="153" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-250x54.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-768x167.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-1536x335.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1.png 1606w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>The Product Opportunity</h2>



<p>As we started to onboard more ML teams onto our platform, we identified two important gaps in our end-to-end support for ML workflows: </p>



<ul><li>A <strong>centralized metadata layer</strong>,<strong> </strong>where we could define our platform entities / entity relationships (e.g., models, evaluations, training sets)</li><li>A <strong>metadata presentation layer</strong>, where users could store, track, and manage the metadata generated from their ML workflows, which is the focus of this blogpost</li></ul>



<p>As the ML Platform team, we knew we wanted a tool where ML engineers could store ML project information and access metadata related to the ML application lifecycle, but weren’t entirely sure what that product would be. As we began exploring, we found that teams were using spreadsheets to track ML metadata and gave us hyper-specific feature requests for their individual problems. We also came away with broader unmet needs such as discovery of ML projects<strong>,</strong> support for ML team collaboration<strong>,</strong> and important product<strong> </strong>gaps within our own ML Platform tooling. These learnings informed the initial scope of our MVP (minimally viable product) and taught us our first product lesson. </p>



<h2>Product Lesson 1: Balancing Product Vision and Product Strategy</h2>



<p>For the first iteration of the product, we took a <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> approach. We focused on building horizontal solutions<strong> </strong>for needs we heard most commonly across all ML practitioner roles, such as being able to collaborate more effectively as an ML team. We also built a vertical solution<strong> </strong>that mapped to a specific platform tooling gap: better evaluation tooling for offline model training for ML engineers. We launched our MVP with an aspirational name and product vision: ML Home, one-stop shop for machine learning at Spotify.</p>



<div><figure><img loading="lazy" width="700" height="282" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-250x101.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-768x309.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-120x48.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021.png 1504w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><p>The initial feedback we received on our MVP fell on two ends of the spectrum. Individual contributions struggled with the broad idea of a “one-stop shop” and wanted to know what concrete problems the product could solve for them today. Leadership wanted to know how many users we would serve over the long run and how big our impact could be.</p><p>Throughout the process of selling our MVP, we learned how difficult it is to balance product vision and product strategy without compromising one for the other. Had we scratched our broader vision based on the initial feedback and focused exclusively on the concrete needs (e.g., I want to see all my training pipelines in one view), we would have risked delivering a narrow point solution. On the other hand, if we over-indexed our roadmap on the broad, ambiguous needs (e.g. I want to collaborate more effectively with my team), we would have delivered a nice-to-have but not a must-have product. </p></div>



<p>By intentionally keeping our <strong>Product Vision broad</strong> <strong>and future-looking </strong>(one-stop shop)<strong>, </strong>we gave ourselves the runway to think bigger about our solution space and our potential impact down the line. And by keeping <strong>Product Strategy concrete</strong> <strong>and iterative</strong> (offline evaluation tool), we were able to ensure that we solved concrete problems that over time helped us ingrain the product into our user’s daily workflows. </p>



<h2>Product Lesson 2: The Limits of MVPs </h2>



<div><figure><img loading="lazy" width="700" height="355" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home.png 1080w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>ML Home</figcaption></figure></div>



<p>As we wrapped up the feedback and adoption drive for the MVP, we learned our second product lesson. It is no secret that driving product adoption is hard, especially for products that are trying to replace existing solutions or market incumbents. We hit the ceiling of our MVP’s potential fairly quickly. We did not see a surge of adoption beyond the handful of users who were involved in the very early ideation process. Most users understood the value proposition of what we were building, but did not see enough depth to switch over from their existing tooling. In retrospect, our expectations of what we delivered and how valuable it would be did not match the depth of our users’ needs. It would have been easy at that stage to dismiss the product entirely, based on early adoption signals. That would have been a mistake. </p>



<p>As we plowed on with more detailed user feedback in our quest to drive more adoption, it became clear to us that we were misplacing our expectations on what role the MVP played in the product development process. <strong>The most valuable end goal of an MVP is to get enough of the vision and strategies out there to help validate or invalidate them. </strong>Our initial MVP helped us test and de-risk our work because we were able to get detailed validation of the workflows from ML teams and lay out the technical foundations for the product. It did not matter how many daily active users we had at that stage, as long as we had enough users (which we did) who saw the value in what we were building. These users continued to attend our user feedback sessions and helped us get the product to a higher and more valuable place.</p>



<h2>Product Lesson 3: Knowing the true Differentiators</h2>



<p>As we moved beyond the MVP phase and started to map out our next steps (focusing on some aspects of the product over others), we learned our third and perhaps most important product lesson. We realized that in order to provide a really valuable product to our users, we needed to not only reach feature parity with existing solutions, but also double down on ML Home’s unique differentiators. In short, ML Home as a product needed to be more compelling than the competing solutions. </p>



<p>For a while, we probed, debated, and stack-ranked specific features and workflows that we felt would be game-changing for our users. Our theory was that if we built a “compelling feature,” it would be able to singularly pass a threshold for users to adopt. In the end, we realized that the unique differentiators for ML Home actually came in the form of our other ML Platform offerings, not any one individual feature. </p>



<p>While some aspects of ML Home could functionally serve as a stand-alone product, by enriching it with training, evaluation, and system metadata generated from the rest of our ML Platform, it became a much more compelling product. <strong>ML Home’s unique differentiator isn’t any one silver-bullet feature but rather the gateway value it provides as the sum of our ML Platform capabilities.</strong></p>



<p>Much of the work we did in this phase was building out our metadata service to consolidate our overall entities and concepts across the platform, but we also spent significant time building flexibility into the product’s interface. For example, annotation capabilities such as tagging and notes became key features that enabled teams to customize and mirror their own workflows. That, paired with a faster, slicker product experience and information-rich model comparisons, tipped the balance in our favor. </p>



<p>By the time we released the second version of ML Home, we had successfully onboarded more ML teams who were actively using the product in their daily workflows. </p>







<div><figure><img loading="lazy" width="700" height="473" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-250x169.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-768x519.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-1536x1039.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-120x81.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Sample ML Project in ML Home. For illustrative purposes only.</figcaption></figure></div>



<h2>Scaling the Product to Our Vision</h2>



<p>Getting closer to<strong> </strong>product-market fit taught us a lot about how to iterate moving forward. We knew that ML Home only served one typical ML workflow. However, in order to be an indispensable product for <em>all</em> ML practitioners, it needed to cover more ground. We also knew that tightly coupling ML Home’s capabilities to our existing ML Platform products resulted in a much higher rate of adoption than stand-alone solutions. Armed with these takeaways, we wireframed a broader vision for the product.</p>



<p>Today, ML Home provides Spotify’s ML practitioners with artifacts and workflow metadata of all models passing through individual components of our ML Platform. It includes capabilities such as the ability to track and evaluate offline experiments, visualize results, track and monitor deployed models, explore features, certify models for production readiness, and much more.  </p>



<p>Through intuitive workflows and simplified information architecture, users are able to quickly spin up a project space to collaborate with their team and discover the 220+ ML projects across Spotify currently listed in ML Home. </p>



<p>We have seen a 200% growth in daily active users since we began our scaling efforts a year ago, and ML Home is now solidly entrenched in the daily workflows of some of the most important ML teams at Spotify. Despite its short tenure in Spotify’s Infrastructure landscape, ML Home is well on its way to becoming the one-stop shop for all things ML at Spotify. <br/></p>



<div><figure><img loading="lazy" width="700" height="476" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-768x523.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home.png 1340w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Conceptual rendering of ML Home. For illustrative purposes only.</figcaption></figure></div>



<h2>Three Key Lessons</h2>



<p>The saying goes that hindsight is 20/20, and it’s true. Looking back, these are the lessons that stick out the most from our product development process: </p>



<ol><li><strong>Product vision vs product strategy. </strong>It is difficult to strike the right balance between an inspiring vision that can support future solutions and a responsive product strategy that addresses today’s problems. But it is crucially important to not conflate the two in the early stages of product development. </li><li><strong>Limits of MVPs.</strong> MVPs provide the most value as a validation and de-risking tool for product strategy and overall direction. </li><li><strong>Know the true differentiators.</strong> It’s worth paying attention to what the real differentiators are for a product. It does not have to be a “compelling” feature; it can simply be opportunities found in the ecosystem that turn the tide for a product’s success. </li></ol>



<h2>Looking Ahead </h2>



<p>ML Home is not done — not even close. We know this because, in the last year, Spotify’s ML community has proposed new and inventive ways to evolve the product. For exampleML engineers saw the potential to build on top of ML Home and proposed we build production readiness certification of ML models in the interface. In addition, we are exploring aspects such as explainability to advance model interpretability and observability to better understand model health. Then there are the ever-inspiring hack week projects that tell us that our product has taken root at Spotify. We are excited for what’s next! </p>



<p>If you are interested in building cutting edge machine learning infrastructure at Spotify, we are <a href="https://www.lifeatspotify.com/jobs" target="_blank" rel="noreferrer noopener">hiring</a> for engineering and product roles across the board. </p>



<h2>Acknowledgments</h2>



<p>ML Home would not exist without the brilliant work of the ML UX team, our teammates from ML Platform, and the generous guidance from Spotify’s ML community. Since the list of individuals to thank would far exceed the words in this post, I will instead mention the individuals whose work made ML Home possible: Johan Bååth, Joshua Baer, Hayden Betts, Martin Bomio, Matt Brown, Keshi Dai, Omar Delarosa, Funmilayo Doro, Gandalf Hernandez, Adam Laiacano, Brian Martin, Daniel Norberg, James O’Dwyer, Ed Samour, Wesley Yee, and Qi Zheng.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning&#xA;</title>
      <link>https://engineering.atspotify.com/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/</link>
      <description>Introduction Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built ML Home, the internal user interface for Spotify’s Ma</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>January 19, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/" title="Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<h2>Introduction </h2>



<p>Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built <strong>ML Home</strong>, the internal user interface for Spotify’s Machine Learning Platform, and the product lessons we learned along the way in our quest to entrench it in Spotify’s ML ecosystem.</p>



<p>It’s a massive understatement to say that machine learning is at the heart of Spotify’s success story. Spotify has delivered beloved audio experiences such as Discover Weekly, Daily Mix, and <a href="https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/" target="_blank" rel="noreferrer noopener">Wrapped</a> on the strength of ML-powered personalized recommendations. Today, almost every part of Spotify has some applied ML systems, and a significant and growing portion of our R&amp;D teams consist of ML engineers and data scientists. In order to support ML systems at the scale and speed that our business requires, and to apply ML responsibly for our listeners, we have platformized sizable parts of the most common ML infrastructure within our Machine Learning Platform. </p>



<h2>Overview of Spotify’s ML Platform<br/></h2>



<p>Since the beginning, our ambition for <a href="https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/" target="_blank" rel="noreferrer noopener">Spotify’s ML Platform</a> was to connect the end-to-end user journey for ML practitioners. We subscribe to the “walking skeleton” model of product development, focusing from the start on end-to-end workflow and subsequently fleshing out functionality once we’ve proven value.</p>



<div><figure><img loading="lazy" width="700" height="154" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-250x55.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-768x169.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow.png 1448w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>In late 2019 / early 2020, our ML Platform consisted of a few components that covered the (supervised) machine learning workflow experience for Spotify’s ML practitioners: </p>



<ul><li>Spotify Kubeflow<strong>, </strong>which is our version of the open source Kubeflow Pipelines platform that helped us standardize ML workflows on the TensorFlow Extended (TFX) ecosystem  </li><li>Jukebox,<strong> </strong>which is based on TensorFlow Transform and powers our feature engineering and management workflows </li><li>Salem, which is based on TensorFlow Serving and helps us standardize model serving and production workflows, and </li><li><a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/" target="_blank" rel="noreferrer noopener">Klio</a>, which is our open source solution for audio processing with Apache Beam and Dataflow</li></ul>



<div><figure><img loading="lazy" width="700" height="153" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-250x54.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-768x167.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-1536x335.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1.png 1606w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>The Product Opportunity</h2>



<p>As we started to onboard more ML teams onto our platform, we identified two important gaps in our end-to-end support for ML workflows: </p>



<ul><li>A <strong>centralized metadata layer</strong>,<strong> </strong>where we could define our platform entities / entity relationships (e.g., models, evaluations, training sets)</li><li>A <strong>metadata presentation layer</strong>, where users could store, track, and manage the metadata generated from their ML workflows, which is the focus of this blogpost</li></ul>



<p>As the ML Platform team, we knew we wanted a tool where ML engineers could store ML project information and access metadata related to the ML application lifecycle, but weren’t entirely sure what that product would be. As we began exploring, we found that teams were using spreadsheets to track ML metadata and gave us hyper-specific feature requests for their individual problems. We also came away with broader unmet needs such as discovery of ML projects<strong>,</strong> support for ML team collaboration<strong>,</strong> and important product<strong> </strong>gaps within our own ML Platform tooling. These learnings informed the initial scope of our MVP (minimally viable product) and taught us our first product lesson. </p>



<h2>Product Lesson 1: Balancing Product Vision and Product Strategy</h2>



<p>For the first iteration of the product, we took a <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> approach. We focused on building horizontal solutions<strong> </strong>for needs we heard most commonly across all ML practitioner roles, such as being able to collaborate more effectively as an ML team. We also built a vertical solution<strong> </strong>that mapped to a specific platform tooling gap: better evaluation tooling for offline model training for ML engineers. We launched our MVP with an aspirational name and product vision: ML Home, one-stop shop for machine learning at Spotify.</p>



<div><figure><img loading="lazy" width="700" height="282" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-250x101.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-768x309.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-120x48.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021.png 1504w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><p>The initial feedback we received on our MVP fell on two ends of the spectrum. Individual contributions struggled with the broad idea of a “one-stop shop” and wanted to know what concrete problems the product could solve for them today. Leadership wanted to know how many users we would serve over the long run and how big our impact could be.</p><p>Throughout the process of selling our MVP, we learned how difficult it is to balance product vision and product strategy without compromising one for the other. Had we scratched our broader vision based on the initial feedback and focused exclusively on the concrete needs (e.g., I want to see all my training pipelines in one view), we would have risked delivering a narrow point solution. On the other hand, if we over-indexed our roadmap on the broad, ambiguous needs (e.g. I want to collaborate more effectively with my team), we would have delivered a nice-to-have but not a must-have product. </p></div>



<p>By intentionally keeping our <strong>Product Vision broad</strong> <strong>and future-looking </strong>(one-stop shop)<strong>, </strong>we gave ourselves the runway to think bigger about our solution space and our potential impact down the line. And by keeping <strong>Product Strategy concrete</strong> <strong>and iterative</strong> (offline evaluation tool), we were able to ensure that we solved concrete problems that over time helped us ingrain the product into our user’s daily workflows. </p>



<h2>Product Lesson 2: The Limits of MVPs </h2>



<div><figure><img loading="lazy" width="700" height="355" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home.png 1080w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>ML Home</figcaption></figure></div>



<p>As we wrapped up the feedback and adoption drive for the MVP, we learned our second product lesson. It is no secret that driving product adoption is hard, especially for products that are trying to replace existing solutions or market incumbents. We hit the ceiling of our MVP’s potential fairly quickly. We did not see a surge of adoption beyond the handful of users who were involved in the very early ideation process. Most users understood the value proposition of what we were building, but did not see enough depth to switch over from their existing tooling. In retrospect, our expectations of what we delivered and how valuable it would be did not match the depth of our users’ needs. It would have been easy at that stage to dismiss the product entirely, based on early adoption signals. That would have been a mistake. </p>



<p>As we plowed on with more detailed user feedback in our quest to drive more adoption, it became clear to us that we were misplacing our expectations on what role the MVP played in the product development process. <strong>The most valuable end goal of an MVP is to get enough of the vision and strategies out there to help validate or invalidate them. </strong>Our initial MVP helped us test and de-risk our work because we were able to get detailed validation of the workflows from ML teams and lay out the technical foundations for the product. It did not matter how many daily active users we had at that stage, as long as we had enough users (which we did) who saw the value in what we were building. These users continued to attend our user feedback sessions and helped us get the product to a higher and more valuable place.</p>



<h2>Product Lesson 3: Knowing the true Differentiators</h2>



<p>As we moved beyond the MVP phase and started to map out our next steps (focusing on some aspects of the product over others), we learned our third and perhaps most important product lesson. We realized that in order to provide a really valuable product to our users, we needed to not only reach feature parity with existing solutions, but also double down on ML Home’s unique differentiators. In short, ML Home as a product needed to be more compelling than the competing solutions. </p>



<p>For a while, we probed, debated, and stack-ranked specific features and workflows that we felt would be game-changing for our users. Our theory was that if we built a “compelling feature,” it would be able to singularly pass a threshold for users to adopt. In the end, we realized that the unique differentiators for ML Home actually came in the form of our other ML Platform offerings, not any one individual feature. </p>



<p>While some aspects of ML Home could functionally serve as a stand-alone product, by enriching it with training, evaluation, and system metadata generated from the rest of our ML Platform, it became a much more compelling product. <strong>ML Home’s unique differentiator isn’t any one silver-bullet feature but rather the gateway value it provides as the sum of our ML Platform capabilities.</strong></p>



<p>Much of the work we did in this phase was building out our metadata service to consolidate our overall entities and concepts across the platform, but we also spent significant time building flexibility into the product’s interface. For example, annotation capabilities such as tagging and notes became key features that enabled teams to customize and mirror their own workflows. That, paired with a faster, slicker product experience and information-rich model comparisons, tipped the balance in our favor. </p>



<p>By the time we released the second version of ML Home, we had successfully onboarded more ML teams who were actively using the product in their daily workflows. </p>







<div><figure><img loading="lazy" width="700" height="473" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-250x169.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-768x519.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-1536x1039.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-120x81.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Sample ML Project in ML Home. For illustrative purposes only.</figcaption></figure></div>



<h2>Scaling the Product to Our Vision</h2>



<p>Getting closer to<strong> </strong>product-market fit taught us a lot about how to iterate moving forward. We knew that ML Home only served one typical ML workflow. However, in order to be an indispensable product for <em>all</em> ML practitioners, it needed to cover more ground. We also knew that tightly coupling ML Home’s capabilities to our existing ML Platform products resulted in a much higher rate of adoption than stand-alone solutions. Armed with these takeaways, we wireframed a broader vision for the product.</p>



<p>Today, ML Home provides Spotify’s ML practitioners with artifacts and workflow metadata of all models passing through individual components of our ML Platform. It includes capabilities such as the ability to track and evaluate offline experiments, visualize results, track and monitor deployed models, explore features, certify models for production readiness, and much more.  </p>



<p>Through intuitive workflows and simplified information architecture, users are able to quickly spin up a project space to collaborate with their team and discover the 220+ ML projects across Spotify currently listed in ML Home. </p>



<p>We have seen a 200% growth in daily active users since we began our scaling efforts a year ago, and ML Home is now solidly entrenched in the daily workflows of some of the most important ML teams at Spotify. Despite its short tenure in Spotify’s Infrastructure landscape, ML Home is well on its way to becoming the one-stop shop for all things ML at Spotify. <br/></p>



<div><figure><img loading="lazy" width="700" height="476" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-768x523.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home.png 1340w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Conceptual rendering of ML Home. For illustrative purposes only.</figcaption></figure></div>



<h2>Three Key Lessons</h2>



<p>The saying goes that hindsight is 20/20, and it’s true. Looking back, these are the lessons that stick out the most from our product development process: </p>



<ol><li><strong>Product vision vs product strategy. </strong>It is difficult to strike the right balance between an inspiring vision that can support future solutions and a responsive product strategy that addresses today’s problems. But it is crucially important to not conflate the two in the early stages of product development. </li><li><strong>Limits of MVPs.</strong> MVPs provide the most value as a validation and de-risking tool for product strategy and overall direction. </li><li><strong>Know the true differentiators.</strong> It’s worth paying attention to what the real differentiators are for a product. It does not have to be a “compelling” feature; it can simply be opportunities found in the ecosystem that turn the tide for a product’s success. </li></ol>



<h2>Looking Ahead </h2>



<p>ML Home is not done — not even close. We know this because, in the last year, Spotify’s ML community has proposed new and inventive ways to evolve the product. For exampleML engineers saw the potential to build on top of ML Home and proposed we build production readiness certification of ML models in the interface. In addition, we are exploring aspects such as explainability to advance model interpretability and observability to better understand model health. Then there are the ever-inspiring hack week projects that tell us that our product has taken root at Spotify. We are excited for what’s next! </p>



<p>If you are interested in building cutting edge machine learning infrastructure at Spotify, we are <a href="https://www.lifeatspotify.com/jobs" target="_blank" rel="noreferrer noopener">hiring</a> for engineering and product roles across the board. </p>



<h2>Acknowledgments</h2>



<p>ML Home would not exist without the brilliant work of the ML UX team, our teammates from ML Platform, and the generous guidance from Spotify’s ML community. Since the list of individuals to thank would far exceed the words in this post, I will instead mention the individuals whose work made ML Home possible: Johan Bååth, Joshua Baer, Hayden Betts, Martin Bomio, Matt Brown, Keshi Dai, Omar Delarosa, Funmilayo Doro, Gandalf Hernandez, Adam Laiacano, Brian Martin, Daniel Norberg, James O’Dwyer, Ed Samour, Wesley Yee, and Qi Zheng.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning&#xA;</title>
      <link>https://engineering.atspotify.com/2022/01/19/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/</link>
      <description>Introduction Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built ML Home, the internal user interface for Spotify’s Ma</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>January 19, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/01/19/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/" title="Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<h2>Introduction </h2>



<p>Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built <strong>ML Home</strong>, the internal user interface for Spotify’s Machine Learning Platform, and the product lessons we learned along the way in our quest to entrench it in Spotify’s ML ecosystem.</p>



<p>It’s a massive understatement to say that machine learning is at the heart of Spotify’s success story. Spotify has delivered beloved audio experiences such as Discover Weekly, Daily Mix, and <a href="https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/" target="_blank" rel="noreferrer noopener">Wrapped</a> on the strength of ML-powered personalized recommendations. Today, almost every part of Spotify has some applied ML systems, and a significant and growing portion of our R&amp;D teams consist of ML engineers and data scientists. In order to support ML systems at the scale and speed that our business requires, and to apply ML responsibly for our listeners, we have platformized sizable parts of the most common ML infrastructure within our Machine Learning Platform. </p>



<h2>Overview of Spotify’s ML Platform<br/></h2>



<p>Since the beginning, our ambition for <a href="https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/" target="_blank" rel="noreferrer noopener">Spotify’s ML Platform</a> was to connect the end-to-end user journey for ML practitioners. We subscribe to the “walking skeleton” model of product development, focusing from the start on end-to-end workflow and subsequently fleshing out functionality once we’ve proven value.</p>



<div><figure><img loading="lazy" width="700" height="154" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-250x55.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-768x169.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow.png 1448w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>In late 2019 / early 2020, our ML Platform consisted of a few components that covered the (supervised) machine learning workflow experience for Spotify’s ML practitioners: </p>



<ul><li>Spotify Kubeflow<strong>, </strong>which is our version of the open source Kubeflow Pipelines platform that helped us standardize ML workflows on the TensorFlow Extended (TFX) ecosystem  </li><li>Jukebox,<strong> </strong>which is based on TensorFlow Transform and powers our feature engineering and management workflows </li><li>Salem, which is based on TensorFlow Serving and helps us standardize model serving and production workflows, and </li><li><a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/" target="_blank" rel="noreferrer noopener">Klio</a>, which is our open source solution for audio processing with Apache Beam and Dataflow</li></ul>



<div><figure><img loading="lazy" width="700" height="153" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-250x54.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-768x167.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-1536x335.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1.png 1606w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>The Product Opportunity</h2>



<p>As we started to onboard more ML teams onto our platform, we identified two important gaps in our end-to-end support for ML workflows: </p>



<ul><li>A <strong>centralized metadata layer</strong>,<strong> </strong>where we could define our platform entities / entity relationships (e.g., models, evaluations, training sets)</li><li>A <strong>metadata presentation layer</strong>, where users could store, track, and manage the metadata generated from their ML workflows, which is the focus of this blogpost</li></ul>



<p>As the ML Platform team, we knew we wanted a tool where ML engineers could store ML project information and access metadata related to the ML application lifecycle, but weren’t entirely sure what that product would be. As we began exploring, we found that teams were using spreadsheets to track ML metadata and gave us hyper-specific feature requests for their individual problems. We also came away with broader unmet needs such as discovery of ML projects<strong>,</strong> support for ML team collaboration<strong>,</strong> and important product<strong> </strong>gaps within our own ML Platform tooling. These learnings informed the initial scope of our MVP (minimally viable product) and taught us our first product lesson. </p>



<h2>Product Lesson 1: Balancing Product Vision and Product Strategy</h2>



<p>For the first iteration of the product, we took a <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> approach. We focused on building horizontal solutions<strong> </strong>for needs we heard most commonly across all ML practitioner roles, such as being able to collaborate more effectively as an ML team. We also built a vertical solution<strong> </strong>that mapped to a specific platform tooling gap: better evaluation tooling for offline model training for ML engineers. We launched our MVP with an aspirational name and product vision: ML Home, one-stop shop for machine learning at Spotify.</p>



<div><figure><img loading="lazy" width="700" height="282" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-250x101.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-768x309.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-120x48.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021.png 1504w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><p>The initial feedback we received on our MVP fell on two ends of the spectrum. Individual contributions struggled with the broad idea of a “one-stop shop” and wanted to know what concrete problems the product could solve for them today. Leadership wanted to know how many users we would serve over the long run and how big our impact could be.</p><p>Throughout the process of selling our MVP, we learned how difficult it is to balance product vision and product strategy without compromising one for the other. Had we scratched our broader vision based on the initial feedback and focused exclusively on the concrete needs (e.g., I want to see all my training pipelines in one view), we would have risked delivering a narrow point solution. On the other hand, if we over-indexed our roadmap on the broad, ambiguous needs (e.g. I want to collaborate more effectively with my team), we would have delivered a nice-to-have but not a must-have product. </p></div>



<p>By intentionally keeping our <strong>Product Vision broad</strong> <strong>and future-looking </strong>(one-stop shop)<strong>, </strong>we gave ourselves the runway to think bigger about our solution space and our potential impact down the line. And by keeping <strong>Product Strategy concrete</strong> <strong>and iterative</strong> (offline evaluation tool), we were able to ensure that we solved concrete problems that over time helped us ingrain the product into our user’s daily workflows. </p>



<h2>Product Lesson 2: The Limits of MVPs </h2>



<div><figure><img loading="lazy" width="700" height="355" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home.png 1080w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>ML Home</figcaption></figure></div>



<p>As we wrapped up the feedback and adoption drive for the MVP, we learned our second product lesson. It is no secret that driving product adoption is hard, especially for products that are trying to replace existing solutions or market incumbents. We hit the ceiling of our MVP’s potential fairly quickly. We did not see a surge of adoption beyond the handful of users who were involved in the very early ideation process. Most users understood the value proposition of what we were building, but did not see enough depth to switch over from their existing tooling. In retrospect, our expectations of what we delivered and how valuable it would be did not match the depth of our users’ needs. It would have been easy at that stage to dismiss the product entirely, based on early adoption signals. That would have been a mistake. </p>



<p>As we plowed on with more detailed user feedback in our quest to drive more adoption, it became clear to us that we were misplacing our expectations on what role the MVP played in the product development process. <strong>The most valuable end goal of an MVP is to get enough of the vision and strategies out there to help validate or invalidate them. </strong>Our initial MVP helped us test and de-risk our work because we were able to get detailed validation of the workflows from ML teams and lay out the technical foundations for the product. It did not matter how many daily active users we had at that stage, as long as we had enough users (which we did) who saw the value in what we were building. These users continued to attend our user feedback sessions and helped us get the product to a higher and more valuable place.</p>



<h2>Product Lesson 3: Knowing the true Differentiators</h2>



<p>As we moved beyond the MVP phase and started to map out our next steps (focusing on some aspects of the product over others), we learned our third and perhaps most important product lesson. We realized that in order to provide a really valuable product to our users, we needed to not only reach feature parity with existing solutions, but also double down on ML Home’s unique differentiators. In short, ML Home as a product needed to be more compelling than the competing solutions. </p>



<p>For a while, we probed, debated, and stack-ranked specific features and workflows that we felt would be game-changing for our users. Our theory was that if we built a “compelling feature,” it would be able to singularly pass a threshold for users to adopt. In the end, we realized that the unique differentiators for ML Home actually came in the form of our other ML Platform offerings, not any one individual feature. </p>



<p>While some aspects of ML Home could functionally serve as a stand-alone product, by enriching it with training, evaluation, and system metadata generated from the rest of our ML Platform, it became a much more compelling product. <strong>ML Home’s unique differentiator isn’t any one silver-bullet feature but rather the gateway value it provides as the sum of our ML Platform capabilities.</strong></p>



<p>Much of the work we did in this phase was building out our metadata service to consolidate our overall entities and concepts across the platform, but we also spent significant time building flexibility into the product’s interface. For example, annotation capabilities such as tagging and notes became key features that enabled teams to customize and mirror their own workflows. That, paired with a faster, slicker product experience and information-rich model comparisons, tipped the balance in our favor. </p>



<p>By the time we released the second version of ML Home, we had successfully onboarded more ML teams who were actively using the product in their daily workflows. </p>







<div><figure><img loading="lazy" width="700" height="473" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-250x169.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-768x519.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-1536x1039.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-120x81.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Sample ML Project in ML Home. For illustrative purposes only.</figcaption></figure></div>



<h2>Scaling the Product to Our Vision</h2>



<p>Getting closer to<strong> </strong>product-market fit taught us a lot about how to iterate moving forward. We knew that ML Home only served one typical ML workflow. However, in order to be an indispensable product for <em>all</em> ML practitioners, it needed to cover more ground. We also knew that tightly coupling ML Home’s capabilities to our existing ML Platform products resulted in a much higher rate of adoption than stand-alone solutions. Armed with these takeaways, we wireframed a broader vision for the product.</p>



<p>Today, ML Home provides Spotify’s ML practitioners with artifacts and workflow metadata of all models passing through individual components of our ML Platform. It includes capabilities such as the ability to track and evaluate offline experiments, visualize results, track and monitor deployed models, explore features, certify models for production readiness, and much more.  </p>



<p>Through intuitive workflows and simplified information architecture, users are able to quickly spin up a project space to collaborate with their team and discover the 220+ ML projects across Spotify currently listed in ML Home. </p>



<p>We have seen a 200% growth in daily active users since we began our scaling efforts a year ago, and ML Home is now solidly entrenched in the daily workflows of some of the most important ML teams at Spotify. Despite its short tenure in Spotify’s Infrastructure landscape, ML Home is well on its way to becoming the one-stop shop for all things ML at Spotify. <br/></p>



<div><figure><img loading="lazy" width="700" height="476" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-768x523.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home.png 1340w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Conceptual rendering of ML Home. For illustrative purposes only.</figcaption></figure></div>



<h2>Three Key Lessons</h2>



<p>The saying goes that hindsight is 20/20, and it’s true. Looking back, these are the lessons that stick out the most from our product development process: </p>



<ol><li><strong>Product vision vs product strategy. </strong>It is difficult to strike the right balance between an inspiring vision that can support future solutions and a responsive product strategy that addresses today’s problems. But it is crucially important to not conflate the two in the early stages of product development. </li><li><strong>Limits of MVPs.</strong> MVPs provide the most value as a validation and de-risking tool for product strategy and overall direction. </li><li><strong>Know the true differentiators.</strong> It’s worth paying attention to what the real differentiators are for a product. It does not have to be a “compelling” feature; it can simply be opportunities found in the ecosystem that turn the tide for a product’s success. </li></ol>



<h2>Looking Ahead </h2>



<p>ML Home is not done — not even close. We know this because, in the last year, Spotify’s ML community has proposed new and inventive ways to evolve the product. For exampleML engineers saw the potential to build on top of ML Home and proposed we build production readiness certification of ML models in the interface. In addition, we are exploring aspects such as explainability to advance model interpretability and observability to better understand model health. Then there are the ever-inspiring hack week projects that tell us that our product has taken root at Spotify. We are excited for what’s next! </p>



<p>If you are interested in building cutting edge machine learning infrastructure at Spotify, we are <a href="https://www.lifeatspotify.com/jobs" target="_blank" rel="noreferrer noopener">hiring</a> for engineering and product roles across the board. </p>



<h2>Acknowledgments</h2>



<p>ML Home would not exist without the brilliant work of the ML UX team, our teammates from ML Platform, and the generous guidance from Spotify’s ML community. Since the list of individuals to thank would far exceed the words in this post, I will instead mention the individuals whose work made ML Home possible: Johan Bååth, Joshua Baer, Hayden Betts, Martin Bomio, Matt Brown, Keshi Dai, Omar Delarosa, Funmilayo Doro, Gandalf Hernandez, Adam Laiacano, Brian Martin, Daniel Norberg, James O’Dwyer, Ed Samour, Wesley Yee, and Qi Zheng.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a></p>

        

            </div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning&#xA;</title>
      <link>https://engineering.atspotify.com/2022/01/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/</link>
      <description>Introduction Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built ML Home, the internal user interface for Spotify’s Ma</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>January 19, 2022</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2022/01/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/" title="Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<h2>Introduction </h2>



<p>Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built <strong>ML Home</strong>, the internal user interface for Spotify’s Machine Learning Platform, and the product lessons we learned along the way in our quest to entrench it in Spotify’s ML ecosystem.</p>



<p>It’s a massive understatement to say that machine learning is at the heart of Spotify’s success story. Spotify has delivered beloved audio experiences such as Discover Weekly, Daily Mix, and <a href="https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/" target="_blank" rel="noreferrer noopener">Wrapped</a> on the strength of ML-powered personalized recommendations. Today, almost every part of Spotify has some applied ML systems, and a significant and growing portion of our R&amp;D teams consist of ML engineers and data scientists. In order to support ML systems at the scale and speed that our business requires, and to apply ML responsibly for our listeners, we have platformized sizable parts of the most common ML infrastructure within our Machine Learning Platform. </p>



<h2>Overview of Spotify’s ML Platform<br/></h2>



<p>Since the beginning, our ambition for <a href="https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/" target="_blank" rel="noreferrer noopener">Spotify’s ML Platform</a> was to connect the end-to-end user journey for ML practitioners. We subscribe to the “walking skeleton” model of product development, focusing from the start on end-to-end workflow and subsequently fleshing out functionality once we’ve proven value.</p>



<div><figure><img loading="lazy" width="700" height="154" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-250x55.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-768x169.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow.png 1448w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>In late 2019 / early 2020, our ML Platform consisted of a few components that covered the (supervised) machine learning workflow experience for Spotify’s ML practitioners: </p>



<ul><li>Spotify Kubeflow<strong>, </strong>which is our version of the open source Kubeflow Pipelines platform that helped us standardize ML workflows on the TensorFlow Extended (TFX) ecosystem  </li><li>Jukebox,<strong> </strong>which is based on TensorFlow Transform and powers our feature engineering and management workflows </li><li>Salem, which is based on TensorFlow Serving and helps us standardize model serving and production workflows, and </li><li><a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/" target="_blank" rel="noreferrer noopener">Klio</a>, which is our open source solution for audio processing with Apache Beam and Dataflow</li></ul>



<div><figure><img loading="lazy" width="700" height="153" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-250x54.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-768x167.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-1536x335.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1.png 1606w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>The Product Opportunity</h2>



<p>As we started to onboard more ML teams onto our platform, we identified two important gaps in our end-to-end support for ML workflows: </p>



<ul><li>A <strong>centralized metadata layer</strong>,<strong> </strong>where we could define our platform entities / entity relationships (e.g., models, evaluations, training sets)</li><li>A <strong>metadata presentation layer</strong>, where users could store, track, and manage the metadata generated from their ML workflows, which is the focus of this blogpost</li></ul>



<p>As the ML Platform team, we knew we wanted a tool where ML engineers could store ML project information and access metadata related to the ML application lifecycle, but weren’t entirely sure what that product would be. As we began exploring, we found that teams were using spreadsheets to track ML metadata and gave us hyper-specific feature requests for their individual problems. We also came away with broader unmet needs such as discovery of ML projects<strong>,</strong> support for ML team collaboration<strong>,</strong> and important product<strong> </strong>gaps within our own ML Platform tooling. These learnings informed the initial scope of our MVP (minimally viable product) and taught us our first product lesson. </p>



<h2>Product Lesson 1: Balancing Product Vision and Product Strategy</h2>



<p>For the first iteration of the product, we took a <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> approach. We focused on building horizontal solutions<strong> </strong>for needs we heard most commonly across all ML practitioner roles, such as being able to collaborate more effectively as an ML team. We also built a vertical solution<strong> </strong>that mapped to a specific platform tooling gap: better evaluation tooling for offline model training for ML engineers. We launched our MVP with an aspirational name and product vision: ML Home, one-stop shop for machine learning at Spotify.</p>



<div><figure><img loading="lazy" width="700" height="282" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-250x101.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-768x309.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-120x48.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021.png 1504w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><p>The initial feedback we received on our MVP fell on two ends of the spectrum. Individual contributions struggled with the broad idea of a “one-stop shop” and wanted to know what concrete problems the product could solve for them today. Leadership wanted to know how many users we would serve over the long run and how big our impact could be.</p><p>Throughout the process of selling our MVP, we learned how difficult it is to balance product vision and product strategy without compromising one for the other. Had we scratched our broader vision based on the initial feedback and focused exclusively on the concrete needs (e.g., I want to see all my training pipelines in one view), we would have risked delivering a narrow point solution. On the other hand, if we over-indexed our roadmap on the broad, ambiguous needs (e.g. I want to collaborate more effectively with my team), we would have delivered a nice-to-have but not a must-have product. </p></div>



<p>By intentionally keeping our <strong>Product Vision broad</strong> <strong>and future-looking </strong>(one-stop shop)<strong>, </strong>we gave ourselves the runway to think bigger about our solution space and our potential impact down the line. And by keeping <strong>Product Strategy concrete</strong> <strong>and iterative</strong> (offline evaluation tool), we were able to ensure that we solved concrete problems that over time helped us ingrain the product into our user’s daily workflows. </p>



<h2>Product Lesson 2: The Limits of MVPs </h2>



<div><figure><img loading="lazy" width="700" height="355" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home.png 1080w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>ML Home</figcaption></figure></div>



<p>As we wrapped up the feedback and adoption drive for the MVP, we learned our second product lesson. It is no secret that driving product adoption is hard, especially for products that are trying to replace existing solutions or market incumbents. We hit the ceiling of our MVP’s potential fairly quickly. We did not see a surge of adoption beyond the handful of users who were involved in the very early ideation process. Most users understood the value proposition of what we were building, but did not see enough depth to switch over from their existing tooling. In retrospect, our expectations of what we delivered and how valuable it would be did not match the depth of our users’ needs. It would have been easy at that stage to dismiss the product entirely, based on early adoption signals. That would have been a mistake. </p>



<p>As we plowed on with more detailed user feedback in our quest to drive more adoption, it became clear to us that we were misplacing our expectations on what role the MVP played in the product development process. <strong>The most valuable end goal of an MVP is to get enough of the vision and strategies out there to help validate or invalidate them. </strong>Our initial MVP helped us test and de-risk our work because we were able to get detailed validation of the workflows from ML teams and lay out the technical foundations for the product. It did not matter how many daily active users we had at that stage, as long as we had enough users (which we did) who saw the value in what we were building. These users continued to attend our user feedback sessions and helped us get the product to a higher and more valuable place.</p>



<h2>Product Lesson 3: Knowing the true Differentiators</h2>



<p>As we moved beyond the MVP phase and started to map out our next steps (focusing on some aspects of the product over others), we learned our third and perhaps most important product lesson. We realized that in order to provide a really valuable product to our users, we needed to not only reach feature parity with existing solutions, but also double down on ML Home’s unique differentiators. In short, ML Home as a product needed to be more compelling than the competing solutions. </p>



<p>For a while, we probed, debated, and stack-ranked specific features and workflows that we felt would be game-changing for our users. Our theory was that if we built a “compelling feature,” it would be able to singularly pass a threshold for users to adopt. In the end, we realized that the unique differentiators for ML Home actually came in the form of our other ML Platform offerings, not any one individual feature. </p>



<p>While some aspects of ML Home could functionally serve as a stand-alone product, by enriching it with training, evaluation, and system metadata generated from the rest of our ML Platform, it became a much more compelling product. <strong>ML Home’s unique differentiator isn’t any one silver-bullet feature but rather the gateway value it provides as the sum of our ML Platform capabilities.</strong></p>



<p>Much of the work we did in this phase was building out our metadata service to consolidate our overall entities and concepts across the platform, but we also spent significant time building flexibility into the product’s interface. For example, annotation capabilities such as tagging and notes became key features that enabled teams to customize and mirror their own workflows. That, paired with a faster, slicker product experience and information-rich model comparisons, tipped the balance in our favor. </p>



<p>By the time we released the second version of ML Home, we had successfully onboarded more ML teams who were actively using the product in their daily workflows. </p>







<div><figure><img loading="lazy" width="700" height="473" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-250x169.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-768x519.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-1536x1039.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-120x81.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Sample ML Project in ML Home. For illustrative purposes only.</figcaption></figure></div>



<h2>Scaling the Product to Our Vision</h2>



<p>Getting closer to<strong> </strong>product-market fit taught us a lot about how to iterate moving forward. We knew that ML Home only served one typical ML workflow. However, in order to be an indispensable product for <em>all</em> ML practitioners, it needed to cover more ground. We also knew that tightly coupling ML Home’s capabilities to our existing ML Platform products resulted in a much higher rate of adoption than stand-alone solutions. Armed with these takeaways, we wireframed a broader vision for the product.</p>



<p>Today, ML Home provides Spotify’s ML practitioners with artifacts and workflow metadata of all models passing through individual components of our ML Platform. It includes capabilities such as the ability to track and evaluate offline experiments, visualize results, track and monitor deployed models, explore features, certify models for production readiness, and much more.  </p>



<p>Through intuitive workflows and simplified information architecture, users are able to quickly spin up a project space to collaborate with their team and discover the 220+ ML projects across Spotify currently listed in ML Home. </p>



<p>We have seen a 200% growth in daily active users since we began our scaling efforts a year ago, and ML Home is now solidly entrenched in the daily workflows of some of the most important ML teams at Spotify. Despite its short tenure in Spotify’s Infrastructure landscape, ML Home is well on its way to becoming the one-stop shop for all things ML at Spotify. <br/></p>



<div><figure><img loading="lazy" width="700" height="476" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-768x523.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home.png 1340w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Conceptual rendering of ML Home. For illustrative purposes only.</figcaption></figure></div>



<h2>Three Key Lessons</h2>



<p>The saying goes that hindsight is 20/20, and it’s true. Looking back, these are the lessons that stick out the most from our product development process: </p>



<ol><li><strong>Product vision vs product strategy. </strong>It is difficult to strike the right balance between an inspiring vision that can support future solutions and a responsive product strategy that addresses today’s problems. But it is crucially important to not conflate the two in the early stages of product development. </li><li><strong>Limits of MVPs.</strong> MVPs provide the most value as a validation and de-risking tool for product strategy and overall direction. </li><li><strong>Know the true differentiators.</strong> It’s worth paying attention to what the real differentiators are for a product. It does not have to be a “compelling” feature; it can simply be opportunities found in the ecosystem that turn the tide for a product’s success. </li></ol>



<h2>Looking Ahead </h2>



<p>ML Home is not done — not even close. We know this because, in the last year, Spotify’s ML community has proposed new and inventive ways to evolve the product. For exampleML engineers saw the potential to build on top of ML Home and proposed we build production readiness certification of ML models in the interface. In addition, we are exploring aspects such as explainability to advance model interpretability and observability to better understand model health. Then there are the ever-inspiring hack week projects that tell us that our product has taken root at Spotify. We are excited for what’s next! </p>



<p>If you are interested in building cutting edge machine learning infrastructure at Spotify, we are <a href="https://www.lifeatspotify.com/jobs" target="_blank" rel="noreferrer noopener">hiring</a> for engineering and product roles across the board. </p>



<h2>Acknowledgments</h2>



<p>ML Home would not exist without the brilliant work of the ML UX team, our teammates from ML Platform, and the generous guidance from Spotify’s ML community. Since the list of individuals to thank would far exceed the words in this post, I will instead mention the individuals whose work made ML Home possible: Johan Bååth, Joshua Baer, Hayden Betts, Martin Bomio, Matt Brown, Keshi Dai, Omar Delarosa, Funmilayo Doro, Gandalf Hernandez, Adam Laiacano, Brian Martin, Daniel Norberg, James O’Dwyer, Ed Samour, Wesley Yee, and Qi Zheng.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Audio Aura Story: Mystical to Mathematical&#xA;</title>
      <link>https://engineering.atspotify.com/the-audio-aura-story-mystical-to-mathematical/</link>
      <description>TL;DR For 2021 Wrapped, we were challenged to visually express a user’s Audio Aura based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like?</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 17, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/the-audio-aura-story-mystical-to-mathematical/" title="The Audio Aura Story: Mystical to Mathematical">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-768x380.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>For 2021 Wrapped, we were challenged to visually express a user’s <a href="https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/" target="_blank" rel="noreferrer noopener">Audio Aura</a> based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like? That’s the 2021 Wrapped Audio Aura story. </p>



<h2><strong>A new Wrapped experience</strong></h2>



<p>Spotify Wrapped is a company-wide effort with 300+ Spotifiers across 20+ teams. My team in particular is a design/engineering team and our main focus is to design, build, and launch the Wrapped personalized user experience to millions of Spotify users around the world. These personalized stories bring users’ annual listening habits to life through creative storytelling. </p>



<p>The personalized experience shows listeners’ data stories such as their top songs, top artists, and top genres of the year. In addition to building these stories, our Brand and Creative team thinks outside the box and ideates new ways to illuminate insights into a user’s audio streams. </p>



<p><em>Imagine.</em> A meeting room full of professionals — a virtual meeting room, of course. Brows furrowed, pens clicking, and fingers scratching heads. We were brainstorming the answer to the question: how do we <em>define</em> a Spotify user’s Audio Aura? To help with this question and guarantee the auras we create are valid, we consulted an aura reader, <a href="https://www.mysticmichaela.com/" target="_blank" rel="noreferrer noopener">Mystic Michaela</a>. </p>



<p>According to Mystic Michaela, auras are “your personal energy signature. Everyone has one, and aura readers see them as a combination of colors, each representative of the traits that make you, you” (check out Michaela’s full take on auras <a href="https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/" target="_blank" rel="noreferrer noopener">here</a>). For the purposes of our Audio Aura story, we can extend the definition to say that a 2021 Spotify Wrapped Audio Aura is a colorful energy made up of two colors that help our listeners understand the vibe or mood of the music they streamed this year.</p>



<h2><strong>Let’s get to the whiteboard </strong></h2>



<h5><strong>Challenge #1</strong> </h5>



<p>Return the top two mood categories and the descriptor associated with their 2021 listening history for each user.</p>



<h5><strong>Challenge #2</strong></h5>



<p>Identify how much of a user’s music is represented by each mood aka the mood weight.</p>



<p>To address the above challenges, we utilized a track mood descriptor dataset, aggregated each user’s listening history, and created six broad mood categories (i.e. “Happy”, “Calm”, “Hopeful”) that would then be narrowed down to two to create the audio aura.</p>



<ol><li>Count the number of streams for a given track and retrieve its top mood descriptor as determined by the mood descriptor dataset.</li><li>Bucket those moods into one of the six mood categories.</li><li>Perform an aggregation to find the total number of streams for each mood category and take the mood descriptor with the highest number of streams.</li><li>Carry out one last calculation to find the percentage of streams of a mood over the total number of streams.</li></ol>



<div><figure><img loading="lazy" width="700" height="505" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-250x180.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-768x554.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-1536x1107.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-120x87.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>In the example above, tracks that are “happy” or “calm” are the most streamed, making them the top two moods found in a specific user’s music. “happy” makes up 47% of the user’s listening with the mood descriptor being “blissful”. “calm” makes up 29% of the user’s listening with the granular descriptor being “chill”.</p>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2><strong>Now, let’s turn these moods into colors</strong></h2>



<p>Dry-erase pen caps click… we’ve identified a user’s top two Audio Aura moods and their mood weights. Now let’s turn this data into colors. Mystic Michaela stepped in to lend her expertise on auras and their colors. With her guidance, we assigned six core colors to the six mood descriptor categories mentioned above (which include “happy”, “calm”, and “hopeful”). To provide secondary colors to the aura visual, our designer represents mood weights through varying levels of contrast. The more a mood is present in a user’s listening, the darker the color. The less a mood is present in a user’s listening, the lighter the color. In the graphic above, the user has a mood weight of 0.47; therefore, the secondary color would be found in the row labeled Encore 100. Their second mood weight was 0.29; therefore, the secondary color would be found in the row labeled Encore 180.</p>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2><strong>Creativity and embracing constraints</strong></h2>



<p>Now that we have a user’s Audio Aura colors, all that’s left is for design and engineering to consider the last question: how can we engineer an ethereal Aura visual?</p>



<p>Our deadline was two weeks away. In our milestones planning, we allocated a few days of engineering per story since we committed to a number of data stories and several new features. We had a few days to explore an elegant aura visual that was feasible on both iOS and Android. Part of the challenge for design is knowing what’s possible for engineering to execute in a dedicated amount of time. And part of the challenge for engineering is accurately estimating the time it will take to build a given design. We had to come up with a solution that made good use of our time and garnered a confident engineering sign-off. Situations like this are very unique to the Wrapped design/engineering experience and through such situations, I’ve learned that creativity comes from embracing constraints and making use of what we have in novel ways. Our designer made use of three elements: </p>



<ol><li>An animated gradient built for last year’s 2020 Wrapped campaign to create a “lava lamp” effect.</li><li>Ribbons built for 2021 Wrapped to create organic and unique shapes.</li><li>A blur effect to blend everything together in a dreamlike fashion.</li></ol>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Since we had already built the animated gradient and ribbons, we were able to build the aura visual with high confidence, on time, and to accurately depict an ethereal aura!</p>



<div><figure><img loading="lazy" width="250" height="501" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-700x1403.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-768x1540.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-766x1536.png 766w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-120x241.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura.png 858w" sizes="(max-width: 250px) 100vw, 250px"/></figure></div>



<h2><strong>To wrap it up</strong></h2>



<p>The 2021 Wrapped Audio Aura story is a peek into the unique experience of engineering and design. We took an esoteric concept and broke it down into its basic elements to build an exciting feature for the Wrapped experience. Our ability to work together with teams across Spotify allows us to find creative solutions, enabling us to create stories that, hopefully, delight our users. If you’re interested in joining our efforts to bring Spotify listeners new experiences, check out our <a href="https://www.lifeatspotify.com/jobs?c=engineering" target="_blank" rel="noreferrer noopener">open roles</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Zela Taino, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Audio Aura Story: Mystical to Mathematical&#xA;</title>
      <link>https://engineering.atspotify.com/the-audio-aura-story-mystical-to-mathematical/</link>
      <description>TL;DR For 2021 Wrapped, we were challenged to visually express a user’s Audio Aura based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like?</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 17, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/the-audio-aura-story-mystical-to-mathematical/" title="The Audio Aura Story: Mystical to Mathematical">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-768x380.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>For 2021 Wrapped, we were challenged to visually express a user’s <a href="https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/" target="_blank" rel="noreferrer noopener">Audio Aura</a> based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like? That’s the 2021 Wrapped Audio Aura story. </p>



<h2><strong>A new Wrapped experience</strong></h2>



<p>Spotify Wrapped is a company-wide effort with 300+ Spotifiers across 20+ teams. My team in particular is a design/engineering team and our main focus is to design, build, and launch the Wrapped personalized user experience to millions of Spotify users around the world. These personalized stories bring users’ annual listening habits to life through creative storytelling. </p>



<p>The personalized experience shows listeners’ data stories such as their top songs, top artists, and top genres of the year. In addition to building these stories, our Brand and Creative team thinks outside the box and ideates new ways to illuminate insights into a user’s audio streams. </p>



<p><em>Imagine.</em> A meeting room full of professionals — a virtual meeting room, of course. Brows furrowed, pens clicking, and fingers scratching heads. We were brainstorming the answer to the question: how do we <em>define</em> a Spotify user’s Audio Aura? To help with this question and guarantee the auras we create are valid, we consulted an aura reader, <a href="https://www.mysticmichaela.com/" target="_blank" rel="noreferrer noopener">Mystic Michaela</a>. </p>



<p>According to Mystic Michaela, auras are “your personal energy signature. Everyone has one, and aura readers see them as a combination of colors, each representative of the traits that make you, you” (check out Michaela’s full take on auras <a href="https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/" target="_blank" rel="noreferrer noopener">here</a>). For the purposes of our Audio Aura story, we can extend the definition to say that a 2021 Spotify Wrapped Audio Aura is a colorful energy made up of two colors that help our listeners understand the vibe or mood of the music they streamed this year.</p>



<h2><strong>Let’s get to the whiteboard </strong></h2>



<h5><strong>Challenge #1</strong> </h5>



<p>Return the top two mood categories and the descriptor associated with their 2021 listening history for each user.</p>



<h5><strong>Challenge #2</strong></h5>



<p>Identify how much of a user’s music is represented by each mood aka the mood weight.</p>



<p>To address the above challenges, we utilized a track mood descriptor dataset, aggregated each user’s listening history, and created six broad mood categories (i.e. “Happy”, “Calm”, “Hopeful”) that would then be narrowed down to two to create the audio aura.</p>



<ol><li>Count the number of streams for a given track and retrieve its top mood descriptor as determined by the mood descriptor dataset.</li><li>Bucket those moods into one of the six mood categories.</li><li>Perform an aggregation to find the total number of streams for each mood category and take the mood descriptor with the highest number of streams.</li><li>Carry out one last calculation to find the percentage of streams of a mood over the total number of streams.</li></ol>



<div><figure><img loading="lazy" width="700" height="505" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-250x180.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-768x554.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-1536x1107.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-120x87.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>In the example above, tracks that are “happy” or “calm” are the most streamed, making them the top two moods found in a specific user’s music. “happy” makes up 47% of the user’s listening with the mood descriptor being “blissful”. “calm” makes up 29% of the user’s listening with the granular descriptor being “chill”.</p>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2><strong>Now, let’s turn these moods into colors</strong></h2>



<p>Dry-erase pen caps click… we’ve identified a user’s top two Audio Aura moods and their mood weights. Now let’s turn this data into colors. Mystic Michaela stepped in to lend her expertise on auras and their colors. With her guidance, we assigned six core colors to the six mood descriptor categories mentioned above (which include “happy”, “calm”, and “hopeful”). To provide secondary colors to the aura visual, our designer represents mood weights through varying levels of contrast. The more a mood is present in a user’s listening, the darker the color. The less a mood is present in a user’s listening, the lighter the color. In the graphic above, the user has a mood weight of 0.47; therefore, the secondary color would be found in the row labeled Encore 100. Their second mood weight was 0.29; therefore, the secondary color would be found in the row labeled Encore 180.</p>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2><strong>Creativity and embracing constraints</strong></h2>



<p>Now that we have a user’s Audio Aura colors, all that’s left is for design and engineering to consider the last question: how can we engineer an ethereal Aura visual?</p>



<p>Our deadline was two weeks away. In our milestones planning, we allocated a few days of engineering per story since we committed to a number of data stories and several new features. We had a few days to explore an elegant aura visual that was feasible on both iOS and Android. Part of the challenge for design is knowing what’s possible for engineering to execute in a dedicated amount of time. And part of the challenge for engineering is accurately estimating the time it will take to build a given design. We had to come up with a solution that made good use of our time and garnered a confident engineering sign-off. Situations like this are very unique to the Wrapped design/engineering experience and through such situations, I’ve learned that creativity comes from embracing constraints and making use of what we have in novel ways. Our designer made use of three elements: </p>



<ol><li>An animated gradient built for last year’s 2020 Wrapped campaign to create a “lava lamp” effect.</li><li>Ribbons built for 2021 Wrapped to create organic and unique shapes.</li><li>A blur effect to blend everything together in a dreamlike fashion.</li></ol>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Since we had already built the animated gradient and ribbons, we were able to build the aura visual with high confidence, on time, and to accurately depict an ethereal aura!</p>



<div><figure><img loading="lazy" width="250" height="501" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-700x1403.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-768x1540.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-766x1536.png 766w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-120x241.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura.png 858w" sizes="(max-width: 250px) 100vw, 250px"/></figure></div>



<h2><strong>To wrap it up</strong></h2>



<p>The 2021 Wrapped Audio Aura story is a peek into the unique experience of engineering and design. We took an esoteric concept and broke it down into its basic elements to build an exciting feature for the Wrapped experience. Our ability to work together with teams across Spotify allows us to find creative solutions, enabling us to create stories that, hopefully, delight our users. If you’re interested in joining our efforts to bring Spotify listeners new experiences, check out our <a href="https://www.lifeatspotify.com/jobs?c=engineering" target="_blank" rel="noreferrer noopener">open roles</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Zela Taino, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Audio Aura Story: Mystical to Mathematical&#xA;</title>
      <link>https://engineering.atspotify.com/2021/12/17/the-audio-aura-story-mystical-to-mathematical/</link>
      <description>TL;DR For 2021 Wrapped, we were challenged to visually express a user’s Audio Aura based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like?</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 17, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/12/17/the-audio-aura-story-mystical-to-mathematical/" title="The Audio Aura Story: Mystical to Mathematical">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-768x380.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>For 2021 Wrapped, we were challenged to visually express a user’s <a href="https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/" target="_blank" rel="noreferrer noopener">Audio Aura</a> based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like? That’s the 2021 Wrapped Audio Aura story. </p>



<h2><strong>A new Wrapped experience</strong></h2>



<p>Spotify Wrapped is a company-wide effort with 300+ Spotifiers across 20+ teams. My team in particular is a design/engineering team and our main focus is to design, build, and launch the Wrapped personalized user experience to millions of Spotify users around the world. These personalized stories bring users’ annual listening habits to life through creative storytelling. </p>



<p>The personalized experience shows listeners’ data stories such as their top songs, top artists, and top genres of the year. In addition to building these stories, our Brand and Creative team thinks outside the box and ideates new ways to illuminate insights into a user’s audio streams. </p>



<p><em>Imagine.</em> A meeting room full of professionals — a virtual meeting room, of course. Brows furrowed, pens clicking, and fingers scratching heads. We were brainstorming the answer to the question: how do we <em>define</em> a Spotify user’s Audio Aura? To help with this question and guarantee the auras we create are valid, we consulted an aura reader, <a href="https://www.mysticmichaela.com/" target="_blank" rel="noreferrer noopener">Mystic Michaela</a>. </p>



<p>According to Mystic Michaela, auras are “your personal energy signature. Everyone has one, and aura readers see them as a combination of colors, each representative of the traits that make you, you” (check out Michaela’s full take on auras <a href="https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/" target="_blank" rel="noreferrer noopener">here</a>). For the purposes of our Audio Aura story, we can extend the definition to say that a 2021 Spotify Wrapped Audio Aura is a colorful energy made up of two colors that help our listeners understand the vibe or mood of the music they streamed this year.</p>



<h2><strong>Let’s get to the whiteboard </strong></h2>



<h5><strong>Challenge #1</strong> </h5>



<p>Return the top two mood categories and the descriptor associated with their 2021 listening history for each user.</p>



<h5><strong>Challenge #2</strong></h5>



<p>Identify how much of a user’s music is represented by each mood aka the mood weight.</p>



<p>To address the above challenges, we utilized a track mood descriptor dataset, aggregated each user’s listening history, and created six broad mood categories (i.e. “Happy”, “Calm”, “Hopeful”) that would then be narrowed down to two to create the audio aura.</p>



<ol><li>Count the number of streams for a given track and retrieve its top mood descriptor as determined by the mood descriptor dataset.</li><li>Bucket those moods into one of the six mood categories.</li><li>Perform an aggregation to find the total number of streams for each mood category and take the mood descriptor with the highest number of streams.</li><li>Carry out one last calculation to find the percentage of streams of a mood over the total number of streams.</li></ol>



<div><figure><img loading="lazy" width="700" height="505" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-250x180.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-768x554.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-1536x1107.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-120x87.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>In the example above, tracks that are “happy” or “calm” are the most streamed, making them the top two moods found in a specific user’s music. “happy” makes up 47% of the user’s listening with the mood descriptor being “blissful”. “calm” makes up 29% of the user’s listening with the granular descriptor being “chill”.</p>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2><strong>Now, let’s turn these moods into colors</strong></h2>



<p>Dry-erase pen caps click… we’ve identified a user’s top two Audio Aura moods and their mood weights. Now let’s turn this data into colors. Mystic Michaela stepped in to lend her expertise on auras and their colors. With her guidance, we assigned six core colors to the six mood descriptor categories mentioned above (which include “happy”, “calm”, and “hopeful”). To provide secondary colors to the aura visual, our designer represents mood weights through varying levels of contrast. The more a mood is present in a user’s listening, the darker the color. The less a mood is present in a user’s listening, the lighter the color. In the graphic above, the user has a mood weight of 0.47; therefore, the secondary color would be found in the row labeled Encore 100. Their second mood weight was 0.29; therefore, the secondary color would be found in the row labeled Encore 180.</p>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2><strong>Creativity and embracing constraints</strong></h2>



<p>Now that we have a user’s Audio Aura colors, all that’s left is for design and engineering to consider the last question: how can we engineer an ethereal Aura visual?</p>



<p>Our deadline was two weeks away. In our milestones planning, we allocated a few days of engineering per story since we committed to a number of data stories and several new features. We had a few days to explore an elegant aura visual that was feasible on both iOS and Android. Part of the challenge for design is knowing what’s possible for engineering to execute in a dedicated amount of time. And part of the challenge for engineering is accurately estimating the time it will take to build a given design. We had to come up with a solution that made good use of our time and garnered a confident engineering sign-off. Situations like this are very unique to the Wrapped design/engineering experience and through such situations, I’ve learned that creativity comes from embracing constraints and making use of what we have in novel ways. Our designer made use of three elements: </p>



<ol><li>An animated gradient built for last year’s 2020 Wrapped campaign to create a “lava lamp” effect.</li><li>Ribbons built for 2021 Wrapped to create organic and unique shapes.</li><li>A blur effect to blend everything together in a dreamlike fashion.</li></ol>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Since we had already built the animated gradient and ribbons, we were able to build the aura visual with high confidence, on time, and to accurately depict an ethereal aura!</p>



<div><figure><img loading="lazy" width="250" height="501" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-700x1403.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-768x1540.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-766x1536.png 766w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-120x241.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura.png 858w" sizes="(max-width: 250px) 100vw, 250px"/></figure></div>



<h2><strong>To wrap it up</strong></h2>



<p>The 2021 Wrapped Audio Aura story is a peek into the unique experience of engineering and design. We took an esoteric concept and broke it down into its basic elements to build an exciting feature for the Wrapped experience. Our ability to work together with teams across Spotify allows us to find creative solutions, enabling us to create stories that, hopefully, delight our users. If you’re interested in joining our efforts to bring Spotify listeners new experiences, check out our <a href="https://www.lifeatspotify.com/jobs?c=engineering" target="_blank" rel="noreferrer noopener">open roles</a>!</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Zela Taino, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Audio Aura Story: Mystical to Mathematical&#xA;</title>
      <link>https://engineering.atspotify.com/2021/12/the-audio-aura-story-mystical-to-mathematical/</link>
      <description>TL;DR For 2021 Wrapped, we were challenged to visually express a user’s Audio Aura based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like?</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 17, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/12/the-audio-aura-story-mystical-to-mathematical/" title="The Audio Aura Story: Mystical to Mathematical">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-768x380.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>For 2021 Wrapped, we were challenged to visually express a user’s <a href="https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/" target="_blank" rel="noreferrer noopener">Audio Aura</a> based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like? That’s the 2021 Wrapped Audio Aura story. </p>



<h2><strong>A new Wrapped experience</strong></h2>



<p>Spotify Wrapped is a company-wide effort with 300+ Spotifiers across 20+ teams. My team in particular is a design/engineering team and our main focus is to design, build, and launch the Wrapped personalized user experience to millions of Spotify users around the world. These personalized stories bring users’ annual listening habits to life through creative storytelling. </p>



<p>The personalized experience shows listeners’ data stories such as their top songs, top artists, and top genres of the year. In addition to building these stories, our Brand and Creative team thinks outside the box and ideates new ways to illuminate insights into a user’s audio streams. </p>



<p><em>Imagine.</em> A meeting room full of professionals — a virtual meeting room, of course. Brows furrowed, pens clicking, and fingers scratching heads. We were brainstorming the answer to the question: how do we <em>define</em> a Spotify user’s Audio Aura? To help with this question and guarantee the auras we create are valid, we consulted an aura reader, <a href="https://www.mysticmichaela.com/" target="_blank" rel="noreferrer noopener">Mystic Michaela</a>. </p>



<p>According to Mystic Michaela, auras are “your personal energy signature. Everyone has one, and aura readers see them as a combination of colors, each representative of the traits that make you, you” (check out Michaela’s full take on auras <a href="https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/" target="_blank" rel="noreferrer noopener">here</a>). For the purposes of our Audio Aura story, we can extend the definition to say that a 2021 Spotify Wrapped Audio Aura is a colorful energy made up of two colors that help our listeners understand the vibe or mood of the music they streamed this year.</p>



<h2><strong>Let’s get to the whiteboard </strong></h2>



<h5><strong>Challenge #1</strong> </h5>



<p>Return the top two mood categories and the descriptor associated with their 2021 listening history for each user.</p>



<h5><strong>Challenge #2</strong></h5>



<p>Identify how much of a user’s music is represented by each mood aka the mood weight.</p>



<p>To address the above challenges, we utilized a track mood descriptor dataset, aggregated each user’s listening history, and created six broad mood categories (i.e. “Happy”, “Calm”, “Hopeful”) that would then be narrowed down to two to create the audio aura.</p>



<ol><li>Count the number of streams for a given track and retrieve its top mood descriptor as determined by the mood descriptor dataset.</li><li>Bucket those moods into one of the six mood categories.</li><li>Perform an aggregation to find the total number of streams for each mood category and take the mood descriptor with the highest number of streams.</li><li>Carry out one last calculation to find the percentage of streams of a mood over the total number of streams.</li></ol>



<div><figure><img loading="lazy" width="700" height="505" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-250x180.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-768x554.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-1536x1107.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-120x87.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>In the example above, tracks that are “happy” or “calm” are the most streamed, making them the top two moods found in a specific user’s music. “happy” makes up 47% of the user’s listening with the mood descriptor being “blissful”. “calm” makes up 29% of the user’s listening with the granular descriptor being “chill”.</p>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2><strong>Now, let’s turn these moods into colors</strong></h2>



<p>Dry-erase pen caps click… we’ve identified a user’s top two Audio Aura moods and their mood weights. Now let’s turn this data into colors. Mystic Michaela stepped in to lend her expertise on auras and their colors. With her guidance, we assigned six core colors to the six mood descriptor categories mentioned above (which include “happy”, “calm”, and “hopeful”). To provide secondary colors to the aura visual, our designer represents mood weights through varying levels of contrast. The more a mood is present in a user’s listening, the darker the color. The less a mood is present in a user’s listening, the lighter the color. In the graphic above, the user has a mood weight of 0.47; therefore, the secondary color would be found in the row labeled Encore 100. Their second mood weight was 0.29; therefore, the secondary color would be found in the row labeled Encore 180.</p>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2><strong>Creativity and embracing constraints</strong></h2>



<p>Now that we have a user’s Audio Aura colors, all that’s left is for design and engineering to consider the last question: how can we engineer an ethereal Aura visual?</p>



<p>Our deadline was two weeks away. In our milestones planning, we allocated a few days of engineering per story since we committed to a number of data stories and several new features. We had a few days to explore an elegant aura visual that was feasible on both iOS and Android. Part of the challenge for design is knowing what’s possible for engineering to execute in a dedicated amount of time. And part of the challenge for engineering is accurately estimating the time it will take to build a given design. We had to come up with a solution that made good use of our time and garnered a confident engineering sign-off. Situations like this are very unique to the Wrapped design/engineering experience and through such situations, I’ve learned that creativity comes from embracing constraints and making use of what we have in novel ways. Our designer made use of three elements: </p>



<ol><li>An animated gradient built for last year’s 2020 Wrapped campaign to create a “lava lamp” effect.</li><li>Ribbons built for 2021 Wrapped to create organic and unique shapes.</li><li>A blur effect to blend everything together in a dreamlike fashion.</li></ol>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Since we had already built the animated gradient and ribbons, we were able to build the aura visual with high confidence, on time, and to accurately depict an ethereal aura!</p>



<div><figure><img loading="lazy" width="250" height="501" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-700x1403.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-768x1540.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-766x1536.png 766w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-120x241.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura.png 858w" sizes="(max-width: 250px) 100vw, 250px"/></figure></div>



<h2><strong>To wrap it up</strong></h2>



<p>The 2021 Wrapped Audio Aura story is a peek into the unique experience of engineering and design. We took an esoteric concept and broke it down into its basic elements to build an exciting feature for the Wrapped experience. Our ability to work together with teams across Spotify allows us to find creative solutions, enabling us to create stories that, hopefully, delight our users. If you’re interested in joining our efforts to bring Spotify listeners new experiences, check out our <a href="https://www.lifeatspotify.com/jobs?c=engineering" target="_blank" rel="noreferrer noopener">open roles</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Zela Taino, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Look Behind Blend: The Personalized Playlist for You…and You&#xA;</title>
      <link>https://engineering.atspotify.com/a-look-behind-blend-the-personalized-playlist-for-youand-you/</link>
      <description>What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. We recently launched a</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/a-look-behind-blend-the-personalized-playlist-for-youand-you/" title="A Look Behind Blend: The Personalized Playlist for You…and You">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png 2097w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-2048x1010.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-120x59.png 120w" sizes="(max-width: 2097px) 100vw, 2097px"/>                    </a>
                        
        </p>

        

        
<p>What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. </p>



<p>We recently launched a new playlist initiative, <a href="https://newsroom.spotify.com/2021-08-31/how-spotifys-newest-personalized-experience-blend-creates-a-playlist-for-you-and-your-bestie/" target="_blank" rel="noreferrer noopener">Blend</a>, where any user can invite any user to generate a playlist wherein the two users’ tastes are combined into one shared playlist. Prior to Blend, the team worked on similar products, Family Mix and Duo Mix. These products create shared playlists for users on the same Family or Duo plan. The products were well received, so we decided to expand this product line, creating a version of opt-in, automatic, shared, personalized playlists that could work for any two users. </p>



<p>Anytime we want to make a new playlist at Spotify, we’re aiming to do something different that we haven’t been able to accomplish before. This means we can’t always lean on our past experiences, and often encounter new challenges that require new solutions. With Blend in particular, we were taking concepts from Family Mix and Duo Mix, and expanding them to a much larger user group. A major complication we saw here was the increase of scale in the number of users we had to deal with. We dealt with unique challenges both in the content creation process, and in the invitation flow, to create a Blend.</p>



<p>Most playlists are composed of a number of attributes and characteristics. For example, with Discover Weekly, our main attribute is discovery. For Daily Mix, our attributes are familiarity and coherency. When we are working with multiple users, however, we have the challenge of taking more attributes into account. Is the playlist:</p>



<ul><li><strong>Relevant:</strong> Does the track we’re selecting for that user reflect their taste? Or is it just a song they accidentally listened to once?<ul><li>This is especially important for track attribution — if we put a user’s profile image next to a song, we need to make sure that this specific user would agree the song listed is representative of their taste.</li></ul></li><li><strong>Coherent: </strong>Does the playlist have flow, or do the tracks feel completely random and unrelated to each other?</li><li><strong>Equal:</strong> Are both users in the Blend represented equally?</li><li><strong>Democratic:</strong> Does music that both users like rise to the top?</li></ul>



<p>One of the core decisions we made for this product was whether it was better to “minimize the misery” or “maximize the joy”. In other words, is it better to pick everyone’s favorite tracks, even if other people in the group wouldn’t like them, or is it better to pick the tracks that everyone is likely to like, even if their favorite songs never get selected? “Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.</p>



<figure><img loading="lazy" width="700" height="477" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-1536x1047.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-2048x1397.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-120x82.png 120w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>It’s a bit simpler to create a Blend with users with similar taste since they listen to a lot of the same music. However, if we have two users with no common music listening history, it’s significantly more difficult to create a perfect Blend. We needed an approach that worked for both types of pairs, while also taking into consideration how any changes to the Blend algorithm impacts all combinations of users.</p>



<p>Between fetching data for both users in the Blend, and trying to come up with the ideal sequence balancing for all of our attributes, creating a Blend is a pretty heavy process. When we tried to come up with the best algorithm, we weren’t so concerned about our latency. Once we were happy with Blend quality, and started to think about scaling the service, we realized how bad our latency had gotten while iterating on the algorithm. We spent a lot of time trying to make the service as fast as possible. What we learned is that our code base had some hot spots in it: some sections of the code were run over 50 times per Blend generation, while other sections of the code were only run once. If we tried to optimize sections of the code that weren’t run many times, we didn’t make much of an impact in our latency. However, when we made improvements to our hot spots, we were able to make a huge difference. The biggest example here was swapping the order of two function calls within an if statement, taking advantage of Java’s short circuiting. This simple code change reduced our latency to 1/10 of its original time.</p>



<p>We were able to make content quality improvements by using both qualitative and quantitative methods. While we normally rely on testing our own playlists when we make changes, we also needed to make sure that we checked several different types of Blends (for example: test a high taste overlap Blend and a low taste overlap Blend). We created some offline metrics to measure how our attributes performed. We also work closely with a Data Curation team, often referred to as the “humans in the loop”. The Data Curation team evaluates and ensures content quality for recommendation systems. </p>



<p>For example, when the team wanted to make a change to make the playlist more coherent, we:</p>



<ul><li>Tested our own playlists — the team had implemented the change for about a month before we evaluated it. During this time, we were able to get a good feel for whether we preferred the change or not.</li><li>Performed a heuristic review, where our Data Curation team reviewed a number of Blends with a variety of taste overlap scores. <ul><li>This process helps identify issues with usability and comprehensibility associated most closely with content quality and with the user experience.</li><li>Utilize a tool called a “Content Recommendation Scorecard”.<ul><li>Score each track over a number of attributes such as relevance and coherence.</li></ul></li><li>From the Content Recommendation Scorecard, we were able to see that the new approach more strongly met our criteria in terms of the attributes we wanted to optimize for.</li><li>The review built enough confidence for the team to roll out the new approach to all users.</li></ul></li></ul>



<p>Creating a social playlist presented a new set of challenges in creating a new playlist algorithm. We had to try to optimize for many attributes: relevance, coherence, equality, and democratic decisions. We also had to consider both high taste overlap users and users who don’t have much taste overlap. </p>



<p>While building out the Blend product, we wanted a way to communicate information to the users about what similarities and differences they have in their music taste. This led to us building out Blend Data Stories, where we can show the users’ information like the artist that brings them together and their taste match score. This year, during the Wrapped Campaign, we gave users a <a href="https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/" target="_blank" rel="noreferrer noopener">Wrapped Blend</a> experience. We modified the Blend Data Stories to use data from Wrapped, to show users information like their top mutual artists and top mutual genre of the year.<br/></p>



<p>We’re still working hard to improve Blend, and build a product that allows our users to feel closer through music, while thinking of more fun ways to grow the social experience in Spotify. If this type of work sounds interesting, our Personalization team is <a href="https://www.lifeatspotify.com/jobs?q=personalization" target="_blank" rel="noreferrer noopener">hiring</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/data-modeling/" rel="tag">data modeling</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Jen Lamere, Senior Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Look Behind Blend: The Personalized Playlist for You…and You&#xA;</title>
      <link>https://engineering.atspotify.com/a-look-behind-blend-the-personalized-playlist-for-youand-you/</link>
      <description>What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. We recently launched a</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/a-look-behind-blend-the-personalized-playlist-for-youand-you/" title="A Look Behind Blend: The Personalized Playlist for You…and You">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png 2097w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-2048x1010.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-120x59.png 120w" sizes="(max-width: 2097px) 100vw, 2097px"/>                    </a>
                        
        </p>

        

        
<p>What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. </p>



<p>We recently launched a new playlist initiative, <a href="https://newsroom.spotify.com/2021-08-31/how-spotifys-newest-personalized-experience-blend-creates-a-playlist-for-you-and-your-bestie/" target="_blank" rel="noreferrer noopener">Blend</a>, where any user can invite any user to generate a playlist wherein the two users’ tastes are combined into one shared playlist. Prior to Blend, the team worked on similar products, Family Mix and Duo Mix. These products create shared playlists for users on the same Family or Duo plan. The products were well received, so we decided to expand this product line, creating a version of opt-in, automatic, shared, personalized playlists that could work for any two users. </p>



<p>Anytime we want to make a new playlist at Spotify, we’re aiming to do something different that we haven’t been able to accomplish before. This means we can’t always lean on our past experiences, and often encounter new challenges that require new solutions. With Blend in particular, we were taking concepts from Family Mix and Duo Mix, and expanding them to a much larger user group. A major complication we saw here was the increase of scale in the number of users we had to deal with. We dealt with unique challenges both in the content creation process, and in the invitation flow, to create a Blend.</p>



<p>Most playlists are composed of a number of attributes and characteristics. For example, with Discover Weekly, our main attribute is discovery. For Daily Mix, our attributes are familiarity and coherency. When we are working with multiple users, however, we have the challenge of taking more attributes into account. Is the playlist:</p>



<ul><li><strong>Relevant:</strong> Does the track we’re selecting for that user reflect their taste? Or is it just a song they accidentally listened to once?<ul><li>This is especially important for track attribution — if we put a user’s profile image next to a song, we need to make sure that this specific user would agree the song listed is representative of their taste.</li></ul></li><li><strong>Coherent: </strong>Does the playlist have flow, or do the tracks feel completely random and unrelated to each other?</li><li><strong>Equal:</strong> Are both users in the Blend represented equally?</li><li><strong>Democratic:</strong> Does music that both users like rise to the top?</li></ul>



<p>One of the core decisions we made for this product was whether it was better to “minimize the misery” or “maximize the joy”. In other words, is it better to pick everyone’s favorite tracks, even if other people in the group wouldn’t like them, or is it better to pick the tracks that everyone is likely to like, even if their favorite songs never get selected? “Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.</p>



<figure><img loading="lazy" width="700" height="477" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-1536x1047.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-2048x1397.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-120x82.png 120w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>It’s a bit simpler to create a Blend with users with similar taste since they listen to a lot of the same music. However, if we have two users with no common music listening history, it’s significantly more difficult to create a perfect Blend. We needed an approach that worked for both types of pairs, while also taking into consideration how any changes to the Blend algorithm impacts all combinations of users.</p>



<p>Between fetching data for both users in the Blend, and trying to come up with the ideal sequence balancing for all of our attributes, creating a Blend is a pretty heavy process. When we tried to come up with the best algorithm, we weren’t so concerned about our latency. Once we were happy with Blend quality, and started to think about scaling the service, we realized how bad our latency had gotten while iterating on the algorithm. We spent a lot of time trying to make the service as fast as possible. What we learned is that our code base had some hot spots in it: some sections of the code were run over 50 times per Blend generation, while other sections of the code were only run once. If we tried to optimize sections of the code that weren’t run many times, we didn’t make much of an impact in our latency. However, when we made improvements to our hot spots, we were able to make a huge difference. The biggest example here was swapping the order of two function calls within an if statement, taking advantage of Java’s short circuiting. This simple code change reduced our latency to 1/10 of its original time.</p>



<p>We were able to make content quality improvements by using both qualitative and quantitative methods. While we normally rely on testing our own playlists when we make changes, we also needed to make sure that we checked several different types of Blends (for example: test a high taste overlap Blend and a low taste overlap Blend). We created some offline metrics to measure how our attributes performed. We also work closely with a Data Curation team, often referred to as the “humans in the loop”. The Data Curation team evaluates and ensures content quality for recommendation systems. </p>



<p>For example, when the team wanted to make a change to make the playlist more coherent, we:</p>



<ul><li>Tested our own playlists — the team had implemented the change for about a month before we evaluated it. During this time, we were able to get a good feel for whether we preferred the change or not.</li><li>Performed a heuristic review, where our Data Curation team reviewed a number of Blends with a variety of taste overlap scores. <ul><li>This process helps identify issues with usability and comprehensibility associated most closely with content quality and with the user experience.</li><li>Utilize a tool called a “Content Recommendation Scorecard”.<ul><li>Score each track over a number of attributes such as relevance and coherence.</li></ul></li><li>From the Content Recommendation Scorecard, we were able to see that the new approach more strongly met our criteria in terms of the attributes we wanted to optimize for.</li><li>The review built enough confidence for the team to roll out the new approach to all users.</li></ul></li></ul>



<p>Creating a social playlist presented a new set of challenges in creating a new playlist algorithm. We had to try to optimize for many attributes: relevance, coherence, equality, and democratic decisions. We also had to consider both high taste overlap users and users who don’t have much taste overlap. </p>



<p>While building out the Blend product, we wanted a way to communicate information to the users about what similarities and differences they have in their music taste. This led to us building out Blend Data Stories, where we can show the users’ information like the artist that brings them together and their taste match score. This year, during the Wrapped Campaign, we gave users a <a href="https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/" target="_blank" rel="noreferrer noopener">Wrapped Blend</a> experience. We modified the Blend Data Stories to use data from Wrapped, to show users information like their top mutual artists and top mutual genre of the year.<br/></p>



<p>We’re still working hard to improve Blend, and build a product that allows our users to feel closer through music, while thinking of more fun ways to grow the social experience in Spotify. If this type of work sounds interesting, our Personalization team is <a href="https://www.lifeatspotify.com/jobs?q=personalization" target="_blank" rel="noreferrer noopener">hiring</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/data-modeling/" rel="tag">data modeling</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Jen Lamere, Senior Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Look Behind Blend: The Personalized Playlist for You…and You&#xA;</title>
      <link>https://engineering.atspotify.com/2021/12/a-look-behind-blend-the-personalized-playlist-for-youand-you/</link>
      <description>What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. We recently launched a</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/12/a-look-behind-blend-the-personalized-playlist-for-youand-you/" title="A Look Behind Blend: The Personalized Playlist for You…and You">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png 2097w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-2048x1010.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-120x59.png 120w" sizes="(max-width: 2097px) 100vw, 2097px"/>                    </a>
                        
        </p>

        

        
<p>What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. </p>



<p>We recently launched a new playlist initiative, <a href="https://newsroom.spotify.com/2021-08-31/how-spotifys-newest-personalized-experience-blend-creates-a-playlist-for-you-and-your-bestie/" target="_blank" rel="noreferrer noopener">Blend</a>, where any user can invite any user to generate a playlist wherein the two users’ tastes are combined into one shared playlist. Prior to Blend, the team worked on similar products, Family Mix and Duo Mix. These products create shared playlists for users on the same Family or Duo plan. The products were well received, so we decided to expand this product line, creating a version of opt-in, automatic, shared, personalized playlists that could work for any two users. </p>



<p>Anytime we want to make a new playlist at Spotify, we’re aiming to do something different that we haven’t been able to accomplish before. This means we can’t always lean on our past experiences, and often encounter new challenges that require new solutions. With Blend in particular, we were taking concepts from Family Mix and Duo Mix, and expanding them to a much larger user group. A major complication we saw here was the increase of scale in the number of users we had to deal with. We dealt with unique challenges both in the content creation process, and in the invitation flow, to create a Blend.</p>



<p>Most playlists are composed of a number of attributes and characteristics. For example, with Discover Weekly, our main attribute is discovery. For Daily Mix, our attributes are familiarity and coherency. When we are working with multiple users, however, we have the challenge of taking more attributes into account. Is the playlist:</p>



<ul><li><strong>Relevant:</strong> Does the track we’re selecting for that user reflect their taste? Or is it just a song they accidentally listened to once?<ul><li>This is especially important for track attribution — if we put a user’s profile image next to a song, we need to make sure that this specific user would agree the song listed is representative of their taste.</li></ul></li><li><strong>Coherent: </strong>Does the playlist have flow, or do the tracks feel completely random and unrelated to each other?</li><li><strong>Equal:</strong> Are both users in the Blend represented equally?</li><li><strong>Democratic:</strong> Does music that both users like rise to the top?</li></ul>



<p>One of the core decisions we made for this product was whether it was better to “minimize the misery” or “maximize the joy”. In other words, is it better to pick everyone’s favorite tracks, even if other people in the group wouldn’t like them, or is it better to pick the tracks that everyone is likely to like, even if their favorite songs never get selected? “Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.</p>



<figure><img loading="lazy" width="700" height="477" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-1536x1047.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-2048x1397.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-120x82.png 120w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>It’s a bit simpler to create a Blend with users with similar taste since they listen to a lot of the same music. However, if we have two users with no common music listening history, it’s significantly more difficult to create a perfect Blend. We needed an approach that worked for both types of pairs, while also taking into consideration how any changes to the Blend algorithm impacts all combinations of users.</p>



<p>Between fetching data for both users in the Blend, and trying to come up with the ideal sequence balancing for all of our attributes, creating a Blend is a pretty heavy process. When we tried to come up with the best algorithm, we weren’t so concerned about our latency. Once we were happy with Blend quality, and started to think about scaling the service, we realized how bad our latency had gotten while iterating on the algorithm. We spent a lot of time trying to make the service as fast as possible. What we learned is that our code base had some hot spots in it: some sections of the code were run over 50 times per Blend generation, while other sections of the code were only run once. If we tried to optimize sections of the code that weren’t run many times, we didn’t make much of an impact in our latency. However, when we made improvements to our hot spots, we were able to make a huge difference. The biggest example here was swapping the order of two function calls within an if statement, taking advantage of Java’s short circuiting. This simple code change reduced our latency to 1/10 of its original time.</p>



<p>We were able to make content quality improvements by using both qualitative and quantitative methods. While we normally rely on testing our own playlists when we make changes, we also needed to make sure that we checked several different types of Blends (for example: test a high taste overlap Blend and a low taste overlap Blend). We created some offline metrics to measure how our attributes performed. We also work closely with a Data Curation team, often referred to as the “humans in the loop”. The Data Curation team evaluates and ensures content quality for recommendation systems. </p>



<p>For example, when the team wanted to make a change to make the playlist more coherent, we:</p>



<ul><li>Tested our own playlists — the team had implemented the change for about a month before we evaluated it. During this time, we were able to get a good feel for whether we preferred the change or not.</li><li>Performed a heuristic review, where our Data Curation team reviewed a number of Blends with a variety of taste overlap scores. <ul><li>This process helps identify issues with usability and comprehensibility associated most closely with content quality and with the user experience.</li><li>Utilize a tool called a “Content Recommendation Scorecard”.<ul><li>Score each track over a number of attributes such as relevance and coherence.</li></ul></li><li>From the Content Recommendation Scorecard, we were able to see that the new approach more strongly met our criteria in terms of the attributes we wanted to optimize for.</li><li>The review built enough confidence for the team to roll out the new approach to all users.</li></ul></li></ul>



<p>Creating a social playlist presented a new set of challenges in creating a new playlist algorithm. We had to try to optimize for many attributes: relevance, coherence, equality, and democratic decisions. We also had to consider both high taste overlap users and users who don’t have much taste overlap. </p>



<p>While building out the Blend product, we wanted a way to communicate information to the users about what similarities and differences they have in their music taste. This led to us building out Blend Data Stories, where we can show the users’ information like the artist that brings them together and their taste match score. This year, during the Wrapped Campaign, we gave users a <a href="https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/" target="_blank" rel="noreferrer noopener">Wrapped Blend</a> experience. We modified the Blend Data Stories to use data from Wrapped, to show users information like their top mutual artists and top mutual genre of the year.<br/></p>



<p>We’re still working hard to improve Blend, and build a product that allows our users to feel closer through music, while thinking of more fun ways to grow the social experience in Spotify. If this type of work sounds interesting, our Personalization team is <a href="https://www.lifeatspotify.com/jobs?q=personalization" target="_blank" rel="noreferrer noopener">hiring</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/data-modeling/" rel="tag">data modeling</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Jen Lamere, Senior Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Look Behind Blend: The Personalized Playlist for You…and You&#xA;</title>
      <link>https://engineering.atspotify.com/2021/12/07/a-look-behind-blend-the-personalized-playlist-for-youand-you/</link>
      <description>What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. We recently launched a</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/12/07/a-look-behind-blend-the-personalized-playlist-for-youand-you/" title="A Look Behind Blend: The Personalized Playlist for You…and You">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png 2097w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-2048x1010.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-120x59.png 120w" sizes="(max-width: 2097px) 100vw, 2097px"/>                    </a>
                        
        </p>

        

        
<p>What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. </p>



<p>We recently launched a new playlist initiative, <a href="https://newsroom.spotify.com/2021-08-31/how-spotifys-newest-personalized-experience-blend-creates-a-playlist-for-you-and-your-bestie/" target="_blank" rel="noreferrer noopener">Blend</a>, where any user can invite any user to generate a playlist wherein the two users’ tastes are combined into one shared playlist. Prior to Blend, the team worked on similar products, Family Mix and Duo Mix. These products create shared playlists for users on the same Family or Duo plan. The products were well received, so we decided to expand this product line, creating a version of opt-in, automatic, shared, personalized playlists that could work for any two users. </p>



<p>Anytime we want to make a new playlist at Spotify, we’re aiming to do something different that we haven’t been able to accomplish before. This means we can’t always lean on our past experiences, and often encounter new challenges that require new solutions. With Blend in particular, we were taking concepts from Family Mix and Duo Mix, and expanding them to a much larger user group. A major complication we saw here was the increase of scale in the number of users we had to deal with. We dealt with unique challenges both in the content creation process, and in the invitation flow, to create a Blend.</p>



<p>Most playlists are composed of a number of attributes and characteristics. For example, with Discover Weekly, our main attribute is discovery. For Daily Mix, our attributes are familiarity and coherency. When we are working with multiple users, however, we have the challenge of taking more attributes into account. Is the playlist:</p>



<ul><li><strong>Relevant:</strong> Does the track we’re selecting for that user reflect their taste? Or is it just a song they accidentally listened to once?<ul><li>This is especially important for track attribution — if we put a user’s profile image next to a song, we need to make sure that this specific user would agree the song listed is representative of their taste.</li></ul></li><li><strong>Coherent: </strong>Does the playlist have flow, or do the tracks feel completely random and unrelated to each other?</li><li><strong>Equal:</strong> Are both users in the Blend represented equally?</li><li><strong>Democratic:</strong> Does music that both users like rise to the top?</li></ul>



<p>One of the core decisions we made for this product was whether it was better to “minimize the misery” or “maximize the joy”. In other words, is it better to pick everyone’s favorite tracks, even if other people in the group wouldn’t like them, or is it better to pick the tracks that everyone is likely to like, even if their favorite songs never get selected? “Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.</p>



<figure><img loading="lazy" width="700" height="477" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-1536x1047.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-2048x1397.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-120x82.png 120w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>It’s a bit simpler to create a Blend with users with similar taste since they listen to a lot of the same music. However, if we have two users with no common music listening history, it’s significantly more difficult to create a perfect Blend. We needed an approach that worked for both types of pairs, while also taking into consideration how any changes to the Blend algorithm impacts all combinations of users.</p>



<p>Between fetching data for both users in the Blend, and trying to come up with the ideal sequence balancing for all of our attributes, creating a Blend is a pretty heavy process. When we tried to come up with the best algorithm, we weren’t so concerned about our latency. Once we were happy with Blend quality, and started to think about scaling the service, we realized how bad our latency had gotten while iterating on the algorithm. We spent a lot of time trying to make the service as fast as possible. What we learned is that our code base had some hot spots in it: some sections of the code were run over 50 times per Blend generation, while other sections of the code were only run once. If we tried to optimize sections of the code that weren’t run many times, we didn’t make much of an impact in our latency. However, when we made improvements to our hot spots, we were able to make a huge difference. The biggest example here was swapping the order of two function calls within an if statement, taking advantage of Java’s short circuiting. This simple code change reduced our latency to 1/10 of its original time.</p>



<p>We were able to make content quality improvements by using both qualitative and quantitative methods. While we normally rely on testing our own playlists when we make changes, we also needed to make sure that we checked several different types of Blends (for example: test a high taste overlap Blend and a low taste overlap Blend). We created some offline metrics to measure how our attributes performed. We also work closely with a Data Curation team, often referred to as the “humans in the loop”. The Data Curation team evaluates and ensures content quality for recommendation systems. </p>



<p>For example, when the team wanted to make a change to make the playlist more coherent, we:</p>



<ul><li>Tested our own playlists — the team had implemented the change for about a month before we evaluated it. During this time, we were able to get a good feel for whether we preferred the change or not.</li><li>Performed a heuristic review, where our Data Curation team reviewed a number of Blends with a variety of taste overlap scores. <ul><li>This process helps identify issues with usability and comprehensibility associated most closely with content quality and with the user experience.</li><li>Utilize a tool called a “Content Recommendation Scorecard”.<ul><li>Score each track over a number of attributes such as relevance and coherence.</li></ul></li><li>From the Content Recommendation Scorecard, we were able to see that the new approach more strongly met our criteria in terms of the attributes we wanted to optimize for.</li><li>The review built enough confidence for the team to roll out the new approach to all users.</li></ul></li></ul>



<p>Creating a social playlist presented a new set of challenges in creating a new playlist algorithm. We had to try to optimize for many attributes: relevance, coherence, equality, and democratic decisions. We also had to consider both high taste overlap users and users who don’t have much taste overlap. </p>



<p>While building out the Blend product, we wanted a way to communicate information to the users about what similarities and differences they have in their music taste. This led to us building out Blend Data Stories, where we can show the users’ information like the artist that brings them together and their taste match score. This year, during the Wrapped Campaign, we gave users a <a href="https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/" target="_blank" rel="noreferrer noopener">Wrapped Blend</a> experience. We modified the Blend Data Stories to use data from Wrapped, to show users information like their top mutual artists and top mutual genre of the year.<br/></p>



<p>We’re still working hard to improve Blend, and build a product that allows our users to feel closer through music, while thinking of more fun ways to grow the social experience in Spotify. If this type of work sounds interesting, our Personalization team is <a href="https://www.lifeatspotify.com/jobs?q=personalization" target="_blank" rel="noreferrer noopener">hiring</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/data-modeling/" rel="tag">data modeling</a></p>

        

            </div></div>]]></content:encoded>
      <author>Published by Jen Lamere, Senior Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Spotify Uses ML to Create the Future of Personalization&#xA;</title>
      <link>https://engineering.atspotify.com/2021/12/02/how-spotify-uses-ml-to-create-the-future-of-personalization/</link>
      <description>Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4904">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 2, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/12/02/how-spotify-uses-ml-to-create-the-future-of-personalization/" title="How Spotify Uses ML to Create the Future of Personalization">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png 1920w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-120x68.png 120w" sizes="(max-width: 1920px) 100vw, 1920px"/>                    </a>
                        
        </p>

        

        
<p>Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But with a library of over 70 million tracks to thumb through, how do our ML models actually go about making these decisions?</p>



<p>Well, Spotify’s VP of Personalization Oskar Stål recently gave a talk at TransformX, a summit for leaders in ML and AI, to discuss just that. Read on to get a glimpse of how ML and reinforcement learning help inform our music and podcast recommendations, and don’t forget to check out Oskar’s presentation <a href="https://www.youtube.com/watch?v=n16LOyba-SE&amp;list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc" target="_blank" rel="noreferrer noopener">here</a> (or below!) to hear even more about the future of ML at Spotify.</p>



<h2><strong>How do we use ML? </strong></h2>



<p>It all starts with data. At the most fundamental level, all sorts of user info — playlists, listening history, interactions with Spotify’s UI, etc. — are fed into our ML models, while keeping trust and responsibility top of mind. Every day, nearly half a trillion events are processed, and the more info our models gather, the smarter they become about making associations between different artists, songs, podcasts, and playlists.</p>



<p>But our ML models even go beyond this, incorporating other factors in their decision-making processes. What time of day is it? Is this playlist for working out or chilling out? Are you on mobile or desktop? By incorporating several of these ML models throughout Spotify’s infrastructure, we’re able to offer increasingly intelligent, specialized recommendations that can, as Oskar puts it, “serve even the narrowest of tastes”.</p>



<p>We aren’t just looking for our users’ instant gratification, though. We want to provide listeners with a lifetime of great audio experiences and be with them on every step of that journey. And that brings us to what we’re working on now.</p>



<h2><strong>The future is reinforcement learning</strong></h2>



<p>Reinforcement learning, or RL, is a type of ML model that responds to its current environment in an effort to maximize the ultimate, long-term reward, whatever that may be. In our case, that reward is our users’ long-term satisfaction with Spotify. RL isn’t about short-term solutions. It’s always playing the long game.</p>



<p>In a general sense, our RL model tries to predict how satisfied our users are with their current experience, and attempts to nudge them toward consuming more fulfilling content in their audio diet to make them happier with the service. In other words, rather than handing users the “empty calories” of a content diet that will only satisfy them in the moment, RL aims to push them to a more sustainable, diverse, and fulfilling content diet that will last a lifetime. </p>



<p>This could mean playing a new dance track we think might fit a user’s current mood, or it could mean suggesting a calming, ambient piece to help them study. Predicting what a user will want 10 minutes from now, a day from now, a week from now, means creating a ton of simulations and running the RL model against those simulations to make it smarter, like a computer playing against itself in chess to get better at the game.</p>



<p>With ML and RL, we’re trying to create a more holistic audio experience, focused on recommendations that ensure long-term satisfaction and enjoyment. Our approach to personalization doesn’t just benefit listeners: better and more satisfying recommendations help out artists, exposing their work to a larger audience more likely to enjoy it. After all, there’s a reason there are 16 billion artist discoveries every month on our platform. And the best is yet to come.</p>



<figure><p>
<iframe loading="lazy" title="VP of Personalization Oskar Stål Talks the Future of ML at TransformX" width="900" height="506" src="https://www.youtube.com/embed/n16LOyba-SE?list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a></p>

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Spotify Uses ML to Create the Future of Personalization&#xA;</title>
      <link>https://engineering.atspotify.com/how-spotify-uses-ml-to-create-the-future-of-personalization/</link>
      <description>Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4904">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 2, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/how-spotify-uses-ml-to-create-the-future-of-personalization/" title="How Spotify Uses ML to Create the Future of Personalization">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png 1920w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-120x68.png 120w" sizes="(max-width: 1920px) 100vw, 1920px"/>                    </a>
                        
        </p>

        

        
<p>Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But with a library of over 70 million tracks to thumb through, how do our ML models actually go about making these decisions?</p>



<p>Well, Spotify’s VP of Personalization Oskar Stål recently gave a talk at TransformX, a summit for leaders in ML and AI, to discuss just that. Read on to get a glimpse of how ML and reinforcement learning help inform our music and podcast recommendations, and don’t forget to check out Oskar’s presentation <a href="https://www.youtube.com/watch?v=n16LOyba-SE&amp;list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc" target="_blank" rel="noreferrer noopener">here</a> (or below!) to hear even more about the future of ML at Spotify.</p>



<h2><strong>How do we use ML? </strong></h2>



<p>It all starts with data. At the most fundamental level, all sorts of user info — playlists, listening history, interactions with Spotify’s UI, etc. — are fed into our ML models, while keeping trust and responsibility top of mind. Every day, nearly half a trillion events are processed, and the more info our models gather, the smarter they become about making associations between different artists, songs, podcasts, and playlists.</p>



<p>But our ML models even go beyond this, incorporating other factors in their decision-making processes. What time of day is it? Is this playlist for working out or chilling out? Are you on mobile or desktop? By incorporating several of these ML models throughout Spotify’s infrastructure, we’re able to offer increasingly intelligent, specialized recommendations that can, as Oskar puts it, “serve even the narrowest of tastes”.</p>



<p>We aren’t just looking for our users’ instant gratification, though. We want to provide listeners with a lifetime of great audio experiences and be with them on every step of that journey. And that brings us to what we’re working on now.</p>



<h2><strong>The future is reinforcement learning</strong></h2>



<p>Reinforcement learning, or RL, is a type of ML model that responds to its current environment in an effort to maximize the ultimate, long-term reward, whatever that may be. In our case, that reward is our users’ long-term satisfaction with Spotify. RL isn’t about short-term solutions. It’s always playing the long game.</p>



<p>In a general sense, our RL model tries to predict how satisfied our users are with their current experience, and attempts to nudge them toward consuming more fulfilling content in their audio diet to make them happier with the service. In other words, rather than handing users the “empty calories” of a content diet that will only satisfy them in the moment, RL aims to push them to a more sustainable, diverse, and fulfilling content diet that will last a lifetime. </p>



<p>This could mean playing a new dance track we think might fit a user’s current mood, or it could mean suggesting a calming, ambient piece to help them study. Predicting what a user will want 10 minutes from now, a day from now, a week from now, means creating a ton of simulations and running the RL model against those simulations to make it smarter, like a computer playing against itself in chess to get better at the game.</p>



<p>With ML and RL, we’re trying to create a more holistic audio experience, focused on recommendations that ensure long-term satisfaction and enjoyment. Our approach to personalization doesn’t just benefit listeners: better and more satisfying recommendations help out artists, exposing their work to a larger audience more likely to enjoy it. After all, there’s a reason there are 16 billion artist discoveries every month on our platform. And the best is yet to come.</p>



<figure><p>
<iframe loading="lazy" title="VP of Personalization Oskar Stål Talks the Future of ML at TransformX" width="900" height="506" src="https://www.youtube.com/embed/n16LOyba-SE?list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Spotify Uses ML to Create the Future of Personalization&#xA;</title>
      <link>https://engineering.atspotify.com/how-spotify-uses-ml-to-create-the-future-of-personalization/</link>
      <description>Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4904">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 2, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/how-spotify-uses-ml-to-create-the-future-of-personalization/" title="How Spotify Uses ML to Create the Future of Personalization">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png 1920w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-120x68.png 120w" sizes="(max-width: 1920px) 100vw, 1920px"/>                    </a>
                        
        </p>

        

        
<p>Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But with a library of over 70 million tracks to thumb through, how do our ML models actually go about making these decisions?</p>



<p>Well, Spotify’s VP of Personalization Oskar Stål recently gave a talk at TransformX, a summit for leaders in ML and AI, to discuss just that. Read on to get a glimpse of how ML and reinforcement learning help inform our music and podcast recommendations, and don’t forget to check out Oskar’s presentation <a href="https://www.youtube.com/watch?v=n16LOyba-SE&amp;list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc" target="_blank" rel="noreferrer noopener">here</a> (or below!) to hear even more about the future of ML at Spotify.</p>



<h2><strong>How do we use ML? </strong></h2>



<p>It all starts with data. At the most fundamental level, all sorts of user info — playlists, listening history, interactions with Spotify’s UI, etc. — are fed into our ML models, while keeping trust and responsibility top of mind. Every day, nearly half a trillion events are processed, and the more info our models gather, the smarter they become about making associations between different artists, songs, podcasts, and playlists.</p>



<p>But our ML models even go beyond this, incorporating other factors in their decision-making processes. What time of day is it? Is this playlist for working out or chilling out? Are you on mobile or desktop? By incorporating several of these ML models throughout Spotify’s infrastructure, we’re able to offer increasingly intelligent, specialized recommendations that can, as Oskar puts it, “serve even the narrowest of tastes”.</p>



<p>We aren’t just looking for our users’ instant gratification, though. We want to provide listeners with a lifetime of great audio experiences and be with them on every step of that journey. And that brings us to what we’re working on now.</p>



<h2><strong>The future is reinforcement learning</strong></h2>



<p>Reinforcement learning, or RL, is a type of ML model that responds to its current environment in an effort to maximize the ultimate, long-term reward, whatever that may be. In our case, that reward is our users’ long-term satisfaction with Spotify. RL isn’t about short-term solutions. It’s always playing the long game.</p>



<p>In a general sense, our RL model tries to predict how satisfied our users are with their current experience, and attempts to nudge them toward consuming more fulfilling content in their audio diet to make them happier with the service. In other words, rather than handing users the “empty calories” of a content diet that will only satisfy them in the moment, RL aims to push them to a more sustainable, diverse, and fulfilling content diet that will last a lifetime. </p>



<p>This could mean playing a new dance track we think might fit a user’s current mood, or it could mean suggesting a calming, ambient piece to help them study. Predicting what a user will want 10 minutes from now, a day from now, a week from now, means creating a ton of simulations and running the RL model against those simulations to make it smarter, like a computer playing against itself in chess to get better at the game.</p>



<p>With ML and RL, we’re trying to create a more holistic audio experience, focused on recommendations that ensure long-term satisfaction and enjoyment. Our approach to personalization doesn’t just benefit listeners: better and more satisfying recommendations help out artists, exposing their work to a larger audience more likely to enjoy it. After all, there’s a reason there are 16 billion artist discoveries every month on our platform. And the best is yet to come.</p>



<figure><p>
<iframe loading="lazy" title="VP of Personalization Oskar Stål Talks the Future of ML at TransformX" width="900" height="506" src="https://www.youtube.com/embed/n16LOyba-SE?list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Spotify Uses ML to Create the Future of Personalization&#xA;</title>
      <link>https://engineering.atspotify.com/2021/12/how-spotify-uses-ml-to-create-the-future-of-personalization/</link>
      <description>Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4904">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 2, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/12/how-spotify-uses-ml-to-create-the-future-of-personalization/" title="How Spotify Uses ML to Create the Future of Personalization">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png 1920w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-120x68.png 120w" sizes="(max-width: 1920px) 100vw, 1920px"/>                    </a>
                        
        </p>

        

        
<p>Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But with a library of over 70 million tracks to thumb through, how do our ML models actually go about making these decisions?</p>



<p>Well, Spotify’s VP of Personalization Oskar Stål recently gave a talk at TransformX, a summit for leaders in ML and AI, to discuss just that. Read on to get a glimpse of how ML and reinforcement learning help inform our music and podcast recommendations, and don’t forget to check out Oskar’s presentation <a href="https://www.youtube.com/watch?v=n16LOyba-SE&amp;list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc" target="_blank" rel="noreferrer noopener">here</a> (or below!) to hear even more about the future of ML at Spotify.</p>



<h2><strong>How do we use ML? </strong></h2>



<p>It all starts with data. At the most fundamental level, all sorts of user info — playlists, listening history, interactions with Spotify’s UI, etc. — are fed into our ML models, while keeping trust and responsibility top of mind. Every day, nearly half a trillion events are processed, and the more info our models gather, the smarter they become about making associations between different artists, songs, podcasts, and playlists.</p>



<p>But our ML models even go beyond this, incorporating other factors in their decision-making processes. What time of day is it? Is this playlist for working out or chilling out? Are you on mobile or desktop? By incorporating several of these ML models throughout Spotify’s infrastructure, we’re able to offer increasingly intelligent, specialized recommendations that can, as Oskar puts it, “serve even the narrowest of tastes”.</p>



<p>We aren’t just looking for our users’ instant gratification, though. We want to provide listeners with a lifetime of great audio experiences and be with them on every step of that journey. And that brings us to what we’re working on now.</p>



<h2><strong>The future is reinforcement learning</strong></h2>



<p>Reinforcement learning, or RL, is a type of ML model that responds to its current environment in an effort to maximize the ultimate, long-term reward, whatever that may be. In our case, that reward is our users’ long-term satisfaction with Spotify. RL isn’t about short-term solutions. It’s always playing the long game.</p>



<p>In a general sense, our RL model tries to predict how satisfied our users are with their current experience, and attempts to nudge them toward consuming more fulfilling content in their audio diet to make them happier with the service. In other words, rather than handing users the “empty calories” of a content diet that will only satisfy them in the moment, RL aims to push them to a more sustainable, diverse, and fulfilling content diet that will last a lifetime. </p>



<p>This could mean playing a new dance track we think might fit a user’s current mood, or it could mean suggesting a calming, ambient piece to help them study. Predicting what a user will want 10 minutes from now, a day from now, a week from now, means creating a ton of simulations and running the RL model against those simulations to make it smarter, like a computer playing against itself in chess to get better at the game.</p>



<p>With ML and RL, we’re trying to create a more holistic audio experience, focused on recommendations that ensure long-term satisfaction and enjoyment. Our approach to personalization doesn’t just benefit listeners: better and more satisfying recommendations help out artists, exposing their work to a larger audience more likely to enjoy it. After all, there’s a reason there are 16 billion artist discoveries every month on our platform. And the best is yet to come.</p>



<figure><p>
<iframe loading="lazy" title="VP of Personalization Oskar Stål Talks the Future of ML at TransformX" width="900" height="506" src="https://www.youtube.com/embed/n16LOyba-SE?list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)&#xA;</title>
      <link>https://engineering.atspotify.com/2021/11/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/</link>
      <description>In Part I of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including: The Podcast Model: Predicts podcasts a listener is likely to listen to in the Shows you might like shelf. The Shortcuts Model: Predicts the listener’s next fa</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/11/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/" title="The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png 2098w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-2048x1009.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-120x59.png 120w" sizes="(max-width: 2098px) 100vw, 2098px"/>                    </a>
                        
        </p>

        

        
<p>In <a href="https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/" target="_blank" rel="noreferrer noopener">Part I</a> of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including:</p>



<ul><li><strong>The Podcast Model:</strong> Predicts podcasts a listener is likely to listen to in the <em>Shows you might like</em> shelf. </li><li><strong>The Shortcuts Model: </strong>Predicts the listener’s next familiar listen in the Shortcuts feature. </li><li><strong>The Playlists Model: </strong>Predicts the playlists a new listener is likely to listen to in the <em>Try something else</em> shelf.  </li></ul>



<p>In this part of the series, we’ll highlight how and why we evaluate our models with different tools, and the hurdles to maintaining these models in production. </p>



<h2>Trust but verify your recommendations… with dashboards</h2>



<p>So let’s talk about what we do with that data — specifically, how we run experiments, and maybe more importantly, how we evaluate our models’ performance.</p>



<h3>Making experimentation simpler</h3>



<h4>How it started: experimenting on a siloed platform</h4>



<p>Not that long ago, after transforming our training data, we would run experiments on a siloed platform specifically geared towards model experimentation, and that was only really used within our team — we did this for both the initial Podcast Model as well as for the Shortcuts Model. This platform could easily launch hundreds of experiments by using a configuration file to specify hyperparameters (it also supported a grid search on specified hyperparameters). And since everything that was submitted to run an experiment was a script, it supported custom evaluation metrics — something that has always been important in our team. While it provided these necessary features, it wasn’t scalable, wasn’t maintained, and had an incomplete UI. Sometimes the compute instances would lose connection with the API (via a periodic ping) and would end up being ghost workers — still running, but not connected to anything.  </p>



<h4>How it’s going: integration with Spotify ML ecosystem</h4>



<p><a href="https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/" target="_blank" rel="noreferrer noopener">Spotify’s managed Kubeflow clusters</a> provide a more scalable approach, modular components, and are compatible with other parts of the Spotify ML infrastructure, so it was an obvious choice to move our experimentation to this platform. Training our models using Kubeflow pipelines is easy and efficient, but running the evaluation we needed and tracking those results were our biggest pain points for two reasons: </p>



<ol><li>As Spotify’s SDK for Kubeflow uses Tensorflow Model Analysis (TFMA), comparing the performance of a non-ML heuristic algorithm to that of a trained model is challenging to set up and requires extra infrastructure. </li><li>We often have custom evaluation metrics that are specific to the model’s task, but they are infinitely more difficult to implement in TFMA than in vanilla Python.</li></ol>



<h3>Evaluating models against simpler (non-ML) solutions</h3>



<p>As I alluded to in an earlier paragraph, we don’t typically start solutions to a problem with an ML solution. We first identify a heuristic, or rule-based, solution, and the most appropriate way to evaluate it.  </p>



<h4>The first step — having a baseline for comparison</h4>



<p>We are often tasked with creating better recommendations for content X, but what are “better recommendations,” and what are they better than? Having a baseline helps answer these questions, giving us something to compare our models against. And a good baseline — usually a heuristic/rule-based solution — is a quick, efficient, but maybe not the most optimal, solution.</p>



<p>Take the Shortcuts Model as an example. We created an initial heuristic that recommended, simply, the most frequently played items from a listener’s short-term listening history. We improved the heuristic over many iterations, then compared it to the performance of the models we trained. Being able to compare these heuristics to the models gave us confidence to say that having a model was an improvement over the heuristics and was worth the extra effort of maintaining, deploying, and monitoring these models.</p>



<h4>Comparing model performance to baseline performance is difficult</h4>



<p>After establishing our baseline and training our model(s), the difficulty lies in how we compare them evenly. In a perfect world we would run infinite A/B tests with hundreds of test cells to compare the performance of all our solutions in the real world, on real listeners. Since it’s not a perfect world, we need reliable offline metrics that act as a proxy for the online metrics we can’t get in those A/B tests.  </p>



<p>When evaluating our recommendations models, we typically use normalized discounted cumulative gain (NDCG@k) as our metric, which can be implemented using Spotify’s Python SDK for Kubeflow pipelines. The question then becomes: how do we do the same for our heuristic? As we’ve mentioned before, transformation logic consistency is paramount, and so is evaluation logic — ideally, we’d have the same evaluation logic and the same evaluation test set of data. Unfortunately, our heuristics are generally written in a Java service and are tested with unit tests (not for performance).</p>



<p>For fairly simple heuristics, we found a way to “train a model” so that its output is the heuristic rule’s output. This allowed us to use the same evaluation and evaluation test set as the models we were comparing against. We took this same approach when coming up with a solution to recommendations in the <em>Try something else</em> shelf for new users on Home. We computed a popularity heuristic based on a listener’s demographics in Tensorflow Transform (TFT) and used the model as a lookup utility (with a fake loss).  </p>



<p>We can’t always fit our problem into such a simple heuristic, as was the case for Shortcuts. The logic used in most of the Shortcuts heuristics was too complex to write in Tensorflow, so we implemented a completely separate offline evaluation pipeline that would gather recommendations made by models and heuristics, and apply custom evaluation functions for comparison.  </p>



<h3>Adding freedom and flexibility to our evaluation tools</h3>



<p>As mentioned earlier, there’s a second pain point we run into often: using custom evaluation metrics in TFMA.</p>



<h4>TFMA is sometimes too rigid</h4>



<p>Spotify’s SDK for Kubeflow only supports evaluations using TFMA, which provides fairly basic metrics out of the box — think: precision, recall, accuracy. The most common metric we typically use is NDCG@k — TFMA provides NDCG, but not NDCG@k. Implementing metrics in TFMA is notoriously difficult; it takes ~120 lines of code to implement NDCG@k in TFMA, but only a single line of code using scikit-learn in Python.</p>



<p>Most recently, we were experimenting with a model that predicts the next playlist that a new user will listen to, and as we have very little information about new users, we wanted to ensure that the model was not just predicting the most popular content. To do so, we were going to evaluate the model with a diversity metric that measured the difference between specific characteristics of items in each playlist. This was nearly impossible to implement in TFMA, so our team contributed to the Python SDK for Kubeflow to support any custom Python evaluation. We have been using this and running our experiments via Kubeflow pipelines since October 2020. </p>



<h4>Compare and track experiment results</h4>



<p>In the pre-Kubeflow world, our experimentation platform allowed for a way to track and compare models — now, we are using Spotify’s internal UI for machine learning, as it easily integrates with our Kubeflow runs. We can view and compare the evaluation scores of our experiments — both NDCG and custom metrics — in the UI. We’ve been using this for a number of our models, and it allows us to track our model deployments as well.</p>



<h3>Looking at more than just the numbers for evaluating recommendations</h3>



<p>I’ve mostly mentioned what metrics we use and why they are important, but there is another incredibly useful way we evaluate our models — sometimes more useful than what a metric can reveal.</p>



<h4>We build custom dashboards to manually evaluate the recs</h4>



<p>Based on past issues, we know that evaluation metrics don’t show the whole picture of how well a model is recommending content. Sometimes, the best way to evaluate a model is by seeing what content it recommends given a specific set of features about a listener. And for this reason, our team built a dashboard that does exactly that. It loads models simply by supplying the storage location of the model, and supports comparison of multiple models given a set of features. We often test and evaluate the recommendations that a new model will provide before deploying it to production by making predictions with different sets of feature values; this gives us an intuition behind what content will be recommended to different users that have these feature values. This has helped us find glaring issues; for example, when developing and testing a new model, we found that it would recommend the same popular playlist to listeners in all European countries. Having this knowledge allowed us to fix and improve the model before deploying it to production.  </p>



<p>Most recently, we have been working on a new model to recommend albums a listener might like based on their locality and what they like to listen to. We have been running experiments comparing evaluation metric values, but we have also been looking at the recommendations on our dashboard. This dashboard gives you the ability to try different features and compare the recommendations across different models — all before the models are used to recommend content to our listeners. At the beginning stages of experimentation and modeling for this project, we noticed that the same album was recommended as the first item no matter what input features (such as user’s country, followed artists, etc.) were used for testing, meaning this album would have been recommended to everyone as the first recommendation. Without this dashboard as a tool, it would have been more challenging to identify this issue and remediate it before the model went live.</p>



<p>While our offline metrics might indicate poor performance, they don’t tell us anything about what the reason might be, whereas this dashboard can show the quality of our recommendations and is extremely useful in finding issues like this.</p>



<p>Through the use of task-specific custom evaluations and dashboards to show evaluation metrics and recommendations per feature set, we have been able to gain deep insight into how our models are behaving, and make our models a little less of a black box. </p>



<h2>The struggles of automated model retraining and deployment</h2>



<p>Let’s dive into our last topic, which is all about maintaining models in production: retraining and automatic deployment.</p>



<h3>But do we actually need to retrain our models?</h3>



<p>It would be really nice if we could train a model once, deploy it, and then not have to do anything except monitor its online performance. Sadly, we’ve never seen this in reality.</p>



<h4>Sometimes the model’s task requires frequent retraining</h4>



<p>Since we first deployed the Podcast Model in Home, we have always had retraining set up for it — and that’s because it only recommends podcast shows that it has seen in training data. So if we didn’t retrain it, it wouldn’t recommend any newly published shows.  </p>



<h4>The rest of the time, it just becomes a tech debt monster</h4>



<p>But in some cases, retraining isn’t necessarily required to capture the full set of possible candidates. For the Shortcuts Model, we didn’t have retraining set up because it only recommends content that the listener has previously listened to (which is always in the serving features). But while retraining wasn’t needed for the Shortcuts Model to operate, the lack of it became one of the biggest sources of ML tech debt. We did not implement retraining for Shortcuts because it wasn’t needed for launching the feature, but have seen that it would have saved us time and effort in the long run had we invested some time in the short term. </p>



<p>It wasn’t until many months after the launch of the model that we saw issues with the quality of recommendations in Shortcuts due to no retraining — some of the features for this model describe the type of content that a listener has listened to, like whether it’s a personalized playlist or an album, etc., and there was a recent addition of a new type of content that was introduced after the model was last trained. As a result, the model didn’t recommend this piece of content in Shortcuts. While this starts to look like the same scenario as the Podcast Model described above, we also saw issues with migrating to different tools and platforms because the model was trained using older versions of libraries.  </p>



<h3>Implement for the short term while waiting for the long-term solution</h3>



<p>Once upon a time, we only had that singular Podcast Model, which was used to generate batch predictions, not real-time predictions. We had a Scio pipeline that used <a href="https://spotify.github.io/zoltar/" target="_blank" rel="noreferrer noopener">Zoltar</a> to predict podcast recommendations for all listeners, and we stored these predictions in our Bigtable instance that holds all of our content recommendations. This was a great start, but fairly inflexible when it came to when and how often we could make predictions for a given listener — and this is important because the listeners’ features could change if they listen to new content or follow new artists, which could provide better information to the model.  </p>



<h4>Building a recommender service for the short term</h4>



<p>Consequently, we built a new service to serve this model and enable online predictions. We could get fresh recommendations for a listener almost instantly, and we could get these recommendations at useful times, such as when a listener follows a new artist. While this was a great improvement to move from offline predictions to online predictions, and an important step in making a better product, we knew we were only going to be in this state for the short term. Spotify’s online serving platform was on the horizon, but not yet ready; the benefits to building a short-term less-than-optimal solution outweighed the benefits of waiting to serve online models until Spotify’s serving solution was production ready.  </p>



<p>With that said, let’s talk about some challenges we faced in building this recommender service, such as how to refresh the local version of a deployed model. Our solution was to poll our internal storage directory every 10 minutes to check if there was a new revision of the model; if so, the service would pull the model down from where it was stored and start using that model to make predictions. Nevermind that we only retrained weekly or that there would be some state at which some machines would have the new revision of a model and others would have the older revision (although this was not something we worried about in our specific use case).  </p>



<h4>The pain of manually deploying models</h4>



<p>This was really a solution to serving models online, and less of a solution to a better process of serving models. Each time we wanted to deploy a model we had to: 1) copy the model to a specific storage location, 2) manually generate a pointer in our internal storage directory for that location, and 3) add this pointer to our recommender service along with the logic to fetch and transform features for the model. If we were to retrain the model, we would have to repeat each of those steps.  </p>



<p>Obviously, this was a cumbersome process, but because we had this short-term solution, we were able to deploy four models to production and tested many others in A/B tests.</p>



<h3>CI/CD — but make it for model training and deployment</h3>



<p>While this recommender service lived a long life of about 10 months, the next obvious step was to migrate to Spotify’s model serving platform, which enabled us to automate retraining and deployment of retrained models.  </p>



<h4>Automating feature transformations without Tensorflow Transform</h4>



<p>The first step in automating retraining is automating train dataset and test dataset curation, fetching the correct features and performing the necessary feature transformations. While feature transformations are generally handled automatically via TFT in a Kubeflow pipeline, we don’t perform our feature transformations in TFT (and therefore not in our experiment pipeline) because many the transformations we perform on the data are fairly complex and would be unnecessarily difficult to do in Tensorflow.  </p>



<p>But because the serving platform provides feature logging, we enabled logging of <em>already transformed</em> features, to which we then apply the correct labels, and separate into train and test sets. These actions are all performed in scheduled pipelines that run weekly and produce weekly datasets for our models to use.</p>



<h4>Migrating from our short-term solution to a long-term solution</h4>



<p>In order to enable feature logging, we had to migrate to the new online model serving platform from our recommender service using Zoltar. It was a matter of dark loading all prediction traffic to the new deployment and then running a simple rollout to start directing traffic to our new deployment instead of using Zoltar to make predictions in our own service. This was an easy migration and provided the benefits that the online serving platform offers — feature logging, faster predictions / lower latencies, less code managed by our team — and it also supports pushing a new model version (from a Kubeflow pipeline), as opposed to constantly polling for a new model version. </p>



<h4>Continuous retraining and automatic deployment</h4>



<p>Now that our models are all deployed via the Spotify serving platform, it enables us to employ CI/CD. We can schedule our models to be retrained via a Kubeflow pipeline, and as part of the Kubeflow pipeline we can ensure that a “bad” model is not accidentally automatically deployed by specifying that it should: 1) check that the evaluation score is greater than our configured threshold, and 2) automatically push it to our serving infrastructure if it is greater than the threshold. This automates a lot of the processes that we had to perform manually not long ago.  </p>



<p>Enabling CI/CD for retraining and model deployment is hard, but it’s becoming easier with the new tools available and makes the quality and reliability of our models better. And at first glance, you might not think you need retraining for a model because of the task it performs, but without it, your model could make predictions in unpredictable ways and increase your tech debt.   </p>



<h2>Conclusion</h2>



<p>Our ML stack has come a long way in recent years, but it’s not perfect by any means. There are still a number of challenges we are tackling — data versioning, model versioning, moving feature transformations to Tensorflow Transform — and better ways to compare offline metrics across both ML and non-ML solutions. But it has decreased the time it takes for us to iterate, experiment, and deploy quality models.</p>



<p>We have adopted and/or built the components we need to successfully and efficiently manage our data, experiment with different models, and support continuous integration and development throughout the deployment and retraining processes. Our ML stack has enabled us to launch numerous models that serve millions of listeners on Home every day.</p>



<p>If you are interested in joining us and helping improve how we recommend content on Home, we are <a href="https://www.lifeatspotify.com/jobs">hiring</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Annie Edmundson, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)&#xA;</title>
      <link>https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/</link>
      <description>In Part I of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including: The Podcast Model: Predicts podcasts a listener is likely to listen to in the Shows you might like shelf. The Shortcuts Model: Predicts the listener’s next fa</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/" title="The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png 2098w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-2048x1009.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-120x59.png 120w" sizes="(max-width: 2098px) 100vw, 2098px"/>                    </a>
                        
        </p>

        

        
<p>In <a href="https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/" target="_blank" rel="noreferrer noopener">Part I</a> of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including:</p>



<ul><li><strong>The Podcast Model:</strong> Predicts podcasts a listener is likely to listen to in the <em>Shows you might like</em> shelf. </li><li><strong>The Shortcuts Model: </strong>Predicts the listener’s next familiar listen in the Shortcuts feature. </li><li><strong>The Playlists Model: </strong>Predicts the playlists a new listener is likely to listen to in the <em>Try something else</em> shelf.  </li></ul>



<p>In this part of the series, we’ll highlight how and why we evaluate our models with different tools, and the hurdles to maintaining these models in production. </p>



<h2>Trust but verify your recommendations… with dashboards</h2>



<p>So let’s talk about what we do with that data — specifically, how we run experiments, and maybe more importantly, how we evaluate our models’ performance.</p>



<h3>Making experimentation simpler</h3>



<h4>How it started: experimenting on a siloed platform</h4>



<p>Not that long ago, after transforming our training data, we would run experiments on a siloed platform specifically geared towards model experimentation, and that was only really used within our team — we did this for both the initial Podcast Model as well as for the Shortcuts Model. This platform could easily launch hundreds of experiments by using a configuration file to specify hyperparameters (it also supported a grid search on specified hyperparameters). And since everything that was submitted to run an experiment was a script, it supported custom evaluation metrics — something that has always been important in our team. While it provided these necessary features, it wasn’t scalable, wasn’t maintained, and had an incomplete UI. Sometimes the compute instances would lose connection with the API (via a periodic ping) and would end up being ghost workers — still running, but not connected to anything.  </p>



<h4>How it’s going: integration with Spotify ML ecosystem</h4>



<p><a href="https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/" target="_blank" rel="noreferrer noopener">Spotify’s managed Kubeflow clusters</a> provide a more scalable approach, modular components, and are compatible with other parts of the Spotify ML infrastructure, so it was an obvious choice to move our experimentation to this platform. Training our models using Kubeflow pipelines is easy and efficient, but running the evaluation we needed and tracking those results were our biggest pain points for two reasons: </p>



<ol><li>As Spotify’s SDK for Kubeflow uses Tensorflow Model Analysis (TFMA), comparing the performance of a non-ML heuristic algorithm to that of a trained model is challenging to set up and requires extra infrastructure. </li><li>We often have custom evaluation metrics that are specific to the model’s task, but they are infinitely more difficult to implement in TFMA than in vanilla Python.</li></ol>



<h3>Evaluating models against simpler (non-ML) solutions</h3>



<p>As I alluded to in an earlier paragraph, we don’t typically start solutions to a problem with an ML solution. We first identify a heuristic, or rule-based, solution, and the most appropriate way to evaluate it.  </p>



<h4>The first step — having a baseline for comparison</h4>



<p>We are often tasked with creating better recommendations for content X, but what are “better recommendations,” and what are they better than? Having a baseline helps answer these questions, giving us something to compare our models against. And a good baseline — usually a heuristic/rule-based solution — is a quick, efficient, but maybe not the most optimal, solution.</p>



<p>Take the Shortcuts Model as an example. We created an initial heuristic that recommended, simply, the most frequently played items from a listener’s short-term listening history. We improved the heuristic over many iterations, then compared it to the performance of the models we trained. Being able to compare these heuristics to the models gave us confidence to say that having a model was an improvement over the heuristics and was worth the extra effort of maintaining, deploying, and monitoring these models.</p>



<h4>Comparing model performance to baseline performance is difficult</h4>



<p>After establishing our baseline and training our model(s), the difficulty lies in how we compare them evenly. In a perfect world we would run infinite A/B tests with hundreds of test cells to compare the performance of all our solutions in the real world, on real listeners. Since it’s not a perfect world, we need reliable offline metrics that act as a proxy for the online metrics we can’t get in those A/B tests.  </p>



<p>When evaluating our recommendations models, we typically use normalized discounted cumulative gain (NDCG@k) as our metric, which can be implemented using Spotify’s Python SDK for Kubeflow pipelines. The question then becomes: how do we do the same for our heuristic? As we’ve mentioned before, transformation logic consistency is paramount, and so is evaluation logic — ideally, we’d have the same evaluation logic and the same evaluation test set of data. Unfortunately, our heuristics are generally written in a Java service and are tested with unit tests (not for performance).</p>



<p>For fairly simple heuristics, we found a way to “train a model” so that its output is the heuristic rule’s output. This allowed us to use the same evaluation and evaluation test set as the models we were comparing against. We took this same approach when coming up with a solution to recommendations in the <em>Try something else</em> shelf for new users on Home. We computed a popularity heuristic based on a listener’s demographics in Tensorflow Transform (TFT) and used the model as a lookup utility (with a fake loss).  </p>



<p>We can’t always fit our problem into such a simple heuristic, as was the case for Shortcuts. The logic used in most of the Shortcuts heuristics was too complex to write in Tensorflow, so we implemented a completely separate offline evaluation pipeline that would gather recommendations made by models and heuristics, and apply custom evaluation functions for comparison.  </p>



<h3>Adding freedom and flexibility to our evaluation tools</h3>



<p>As mentioned earlier, there’s a second pain point we run into often: using custom evaluation metrics in TFMA.</p>



<h4>TFMA is sometimes too rigid</h4>



<p>Spotify’s SDK for Kubeflow only supports evaluations using TFMA, which provides fairly basic metrics out of the box — think: precision, recall, accuracy. The most common metric we typically use is NDCG@k — TFMA provides NDCG, but not NDCG@k. Implementing metrics in TFMA is notoriously difficult; it takes ~120 lines of code to implement NDCG@k in TFMA, but only a single line of code using scikit-learn in Python.</p>



<p>Most recently, we were experimenting with a model that predicts the next playlist that a new user will listen to, and as we have very little information about new users, we wanted to ensure that the model was not just predicting the most popular content. To do so, we were going to evaluate the model with a diversity metric that measured the difference between specific characteristics of items in each playlist. This was nearly impossible to implement in TFMA, so our team contributed to the Python SDK for Kubeflow to support any custom Python evaluation. We have been using this and running our experiments via Kubeflow pipelines since October 2020. </p>



<h4>Compare and track experiment results</h4>



<p>In the pre-Kubeflow world, our experimentation platform allowed for a way to track and compare models — now, we are using Spotify’s internal UI for machine learning, as it easily integrates with our Kubeflow runs. We can view and compare the evaluation scores of our experiments — both NDCG and custom metrics — in the UI. We’ve been using this for a number of our models, and it allows us to track our model deployments as well.</p>



<h3>Looking at more than just the numbers for evaluating recommendations</h3>



<p>I’ve mostly mentioned what metrics we use and why they are important, but there is another incredibly useful way we evaluate our models — sometimes more useful than what a metric can reveal.</p>



<h4>We build custom dashboards to manually evaluate the recs</h4>



<p>Based on past issues, we know that evaluation metrics don’t show the whole picture of how well a model is recommending content. Sometimes, the best way to evaluate a model is by seeing what content it recommends given a specific set of features about a listener. And for this reason, our team built a dashboard that does exactly that. It loads models simply by supplying the storage location of the model, and supports comparison of multiple models given a set of features. We often test and evaluate the recommendations that a new model will provide before deploying it to production by making predictions with different sets of feature values; this gives us an intuition behind what content will be recommended to different users that have these feature values. This has helped us find glaring issues; for example, when developing and testing a new model, we found that it would recommend the same popular playlist to listeners in all European countries. Having this knowledge allowed us to fix and improve the model before deploying it to production.  </p>



<p>Most recently, we have been working on a new model to recommend albums a listener might like based on their locality and what they like to listen to. We have been running experiments comparing evaluation metric values, but we have also been looking at the recommendations on our dashboard. This dashboard gives you the ability to try different features and compare the recommendations across different models — all before the models are used to recommend content to our listeners. At the beginning stages of experimentation and modeling for this project, we noticed that the same album was recommended as the first item no matter what input features (such as user’s country, followed artists, etc.) were used for testing, meaning this album would have been recommended to everyone as the first recommendation. Without this dashboard as a tool, it would have been more challenging to identify this issue and remediate it before the model went live.</p>



<p>While our offline metrics might indicate poor performance, they don’t tell us anything about what the reason might be, whereas this dashboard can show the quality of our recommendations and is extremely useful in finding issues like this.</p>



<p>Through the use of task-specific custom evaluations and dashboards to show evaluation metrics and recommendations per feature set, we have been able to gain deep insight into how our models are behaving, and make our models a little less of a black box. </p>



<h2>The struggles of automated model retraining and deployment</h2>



<p>Let’s dive into our last topic, which is all about maintaining models in production: retraining and automatic deployment.</p>



<h3>But do we actually need to retrain our models?</h3>



<p>It would be really nice if we could train a model once, deploy it, and then not have to do anything except monitor its online performance. Sadly, we’ve never seen this in reality.</p>



<h4>Sometimes the model’s task requires frequent retraining</h4>



<p>Since we first deployed the Podcast Model in Home, we have always had retraining set up for it — and that’s because it only recommends podcast shows that it has seen in training data. So if we didn’t retrain it, it wouldn’t recommend any newly published shows.  </p>



<h4>The rest of the time, it just becomes a tech debt monster</h4>



<p>But in some cases, retraining isn’t necessarily required to capture the full set of possible candidates. For the Shortcuts Model, we didn’t have retraining set up because it only recommends content that the listener has previously listened to (which is always in the serving features). But while retraining wasn’t needed for the Shortcuts Model to operate, the lack of it became one of the biggest sources of ML tech debt. We did not implement retraining for Shortcuts because it wasn’t needed for launching the feature, but have seen that it would have saved us time and effort in the long run had we invested some time in the short term. </p>



<p>It wasn’t until many months after the launch of the model that we saw issues with the quality of recommendations in Shortcuts due to no retraining — some of the features for this model describe the type of content that a listener has listened to, like whether it’s a personalized playlist or an album, etc., and there was a recent addition of a new type of content that was introduced after the model was last trained. As a result, the model didn’t recommend this piece of content in Shortcuts. While this starts to look like the same scenario as the Podcast Model described above, we also saw issues with migrating to different tools and platforms because the model was trained using older versions of libraries.  </p>



<h3>Implement for the short term while waiting for the long-term solution</h3>



<p>Once upon a time, we only had that singular Podcast Model, which was used to generate batch predictions, not real-time predictions. We had a Scio pipeline that used <a href="https://spotify.github.io/zoltar/" target="_blank" rel="noreferrer noopener">Zoltar</a> to predict podcast recommendations for all listeners, and we stored these predictions in our Bigtable instance that holds all of our content recommendations. This was a great start, but fairly inflexible when it came to when and how often we could make predictions for a given listener — and this is important because the listeners’ features could change if they listen to new content or follow new artists, which could provide better information to the model.  </p>



<h4>Building a recommender service for the short term</h4>



<p>Consequently, we built a new service to serve this model and enable online predictions. We could get fresh recommendations for a listener almost instantly, and we could get these recommendations at useful times, such as when a listener follows a new artist. While this was a great improvement to move from offline predictions to online predictions, and an important step in making a better product, we knew we were only going to be in this state for the short term. Spotify’s online serving platform was on the horizon, but not yet ready; the benefits to building a short-term less-than-optimal solution outweighed the benefits of waiting to serve online models until Spotify’s serving solution was production ready.  </p>



<p>With that said, let’s talk about some challenges we faced in building this recommender service, such as how to refresh the local version of a deployed model. Our solution was to poll our internal storage directory every 10 minutes to check if there was a new revision of the model; if so, the service would pull the model down from where it was stored and start using that model to make predictions. Nevermind that we only retrained weekly or that there would be some state at which some machines would have the new revision of a model and others would have the older revision (although this was not something we worried about in our specific use case).  </p>



<h4>The pain of manually deploying models</h4>



<p>This was really a solution to serving models online, and less of a solution to a better process of serving models. Each time we wanted to deploy a model we had to: 1) copy the model to a specific storage location, 2) manually generate a pointer in our internal storage directory for that location, and 3) add this pointer to our recommender service along with the logic to fetch and transform features for the model. If we were to retrain the model, we would have to repeat each of those steps.  </p>



<p>Obviously, this was a cumbersome process, but because we had this short-term solution, we were able to deploy four models to production and tested many others in A/B tests.</p>



<h3>CI/CD — but make it for model training and deployment</h3>



<p>While this recommender service lived a long life of about 10 months, the next obvious step was to migrate to Spotify’s model serving platform, which enabled us to automate retraining and deployment of retrained models.  </p>



<h4>Automating feature transformations without Tensorflow Transform</h4>



<p>The first step in automating retraining is automating train dataset and test dataset curation, fetching the correct features and performing the necessary feature transformations. While feature transformations are generally handled automatically via TFT in a Kubeflow pipeline, we don’t perform our feature transformations in TFT (and therefore not in our experiment pipeline) because many the transformations we perform on the data are fairly complex and would be unnecessarily difficult to do in Tensorflow.  </p>



<p>But because the serving platform provides feature logging, we enabled logging of <em>already transformed</em> features, to which we then apply the correct labels, and separate into train and test sets. These actions are all performed in scheduled pipelines that run weekly and produce weekly datasets for our models to use.</p>



<h4>Migrating from our short-term solution to a long-term solution</h4>



<p>In order to enable feature logging, we had to migrate to the new online model serving platform from our recommender service using Zoltar. It was a matter of dark loading all prediction traffic to the new deployment and then running a simple rollout to start directing traffic to our new deployment instead of using Zoltar to make predictions in our own service. This was an easy migration and provided the benefits that the online serving platform offers — feature logging, faster predictions / lower latencies, less code managed by our team — and it also supports pushing a new model version (from a Kubeflow pipeline), as opposed to constantly polling for a new model version. </p>



<h4>Continuous retraining and automatic deployment</h4>



<p>Now that our models are all deployed via the Spotify serving platform, it enables us to employ CI/CD. We can schedule our models to be retrained via a Kubeflow pipeline, and as part of the Kubeflow pipeline we can ensure that a “bad” model is not accidentally automatically deployed by specifying that it should: 1) check that the evaluation score is greater than our configured threshold, and 2) automatically push it to our serving infrastructure if it is greater than the threshold. This automates a lot of the processes that we had to perform manually not long ago.  </p>



<p>Enabling CI/CD for retraining and model deployment is hard, but it’s becoming easier with the new tools available and makes the quality and reliability of our models better. And at first glance, you might not think you need retraining for a model because of the task it performs, but without it, your model could make predictions in unpredictable ways and increase your tech debt.   </p>



<h2>Conclusion</h2>



<p>Our ML stack has come a long way in recent years, but it’s not perfect by any means. There are still a number of challenges we are tackling — data versioning, model versioning, moving feature transformations to Tensorflow Transform — and better ways to compare offline metrics across both ML and non-ML solutions. But it has decreased the time it takes for us to iterate, experiment, and deploy quality models.</p>



<p>We have adopted and/or built the components we need to successfully and efficiently manage our data, experiment with different models, and support continuous integration and development throughout the deployment and retraining processes. Our ML stack has enabled us to launch numerous models that serve millions of listeners on Home every day.</p>



<p>If you are interested in joining us and helping improve how we recommend content on Home, we are <a href="https://www.lifeatspotify.com/jobs">hiring</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Annie Edmundson, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)&#xA;</title>
      <link>https://engineering.atspotify.com/2021/11/18/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/</link>
      <description>In Part I of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including: The Podcast Model: Predicts podcasts a listener is likely to listen to in the Shows you might like shelf. The Shortcuts Model: Predicts the listener’s next fa</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/11/18/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/" title="The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png 2098w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-2048x1009.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-120x59.png 120w" sizes="(max-width: 2098px) 100vw, 2098px"/>                    </a>
                        
        </p>

        

        
<p>In <a href="https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/" target="_blank" rel="noreferrer noopener">Part I</a> of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including:</p>



<ul><li><strong>The Podcast Model:</strong> Predicts podcasts a listener is likely to listen to in the <em>Shows you might like</em> shelf. </li><li><strong>The Shortcuts Model: </strong>Predicts the listener’s next familiar listen in the Shortcuts feature. </li><li><strong>The Playlists Model: </strong>Predicts the playlists a new listener is likely to listen to in the <em>Try something else</em> shelf.  </li></ul>



<p>In this part of the series, we’ll highlight how and why we evaluate our models with different tools, and the hurdles to maintaining these models in production. </p>



<h2>Trust but verify your recommendations… with dashboards</h2>



<p>So let’s talk about what we do with that data — specifically, how we run experiments, and maybe more importantly, how we evaluate our models’ performance.</p>



<h3>Making experimentation simpler</h3>



<h4>How it started: experimenting on a siloed platform</h4>



<p>Not that long ago, after transforming our training data, we would run experiments on a siloed platform specifically geared towards model experimentation, and that was only really used within our team — we did this for both the initial Podcast Model as well as for the Shortcuts Model. This platform could easily launch hundreds of experiments by using a configuration file to specify hyperparameters (it also supported a grid search on specified hyperparameters). And since everything that was submitted to run an experiment was a script, it supported custom evaluation metrics — something that has always been important in our team. While it provided these necessary features, it wasn’t scalable, wasn’t maintained, and had an incomplete UI. Sometimes the compute instances would lose connection with the API (via a periodic ping) and would end up being ghost workers — still running, but not connected to anything.  </p>



<h4>How it’s going: integration with Spotify ML ecosystem</h4>



<p><a href="https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/" target="_blank" rel="noreferrer noopener">Spotify’s managed Kubeflow clusters</a> provide a more scalable approach, modular components, and are compatible with other parts of the Spotify ML infrastructure, so it was an obvious choice to move our experimentation to this platform. Training our models using Kubeflow pipelines is easy and efficient, but running the evaluation we needed and tracking those results were our biggest pain points for two reasons: </p>



<ol><li>As Spotify’s SDK for Kubeflow uses Tensorflow Model Analysis (TFMA), comparing the performance of a non-ML heuristic algorithm to that of a trained model is challenging to set up and requires extra infrastructure. </li><li>We often have custom evaluation metrics that are specific to the model’s task, but they are infinitely more difficult to implement in TFMA than in vanilla Python.</li></ol>



<h3>Evaluating models against simpler (non-ML) solutions</h3>



<p>As I alluded to in an earlier paragraph, we don’t typically start solutions to a problem with an ML solution. We first identify a heuristic, or rule-based, solution, and the most appropriate way to evaluate it.  </p>



<h4>The first step — having a baseline for comparison</h4>



<p>We are often tasked with creating better recommendations for content X, but what are “better recommendations,” and what are they better than? Having a baseline helps answer these questions, giving us something to compare our models against. And a good baseline — usually a heuristic/rule-based solution — is a quick, efficient, but maybe not the most optimal, solution.</p>



<p>Take the Shortcuts Model as an example. We created an initial heuristic that recommended, simply, the most frequently played items from a listener’s short-term listening history. We improved the heuristic over many iterations, then compared it to the performance of the models we trained. Being able to compare these heuristics to the models gave us confidence to say that having a model was an improvement over the heuristics and was worth the extra effort of maintaining, deploying, and monitoring these models.</p>



<h4>Comparing model performance to baseline performance is difficult</h4>



<p>After establishing our baseline and training our model(s), the difficulty lies in how we compare them evenly. In a perfect world we would run infinite A/B tests with hundreds of test cells to compare the performance of all our solutions in the real world, on real listeners. Since it’s not a perfect world, we need reliable offline metrics that act as a proxy for the online metrics we can’t get in those A/B tests.  </p>



<p>When evaluating our recommendations models, we typically use normalized discounted cumulative gain (NDCG@k) as our metric, which can be implemented using Spotify’s Python SDK for Kubeflow pipelines. The question then becomes: how do we do the same for our heuristic? As we’ve mentioned before, transformation logic consistency is paramount, and so is evaluation logic — ideally, we’d have the same evaluation logic and the same evaluation test set of data. Unfortunately, our heuristics are generally written in a Java service and are tested with unit tests (not for performance).</p>



<p>For fairly simple heuristics, we found a way to “train a model” so that its output is the heuristic rule’s output. This allowed us to use the same evaluation and evaluation test set as the models we were comparing against. We took this same approach when coming up with a solution to recommendations in the <em>Try something else</em> shelf for new users on Home. We computed a popularity heuristic based on a listener’s demographics in Tensorflow Transform (TFT) and used the model as a lookup utility (with a fake loss).  </p>



<p>We can’t always fit our problem into such a simple heuristic, as was the case for Shortcuts. The logic used in most of the Shortcuts heuristics was too complex to write in Tensorflow, so we implemented a completely separate offline evaluation pipeline that would gather recommendations made by models and heuristics, and apply custom evaluation functions for comparison.  </p>



<h3>Adding freedom and flexibility to our evaluation tools</h3>



<p>As mentioned earlier, there’s a second pain point we run into often: using custom evaluation metrics in TFMA.</p>



<h4>TFMA is sometimes too rigid</h4>



<p>Spotify’s SDK for Kubeflow only supports evaluations using TFMA, which provides fairly basic metrics out of the box — think: precision, recall, accuracy. The most common metric we typically use is NDCG@k — TFMA provides NDCG, but not NDCG@k. Implementing metrics in TFMA is notoriously difficult; it takes ~120 lines of code to implement NDCG@k in TFMA, but only a single line of code using scikit-learn in Python.</p>



<p>Most recently, we were experimenting with a model that predicts the next playlist that a new user will listen to, and as we have very little information about new users, we wanted to ensure that the model was not just predicting the most popular content. To do so, we were going to evaluate the model with a diversity metric that measured the difference between specific characteristics of items in each playlist. This was nearly impossible to implement in TFMA, so our team contributed to the Python SDK for Kubeflow to support any custom Python evaluation. We have been using this and running our experiments via Kubeflow pipelines since October 2020. </p>



<h4>Compare and track experiment results</h4>



<p>In the pre-Kubeflow world, our experimentation platform allowed for a way to track and compare models — now, we are using Spotify’s internal UI for machine learning, as it easily integrates with our Kubeflow runs. We can view and compare the evaluation scores of our experiments — both NDCG and custom metrics — in the UI. We’ve been using this for a number of our models, and it allows us to track our model deployments as well.</p>



<h3>Looking at more than just the numbers for evaluating recommendations</h3>



<p>I’ve mostly mentioned what metrics we use and why they are important, but there is another incredibly useful way we evaluate our models — sometimes more useful than what a metric can reveal.</p>



<h4>We build custom dashboards to manually evaluate the recs</h4>



<p>Based on past issues, we know that evaluation metrics don’t show the whole picture of how well a model is recommending content. Sometimes, the best way to evaluate a model is by seeing what content it recommends given a specific set of features about a listener. And for this reason, our team built a dashboard that does exactly that. It loads models simply by supplying the storage location of the model, and supports comparison of multiple models given a set of features. We often test and evaluate the recommendations that a new model will provide before deploying it to production by making predictions with different sets of feature values; this gives us an intuition behind what content will be recommended to different users that have these feature values. This has helped us find glaring issues; for example, when developing and testing a new model, we found that it would recommend the same popular playlist to listeners in all European countries. Having this knowledge allowed us to fix and improve the model before deploying it to production.  </p>



<p>Most recently, we have been working on a new model to recommend albums a listener might like based on their locality and what they like to listen to. We have been running experiments comparing evaluation metric values, but we have also been looking at the recommendations on our dashboard. This dashboard gives you the ability to try different features and compare the recommendations across different models — all before the models are used to recommend content to our listeners. At the beginning stages of experimentation and modeling for this project, we noticed that the same album was recommended as the first item no matter what input features (such as user’s country, followed artists, etc.) were used for testing, meaning this album would have been recommended to everyone as the first recommendation. Without this dashboard as a tool, it would have been more challenging to identify this issue and remediate it before the model went live.</p>



<p>While our offline metrics might indicate poor performance, they don’t tell us anything about what the reason might be, whereas this dashboard can show the quality of our recommendations and is extremely useful in finding issues like this.</p>



<p>Through the use of task-specific custom evaluations and dashboards to show evaluation metrics and recommendations per feature set, we have been able to gain deep insight into how our models are behaving, and make our models a little less of a black box. </p>



<h2>The struggles of automated model retraining and deployment</h2>



<p>Let’s dive into our last topic, which is all about maintaining models in production: retraining and automatic deployment.</p>



<h3>But do we actually need to retrain our models?</h3>



<p>It would be really nice if we could train a model once, deploy it, and then not have to do anything except monitor its online performance. Sadly, we’ve never seen this in reality.</p>



<h4>Sometimes the model’s task requires frequent retraining</h4>



<p>Since we first deployed the Podcast Model in Home, we have always had retraining set up for it — and that’s because it only recommends podcast shows that it has seen in training data. So if we didn’t retrain it, it wouldn’t recommend any newly published shows.  </p>



<h4>The rest of the time, it just becomes a tech debt monster</h4>



<p>But in some cases, retraining isn’t necessarily required to capture the full set of possible candidates. For the Shortcuts Model, we didn’t have retraining set up because it only recommends content that the listener has previously listened to (which is always in the serving features). But while retraining wasn’t needed for the Shortcuts Model to operate, the lack of it became one of the biggest sources of ML tech debt. We did not implement retraining for Shortcuts because it wasn’t needed for launching the feature, but have seen that it would have saved us time and effort in the long run had we invested some time in the short term. </p>



<p>It wasn’t until many months after the launch of the model that we saw issues with the quality of recommendations in Shortcuts due to no retraining — some of the features for this model describe the type of content that a listener has listened to, like whether it’s a personalized playlist or an album, etc., and there was a recent addition of a new type of content that was introduced after the model was last trained. As a result, the model didn’t recommend this piece of content in Shortcuts. While this starts to look like the same scenario as the Podcast Model described above, we also saw issues with migrating to different tools and platforms because the model was trained using older versions of libraries.  </p>



<h3>Implement for the short term while waiting for the long-term solution</h3>



<p>Once upon a time, we only had that singular Podcast Model, which was used to generate batch predictions, not real-time predictions. We had a Scio pipeline that used <a href="https://spotify.github.io/zoltar/" target="_blank" rel="noreferrer noopener">Zoltar</a> to predict podcast recommendations for all listeners, and we stored these predictions in our Bigtable instance that holds all of our content recommendations. This was a great start, but fairly inflexible when it came to when and how often we could make predictions for a given listener — and this is important because the listeners’ features could change if they listen to new content or follow new artists, which could provide better information to the model.  </p>



<h4>Building a recommender service for the short term</h4>



<p>Consequently, we built a new service to serve this model and enable online predictions. We could get fresh recommendations for a listener almost instantly, and we could get these recommendations at useful times, such as when a listener follows a new artist. While this was a great improvement to move from offline predictions to online predictions, and an important step in making a better product, we knew we were only going to be in this state for the short term. Spotify’s online serving platform was on the horizon, but not yet ready; the benefits to building a short-term less-than-optimal solution outweighed the benefits of waiting to serve online models until Spotify’s serving solution was production ready.  </p>



<p>With that said, let’s talk about some challenges we faced in building this recommender service, such as how to refresh the local version of a deployed model. Our solution was to poll our internal storage directory every 10 minutes to check if there was a new revision of the model; if so, the service would pull the model down from where it was stored and start using that model to make predictions. Nevermind that we only retrained weekly or that there would be some state at which some machines would have the new revision of a model and others would have the older revision (although this was not something we worried about in our specific use case).  </p>



<h4>The pain of manually deploying models</h4>



<p>This was really a solution to serving models online, and less of a solution to a better process of serving models. Each time we wanted to deploy a model we had to: 1) copy the model to a specific storage location, 2) manually generate a pointer in our internal storage directory for that location, and 3) add this pointer to our recommender service along with the logic to fetch and transform features for the model. If we were to retrain the model, we would have to repeat each of those steps.  </p>



<p>Obviously, this was a cumbersome process, but because we had this short-term solution, we were able to deploy four models to production and tested many others in A/B tests.</p>



<h3>CI/CD — but make it for model training and deployment</h3>



<p>While this recommender service lived a long life of about 10 months, the next obvious step was to migrate to Spotify’s model serving platform, which enabled us to automate retraining and deployment of retrained models.  </p>



<h4>Automating feature transformations without Tensorflow Transform</h4>



<p>The first step in automating retraining is automating train dataset and test dataset curation, fetching the correct features and performing the necessary feature transformations. While feature transformations are generally handled automatically via TFT in a Kubeflow pipeline, we don’t perform our feature transformations in TFT (and therefore not in our experiment pipeline) because many the transformations we perform on the data are fairly complex and would be unnecessarily difficult to do in Tensorflow.  </p>



<p>But because the serving platform provides feature logging, we enabled logging of <em>already transformed</em> features, to which we then apply the correct labels, and separate into train and test sets. These actions are all performed in scheduled pipelines that run weekly and produce weekly datasets for our models to use.</p>



<h4>Migrating from our short-term solution to a long-term solution</h4>



<p>In order to enable feature logging, we had to migrate to the new online model serving platform from our recommender service using Zoltar. It was a matter of dark loading all prediction traffic to the new deployment and then running a simple rollout to start directing traffic to our new deployment instead of using Zoltar to make predictions in our own service. This was an easy migration and provided the benefits that the online serving platform offers — feature logging, faster predictions / lower latencies, less code managed by our team — and it also supports pushing a new model version (from a Kubeflow pipeline), as opposed to constantly polling for a new model version. </p>



<h4>Continuous retraining and automatic deployment</h4>



<p>Now that our models are all deployed via the Spotify serving platform, it enables us to employ CI/CD. We can schedule our models to be retrained via a Kubeflow pipeline, and as part of the Kubeflow pipeline we can ensure that a “bad” model is not accidentally automatically deployed by specifying that it should: 1) check that the evaluation score is greater than our configured threshold, and 2) automatically push it to our serving infrastructure if it is greater than the threshold. This automates a lot of the processes that we had to perform manually not long ago.  </p>



<p>Enabling CI/CD for retraining and model deployment is hard, but it’s becoming easier with the new tools available and makes the quality and reliability of our models better. And at first glance, you might not think you need retraining for a model because of the task it performs, but without it, your model could make predictions in unpredictable ways and increase your tech debt.   </p>



<h2>Conclusion</h2>



<p>Our ML stack has come a long way in recent years, but it’s not perfect by any means. There are still a number of challenges we are tackling — data versioning, model versioning, moving feature transformations to Tensorflow Transform — and better ways to compare offline metrics across both ML and non-ML solutions. But it has decreased the time it takes for us to iterate, experiment, and deploy quality models.</p>



<p>We have adopted and/or built the components we need to successfully and efficiently manage our data, experiment with different models, and support continuous integration and development throughout the deployment and retraining processes. Our ML stack has enabled us to launch numerous models that serve millions of listeners on Home every day.</p>



<p>If you are interested in joining us and helping improve how we recommend content on Home, we are <a href="https://www.lifeatspotify.com/jobs">hiring</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a></p>

        

            </div></div>]]></content:encoded>
      <author>Published by Annie Edmundson, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)&#xA;</title>
      <link>https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/</link>
      <description>In Part I of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including: The Podcast Model: Predicts podcasts a listener is likely to listen to in the Shows you might like shelf. The Shortcuts Model: Predicts the listener’s next fa</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/" title="The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png 2098w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-2048x1009.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-120x59.png 120w" sizes="(max-width: 2098px) 100vw, 2098px"/>                    </a>
                        
        </p>

        

        
<p>In <a href="https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/" target="_blank" rel="noreferrer noopener">Part I</a> of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including:</p>



<ul><li><strong>The Podcast Model:</strong> Predicts podcasts a listener is likely to listen to in the <em>Shows you might like</em> shelf. </li><li><strong>The Shortcuts Model: </strong>Predicts the listener’s next familiar listen in the Shortcuts feature. </li><li><strong>The Playlists Model: </strong>Predicts the playlists a new listener is likely to listen to in the <em>Try something else</em> shelf.  </li></ul>



<p>In this part of the series, we’ll highlight how and why we evaluate our models with different tools, and the hurdles to maintaining these models in production. </p>



<h2>Trust but verify your recommendations… with dashboards</h2>



<p>So let’s talk about what we do with that data — specifically, how we run experiments, and maybe more importantly, how we evaluate our models’ performance.</p>



<h3>Making experimentation simpler</h3>



<h4>How it started: experimenting on a siloed platform</h4>



<p>Not that long ago, after transforming our training data, we would run experiments on a siloed platform specifically geared towards model experimentation, and that was only really used within our team — we did this for both the initial Podcast Model as well as for the Shortcuts Model. This platform could easily launch hundreds of experiments by using a configuration file to specify hyperparameters (it also supported a grid search on specified hyperparameters). And since everything that was submitted to run an experiment was a script, it supported custom evaluation metrics — something that has always been important in our team. While it provided these necessary features, it wasn’t scalable, wasn’t maintained, and had an incomplete UI. Sometimes the compute instances would lose connection with the API (via a periodic ping) and would end up being ghost workers — still running, but not connected to anything.  </p>



<h4>How it’s going: integration with Spotify ML ecosystem</h4>



<p><a href="https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/" target="_blank" rel="noreferrer noopener">Spotify’s managed Kubeflow clusters</a> provide a more scalable approach, modular components, and are compatible with other parts of the Spotify ML infrastructure, so it was an obvious choice to move our experimentation to this platform. Training our models using Kubeflow pipelines is easy and efficient, but running the evaluation we needed and tracking those results were our biggest pain points for two reasons: </p>



<ol><li>As Spotify’s SDK for Kubeflow uses Tensorflow Model Analysis (TFMA), comparing the performance of a non-ML heuristic algorithm to that of a trained model is challenging to set up and requires extra infrastructure. </li><li>We often have custom evaluation metrics that are specific to the model’s task, but they are infinitely more difficult to implement in TFMA than in vanilla Python.</li></ol>



<h3>Evaluating models against simpler (non-ML) solutions</h3>



<p>As I alluded to in an earlier paragraph, we don’t typically start solutions to a problem with an ML solution. We first identify a heuristic, or rule-based, solution, and the most appropriate way to evaluate it.  </p>



<h4>The first step — having a baseline for comparison</h4>



<p>We are often tasked with creating better recommendations for content X, but what are “better recommendations,” and what are they better than? Having a baseline helps answer these questions, giving us something to compare our models against. And a good baseline — usually a heuristic/rule-based solution — is a quick, efficient, but maybe not the most optimal, solution.</p>



<p>Take the Shortcuts Model as an example. We created an initial heuristic that recommended, simply, the most frequently played items from a listener’s short-term listening history. We improved the heuristic over many iterations, then compared it to the performance of the models we trained. Being able to compare these heuristics to the models gave us confidence to say that having a model was an improvement over the heuristics and was worth the extra effort of maintaining, deploying, and monitoring these models.</p>



<h4>Comparing model performance to baseline performance is difficult</h4>



<p>After establishing our baseline and training our model(s), the difficulty lies in how we compare them evenly. In a perfect world we would run infinite A/B tests with hundreds of test cells to compare the performance of all our solutions in the real world, on real listeners. Since it’s not a perfect world, we need reliable offline metrics that act as a proxy for the online metrics we can’t get in those A/B tests.  </p>



<p>When evaluating our recommendations models, we typically use normalized discounted cumulative gain (NDCG@k) as our metric, which can be implemented using Spotify’s Python SDK for Kubeflow pipelines. The question then becomes: how do we do the same for our heuristic? As we’ve mentioned before, transformation logic consistency is paramount, and so is evaluation logic — ideally, we’d have the same evaluation logic and the same evaluation test set of data. Unfortunately, our heuristics are generally written in a Java service and are tested with unit tests (not for performance).</p>



<p>For fairly simple heuristics, we found a way to “train a model” so that its output is the heuristic rule’s output. This allowed us to use the same evaluation and evaluation test set as the models we were comparing against. We took this same approach when coming up with a solution to recommendations in the <em>Try something else</em> shelf for new users on Home. We computed a popularity heuristic based on a listener’s demographics in Tensorflow Transform (TFT) and used the model as a lookup utility (with a fake loss).  </p>



<p>We can’t always fit our problem into such a simple heuristic, as was the case for Shortcuts. The logic used in most of the Shortcuts heuristics was too complex to write in Tensorflow, so we implemented a completely separate offline evaluation pipeline that would gather recommendations made by models and heuristics, and apply custom evaluation functions for comparison.  </p>



<h3>Adding freedom and flexibility to our evaluation tools</h3>



<p>As mentioned earlier, there’s a second pain point we run into often: using custom evaluation metrics in TFMA.</p>



<h4>TFMA is sometimes too rigid</h4>



<p>Spotify’s SDK for Kubeflow only supports evaluations using TFMA, which provides fairly basic metrics out of the box — think: precision, recall, accuracy. The most common metric we typically use is NDCG@k — TFMA provides NDCG, but not NDCG@k. Implementing metrics in TFMA is notoriously difficult; it takes ~120 lines of code to implement NDCG@k in TFMA, but only a single line of code using scikit-learn in Python.</p>



<p>Most recently, we were experimenting with a model that predicts the next playlist that a new user will listen to, and as we have very little information about new users, we wanted to ensure that the model was not just predicting the most popular content. To do so, we were going to evaluate the model with a diversity metric that measured the difference between specific characteristics of items in each playlist. This was nearly impossible to implement in TFMA, so our team contributed to the Python SDK for Kubeflow to support any custom Python evaluation. We have been using this and running our experiments via Kubeflow pipelines since October 2020. </p>



<h4>Compare and track experiment results</h4>



<p>In the pre-Kubeflow world, our experimentation platform allowed for a way to track and compare models — now, we are using Spotify’s internal UI for machine learning, as it easily integrates with our Kubeflow runs. We can view and compare the evaluation scores of our experiments — both NDCG and custom metrics — in the UI. We’ve been using this for a number of our models, and it allows us to track our model deployments as well.</p>



<h3>Looking at more than just the numbers for evaluating recommendations</h3>



<p>I’ve mostly mentioned what metrics we use and why they are important, but there is another incredibly useful way we evaluate our models — sometimes more useful than what a metric can reveal.</p>



<h4>We build custom dashboards to manually evaluate the recs</h4>



<p>Based on past issues, we know that evaluation metrics don’t show the whole picture of how well a model is recommending content. Sometimes, the best way to evaluate a model is by seeing what content it recommends given a specific set of features about a listener. And for this reason, our team built a dashboard that does exactly that. It loads models simply by supplying the storage location of the model, and supports comparison of multiple models given a set of features. We often test and evaluate the recommendations that a new model will provide before deploying it to production by making predictions with different sets of feature values; this gives us an intuition behind what content will be recommended to different users that have these feature values. This has helped us find glaring issues; for example, when developing and testing a new model, we found that it would recommend the same popular playlist to listeners in all European countries. Having this knowledge allowed us to fix and improve the model before deploying it to production.  </p>



<p>Most recently, we have been working on a new model to recommend albums a listener might like based on their locality and what they like to listen to. We have been running experiments comparing evaluation metric values, but we have also been looking at the recommendations on our dashboard. This dashboard gives you the ability to try different features and compare the recommendations across different models — all before the models are used to recommend content to our listeners. At the beginning stages of experimentation and modeling for this project, we noticed that the same album was recommended as the first item no matter what input features (such as user’s country, followed artists, etc.) were used for testing, meaning this album would have been recommended to everyone as the first recommendation. Without this dashboard as a tool, it would have been more challenging to identify this issue and remediate it before the model went live.</p>



<p>While our offline metrics might indicate poor performance, they don’t tell us anything about what the reason might be, whereas this dashboard can show the quality of our recommendations and is extremely useful in finding issues like this.</p>



<p>Through the use of task-specific custom evaluations and dashboards to show evaluation metrics and recommendations per feature set, we have been able to gain deep insight into how our models are behaving, and make our models a little less of a black box. </p>



<h2>The struggles of automated model retraining and deployment</h2>



<p>Let’s dive into our last topic, which is all about maintaining models in production: retraining and automatic deployment.</p>



<h3>But do we actually need to retrain our models?</h3>



<p>It would be really nice if we could train a model once, deploy it, and then not have to do anything except monitor its online performance. Sadly, we’ve never seen this in reality.</p>



<h4>Sometimes the model’s task requires frequent retraining</h4>



<p>Since we first deployed the Podcast Model in Home, we have always had retraining set up for it — and that’s because it only recommends podcast shows that it has seen in training data. So if we didn’t retrain it, it wouldn’t recommend any newly published shows.  </p>



<h4>The rest of the time, it just becomes a tech debt monster</h4>



<p>But in some cases, retraining isn’t necessarily required to capture the full set of possible candidates. For the Shortcuts Model, we didn’t have retraining set up because it only recommends content that the listener has previously listened to (which is always in the serving features). But while retraining wasn’t needed for the Shortcuts Model to operate, the lack of it became one of the biggest sources of ML tech debt. We did not implement retraining for Shortcuts because it wasn’t needed for launching the feature, but have seen that it would have saved us time and effort in the long run had we invested some time in the short term. </p>



<p>It wasn’t until many months after the launch of the model that we saw issues with the quality of recommendations in Shortcuts due to no retraining — some of the features for this model describe the type of content that a listener has listened to, like whether it’s a personalized playlist or an album, etc., and there was a recent addition of a new type of content that was introduced after the model was last trained. As a result, the model didn’t recommend this piece of content in Shortcuts. While this starts to look like the same scenario as the Podcast Model described above, we also saw issues with migrating to different tools and platforms because the model was trained using older versions of libraries.  </p>



<h3>Implement for the short term while waiting for the long-term solution</h3>



<p>Once upon a time, we only had that singular Podcast Model, which was used to generate batch predictions, not real-time predictions. We had a Scio pipeline that used <a href="https://spotify.github.io/zoltar/" target="_blank" rel="noreferrer noopener">Zoltar</a> to predict podcast recommendations for all listeners, and we stored these predictions in our Bigtable instance that holds all of our content recommendations. This was a great start, but fairly inflexible when it came to when and how often we could make predictions for a given listener — and this is important because the listeners’ features could change if they listen to new content or follow new artists, which could provide better information to the model.  </p>



<h4>Building a recommender service for the short term</h4>



<p>Consequently, we built a new service to serve this model and enable online predictions. We could get fresh recommendations for a listener almost instantly, and we could get these recommendations at useful times, such as when a listener follows a new artist. While this was a great improvement to move from offline predictions to online predictions, and an important step in making a better product, we knew we were only going to be in this state for the short term. Spotify’s online serving platform was on the horizon, but not yet ready; the benefits to building a short-term less-than-optimal solution outweighed the benefits of waiting to serve online models until Spotify’s serving solution was production ready.  </p>



<p>With that said, let’s talk about some challenges we faced in building this recommender service, such as how to refresh the local version of a deployed model. Our solution was to poll our internal storage directory every 10 minutes to check if there was a new revision of the model; if so, the service would pull the model down from where it was stored and start using that model to make predictions. Nevermind that we only retrained weekly or that there would be some state at which some machines would have the new revision of a model and others would have the older revision (although this was not something we worried about in our specific use case).  </p>



<h4>The pain of manually deploying models</h4>



<p>This was really a solution to serving models online, and less of a solution to a better process of serving models. Each time we wanted to deploy a model we had to: 1) copy the model to a specific storage location, 2) manually generate a pointer in our internal storage directory for that location, and 3) add this pointer to our recommender service along with the logic to fetch and transform features for the model. If we were to retrain the model, we would have to repeat each of those steps.  </p>



<p>Obviously, this was a cumbersome process, but because we had this short-term solution, we were able to deploy four models to production and tested many others in A/B tests.</p>



<h3>CI/CD — but make it for model training and deployment</h3>



<p>While this recommender service lived a long life of about 10 months, the next obvious step was to migrate to Spotify’s model serving platform, which enabled us to automate retraining and deployment of retrained models.  </p>



<h4>Automating feature transformations without Tensorflow Transform</h4>



<p>The first step in automating retraining is automating train dataset and test dataset curation, fetching the correct features and performing the necessary feature transformations. While feature transformations are generally handled automatically via TFT in a Kubeflow pipeline, we don’t perform our feature transformations in TFT (and therefore not in our experiment pipeline) because many the transformations we perform on the data are fairly complex and would be unnecessarily difficult to do in Tensorflow.  </p>



<p>But because the serving platform provides feature logging, we enabled logging of <em>already transformed</em> features, to which we then apply the correct labels, and separate into train and test sets. These actions are all performed in scheduled pipelines that run weekly and produce weekly datasets for our models to use.</p>



<h4>Migrating from our short-term solution to a long-term solution</h4>



<p>In order to enable feature logging, we had to migrate to the new online model serving platform from our recommender service using Zoltar. It was a matter of dark loading all prediction traffic to the new deployment and then running a simple rollout to start directing traffic to our new deployment instead of using Zoltar to make predictions in our own service. This was an easy migration and provided the benefits that the online serving platform offers — feature logging, faster predictions / lower latencies, less code managed by our team — and it also supports pushing a new model version (from a Kubeflow pipeline), as opposed to constantly polling for a new model version. </p>



<h4>Continuous retraining and automatic deployment</h4>



<p>Now that our models are all deployed via the Spotify serving platform, it enables us to employ CI/CD. We can schedule our models to be retrained via a Kubeflow pipeline, and as part of the Kubeflow pipeline we can ensure that a “bad” model is not accidentally automatically deployed by specifying that it should: 1) check that the evaluation score is greater than our configured threshold, and 2) automatically push it to our serving infrastructure if it is greater than the threshold. This automates a lot of the processes that we had to perform manually not long ago.  </p>



<p>Enabling CI/CD for retraining and model deployment is hard, but it’s becoming easier with the new tools available and makes the quality and reliability of our models better. And at first glance, you might not think you need retraining for a model because of the task it performs, but without it, your model could make predictions in unpredictable ways and increase your tech debt.   </p>



<h2>Conclusion</h2>



<p>Our ML stack has come a long way in recent years, but it’s not perfect by any means. There are still a number of challenges we are tackling — data versioning, model versioning, moving feature transformations to Tensorflow Transform — and better ways to compare offline metrics across both ML and non-ML solutions. But it has decreased the time it takes for us to iterate, experiment, and deploy quality models.</p>



<p>We have adopted and/or built the components we need to successfully and efficiently manage our data, experiment with different models, and support continuous integration and development throughout the deployment and retraining processes. Our ML stack has enabled us to launch numerous models that serve millions of listeners on Home every day.</p>



<p>If you are interested in joining us and helping improve how we recommend content on Home, we are <a href="https://www.lifeatspotify.com/jobs">hiring</a>!</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Annie Edmundson, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 18 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%&#xA;</title>
      <link>https://engineering.atspotify.com/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/</link>
      <description>At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain. We are excited to be open sourcing X</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/" title="Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain.</p>



<p>We are excited to be open sourcing <a href="https://github.com/spotify/XCRemoteCache" target="_blank" rel="noreferrer noopener">XCRemoteCache</a>, the library we created to mitigate long local builds. As the name suggests, this library is a remote caching implementation for iOS projects with an aim to reuse Xcode target artifacts generated on Continuous Integration (CI) machines. It supports Objective-C, Swift, and ObjC+Swift targets and can be easily integrated with existing Xcode projects, including ones managed by CocoaPods or Carthage.</p>



<p>Best of all, XCRemoteCache resulted in a <strong>70% decrease in clean build times</strong> (we classify a build as <em>clean</em> when at least 50% of all targets compile at least one file).</p>



<h2><strong>Background</strong></h2>



<p>Using our Xcode build metrics (for more details on how this works, take a look at our open source <a href="https://xcmetrics.io/" target="_blank" rel="noreferrer noopener">XCMetrics project</a>), we found out that it often takes our developers more than 10 minutes to build the main Spotify iOS application. Even though the number of these builds is relatively small (less than 3% of all builds), they take more than 50% of global building times (Figure 1). </p>



<p>After some investigation, it was revealed that long-lasting builds usually happen after rebasing or merging remote branches. Implementing a remote cache solution was the perfect fit.</p>



<figure><img loading="lazy" width="700" height="354" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-768x389.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times.png 1205w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: Distribution of local machines’ build times and their total build times.</figcaption></figure>



<h2><strong>Remote cache principle</strong></h2>



<p>A remote cache is a popular technique to speed up builds of big applications by applying the “compile once, use everywhere” approach. As long as all input files and compilation parameters are the same, instead of building a target locally, one can download artifacts that were built and shared from some other machine. A key success factor for remote caching is finding an optimal caching level. Caching units that are too granular, where every single piece of the compilation step is cacheable, may lead to extensive network traffic overhead, which can offset CPU savings. On the other hand, putting the entire codebase into a single cacheable unit may significantly degrade the caching hit rate; every single local change invalidates remotely available cache artifacts, triggering a full build, locally.</p>



<p>The main Spotify iOS application is highly modularized and contains more than 400 independent modules configured as separate Xcode targets. Applying target-level caching was natural, and as we found out later, the right decision.</p>



<h2><strong>Designing the remote cache solution </strong></h2>



<p>In the design phase, our aim was to come up with a solution generic enough that it could be applied to a broad range of iOS applications with minimal or no project changes. That was an ambitious goal given how the Xcode build system works. Before going straight to the applied solution, let’s consider how an Xcode build actually works. </p>



<h2><strong>How does XCRemoteCache work?</strong></h2>



<p>In general, all caching mechanisms use a fingerprint of input files to recognize if build products can be reused. However, finding a precise set of these files is a nontrivial task. The Xcode build system is very liberal when it comes to dependency attribution. It tries to optimistically find a definition of the dependency (header files, <code>.swiftmodule</code>, etc.) in all available search paths — provided either by the developer in the Xcode project settings or in the current build product directory. As a result, developers don’t have to explicitly specify all dependencies a target uses, but just have to make sure that those dependencies will be placed in a correct location <em>before</em> Xcode actually needs them. The fact that compilers are able to implicitly find required dependencies, hinders the simple fingerprint generation by hashing all available files in header and framework search paths — a list of files to consider in the fingerprinting would often be too broad.</p>



<p>On the other hand, we observed that Xcode works quite well for local incremental builds and executes only a narrow subset of steps that were affected since the last build. In other words, the Xcode build system knows which files are the actual input files for the compilation, but that list is generated as compiler’s output (.d files) and is not available ahead of a compilation.</p>



<p>XCRemoteCache applies a unique approach to automatically identify all input files of the compilation based on Git history and dependency lists provided as a compilation output. The generation side, called <em>producer mode</em>, along with the compilation product, also uploads a meta file. That file contains a list of all compilation files the compiler used and the full SHA-1 (Secure Hash Algorithm 1) commit identifier it was built against. The producer mode should run on CI for each primary branch (like <code>master</code> or <code>develop</code>) commit, as a part of the post-merge phase.</p>



<p>On the consumer side (aka <em>consumer mode</em>), XCRemoteCache finds the most recent history commit for which the remote server contains build artifacts and builds a fingerprint based on input files provided in the meta file. Let’s imagine two developers, <code>A</code> and <code>B</code>, working on their local branches <code>featureA </code>and <code>featureB</code>, branching out from <code>master </code>on <code>Commit1</code> and <code>Commit3</code>, respectively (Figure 2). A CI job that produces and uploads cache artifacts to the central server finished its work only for commits 1, 2, and 4. For some reason, <code>Commit3</code> artifacts are not ready — either the build is in progress or it has failed. Developer A’s machine will reuse the artifacts generated for <code>Commit1</code>, while Developer B’s takes them from <code>Commit2</code> — it tried with <code>Commit3</code>, but they are not ready yet.</p>



<figure><img loading="lazy" width="700" height="440" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-250x157.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-768x483.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts.png 1205w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: Finding a commit with artifacts to reuse.</figcaption></figure>



<div><p>With that procedure, XCRemoteCache gets a strict list of input files almost for free.</p><p>Assuming we have an app split into several independent targets and local branches that don’t divert much from a primary branch, the caching hit rate will be high, minus only these targets that contain changes comparing the commit of which remote artifacts are used. </p></div>



<h3>Build artifacts portability</h3>



<p>Another problem to consider is “build artifacts portability” between multiple machines. Several types of compilation output files include absolute paths, so for full compatibility, some kind of normalization is required. For iOS projects, such a step is required for <code>.swiftmodule</code> files and debug symbols. </p>



<p>Projects cloned to <code>/dir1</code> and <code>/dir2</code> generate <code>.swiftmodule </code>files that do not match on a byte level. Swiftmodule represents an inter-module API (Swift counterparts to .h) that can be included in the list of fingerprinting files. To overcome falsy cache misses if two machines don’t have the same absolute source root paths, XCRemoteCache carries an extra fingerprint file (called a fingerprint override) next to the <code>.swiftmodule </code>that includes a fingerprint of all the files used in the compilation step. A fingerprint override is path agnostic, so contrary to the <code>.swiftmodule</code> file, it can be used as a byte-level stable fingerprint of a Swift target. </p>



<p>Debug symbols, other path-sensitive files, are appended to the binary package to associate the machine code with the corresponding source location when debugging. The XCRemoteCache leverages support for LLDB runtime path rewrites using <code>settings set target.source-map</code>. Both producer and consumer pass <code>debug-prefix-map</code> parameters to the Swift and Clang compilers to align the source root of all debug symbols making the LLDB source mapping a simple, single-line command.</p>



<h2><strong>The Spotify story</strong></h2>



<p>At Spotify, we have fast, well-optimized CI jobs that no longer slow down our development feedback loop (please head to <a href="https://engineering.atspotify.com/2020/05/01/how-we-gave-superpowers-to-our-macos-ci/" target="_blank" rel="noreferrer noopener">How We Gave Superpowers to Our macOS CI</a> to read more about that), so we focused on applying XCRemoteCache on local machines. Keep in mind that the tool is able to work on CI machines and accelerate PR jobs, too.</p>



<p>In controlled conditions, XCRemoteCache was able to cut the very first iOS Spotify application build time by 85%. This was a great achievement, but in practice, developers introduce changes locally, and some targets have to be compiled locally. Our goal was to evaluate real-world scenarios to understand their true impact. To estimate that, we rolled out the remote cache to 50% of our developers for a week and compared all build metrics. </p>



<p>Results exceeded our expectations — we observed a huge improvement of the local build times: <strong>median clean build and incremental build times decreased by 70% and 6%, respectively. </strong>We classify builds as <em>clean</em> when at least 50% of all targets compile at least one file.  Other builds that compile at least one file are <em>incremental</em>.</p>



<p>So, we enabled XCRemoteCache into our main application more than a year ago, and since then, it has worked flawlessly. We get very positive feedback from our developers and, as a side effect, we’ve seen the portion of clean builds almost double compared to the pre-remote cache times. Developers nowadays are twice more likely to rebase their working branches, eventually leading to fewer conflicts when creating a pull request. </p>



<h2><strong>How to integrate XCRemoteCache into your existing project</strong></h2>



<p>Now that XCRemoteCache is open source, you can apply it to your own project with minimum effort. It supports multiple project setups, including the ones managed by CocoaPods, Carthage, or any other custom dependency management. Keep in mind, though, that for best results, your project should be split into several targets or you risk frequent cache invalidations, as described above.</p>



<div><p>For seamless integration, we are open sourcing a <a href="https://github.com/spotify/XCRemoteCache/tree/master/cocoapods-plugin" target="_blank" rel="noreferrer noopener">CocoaPods plugin</a> and provide an automated script to modify the existing .xcodeproj project.</p><p>XCRemoteCache works with any HTTP server that supports PUT, HEAD, and GET requests. You are free to pick a server that works best for you, including the two popular storage options provided by Amazon S3 and Google’s Google Cloud Storage. We also provide a simple docker image that hosts a local server, perfect for the development phase.</p></div>



<p>With that in place, you should be able to try out XCRemoteCache within minutes. For a list of integration steps, head to the <a href="https://github.com/spotify/XCRemoteCache#how-to-integrate-xcremotecache-with-your-xcode-project" target="_blank" rel="noreferrer noopener"><em>How-to</em></a><em> </em>section in the GitHub repo.</p>



<h2><strong>Contributing to XCRemoteCache</strong></h2>



<p>The XCRemoteCache tool is written fully in Swift, so iOS developers can easily familiarize themselves with the codebase and potentially contribute to it. We tried to cover most of the common scenarios but, keeping in mind that Xcode projects may have very custom setups, some of them may not be compatible right now. Therefore, any inputs from the community, both raising issues or pull requests, are very welcome. We believe that, together, we will be able to move the project even further and support a wider audience of iOS developers. </p>



<p>If you want to contribute to the codebase, make sure to check out our <a href="https://github.com/spotify/XCRemoteCache/blob/master/docs/Development.md" target="_blank" rel="noreferrer noopener">development</a> guide. And if you want to work full-time on tools like that, please check out our <a rel="noreferrer noopener" href="https://www.lifeatspotify.com/jobs" target="_blank">open job positions</a>.</p>



<p>I want to personally thank Erick Camacho for his help in preparing this blog post.</p>



<p><em>Xcode is a trademark of Apple Inc., registered in the U.S. and other countries.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Bartosz Polaczyk, Senior Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%&#xA;</title>
      <link>https://engineering.atspotify.com/2021/11/16/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/</link>
      <description>At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain. We are excited to be open sourcing X</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/11/16/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/" title="Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain.</p>



<p>We are excited to be open sourcing <a href="https://github.com/spotify/XCRemoteCache" target="_blank" rel="noreferrer noopener">XCRemoteCache</a>, the library we created to mitigate long local builds. As the name suggests, this library is a remote caching implementation for iOS projects with an aim to reuse Xcode target artifacts generated on Continuous Integration (CI) machines. It supports Objective-C, Swift, and ObjC+Swift targets and can be easily integrated with existing Xcode projects, including ones managed by CocoaPods or Carthage.</p>



<p>Best of all, XCRemoteCache resulted in a <strong>70% decrease in clean build times</strong> (we classify a build as <em>clean</em> when at least 50% of all targets compile at least one file).</p>



<h2><strong>Background</strong></h2>



<p>Using our Xcode build metrics (for more details on how this works, take a look at our open source <a href="https://xcmetrics.io/" target="_blank" rel="noreferrer noopener">XCMetrics project</a>), we found out that it often takes our developers more than 10 minutes to build the main Spotify iOS application. Even though the number of these builds is relatively small (less than 3% of all builds), they take more than 50% of global building times (Figure 1). </p>



<p>After some investigation, it was revealed that long-lasting builds usually happen after rebasing or merging remote branches. Implementing a remote cache solution was the perfect fit.</p>



<figure><img loading="lazy" width="700" height="354" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-768x389.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times.png 1205w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: Distribution of local machines’ build times and their total build times.</figcaption></figure>



<h2><strong>Remote cache principle</strong></h2>



<p>A remote cache is a popular technique to speed up builds of big applications by applying the “compile once, use everywhere” approach. As long as all input files and compilation parameters are the same, instead of building a target locally, one can download artifacts that were built and shared from some other machine. A key success factor for remote caching is finding an optimal caching level. Caching units that are too granular, where every single piece of the compilation step is cacheable, may lead to extensive network traffic overhead, which can offset CPU savings. On the other hand, putting the entire codebase into a single cacheable unit may significantly degrade the caching hit rate; every single local change invalidates remotely available cache artifacts, triggering a full build, locally.</p>



<p>The main Spotify iOS application is highly modularized and contains more than 400 independent modules configured as separate Xcode targets. Applying target-level caching was natural, and as we found out later, the right decision.</p>



<h2><strong>Designing the remote cache solution </strong></h2>



<p>In the design phase, our aim was to come up with a solution generic enough that it could be applied to a broad range of iOS applications with minimal or no project changes. That was an ambitious goal given how the Xcode build system works. Before going straight to the applied solution, let’s consider how an Xcode build actually works. </p>



<h2><strong>How does XCRemoteCache work?</strong></h2>



<p>In general, all caching mechanisms use a fingerprint of input files to recognize if build products can be reused. However, finding a precise set of these files is a nontrivial task. The Xcode build system is very liberal when it comes to dependency attribution. It tries to optimistically find a definition of the dependency (header files, <code>.swiftmodule</code>, etc.) in all available search paths — provided either by the developer in the Xcode project settings or in the current build product directory. As a result, developers don’t have to explicitly specify all dependencies a target uses, but just have to make sure that those dependencies will be placed in a correct location <em>before</em> Xcode actually needs them. The fact that compilers are able to implicitly find required dependencies, hinders the simple fingerprint generation by hashing all available files in header and framework search paths — a list of files to consider in the fingerprinting would often be too broad.</p>



<p>On the other hand, we observed that Xcode works quite well for local incremental builds and executes only a narrow subset of steps that were affected since the last build. In other words, the Xcode build system knows which files are the actual input files for the compilation, but that list is generated as compiler’s output (.d files) and is not available ahead of a compilation.</p>



<p>XCRemoteCache applies a unique approach to automatically identify all input files of the compilation based on Git history and dependency lists provided as a compilation output. The generation side, called <em>producer mode</em>, along with the compilation product, also uploads a meta file. That file contains a list of all compilation files the compiler used and the full SHA-1 (Secure Hash Algorithm 1) commit identifier it was built against. The producer mode should run on CI for each primary branch (like <code>master</code> or <code>develop</code>) commit, as a part of the post-merge phase.</p>



<p>On the consumer side (aka <em>consumer mode</em>), XCRemoteCache finds the most recent history commit for which the remote server contains build artifacts and builds a fingerprint based on input files provided in the meta file. Let’s imagine two developers, <code>A</code> and <code>B</code>, working on their local branches <code>featureA </code>and <code>featureB</code>, branching out from <code>master </code>on <code>Commit1</code> and <code>Commit3</code>, respectively (Figure 2). A CI job that produces and uploads cache artifacts to the central server finished its work only for commits 1, 2, and 4. For some reason, <code>Commit3</code> artifacts are not ready — either the build is in progress or it has failed. Developer A’s machine will reuse the artifacts generated for <code>Commit1</code>, while Developer B’s takes them from <code>Commit2</code> — it tried with <code>Commit3</code>, but they are not ready yet.</p>



<figure><img loading="lazy" width="700" height="440" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-250x157.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-768x483.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts.png 1205w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: Finding a commit with artifacts to reuse.</figcaption></figure>



<div><p>With that procedure, XCRemoteCache gets a strict list of input files almost for free.</p><p>Assuming we have an app split into several independent targets and local branches that don’t divert much from a primary branch, the caching hit rate will be high, minus only these targets that contain changes comparing the commit of which remote artifacts are used. </p></div>



<h3>Build artifacts portability</h3>



<p>Another problem to consider is “build artifacts portability” between multiple machines. Several types of compilation output files include absolute paths, so for full compatibility, some kind of normalization is required. For iOS projects, such a step is required for <code>.swiftmodule</code> files and debug symbols. </p>



<p>Projects cloned to <code>/dir1</code> and <code>/dir2</code> generate <code>.swiftmodule </code>files that do not match on a byte level. Swiftmodule represents an inter-module API (Swift counterparts to .h) that can be included in the list of fingerprinting files. To overcome falsy cache misses if two machines don’t have the same absolute source root paths, XCRemoteCache carries an extra fingerprint file (called a fingerprint override) next to the <code>.swiftmodule </code>that includes a fingerprint of all the files used in the compilation step. A fingerprint override is path agnostic, so contrary to the <code>.swiftmodule</code> file, it can be used as a byte-level stable fingerprint of a Swift target. </p>



<p>Debug symbols, other path-sensitive files, are appended to the binary package to associate the machine code with the corresponding source location when debugging. The XCRemoteCache leverages support for LLDB runtime path rewrites using <code>settings set target.source-map</code>. Both producer and consumer pass <code>debug-prefix-map</code> parameters to the Swift and Clang compilers to align the source root of all debug symbols making the LLDB source mapping a simple, single-line command.</p>



<h2><strong>The Spotify story</strong></h2>



<p>At Spotify, we have fast, well-optimized CI jobs that no longer slow down our development feedback loop (please head to <a href="https://engineering.atspotify.com/2020/05/01/how-we-gave-superpowers-to-our-macos-ci/" target="_blank" rel="noreferrer noopener">How We Gave Superpowers to Our macOS CI</a> to read more about that), so we focused on applying XCRemoteCache on local machines. Keep in mind that the tool is able to work on CI machines and accelerate PR jobs, too.</p>



<p>In controlled conditions, XCRemoteCache was able to cut the very first iOS Spotify application build time by 85%. This was a great achievement, but in practice, developers introduce changes locally, and some targets have to be compiled locally. Our goal was to evaluate real-world scenarios to understand their true impact. To estimate that, we rolled out the remote cache to 50% of our developers for a week and compared all build metrics. </p>



<p>Results exceeded our expectations — we observed a huge improvement of the local build times: <strong>median clean build and incremental build times decreased by 70% and 6%, respectively. </strong>We classify builds as <em>clean</em> when at least 50% of all targets compile at least one file.  Other builds that compile at least one file are <em>incremental</em>.</p>



<p>So, we enabled XCRemoteCache into our main application more than a year ago, and since then, it has worked flawlessly. We get very positive feedback from our developers and, as a side effect, we’ve seen the portion of clean builds almost double compared to the pre-remote cache times. Developers nowadays are twice more likely to rebase their working branches, eventually leading to fewer conflicts when creating a pull request. </p>



<h2><strong>How to integrate XCRemoteCache into your existing project</strong></h2>



<p>Now that XCRemoteCache is open source, you can apply it to your own project with minimum effort. It supports multiple project setups, including the ones managed by CocoaPods, Carthage, or any other custom dependency management. Keep in mind, though, that for best results, your project should be split into several targets or you risk frequent cache invalidations, as described above.</p>



<div><p>For seamless integration, we are open sourcing a <a href="https://github.com/spotify/XCRemoteCache/tree/master/cocoapods-plugin" target="_blank" rel="noreferrer noopener">CocoaPods plugin</a> and provide an automated script to modify the existing .xcodeproj project.</p><p>XCRemoteCache works with any HTTP server that supports PUT, HEAD, and GET requests. You are free to pick a server that works best for you, including the two popular storage options provided by Amazon S3 and Google’s Google Cloud Storage. We also provide a simple docker image that hosts a local server, perfect for the development phase.</p></div>



<p>With that in place, you should be able to try out XCRemoteCache within minutes. For a list of integration steps, head to the <a href="https://github.com/spotify/XCRemoteCache#how-to-integrate-xcremotecache-with-your-xcode-project" target="_blank" rel="noreferrer noopener"><em>How-to</em></a><em> </em>section in the GitHub repo.</p>



<h2><strong>Contributing to XCRemoteCache</strong></h2>



<p>The XCRemoteCache tool is written fully in Swift, so iOS developers can easily familiarize themselves with the codebase and potentially contribute to it. We tried to cover most of the common scenarios but, keeping in mind that Xcode projects may have very custom setups, some of them may not be compatible right now. Therefore, any inputs from the community, both raising issues or pull requests, are very welcome. We believe that, together, we will be able to move the project even further and support a wider audience of iOS developers. </p>



<p>If you want to contribute to the codebase, make sure to check out our <a href="https://github.com/spotify/XCRemoteCache/blob/master/docs/Development.md" target="_blank" rel="noreferrer noopener">development</a> guide. And if you want to work full-time on tools like that, please check out our <a rel="noreferrer noopener" href="https://www.lifeatspotify.com/jobs" target="_blank">open job positions</a>.</p>



<p>I want to personally thank Erick Camacho for his help in preparing this blog post.</p>



<p><em>Xcode is a trademark of Apple Inc., registered in the U.S. and other countries.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a></p>

        

            </div></div>]]></content:encoded>
      <author>Published by Bartosz Polaczyk, Senior Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%&#xA;</title>
      <link>https://engineering.atspotify.com/2021/11/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/</link>
      <description>At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain. We are excited to be open sourcing X</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/11/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/" title="Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain.</p>



<p>We are excited to be open sourcing <a href="https://github.com/spotify/XCRemoteCache" target="_blank" rel="noreferrer noopener">XCRemoteCache</a>, the library we created to mitigate long local builds. As the name suggests, this library is a remote caching implementation for iOS projects with an aim to reuse Xcode target artifacts generated on Continuous Integration (CI) machines. It supports Objective-C, Swift, and ObjC+Swift targets and can be easily integrated with existing Xcode projects, including ones managed by CocoaPods or Carthage.</p>



<p>Best of all, XCRemoteCache resulted in a <strong>70% decrease in clean build times</strong> (we classify a build as <em>clean</em> when at least 50% of all targets compile at least one file).</p>



<h2><strong>Background</strong></h2>



<p>Using our Xcode build metrics (for more details on how this works, take a look at our open source <a href="https://xcmetrics.io/" target="_blank" rel="noreferrer noopener">XCMetrics project</a>), we found out that it often takes our developers more than 10 minutes to build the main Spotify iOS application. Even though the number of these builds is relatively small (less than 3% of all builds), they take more than 50% of global building times (Figure 1). </p>



<p>After some investigation, it was revealed that long-lasting builds usually happen after rebasing or merging remote branches. Implementing a remote cache solution was the perfect fit.</p>



<figure><img loading="lazy" width="700" height="354" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-768x389.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times.png 1205w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: Distribution of local machines’ build times and their total build times.</figcaption></figure>



<h2><strong>Remote cache principle</strong></h2>



<p>A remote cache is a popular technique to speed up builds of big applications by applying the “compile once, use everywhere” approach. As long as all input files and compilation parameters are the same, instead of building a target locally, one can download artifacts that were built and shared from some other machine. A key success factor for remote caching is finding an optimal caching level. Caching units that are too granular, where every single piece of the compilation step is cacheable, may lead to extensive network traffic overhead, which can offset CPU savings. On the other hand, putting the entire codebase into a single cacheable unit may significantly degrade the caching hit rate; every single local change invalidates remotely available cache artifacts, triggering a full build, locally.</p>



<p>The main Spotify iOS application is highly modularized and contains more than 400 independent modules configured as separate Xcode targets. Applying target-level caching was natural, and as we found out later, the right decision.</p>



<h2><strong>Designing the remote cache solution </strong></h2>



<p>In the design phase, our aim was to come up with a solution generic enough that it could be applied to a broad range of iOS applications with minimal or no project changes. That was an ambitious goal given how the Xcode build system works. Before going straight to the applied solution, let’s consider how an Xcode build actually works. </p>



<h2><strong>How does XCRemoteCache work?</strong></h2>



<p>In general, all caching mechanisms use a fingerprint of input files to recognize if build products can be reused. However, finding a precise set of these files is a nontrivial task. The Xcode build system is very liberal when it comes to dependency attribution. It tries to optimistically find a definition of the dependency (header files, <code>.swiftmodule</code>, etc.) in all available search paths — provided either by the developer in the Xcode project settings or in the current build product directory. As a result, developers don’t have to explicitly specify all dependencies a target uses, but just have to make sure that those dependencies will be placed in a correct location <em>before</em> Xcode actually needs them. The fact that compilers are able to implicitly find required dependencies, hinders the simple fingerprint generation by hashing all available files in header and framework search paths — a list of files to consider in the fingerprinting would often be too broad.</p>



<p>On the other hand, we observed that Xcode works quite well for local incremental builds and executes only a narrow subset of steps that were affected since the last build. In other words, the Xcode build system knows which files are the actual input files for the compilation, but that list is generated as compiler’s output (.d files) and is not available ahead of a compilation.</p>



<p>XCRemoteCache applies a unique approach to automatically identify all input files of the compilation based on Git history and dependency lists provided as a compilation output. The generation side, called <em>producer mode</em>, along with the compilation product, also uploads a meta file. That file contains a list of all compilation files the compiler used and the full SHA-1 (Secure Hash Algorithm 1) commit identifier it was built against. The producer mode should run on CI for each primary branch (like <code>master</code> or <code>develop</code>) commit, as a part of the post-merge phase.</p>



<p>On the consumer side (aka <em>consumer mode</em>), XCRemoteCache finds the most recent history commit for which the remote server contains build artifacts and builds a fingerprint based on input files provided in the meta file. Let’s imagine two developers, <code>A</code> and <code>B</code>, working on their local branches <code>featureA </code>and <code>featureB</code>, branching out from <code>master </code>on <code>Commit1</code> and <code>Commit3</code>, respectively (Figure 2). A CI job that produces and uploads cache artifacts to the central server finished its work only for commits 1, 2, and 4. For some reason, <code>Commit3</code> artifacts are not ready — either the build is in progress or it has failed. Developer A’s machine will reuse the artifacts generated for <code>Commit1</code>, while Developer B’s takes them from <code>Commit2</code> — it tried with <code>Commit3</code>, but they are not ready yet.</p>



<figure><img loading="lazy" width="700" height="440" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-250x157.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-768x483.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts.png 1205w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: Finding a commit with artifacts to reuse.</figcaption></figure>



<div><p>With that procedure, XCRemoteCache gets a strict list of input files almost for free.</p><p>Assuming we have an app split into several independent targets and local branches that don’t divert much from a primary branch, the caching hit rate will be high, minus only these targets that contain changes comparing the commit of which remote artifacts are used. </p></div>



<h3>Build artifacts portability</h3>



<p>Another problem to consider is “build artifacts portability” between multiple machines. Several types of compilation output files include absolute paths, so for full compatibility, some kind of normalization is required. For iOS projects, such a step is required for <code>.swiftmodule</code> files and debug symbols. </p>



<p>Projects cloned to <code>/dir1</code> and <code>/dir2</code> generate <code>.swiftmodule </code>files that do not match on a byte level. Swiftmodule represents an inter-module API (Swift counterparts to .h) that can be included in the list of fingerprinting files. To overcome falsy cache misses if two machines don’t have the same absolute source root paths, XCRemoteCache carries an extra fingerprint file (called a fingerprint override) next to the <code>.swiftmodule </code>that includes a fingerprint of all the files used in the compilation step. A fingerprint override is path agnostic, so contrary to the <code>.swiftmodule</code> file, it can be used as a byte-level stable fingerprint of a Swift target. </p>



<p>Debug symbols, other path-sensitive files, are appended to the binary package to associate the machine code with the corresponding source location when debugging. The XCRemoteCache leverages support for LLDB runtime path rewrites using <code>settings set target.source-map</code>. Both producer and consumer pass <code>debug-prefix-map</code> parameters to the Swift and Clang compilers to align the source root of all debug symbols making the LLDB source mapping a simple, single-line command.</p>



<h2><strong>The Spotify story</strong></h2>



<p>At Spotify, we have fast, well-optimized CI jobs that no longer slow down our development feedback loop (please head to <a href="https://engineering.atspotify.com/2020/05/01/how-we-gave-superpowers-to-our-macos-ci/" target="_blank" rel="noreferrer noopener">How We Gave Superpowers to Our macOS CI</a> to read more about that), so we focused on applying XCRemoteCache on local machines. Keep in mind that the tool is able to work on CI machines and accelerate PR jobs, too.</p>



<p>In controlled conditions, XCRemoteCache was able to cut the very first iOS Spotify application build time by 85%. This was a great achievement, but in practice, developers introduce changes locally, and some targets have to be compiled locally. Our goal was to evaluate real-world scenarios to understand their true impact. To estimate that, we rolled out the remote cache to 50% of our developers for a week and compared all build metrics. </p>



<p>Results exceeded our expectations — we observed a huge improvement of the local build times: <strong>median clean build and incremental build times decreased by 70% and 6%, respectively. </strong>We classify builds as <em>clean</em> when at least 50% of all targets compile at least one file.  Other builds that compile at least one file are <em>incremental</em>.</p>



<p>So, we enabled XCRemoteCache into our main application more than a year ago, and since then, it has worked flawlessly. We get very positive feedback from our developers and, as a side effect, we’ve seen the portion of clean builds almost double compared to the pre-remote cache times. Developers nowadays are twice more likely to rebase their working branches, eventually leading to fewer conflicts when creating a pull request. </p>



<h2><strong>How to integrate XCRemoteCache into your existing project</strong></h2>



<p>Now that XCRemoteCache is open source, you can apply it to your own project with minimum effort. It supports multiple project setups, including the ones managed by CocoaPods, Carthage, or any other custom dependency management. Keep in mind, though, that for best results, your project should be split into several targets or you risk frequent cache invalidations, as described above.</p>



<div><p>For seamless integration, we are open sourcing a <a href="https://github.com/spotify/XCRemoteCache/tree/master/cocoapods-plugin" target="_blank" rel="noreferrer noopener">CocoaPods plugin</a> and provide an automated script to modify the existing .xcodeproj project.</p><p>XCRemoteCache works with any HTTP server that supports PUT, HEAD, and GET requests. You are free to pick a server that works best for you, including the two popular storage options provided by Amazon S3 and Google’s Google Cloud Storage. We also provide a simple docker image that hosts a local server, perfect for the development phase.</p></div>



<p>With that in place, you should be able to try out XCRemoteCache within minutes. For a list of integration steps, head to the <a href="https://github.com/spotify/XCRemoteCache#how-to-integrate-xcremotecache-with-your-xcode-project" target="_blank" rel="noreferrer noopener"><em>How-to</em></a><em> </em>section in the GitHub repo.</p>



<h2><strong>Contributing to XCRemoteCache</strong></h2>



<p>The XCRemoteCache tool is written fully in Swift, so iOS developers can easily familiarize themselves with the codebase and potentially contribute to it. We tried to cover most of the common scenarios but, keeping in mind that Xcode projects may have very custom setups, some of them may not be compatible right now. Therefore, any inputs from the community, both raising issues or pull requests, are very welcome. We believe that, together, we will be able to move the project even further and support a wider audience of iOS developers. </p>



<p>If you want to contribute to the codebase, make sure to check out our <a href="https://github.com/spotify/XCRemoteCache/blob/master/docs/Development.md" target="_blank" rel="noreferrer noopener">development</a> guide. And if you want to work full-time on tools like that, please check out our <a rel="noreferrer noopener" href="https://www.lifeatspotify.com/jobs" target="_blank">open job positions</a>.</p>



<p>I want to personally thank Erick Camacho for his help in preparing this blog post.</p>



<p><em>Xcode is a trademark of Apple Inc., registered in the U.S. and other countries.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Bartosz Polaczyk, Senior Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%&#xA;</title>
      <link>https://engineering.atspotify.com/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/</link>
      <description>At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain. We are excited to be open sourcing X</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/" title="Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain.</p>



<p>We are excited to be open sourcing <a href="https://github.com/spotify/XCRemoteCache" target="_blank" rel="noreferrer noopener">XCRemoteCache</a>, the library we created to mitigate long local builds. As the name suggests, this library is a remote caching implementation for iOS projects with an aim to reuse Xcode target artifacts generated on Continuous Integration (CI) machines. It supports Objective-C, Swift, and ObjC+Swift targets and can be easily integrated with existing Xcode projects, including ones managed by CocoaPods or Carthage.</p>



<p>Best of all, XCRemoteCache resulted in a <strong>70% decrease in clean build times</strong> (we classify a build as <em>clean</em> when at least 50% of all targets compile at least one file).</p>



<h2><strong>Background</strong></h2>



<p>Using our Xcode build metrics (for more details on how this works, take a look at our open source <a href="https://xcmetrics.io/" target="_blank" rel="noreferrer noopener">XCMetrics project</a>), we found out that it often takes our developers more than 10 minutes to build the main Spotify iOS application. Even though the number of these builds is relatively small (less than 3% of all builds), they take more than 50% of global building times (Figure 1). </p>



<p>After some investigation, it was revealed that long-lasting builds usually happen after rebasing or merging remote branches. Implementing a remote cache solution was the perfect fit.</p>



<figure><img loading="lazy" width="700" height="354" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-768x389.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times.png 1205w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: Distribution of local machines’ build times and their total build times.</figcaption></figure>



<h2><strong>Remote cache principle</strong></h2>



<p>A remote cache is a popular technique to speed up builds of big applications by applying the “compile once, use everywhere” approach. As long as all input files and compilation parameters are the same, instead of building a target locally, one can download artifacts that were built and shared from some other machine. A key success factor for remote caching is finding an optimal caching level. Caching units that are too granular, where every single piece of the compilation step is cacheable, may lead to extensive network traffic overhead, which can offset CPU savings. On the other hand, putting the entire codebase into a single cacheable unit may significantly degrade the caching hit rate; every single local change invalidates remotely available cache artifacts, triggering a full build, locally.</p>



<p>The main Spotify iOS application is highly modularized and contains more than 400 independent modules configured as separate Xcode targets. Applying target-level caching was natural, and as we found out later, the right decision.</p>



<h2><strong>Designing the remote cache solution </strong></h2>



<p>In the design phase, our aim was to come up with a solution generic enough that it could be applied to a broad range of iOS applications with minimal or no project changes. That was an ambitious goal given how the Xcode build system works. Before going straight to the applied solution, let’s consider how an Xcode build actually works. </p>



<h2><strong>How does XCRemoteCache work?</strong></h2>



<p>In general, all caching mechanisms use a fingerprint of input files to recognize if build products can be reused. However, finding a precise set of these files is a nontrivial task. The Xcode build system is very liberal when it comes to dependency attribution. It tries to optimistically find a definition of the dependency (header files, <code>.swiftmodule</code>, etc.) in all available search paths — provided either by the developer in the Xcode project settings or in the current build product directory. As a result, developers don’t have to explicitly specify all dependencies a target uses, but just have to make sure that those dependencies will be placed in a correct location <em>before</em> Xcode actually needs them. The fact that compilers are able to implicitly find required dependencies, hinders the simple fingerprint generation by hashing all available files in header and framework search paths — a list of files to consider in the fingerprinting would often be too broad.</p>



<p>On the other hand, we observed that Xcode works quite well for local incremental builds and executes only a narrow subset of steps that were affected since the last build. In other words, the Xcode build system knows which files are the actual input files for the compilation, but that list is generated as compiler’s output (.d files) and is not available ahead of a compilation.</p>



<p>XCRemoteCache applies a unique approach to automatically identify all input files of the compilation based on Git history and dependency lists provided as a compilation output. The generation side, called <em>producer mode</em>, along with the compilation product, also uploads a meta file. That file contains a list of all compilation files the compiler used and the full SHA-1 (Secure Hash Algorithm 1) commit identifier it was built against. The producer mode should run on CI for each primary branch (like <code>master</code> or <code>develop</code>) commit, as a part of the post-merge phase.</p>



<p>On the consumer side (aka <em>consumer mode</em>), XCRemoteCache finds the most recent history commit for which the remote server contains build artifacts and builds a fingerprint based on input files provided in the meta file. Let’s imagine two developers, <code>A</code> and <code>B</code>, working on their local branches <code>featureA </code>and <code>featureB</code>, branching out from <code>master </code>on <code>Commit1</code> and <code>Commit3</code>, respectively (Figure 2). A CI job that produces and uploads cache artifacts to the central server finished its work only for commits 1, 2, and 4. For some reason, <code>Commit3</code> artifacts are not ready — either the build is in progress or it has failed. Developer A’s machine will reuse the artifacts generated for <code>Commit1</code>, while Developer B’s takes them from <code>Commit2</code> — it tried with <code>Commit3</code>, but they are not ready yet.</p>



<figure><img loading="lazy" width="700" height="440" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-250x157.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-768x483.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts.png 1205w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: Finding a commit with artifacts to reuse.</figcaption></figure>



<div><p>With that procedure, XCRemoteCache gets a strict list of input files almost for free.</p><p>Assuming we have an app split into several independent targets and local branches that don’t divert much from a primary branch, the caching hit rate will be high, minus only these targets that contain changes comparing the commit of which remote artifacts are used. </p></div>



<h3>Build artifacts portability</h3>



<p>Another problem to consider is “build artifacts portability” between multiple machines. Several types of compilation output files include absolute paths, so for full compatibility, some kind of normalization is required. For iOS projects, such a step is required for <code>.swiftmodule</code> files and debug symbols. </p>



<p>Projects cloned to <code>/dir1</code> and <code>/dir2</code> generate <code>.swiftmodule </code>files that do not match on a byte level. Swiftmodule represents an inter-module API (Swift counterparts to .h) that can be included in the list of fingerprinting files. To overcome falsy cache misses if two machines don’t have the same absolute source root paths, XCRemoteCache carries an extra fingerprint file (called a fingerprint override) next to the <code>.swiftmodule </code>that includes a fingerprint of all the files used in the compilation step. A fingerprint override is path agnostic, so contrary to the <code>.swiftmodule</code> file, it can be used as a byte-level stable fingerprint of a Swift target. </p>



<p>Debug symbols, other path-sensitive files, are appended to the binary package to associate the machine code with the corresponding source location when debugging. The XCRemoteCache leverages support for LLDB runtime path rewrites using <code>settings set target.source-map</code>. Both producer and consumer pass <code>debug-prefix-map</code> parameters to the Swift and Clang compilers to align the source root of all debug symbols making the LLDB source mapping a simple, single-line command.</p>



<h2><strong>The Spotify story</strong></h2>



<p>At Spotify, we have fast, well-optimized CI jobs that no longer slow down our development feedback loop (please head to <a href="https://engineering.atspotify.com/2020/05/01/how-we-gave-superpowers-to-our-macos-ci/" target="_blank" rel="noreferrer noopener">How We Gave Superpowers to Our macOS CI</a> to read more about that), so we focused on applying XCRemoteCache on local machines. Keep in mind that the tool is able to work on CI machines and accelerate PR jobs, too.</p>



<p>In controlled conditions, XCRemoteCache was able to cut the very first iOS Spotify application build time by 85%. This was a great achievement, but in practice, developers introduce changes locally, and some targets have to be compiled locally. Our goal was to evaluate real-world scenarios to understand their true impact. To estimate that, we rolled out the remote cache to 50% of our developers for a week and compared all build metrics. </p>



<p>Results exceeded our expectations — we observed a huge improvement of the local build times: <strong>median clean build and incremental build times decreased by 70% and 6%, respectively. </strong>We classify builds as <em>clean</em> when at least 50% of all targets compile at least one file.  Other builds that compile at least one file are <em>incremental</em>.</p>



<p>So, we enabled XCRemoteCache into our main application more than a year ago, and since then, it has worked flawlessly. We get very positive feedback from our developers and, as a side effect, we’ve seen the portion of clean builds almost double compared to the pre-remote cache times. Developers nowadays are twice more likely to rebase their working branches, eventually leading to fewer conflicts when creating a pull request. </p>



<h2><strong>How to integrate XCRemoteCache into your existing project</strong></h2>



<p>Now that XCRemoteCache is open source, you can apply it to your own project with minimum effort. It supports multiple project setups, including the ones managed by CocoaPods, Carthage, or any other custom dependency management. Keep in mind, though, that for best results, your project should be split into several targets or you risk frequent cache invalidations, as described above.</p>



<div><p>For seamless integration, we are open sourcing a <a href="https://github.com/spotify/XCRemoteCache/tree/master/cocoapods-plugin" target="_blank" rel="noreferrer noopener">CocoaPods plugin</a> and provide an automated script to modify the existing .xcodeproj project.</p><p>XCRemoteCache works with any HTTP server that supports PUT, HEAD, and GET requests. You are free to pick a server that works best for you, including the two popular storage options provided by Amazon S3 and Google’s Google Cloud Storage. We also provide a simple docker image that hosts a local server, perfect for the development phase.</p></div>



<p>With that in place, you should be able to try out XCRemoteCache within minutes. For a list of integration steps, head to the <a href="https://github.com/spotify/XCRemoteCache#how-to-integrate-xcremotecache-with-your-xcode-project" target="_blank" rel="noreferrer noopener"><em>How-to</em></a><em> </em>section in the GitHub repo.</p>



<h2><strong>Contributing to XCRemoteCache</strong></h2>



<p>The XCRemoteCache tool is written fully in Swift, so iOS developers can easily familiarize themselves with the codebase and potentially contribute to it. We tried to cover most of the common scenarios but, keeping in mind that Xcode projects may have very custom setups, some of them may not be compatible right now. Therefore, any inputs from the community, both raising issues or pull requests, are very welcome. We believe that, together, we will be able to move the project even further and support a wider audience of iOS developers. </p>



<p>If you want to contribute to the codebase, make sure to check out our <a href="https://github.com/spotify/XCRemoteCache/blob/master/docs/Development.md" target="_blank" rel="noreferrer noopener">development</a> guide. And if you want to work full-time on tools like that, please check out our <a rel="noreferrer noopener" href="https://www.lifeatspotify.com/jobs" target="_blank">open job positions</a>.</p>



<p>I want to personally thank Erick Camacho for his help in preparing this blog post.</p>



<p><em>Xcode is a trademark of Apple Inc., registered in the U.S. and other countries.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Bartosz Polaczyk, Senior Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)&#xA;</title>
      <link>https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/</link>
      <description>At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the les</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/" title="The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the lessons we’ve learned from building the ML stack that serves these models.</p>



<p>Machine learning is central to how we personalize the Home page user experience and connect listeners to the creators that are most relevant to them. Like many recommendation systems, the <a href="https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/">Spotify Home page recommendations are powered</a> by two stages: </p>



<p><strong>Stage 1: Candidate generation:</strong> The best albums, playlists, artists, and podcasts are selected for each listener.</p>



<p><strong>Stage 2: Ranking:</strong> Candidates are ranked in the best order for each listener.  </p>



<p>In Part I of this series, we’ll focus on the first stage — the machine learning solutions we’ve built to personalize the content for listeners’ Home pages and, specifically, the lessons we’ve learned in building, experimenting, and deploying these models. </p>



<h2>Home @ Spotify</h2>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png" alt="" width="303" height="334" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-250x276.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-768x847.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-1393x1536.png 1393w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-120x132.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home.png 1642w" sizes="(max-width: 303px) 100vw, 303px"/></figure></div>



<p>The Home page consists of cards — the square items that represent an album, playlist, etc. — and shelves — the horizontal rows that contain multiple cards. We generate personalized content for listeners’ Home pages, algorithmically curating the music and podcasts that are shown to listeners in the shelves on Home. Some content is generated via heuristics and rules and some content is manually curated by editors, while other content is generated via predictions using trained models. We currently have a number of models running in production, each one powering content curation for a different shelf, but we will be discussing three of those models in this post, including: </p>



<ul><li><strong>The Podcast Model:</strong> Predicts podcasts a listener is likely to listen to in the <em>Shows you might like</em> shelf. </li><li><strong>The Shortcuts Model: </strong>Predicts the listener’s next familiar listen in the Shortcuts feature. </li><li><strong>The Playlists Model: </strong>Predicts the playlists a new listener is likely to listen to in the <em>Try something else</em> shelf.  </li></ul>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png" alt="" width="411" height="375" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-250x228.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-768x700.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-1536x1400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-120x109.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2.png 1999w" sizes="(max-width: 411px) 100vw, 411px"/></figure></div>



<p>Since we launched our first model to recommend content on Home, we have worked to improve our ML stack and processes in order to experiment and productionize models more quickly and reliably.</p>



<h2>The road to simplicity and automation</h2>



<p>As anyone who may have contributed to operationalizing an ML model knows, moving a model from experimentation to production is no easy feat. There are numerous challenges in managing the data that goes into a model, running and tracking experiments, and monitoring and retraining models. While we have always tried to keep our ML infrastructure simple, and as close to the sources of features as possible, it has become drastically easier for our squads to deploy and maintain models now than when we started.</p>



<p>At a high level, an ML workflow can be broken down into three main phases: 1) data management, 2) experimentation, and 3) operationalization.  </p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png" alt="" width="784" height="164" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-250x52.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-768x160.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-120x25.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow.png 1351w" sizes="(max-width: 784px) 100vw, 784px"/></figure></div>



<p>It’s common to iteratively work on the training and evaluation phase until a final model version is selected as the best. This model is then deployed to production systems and can start making predictions for listeners. Similar to most production systems, models (and the services/pipelines that serve them) should be monitored closely. To keep a model up to date (which is more important for some tasks than others; more to come on this), retraining and model versioning are the last steps in our workflow. This part of our stack and workflow has had significant changes since our first model — making batch predictions (offline) of content listeners are likely to stream — to now, where all our models are served in real time. The figure below shows where our machine learning stack started and where we are now:</p>



<div><figure><img loading="lazy" width="700" height="533" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going.png 1012w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Our current ML stack automates a lot of the processes involved in maintaining models in production (with online serving): we have automated feature logging instrumented in our serving infrastructure, with both scheduled Scio pipelines to transform these features and Kubeflow pipelines to retrain weekly. We have also implemented data validation of our training and serving features (as well as validation between subsequent training datasets) to verify our features are consistent and follow the same distributions at training and inference times. In our Kubeflow pipelines, we have components that check the evaluation score and automatically push the model to production if the score is above our threshold. With this stack, we monitor and alert on the automatic data validation pipeline, as well as the online deployments of our models — allowing us to handle any issues as soon as they arise.</p>



<p>With a lot of effort and many lessons learned, our ML stack has evolved to make these processes automated and more reliable, enabling us to iterate faster to improve our models and increase our engineering productivity. </p>



<h2>How we unified training and serving data</h2>



<p>When we first start to think about a problem, we always dig into the data first — what data would be useful? What data is available? And then we take a really close look at the data that will be used for features, characterizing what is in the dataset and identifying the edge cases in the data. We feel fairly confident about the contents of the data used for our training features as well as what the transformed data looks like, but features fetched and transformed at <em>serving</em> time are an entirely different story. </p>



<h3>Batch training data and batch predictions</h3>



<p>Historically, we have had one set of infrastructure for fetching and transforming features during experimentation (training) and a different set of infrastructure for fetching and transforming features for making predictions (serving). </p>



<h3>Then we started to make online predictions (… with the wrong data)</h3>



<p>When we changed the Podcast Model from making batch offline predictions to serving in real time, we set up a new service that could support this — this new service had to fetch and transform features, make the prediction, and respond to the request. The important part here is that the feature processing and transformation was now in a different place than where the corresponding training feature processing took place. And, unfortunately, models are like black boxes, so testing the output is difficult, if not impossible. A while ago, we discovered that we had been transforming one of the model’s features slightly differently at training time than at serving time, leading to potentially degraded recommendations — and there was no way to detect this, so it continued to happen for four months.Think about this for just a second. Such a simple part of our stack — at most, a few lines of code — was doing the wrong thing and impacted the recommendations produced by our model. Our short-term fix was to simply change the one line of code in our prediction service that was causing the issue, but we knew long term that we needed to either have a single source of data for both training and serving, or we needed to ensure that data was produced and transformed the same way in both stages.</p>



<h3>One transformation implementation to rule them all</h3>



<p>Our first approach was to make any feature processing and transformation occur in the same code path, so that training and serving features would be processed identically. Taking the Shortcuts Model as an example again, our goal was to get rid of the Python service that transformed training features — this service was always running and constantly checking, on all days, to see if it was a Monday; if so, then it would request data from the necessary service (at a rate-limited 5 requests/second) and transform them into features; ideally, this would have been implemented as a pipeline, but we couldn’t schedule and orchestrate it because the process took more than 24 hours. There were many reasons we wanted to migrate away from this approach, but logging features when the only data source for features is a different service (owned by a different squad) proved difficult. Using our serving infrastructure’s feature logging capabilities, we could automatically log already transformed features, which could later be used for training. At this point, all of our features for training and serving were being transformed by code in the Java service. And we now use this feature logging for all of our models both to solve this problem, and also because it reduces the amount of additional infrastructure we need to support.  </p>



<h3>But wait, we can do more by validating our data</h3>



<p>The second approach we took to ensure our training and serving features did not differ was to use Tensorflow Data Validation (TFDV) to compare training and serving data schemas and feature distributions on a daily basis. The alerting we have added to our data validation pipeline allows us to detect significant differences in our feature sets — it uses the Chebyshev distance metric, which compares the distance between two vectors, and can help alert us to drift in training and serving features.  </p>



<p>While we knew that understanding what is in our data is crucial, we quickly learned that it’s easy to make mistakes when moving models to production because the data often uses a different processing library. We didn’t expect many data differences, but validating and alerting on issues lets us know if something changed, and how we should remediate the issue.</p>



<p>Stay tuned for Part II as we take a closer look at how we evaluate our models using offline and online metrics, why it’s so important to actually look at the recommendations we are making, and the challenges we faced in our journey to CI/CD in model retraining. </p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Annie Edmundson, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)&#xA;</title>
      <link>https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/</link>
      <description>At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the les</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/" title="The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the lessons we’ve learned from building the ML stack that serves these models.</p>



<p>Machine learning is central to how we personalize the Home page user experience and connect listeners to the creators that are most relevant to them. Like many recommendation systems, the <a href="https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/">Spotify Home page recommendations are powered</a> by two stages: </p>



<p><strong>Stage 1: Candidate generation:</strong> The best albums, playlists, artists, and podcasts are selected for each listener.</p>



<p><strong>Stage 2: Ranking:</strong> Candidates are ranked in the best order for each listener.  </p>



<p>In Part I of this series, we’ll focus on the first stage — the machine learning solutions we’ve built to personalize the content for listeners’ Home pages and, specifically, the lessons we’ve learned in building, experimenting, and deploying these models. </p>



<h2>Home @ Spotify</h2>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png" alt="" width="303" height="334" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-250x276.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-768x847.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-1393x1536.png 1393w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-120x132.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home.png 1642w" sizes="(max-width: 303px) 100vw, 303px"/></figure></div>



<p>The Home page consists of cards — the square items that represent an album, playlist, etc. — and shelves — the horizontal rows that contain multiple cards. We generate personalized content for listeners’ Home pages, algorithmically curating the music and podcasts that are shown to listeners in the shelves on Home. Some content is generated via heuristics and rules and some content is manually curated by editors, while other content is generated via predictions using trained models. We currently have a number of models running in production, each one powering content curation for a different shelf, but we will be discussing three of those models in this post, including: </p>



<ul><li><strong>The Podcast Model:</strong> Predicts podcasts a listener is likely to listen to in the <em>Shows you might like</em> shelf. </li><li><strong>The Shortcuts Model: </strong>Predicts the listener’s next familiar listen in the Shortcuts feature. </li><li><strong>The Playlists Model: </strong>Predicts the playlists a new listener is likely to listen to in the <em>Try something else</em> shelf.  </li></ul>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png" alt="" width="411" height="375" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-250x228.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-768x700.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-1536x1400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-120x109.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2.png 1999w" sizes="(max-width: 411px) 100vw, 411px"/></figure></div>



<p>Since we launched our first model to recommend content on Home, we have worked to improve our ML stack and processes in order to experiment and productionize models more quickly and reliably.</p>



<h2>The road to simplicity and automation</h2>



<p>As anyone who may have contributed to operationalizing an ML model knows, moving a model from experimentation to production is no easy feat. There are numerous challenges in managing the data that goes into a model, running and tracking experiments, and monitoring and retraining models. While we have always tried to keep our ML infrastructure simple, and as close to the sources of features as possible, it has become drastically easier for our squads to deploy and maintain models now than when we started.</p>



<p>At a high level, an ML workflow can be broken down into three main phases: 1) data management, 2) experimentation, and 3) operationalization.  </p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png" alt="" width="784" height="164" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-250x52.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-768x160.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-120x25.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow.png 1351w" sizes="(max-width: 784px) 100vw, 784px"/></figure></div>



<p>It’s common to iteratively work on the training and evaluation phase until a final model version is selected as the best. This model is then deployed to production systems and can start making predictions for listeners. Similar to most production systems, models (and the services/pipelines that serve them) should be monitored closely. To keep a model up to date (which is more important for some tasks than others; more to come on this), retraining and model versioning are the last steps in our workflow. This part of our stack and workflow has had significant changes since our first model — making batch predictions (offline) of content listeners are likely to stream — to now, where all our models are served in real time. The figure below shows where our machine learning stack started and where we are now:</p>



<div><figure><img loading="lazy" width="700" height="533" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going.png 1012w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Our current ML stack automates a lot of the processes involved in maintaining models in production (with online serving): we have automated feature logging instrumented in our serving infrastructure, with both scheduled Scio pipelines to transform these features and Kubeflow pipelines to retrain weekly. We have also implemented data validation of our training and serving features (as well as validation between subsequent training datasets) to verify our features are consistent and follow the same distributions at training and inference times. In our Kubeflow pipelines, we have components that check the evaluation score and automatically push the model to production if the score is above our threshold. With this stack, we monitor and alert on the automatic data validation pipeline, as well as the online deployments of our models — allowing us to handle any issues as soon as they arise.</p>



<p>With a lot of effort and many lessons learned, our ML stack has evolved to make these processes automated and more reliable, enabling us to iterate faster to improve our models and increase our engineering productivity. </p>



<h2>How we unified training and serving data</h2>



<p>When we first start to think about a problem, we always dig into the data first — what data would be useful? What data is available? And then we take a really close look at the data that will be used for features, characterizing what is in the dataset and identifying the edge cases in the data. We feel fairly confident about the contents of the data used for our training features as well as what the transformed data looks like, but features fetched and transformed at <em>serving</em> time are an entirely different story. </p>



<h3>Batch training data and batch predictions</h3>



<p>Historically, we have had one set of infrastructure for fetching and transforming features during experimentation (training) and a different set of infrastructure for fetching and transforming features for making predictions (serving). </p>



<h3>Then we started to make online predictions (… with the wrong data)</h3>



<p>When we changed the Podcast Model from making batch offline predictions to serving in real time, we set up a new service that could support this — this new service had to fetch and transform features, make the prediction, and respond to the request. The important part here is that the feature processing and transformation was now in a different place than where the corresponding training feature processing took place. And, unfortunately, models are like black boxes, so testing the output is difficult, if not impossible. A while ago, we discovered that we had been transforming one of the model’s features slightly differently at training time than at serving time, leading to potentially degraded recommendations — and there was no way to detect this, so it continued to happen for four months.Think about this for just a second. Such a simple part of our stack — at most, a few lines of code — was doing the wrong thing and impacted the recommendations produced by our model. Our short-term fix was to simply change the one line of code in our prediction service that was causing the issue, but we knew long term that we needed to either have a single source of data for both training and serving, or we needed to ensure that data was produced and transformed the same way in both stages.</p>



<h3>One transformation implementation to rule them all</h3>



<p>Our first approach was to make any feature processing and transformation occur in the same code path, so that training and serving features would be processed identically. Taking the Shortcuts Model as an example again, our goal was to get rid of the Python service that transformed training features — this service was always running and constantly checking, on all days, to see if it was a Monday; if so, then it would request data from the necessary service (at a rate-limited 5 requests/second) and transform them into features; ideally, this would have been implemented as a pipeline, but we couldn’t schedule and orchestrate it because the process took more than 24 hours. There were many reasons we wanted to migrate away from this approach, but logging features when the only data source for features is a different service (owned by a different squad) proved difficult. Using our serving infrastructure’s feature logging capabilities, we could automatically log already transformed features, which could later be used for training. At this point, all of our features for training and serving were being transformed by code in the Java service. And we now use this feature logging for all of our models both to solve this problem, and also because it reduces the amount of additional infrastructure we need to support.  </p>



<h3>But wait, we can do more by validating our data</h3>



<p>The second approach we took to ensure our training and serving features did not differ was to use Tensorflow Data Validation (TFDV) to compare training and serving data schemas and feature distributions on a daily basis. The alerting we have added to our data validation pipeline allows us to detect significant differences in our feature sets — it uses the Chebyshev distance metric, which compares the distance between two vectors, and can help alert us to drift in training and serving features.  </p>



<p>While we knew that understanding what is in our data is crucial, we quickly learned that it’s easy to make mistakes when moving models to production because the data often uses a different processing library. We didn’t expect many data differences, but validating and alerting on issues lets us know if something changed, and how we should remediate the issue.</p>



<p>Stay tuned for Part II as we take a closer look at how we evaluate our models using offline and online metrics, why it’s so important to actually look at the recommendations we are making, and the challenges we faced in our journey to CI/CD in model retraining. </p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Annie Edmundson, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)&#xA;</title>
      <link>https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/</link>
      <description>At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the les</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/" title="The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the lessons we’ve learned from building the ML stack that serves these models.</p>



<p>Machine learning is central to how we personalize the Home page user experience and connect listeners to the creators that are most relevant to them. Like many recommendation systems, the <a href="https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/">Spotify Home page recommendations are powered</a> by two stages: </p>



<p><strong>Stage 1: Candidate generation:</strong> The best albums, playlists, artists, and podcasts are selected for each listener.</p>



<p><strong>Stage 2: Ranking:</strong> Candidates are ranked in the best order for each listener.  </p>



<p>In Part I of this series, we’ll focus on the first stage — the machine learning solutions we’ve built to personalize the content for listeners’ Home pages and, specifically, the lessons we’ve learned in building, experimenting, and deploying these models. </p>



<h2>Home @ Spotify</h2>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png" alt="" width="303" height="334" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-250x276.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-768x847.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-1393x1536.png 1393w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-120x132.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home.png 1642w" sizes="(max-width: 303px) 100vw, 303px"/></figure></div>



<p>The Home page consists of cards — the square items that represent an album, playlist, etc. — and shelves — the horizontal rows that contain multiple cards. We generate personalized content for listeners’ Home pages, algorithmically curating the music and podcasts that are shown to listeners in the shelves on Home. Some content is generated via heuristics and rules and some content is manually curated by editors, while other content is generated via predictions using trained models. We currently have a number of models running in production, each one powering content curation for a different shelf, but we will be discussing three of those models in this post, including: </p>



<ul><li><strong>The Podcast Model:</strong> Predicts podcasts a listener is likely to listen to in the <em>Shows you might like</em> shelf. </li><li><strong>The Shortcuts Model: </strong>Predicts the listener’s next familiar listen in the Shortcuts feature. </li><li><strong>The Playlists Model: </strong>Predicts the playlists a new listener is likely to listen to in the <em>Try something else</em> shelf.  </li></ul>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png" alt="" width="411" height="375" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-250x228.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-768x700.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-1536x1400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-120x109.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2.png 1999w" sizes="(max-width: 411px) 100vw, 411px"/></figure></div>



<p>Since we launched our first model to recommend content on Home, we have worked to improve our ML stack and processes in order to experiment and productionize models more quickly and reliably.</p>



<h2>The road to simplicity and automation</h2>



<p>As anyone who may have contributed to operationalizing an ML model knows, moving a model from experimentation to production is no easy feat. There are numerous challenges in managing the data that goes into a model, running and tracking experiments, and monitoring and retraining models. While we have always tried to keep our ML infrastructure simple, and as close to the sources of features as possible, it has become drastically easier for our squads to deploy and maintain models now than when we started.</p>



<p>At a high level, an ML workflow can be broken down into three main phases: 1) data management, 2) experimentation, and 3) operationalization.  </p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png" alt="" width="784" height="164" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-250x52.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-768x160.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-120x25.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow.png 1351w" sizes="(max-width: 784px) 100vw, 784px"/></figure></div>



<p>It’s common to iteratively work on the training and evaluation phase until a final model version is selected as the best. This model is then deployed to production systems and can start making predictions for listeners. Similar to most production systems, models (and the services/pipelines that serve them) should be monitored closely. To keep a model up to date (which is more important for some tasks than others; more to come on this), retraining and model versioning are the last steps in our workflow. This part of our stack and workflow has had significant changes since our first model — making batch predictions (offline) of content listeners are likely to stream — to now, where all our models are served in real time. The figure below shows where our machine learning stack started and where we are now:</p>



<div><figure><img loading="lazy" width="700" height="533" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going.png 1012w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Our current ML stack automates a lot of the processes involved in maintaining models in production (with online serving): we have automated feature logging instrumented in our serving infrastructure, with both scheduled Scio pipelines to transform these features and Kubeflow pipelines to retrain weekly. We have also implemented data validation of our training and serving features (as well as validation between subsequent training datasets) to verify our features are consistent and follow the same distributions at training and inference times. In our Kubeflow pipelines, we have components that check the evaluation score and automatically push the model to production if the score is above our threshold. With this stack, we monitor and alert on the automatic data validation pipeline, as well as the online deployments of our models — allowing us to handle any issues as soon as they arise.</p>



<p>With a lot of effort and many lessons learned, our ML stack has evolved to make these processes automated and more reliable, enabling us to iterate faster to improve our models and increase our engineering productivity. </p>



<h2>How we unified training and serving data</h2>



<p>When we first start to think about a problem, we always dig into the data first — what data would be useful? What data is available? And then we take a really close look at the data that will be used for features, characterizing what is in the dataset and identifying the edge cases in the data. We feel fairly confident about the contents of the data used for our training features as well as what the transformed data looks like, but features fetched and transformed at <em>serving</em> time are an entirely different story. </p>



<h3>Batch training data and batch predictions</h3>



<p>Historically, we have had one set of infrastructure for fetching and transforming features during experimentation (training) and a different set of infrastructure for fetching and transforming features for making predictions (serving). </p>



<h3>Then we started to make online predictions (… with the wrong data)</h3>



<p>When we changed the Podcast Model from making batch offline predictions to serving in real time, we set up a new service that could support this — this new service had to fetch and transform features, make the prediction, and respond to the request. The important part here is that the feature processing and transformation was now in a different place than where the corresponding training feature processing took place. And, unfortunately, models are like black boxes, so testing the output is difficult, if not impossible. A while ago, we discovered that we had been transforming one of the model’s features slightly differently at training time than at serving time, leading to potentially degraded recommendations — and there was no way to detect this, so it continued to happen for four months.Think about this for just a second. Such a simple part of our stack — at most, a few lines of code — was doing the wrong thing and impacted the recommendations produced by our model. Our short-term fix was to simply change the one line of code in our prediction service that was causing the issue, but we knew long term that we needed to either have a single source of data for both training and serving, or we needed to ensure that data was produced and transformed the same way in both stages.</p>



<h3>One transformation implementation to rule them all</h3>



<p>Our first approach was to make any feature processing and transformation occur in the same code path, so that training and serving features would be processed identically. Taking the Shortcuts Model as an example again, our goal was to get rid of the Python service that transformed training features — this service was always running and constantly checking, on all days, to see if it was a Monday; if so, then it would request data from the necessary service (at a rate-limited 5 requests/second) and transform them into features; ideally, this would have been implemented as a pipeline, but we couldn’t schedule and orchestrate it because the process took more than 24 hours. There were many reasons we wanted to migrate away from this approach, but logging features when the only data source for features is a different service (owned by a different squad) proved difficult. Using our serving infrastructure’s feature logging capabilities, we could automatically log already transformed features, which could later be used for training. At this point, all of our features for training and serving were being transformed by code in the Java service. And we now use this feature logging for all of our models both to solve this problem, and also because it reduces the amount of additional infrastructure we need to support.  </p>



<h3>But wait, we can do more by validating our data</h3>



<p>The second approach we took to ensure our training and serving features did not differ was to use Tensorflow Data Validation (TFDV) to compare training and serving data schemas and feature distributions on a daily basis. The alerting we have added to our data validation pipeline allows us to detect significant differences in our feature sets — it uses the Chebyshev distance metric, which compares the distance between two vectors, and can help alert us to drift in training and serving features.  </p>



<p>While we knew that understanding what is in our data is crucial, we quickly learned that it’s easy to make mistakes when moving models to production because the data often uses a different processing library. We didn’t expect many data differences, but validating and alerting on issues lets us know if something changed, and how we should remediate the issue.</p>



<p>Stay tuned for Part II as we take a closer look at how we evaluate our models using offline and online metrics, why it’s so important to actually look at the recommendations we are making, and the challenges we faced in our journey to CI/CD in model retraining. </p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a></p>

        

            </div></div>]]></content:encoded>
      <author>Published by Annie Edmundson, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)&#xA;</title>
      <link>https://engineering.atspotify.com/2021/11/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/</link>
      <description>At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the les</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/11/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/" title="The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-120x59.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the lessons we’ve learned from building the ML stack that serves these models.</p>



<p>Machine learning is central to how we personalize the Home page user experience and connect listeners to the creators that are most relevant to them. Like many recommendation systems, the <a href="https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/">Spotify Home page recommendations are powered</a> by two stages: </p>



<p><strong>Stage 1: Candidate generation:</strong> The best albums, playlists, artists, and podcasts are selected for each listener.</p>



<p><strong>Stage 2: Ranking:</strong> Candidates are ranked in the best order for each listener.  </p>



<p>In Part I of this series, we’ll focus on the first stage — the machine learning solutions we’ve built to personalize the content for listeners’ Home pages and, specifically, the lessons we’ve learned in building, experimenting, and deploying these models. </p>



<h2>Home @ Spotify</h2>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png" alt="" width="303" height="334" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-250x276.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-768x847.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-1393x1536.png 1393w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-120x132.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home.png 1642w" sizes="(max-width: 303px) 100vw, 303px"/></figure></div>



<p>The Home page consists of cards — the square items that represent an album, playlist, etc. — and shelves — the horizontal rows that contain multiple cards. We generate personalized content for listeners’ Home pages, algorithmically curating the music and podcasts that are shown to listeners in the shelves on Home. Some content is generated via heuristics and rules and some content is manually curated by editors, while other content is generated via predictions using trained models. We currently have a number of models running in production, each one powering content curation for a different shelf, but we will be discussing three of those models in this post, including: </p>



<ul><li><strong>The Podcast Model:</strong> Predicts podcasts a listener is likely to listen to in the <em>Shows you might like</em> shelf. </li><li><strong>The Shortcuts Model: </strong>Predicts the listener’s next familiar listen in the Shortcuts feature. </li><li><strong>The Playlists Model: </strong>Predicts the playlists a new listener is likely to listen to in the <em>Try something else</em> shelf.  </li></ul>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png" alt="" width="411" height="375" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-250x228.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-768x700.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-1536x1400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-120x109.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2.png 1999w" sizes="(max-width: 411px) 100vw, 411px"/></figure></div>



<p>Since we launched our first model to recommend content on Home, we have worked to improve our ML stack and processes in order to experiment and productionize models more quickly and reliably.</p>



<h2>The road to simplicity and automation</h2>



<p>As anyone who may have contributed to operationalizing an ML model knows, moving a model from experimentation to production is no easy feat. There are numerous challenges in managing the data that goes into a model, running and tracking experiments, and monitoring and retraining models. While we have always tried to keep our ML infrastructure simple, and as close to the sources of features as possible, it has become drastically easier for our squads to deploy and maintain models now than when we started.</p>



<p>At a high level, an ML workflow can be broken down into three main phases: 1) data management, 2) experimentation, and 3) operationalization.  </p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png" alt="" width="784" height="164" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-250x52.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-768x160.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-120x25.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow.png 1351w" sizes="(max-width: 784px) 100vw, 784px"/></figure></div>



<p>It’s common to iteratively work on the training and evaluation phase until a final model version is selected as the best. This model is then deployed to production systems and can start making predictions for listeners. Similar to most production systems, models (and the services/pipelines that serve them) should be monitored closely. To keep a model up to date (which is more important for some tasks than others; more to come on this), retraining and model versioning are the last steps in our workflow. This part of our stack and workflow has had significant changes since our first model — making batch predictions (offline) of content listeners are likely to stream — to now, where all our models are served in real time. The figure below shows where our machine learning stack started and where we are now:</p>



<div><figure><img loading="lazy" width="700" height="533" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going.png 1012w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Our current ML stack automates a lot of the processes involved in maintaining models in production (with online serving): we have automated feature logging instrumented in our serving infrastructure, with both scheduled Scio pipelines to transform these features and Kubeflow pipelines to retrain weekly. We have also implemented data validation of our training and serving features (as well as validation between subsequent training datasets) to verify our features are consistent and follow the same distributions at training and inference times. In our Kubeflow pipelines, we have components that check the evaluation score and automatically push the model to production if the score is above our threshold. With this stack, we monitor and alert on the automatic data validation pipeline, as well as the online deployments of our models — allowing us to handle any issues as soon as they arise.</p>



<p>With a lot of effort and many lessons learned, our ML stack has evolved to make these processes automated and more reliable, enabling us to iterate faster to improve our models and increase our engineering productivity. </p>



<h2>How we unified training and serving data</h2>



<p>When we first start to think about a problem, we always dig into the data first — what data would be useful? What data is available? And then we take a really close look at the data that will be used for features, characterizing what is in the dataset and identifying the edge cases in the data. We feel fairly confident about the contents of the data used for our training features as well as what the transformed data looks like, but features fetched and transformed at <em>serving</em> time are an entirely different story. </p>



<h3>Batch training data and batch predictions</h3>



<p>Historically, we have had one set of infrastructure for fetching and transforming features during experimentation (training) and a different set of infrastructure for fetching and transforming features for making predictions (serving). </p>



<h3>Then we started to make online predictions (… with the wrong data)</h3>



<p>When we changed the Podcast Model from making batch offline predictions to serving in real time, we set up a new service that could support this — this new service had to fetch and transform features, make the prediction, and respond to the request. The important part here is that the feature processing and transformation was now in a different place than where the corresponding training feature processing took place. And, unfortunately, models are like black boxes, so testing the output is difficult, if not impossible. A while ago, we discovered that we had been transforming one of the model’s features slightly differently at training time than at serving time, leading to potentially degraded recommendations — and there was no way to detect this, so it continued to happen for four months.Think about this for just a second. Such a simple part of our stack — at most, a few lines of code — was doing the wrong thing and impacted the recommendations produced by our model. Our short-term fix was to simply change the one line of code in our prediction service that was causing the issue, but we knew long term that we needed to either have a single source of data for both training and serving, or we needed to ensure that data was produced and transformed the same way in both stages.</p>



<h3>One transformation implementation to rule them all</h3>



<p>Our first approach was to make any feature processing and transformation occur in the same code path, so that training and serving features would be processed identically. Taking the Shortcuts Model as an example again, our goal was to get rid of the Python service that transformed training features — this service was always running and constantly checking, on all days, to see if it was a Monday; if so, then it would request data from the necessary service (at a rate-limited 5 requests/second) and transform them into features; ideally, this would have been implemented as a pipeline, but we couldn’t schedule and orchestrate it because the process took more than 24 hours. There were many reasons we wanted to migrate away from this approach, but logging features when the only data source for features is a different service (owned by a different squad) proved difficult. Using our serving infrastructure’s feature logging capabilities, we could automatically log already transformed features, which could later be used for training. At this point, all of our features for training and serving were being transformed by code in the Java service. And we now use this feature logging for all of our models both to solve this problem, and also because it reduces the amount of additional infrastructure we need to support.  </p>



<h3>But wait, we can do more by validating our data</h3>



<p>The second approach we took to ensure our training and serving features did not differ was to use Tensorflow Data Validation (TFDV) to compare training and serving data schemas and feature distributions on a daily basis. The alerting we have added to our data validation pipeline allows us to detect significant differences in our feature sets — it uses the Chebyshev distance metric, which compares the distance between two vectors, and can help alert us to drift in training and serving features.  </p>



<p>While we knew that understanding what is in our data is crucial, we quickly learned that it’s easy to make mistakes when moving models to production because the data often uses a different processing library. We didn’t expect many data differences, but validating and alerting on issues lets us know if something changed, and how we should remediate the issue.</p>



<p>Stay tuned for Part II as we take a closer look at how we evaluate our models using offline and online metrics, why it’s so important to actually look at the recommendations we are making, and the challenges we faced in our journey to CI/CD in model retraining. </p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Annie Edmundson, Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration&#xA;</title>
      <link>https://engineering.atspotify.com/2021/10/20/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/</link>
      <description>At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infra</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 20, 2021</span>
                <span>
                    Published by Flavio Santos (Data Infrastructure Engineer) and Robert Stephenson (Senior Product Manager)                </span>
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/10/20/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/" title="Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-768x381.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-2048x1015.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-120x59.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infrastructure (EDI). Throughout this blog post we make a distinction between the internal users of the EDI, who are Spotify Engineers, Data Scientists, PMs and squads, and end users, who use Spotify as a service and audio platform.</p>



<p>In 2016, we redesigned the EDI in Google Cloud Platform (GCP) when Spotify migrated to the cloud, and we documented the journey in three blog posts (<a href="https://engineering.atspotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/" target="_blank" rel="noreferrer noopener">Part I</a>, <a href="https://engineering.atspotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/" target="_blank" rel="noreferrer noopener">Part II</a>, and <a href="https://engineering.atspotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/" target="_blank" rel="noreferrer noopener">Part III</a>). Not everything went as planned, and we wrote about our learnings from operating our cloud-native EDI in <a href="https://engineering.atspotify.com/2019/11/12/spotifys-event-delivery-life-in-the-cloud/" target="_blank" rel="noreferrer noopener">Part IV</a>. Our design was optimized to make it quick and easy for internal developers to instrument and log the data they needed. We then extended it to adapt to the General Data Protection Regulation (GDPR), we introduced streaming event delivery in addition to batch, and we brought BigQuery to our data community. We also improved operational stability and the quality of life of our on-call engineers. The peak traffic increased from 1.5M events per second to nearly 8M, and we were ready for that massive scale increase. This increased the total volume of data which we ingested daily to nearly 70TB! (Figure 1).</p>



<div><figure><img loading="lazy" width="700" height="436" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-768x478.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1.png 1480w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: Average total volume (TB) of events stored daily by our <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Extract,_transform,_load" target="_blank">ETL process</a> (after compression).</figcaption></figure></div>



<p>However, with that high adoption and traffic increase we discovered some bottlenecks. Our internal users had feature requests and needed more from the system. Now our incomplete and low-quality data was degrading the productivity of the Spotify data community. Whoops!</p>



<h2>What was hurting us?</h2>



<p>When we designed and built the initial EDI, our team had the mission statement to “provide infrastructure for teams at Spotify to reliably collect data, and make it available, safely and efficiently.” The use cases we focused on were well supported, such as music streaming and application monitoring. As other use cases started to appear, the assumptions we made when building the system had to be revisited. During three years of operating and scaling the existing EDI, we gathered a lot of feedback from our internal users and learned a lot about our limitations.</p>



<h3>Data loss</h3>



<p>Most events generated on mobile clients were sent in a fire-and-forget fashion. This might seem surprising, but because end users can enjoy Spotify while offline, there are some complications around deduplication of data that is re-sent. For example, if we detect that we are missing a data point, we don’t necessarily know if it is actually lost, or just has not arrived yet due to the user being offline, in a tunnel, or maybe having a flaky network connection. This leads to a small percentage of data loss for nearly all the data we collect, which is not acceptable for some types of data. Furthermore, this problem is compounded for datasets generated from a combination of multiple event types in order to “connect the dots” in user journeys where, for example, a single lost event can compromise the whole journey. While we had some specific client code and algorithms to reliably deliver business-critical data exactly once, it was not done in a way that we could extend to all 600+ event types that we had at that time.</p>



<h3>Control plane UX</h3>



<p>The workflow for a customer to progress from “instrumentation to insights” took far too long. Under normal circumstances it would take a customer a week to go through this workflow and get their data. One issue was that multiple components in the EDI had to be schema aware. For example, the receiver service, which is the entry point of the infrastructure, uses the schemas to validate that incoming data is well formed. Due to some tech debt, it took a few hours to propagate the schemas for this validation. This was an eternity in terms of iteration time. Since this process was so painful, some teams tried to instrument their features or services, but then gave up. Some other teams would shoehorn their data into existing data events. This led to gaps in what was instrumented, and a data-quality nightmare.</p>



<h3>Backwards compatible? Or stuck in the past?</h3>



<p>For strategic reasons, it was critical, in 2016, that we build the EDI in GCP and migrate over as quickly as possible. A key decision we took to make this happen was to stay backwards compatible to minimize the migration time. That meant we had to stick with some historical design choices that we would not have if we had built this EDI from scratch. For example:</p>



<p><strong>Tab-separated values (TSV):</strong> All data events were sent as TSV strings. The schemas were parsed and converted to Avro with a Python library created in 2007. The schema-aware tooling for parsing the TSV data was the main cause for the painful control plane UX mentioned earlier.</p>



<p><strong>Stateful services:</strong> Data events were first stored on disk and then forwarded to the EDI. This made us resilient to crashes, but made us vulnerable to data loss if a machine was taken down. Furthermore, Spotify could not take advantage of auto-scaling mechanisms or Kubernetes (without difficult workarounds) because the EDI made our service ecosystem stateful.</p>



<p><strong>Legacy perimeter:</strong> Since data events were forwarded from disk to our EDI, all events triggered by Spotify clients needed to be emitted from our perimeter servers. These servers had to keep events on disk and were tightly coupled to our legacy logging mechanism. This caused some pain to perimeter administrators and hindered architectural innovations. Besides the additional complexity in the perimeter, the shared ownership of different teams with different goals caused alignment problems.</p>



<h2>The situation</h2>



<p>We had hundreds of services sending events through a legacy EDI by logging data to disk. After being ingested by the infrastructure, events were consumed by hundreds of downstream data pipelines to produce derivative datasets (Figure 2). Our goal was to build a platform that takes advantage of the modern landscape in the cloud while also enabling legacy event types to be migrated easily. The workflow to create new events should be frictionless, while still following our data governance principles and applicable privacy laws.</p>



<div><figure><img loading="lazy" width="591" height="302" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png 591w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-250x128.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-120x61.png 120w" sizes="(max-width: 591px) 100vw, 591px"/><figcaption>Figure 2. Events produced by our internal services go through the legacy EDI and are consumed by hundreds of data pipelines.</figcaption></figure></div>



<p>Transitioning event logging to a new infrastructure would need to take into consideration the long tail that mobile app updates have. A new version of our mobile apps takes several months to gain adoption from a high percentage of Spotify end users. We knew that we would have traffic coming to both the old and new EDIs for quite some time. Moreover, events emitted from embedded devices, such as TVs and speakers, would need special treatment as some of these devices are unlikely to ever be upgraded. We call this challenge “The Long Tail Problem”.</p>



<h2>The strategy</h2>



<p>We partially solved “The Long Tail Problem” by designing a data transformation pipeline that reads events from legacy clients, converts them, and feeds them from the legacy EDI into the new infrastructure (Figure 3). Since we were breaking backwards compatibility, we took the opportunity to update our data model. The transformation to the new data model would not have all the necessary information available, so missing or inaccurate fields were expected occasionally. But since this transformation only applied to legacy clients, it would decrease as end users upgraded to the latest version of Spotify. This traffic would become negligible, eventually.</p>



<div><figure><img loading="lazy" width="700" height="355" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3.png 1286w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 3: To handle clients which had not yet upgraded to the latest version, we implemented a job to export legacy data to the new EDI and transform it to our new data model.</figcaption></figure></div>



<p>We gave data producers two options to adopt the new EDI: either redesign their instrumentation using the new data model, or stick with what they have and turn on exporting data from the legacy EDI to the new EDI. After producers onboarded, event consumers would migrate to read data from the new EDI. If producers and consumers agree to use the exporter, they would first need to update any downstream pipelines to read from the new infrastructure before making client-side changes.</p>



<h2>Get in production with real use cases ASAP</h2>



<p>In order to validate our decisions, we had to find early adopters to start producing events with the new infrastructure. We presented the advantages and explained the limitations of our alpha product to potential interested teams. It was important to be able to experiment, break, and fix issues fast and safely without worrying about affecting critical production systems or data. Setting expectations with our internal users was important so we could make breaking changes when our assumptions were wrong.</p>



<p>Next, we found at least one real use case to migrate. We were looking for something specific, since different event types have different levels of importance, timeliness requirements, and downstream dependencies. We reached out to event owners to understand how their data was being used and how we could help them migrate.</p>



<p>Given a set of eligible event types, we identified use cases that were satisfied by the limited features we had built so far. Learning which features our internal users were missing also helped prioritize our roadmap. The more features we added to the new EDI, the more event types we could onboard. We periodically revisited our design decisions and assumptions in order to identify potential problems in the new infrastructure as quickly as possible.</p>



<p>Once we had a prototype that was working with real production traffic, we solidified the interfaces and data model and helped the alpha internal users adapt to the changes (Figure 4). This enabled us to decouple the significant work of migrating the 600+ event types which were running on the legacy infrastructure, and actually building the new EDI behind the abstractions.</p>



<div><figure><img loading="lazy" width="700" height="187" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-250x67.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-768x206.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-1536x411.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-120x32.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 4: New interfaces for the prototype infrastructure, so we could concurrently migrate internal users to the new Event Delivery Infrastructure while building it.</figcaption></figure></div>



<h2>Just-in-time optimizations</h2>



<p>Prematurely optimizing is generally a bad idea without motivating metrics. We always want to be as efficient as possible, but we had to prioritize and make trade-offs. Part of the challenge was to find a good balance between the desired efficiency of our infrastructure and the features we absolutely needed to release in order to accomplish our goals.</p>



<p>We learned from the first EDI that we needed to design for our targeted service availability from the start. Since transactional data collection was not required, there was no need for 100% delivery. We instead had to determine what level of service availability was acceptable and understand the trade-offs associated with that.</p>



<h2>The design changes and decisions</h2>



<p>We have two main interfaces to the EDI. The control plane is the starting point where internal users declare their events, design their schemas, and bind them to specific SDKs. The data plane receives events sent by those SDKs, divides them by event type, and makes them available as batch datasets or data-streaming topics. The events go through several other components that deduplicate, translate to our well-designed data model, and pseudonymize personal information. The output data is reliably stored for Spotify’s data community to consume and build data pipelines. Due to the evolving needs of internal users, as well as operational overhead and scalability concerns, we needed to make changes between the old and new infrastructure.</p>



<h3>Client re-sends and new deduplication</h3>



<p>In order to reduce event loss and improve reliability, we implemented client resends. Due to connection stability and offline mode, these resends may happen immediately, within a few minutes, or maybe several days later. They may never happen! It’s actually impossible to tell if an event has been lost in transport, or if a user has used Spotify and then dropped their phone in the ocean, causing data loss. The combination of resend strategies and flaky network connections complicates things and introduces duplicate events.</p>



<p>In the old infrastructure, we only deduplicate events within a small window of hours. However, due to the significant increase of duplicates, we hit some bottlenecks and decided to redesign the job. The biggest changes in the new job are the introduction of event message identifiers, and the adoption of Google’s Dataflow processing service instead of Hadoop. The event message identifiers were used to generate lookup indices and remove duplicates. This new strategy allowed us to look back across multiple weeks.</p>



<h3>Receiver service — offline to online</h3>



<p>The legacy EDI used files on disk to store events before they were sent to a receiver service.  Spotify’s access point or other backend services would have their own availability guarantees, and we would read the data from disk eventually. In the new EDI, our receiver service needs its own availability guarantees, which was a paradigm shift in our infrastructure and for our team as <a href="https://sre.google/" target="_blank" rel="noreferrer noopener">SREs</a>. Furthermore, those files on disk were a blocker for Spotify to leverage auto-scaling fleets.</p>



<p>In the new EDI, we have the receiver service as a highly available API used by SDKs to send events. In case of a receiver service outage, events would be temporarily stored on clients and, eventually, re-sent according to a predefined retry policy.</p>



<h3>Managed Dataflow</h3>



<p>We wanted the new EDI to leverage cloud-managed services as much as possible. By rebuilding the architecture to run in the cloud, we can offload management responsibilities to Google, and our team can focus on providing additional value.</p>



<p>When building the legacy EDI, we needed to migrate a heavy Hadoop job from our on-premise cluster to the cloud. The easiest way was to run the same job on Google’s managed Hadoop solution, Dataproc, so that’s what we did. In the new EDI, the new implementation of that job uses <a href="https://spotify.github.io/scio/" target="_blank" rel="noreferrer noopener">Scio</a> (Scala API for Apache Beam) and runs on Google’s Dataflow instead. We considered Spark or Flink, but those had to run over Hadoop, which goes against our strategy to save us operational burden and cost.</p>



<p>By using Dataflow, we no longer needed to keep long-lived Hadoop clusters to execute our jobs. These clusters had to be big enough to process the largest job without issues, and were overkill for almost everything else. Maintaining these clusters was incredibly expensive. Conversely, Dataflow recycles clusters for every job and supports auto-scaling, allowing us to use and pay only for the resources we need.</p>



<h2>Wrap-up</h2>



<p>Once we decided to redesign our EDI, we evaluated new technologies and adopted new paradigms available to us in the cloud. We had been operating our old infrastructure for years, and that helped us to understand the main pain points and fragilities. We made decisions based on the technical direction of the company, the industry state of the art, and the known scalability issues with existing components. </p>



<p>We started by first designing new components for the new EDI, which we hacked together into a proof of concept and quickly evolved to a more robust prototype that could be used in production. Shipping as soon as possible was critical to validate the infrastructure end to end and catch issues fast. Having the internal users onboarded early was an important forcing function to keep quality and operational maturity high. Next, we solidified the interfaces to the prototype infrastructure and scaled up traffic by onboarding many noncritical event types. With the interfaces stable, we could improve or change out the internals without friction. This approach decoupled the mass migration from actually rebuilding the infrastructure and reduced wall-clock project time significantly.</p>



<p>As we neared the end of the migration, we had thrown out nearly all the old, obsolete infrastructure in favor of the state of the art. We successfully changed the wheels of the moving bus, and gave Spotify’s data community a smooth ride.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration&#xA;</title>
      <link>https://engineering.atspotify.com/2021/10/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/</link>
      <description>At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infra</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 20, 2021</span>
                <span>
                    Published by Flavio Santos (Data Infrastructure Engineer) and Robert Stephenson (Senior Product Manager)                </span>
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/10/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/" title="Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-768x381.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-2048x1015.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-120x59.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infrastructure (EDI). Throughout this blog post we make a distinction between the internal users of the EDI, who are Spotify Engineers, Data Scientists, PMs and squads, and end users, who use Spotify as a service and audio platform.</p>



<p>In 2016, we redesigned the EDI in Google Cloud Platform (GCP) when Spotify migrated to the cloud, and we documented the journey in three blog posts (<a href="https://engineering.atspotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/" target="_blank" rel="noreferrer noopener">Part I</a>, <a href="https://engineering.atspotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/" target="_blank" rel="noreferrer noopener">Part II</a>, and <a href="https://engineering.atspotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/" target="_blank" rel="noreferrer noopener">Part III</a>). Not everything went as planned, and we wrote about our learnings from operating our cloud-native EDI in <a href="https://engineering.atspotify.com/2019/11/12/spotifys-event-delivery-life-in-the-cloud/" target="_blank" rel="noreferrer noopener">Part IV</a>. Our design was optimized to make it quick and easy for internal developers to instrument and log the data they needed. We then extended it to adapt to the General Data Protection Regulation (GDPR), we introduced streaming event delivery in addition to batch, and we brought BigQuery to our data community. We also improved operational stability and the quality of life of our on-call engineers. The peak traffic increased from 1.5M events per second to nearly 8M, and we were ready for that massive scale increase. This increased the total volume of data which we ingested daily to nearly 70TB! (Figure 1).</p>



<div><figure><img loading="lazy" width="700" height="436" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-768x478.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1.png 1480w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: Average total volume (TB) of events stored daily by our <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Extract,_transform,_load" target="_blank">ETL process</a> (after compression).</figcaption></figure></div>



<p>However, with that high adoption and traffic increase we discovered some bottlenecks. Our internal users had feature requests and needed more from the system. Now our incomplete and low-quality data was degrading the productivity of the Spotify data community. Whoops!</p>



<h2>What was hurting us?</h2>



<p>When we designed and built the initial EDI, our team had the mission statement to “provide infrastructure for teams at Spotify to reliably collect data, and make it available, safely and efficiently.” The use cases we focused on were well supported, such as music streaming and application monitoring. As other use cases started to appear, the assumptions we made when building the system had to be revisited. During three years of operating and scaling the existing EDI, we gathered a lot of feedback from our internal users and learned a lot about our limitations.</p>



<h3>Data loss</h3>



<p>Most events generated on mobile clients were sent in a fire-and-forget fashion. This might seem surprising, but because end users can enjoy Spotify while offline, there are some complications around deduplication of data that is re-sent. For example, if we detect that we are missing a data point, we don’t necessarily know if it is actually lost, or just has not arrived yet due to the user being offline, in a tunnel, or maybe having a flaky network connection. This leads to a small percentage of data loss for nearly all the data we collect, which is not acceptable for some types of data. Furthermore, this problem is compounded for datasets generated from a combination of multiple event types in order to “connect the dots” in user journeys where, for example, a single lost event can compromise the whole journey. While we had some specific client code and algorithms to reliably deliver business-critical data exactly once, it was not done in a way that we could extend to all 600+ event types that we had at that time.</p>



<h3>Control plane UX</h3>



<p>The workflow for a customer to progress from “instrumentation to insights” took far too long. Under normal circumstances it would take a customer a week to go through this workflow and get their data. One issue was that multiple components in the EDI had to be schema aware. For example, the receiver service, which is the entry point of the infrastructure, uses the schemas to validate that incoming data is well formed. Due to some tech debt, it took a few hours to propagate the schemas for this validation. This was an eternity in terms of iteration time. Since this process was so painful, some teams tried to instrument their features or services, but then gave up. Some other teams would shoehorn their data into existing data events. This led to gaps in what was instrumented, and a data-quality nightmare.</p>



<h3>Backwards compatible? Or stuck in the past?</h3>



<p>For strategic reasons, it was critical, in 2016, that we build the EDI in GCP and migrate over as quickly as possible. A key decision we took to make this happen was to stay backwards compatible to minimize the migration time. That meant we had to stick with some historical design choices that we would not have if we had built this EDI from scratch. For example:</p>



<p><strong>Tab-separated values (TSV):</strong> All data events were sent as TSV strings. The schemas were parsed and converted to Avro with a Python library created in 2007. The schema-aware tooling for parsing the TSV data was the main cause for the painful control plane UX mentioned earlier.</p>



<p><strong>Stateful services:</strong> Data events were first stored on disk and then forwarded to the EDI. This made us resilient to crashes, but made us vulnerable to data loss if a machine was taken down. Furthermore, Spotify could not take advantage of auto-scaling mechanisms or Kubernetes (without difficult workarounds) because the EDI made our service ecosystem stateful.</p>



<p><strong>Legacy perimeter:</strong> Since data events were forwarded from disk to our EDI, all events triggered by Spotify clients needed to be emitted from our perimeter servers. These servers had to keep events on disk and were tightly coupled to our legacy logging mechanism. This caused some pain to perimeter administrators and hindered architectural innovations. Besides the additional complexity in the perimeter, the shared ownership of different teams with different goals caused alignment problems.</p>



<h2>The situation</h2>



<p>We had hundreds of services sending events through a legacy EDI by logging data to disk. After being ingested by the infrastructure, events were consumed by hundreds of downstream data pipelines to produce derivative datasets (Figure 2). Our goal was to build a platform that takes advantage of the modern landscape in the cloud while also enabling legacy event types to be migrated easily. The workflow to create new events should be frictionless, while still following our data governance principles and applicable privacy laws.</p>



<div><figure><img loading="lazy" width="591" height="302" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png 591w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-250x128.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-120x61.png 120w" sizes="(max-width: 591px) 100vw, 591px"/><figcaption>Figure 2: Events produced by our internal services go through the legacy EDI and are consumed by hundreds of data pipelines.</figcaption></figure></div>



<p>Transitioning event logging to a new infrastructure would need to take into consideration the long tail that mobile app updates have. A new version of our mobile apps takes several months to gain adoption from a high percentage of Spotify end users. We knew that we would have traffic coming to both the old and new EDIs for quite some time. Moreover, events emitted from embedded devices, such as TVs and speakers, would need special treatment as some of these devices are unlikely to ever be upgraded. We call this challenge “The Long Tail Problem”.</p>



<h2>The strategy</h2>



<p>We partially solved “The Long Tail Problem” by designing a data transformation pipeline that reads events from legacy clients, converts them, and feeds them from the legacy EDI into the new infrastructure (Figure 3). Since we were breaking backwards compatibility, we took the opportunity to update our data model. The transformation to the new data model would not have all the necessary information available, so missing or inaccurate fields were expected occasionally. But since this transformation only applied to legacy clients, it would decrease as end users upgraded to the latest version of Spotify. This traffic would become negligible, eventually.</p>



<div><figure><img loading="lazy" width="700" height="355" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3.png 1286w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 3: To handle clients which had not yet upgraded to the latest version, we implemented a job to export legacy data to the new EDI and transform it to our new data model.</figcaption></figure></div>



<p>We gave data producers two options to adopt the new EDI: either redesign their instrumentation using the new data model, or stick with what they have and turn on exporting data from the legacy EDI to the new EDI. After producers onboarded, event consumers would migrate to read data from the new EDI. If producers and consumers agree to use the exporter, they would first need to update any downstream pipelines to read from the new infrastructure before making client-side changes.</p>



<h2>Get in production with real use cases ASAP</h2>



<p>In order to validate our decisions, we had to find early adopters to start producing events with the new infrastructure. We presented the advantages and explained the limitations of our alpha product to potential interested teams. It was important to be able to experiment, break, and fix issues fast and safely without worrying about affecting critical production systems or data. Setting expectations with our internal users was important so we could make breaking changes when our assumptions were wrong.</p>



<p>Next, we found at least one real use case to migrate. We were looking for something specific, since different event types have different levels of importance, timeliness requirements, and downstream dependencies. We reached out to event owners to understand how their data was being used and how we could help them migrate.</p>



<p>Given a set of eligible event types, we identified use cases that were satisfied by the limited features we had built so far. Learning which features our internal users were missing also helped prioritize our roadmap. The more features we added to the new EDI, the more event types we could onboard. We periodically revisited our design decisions and assumptions in order to identify potential problems in the new infrastructure as quickly as possible.</p>



<p>Once we had a prototype that was working with real production traffic, we solidified the interfaces and data model and helped the alpha internal users adapt to the changes (Figure 4). This enabled us to decouple the significant work of migrating the 600+ event types which were running on the legacy infrastructure, and actually building the new EDI behind the abstractions.</p>



<div><figure><img loading="lazy" width="700" height="187" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-250x67.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-768x206.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-1536x411.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-120x32.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 4: New interfaces for the prototype infrastructure, so we could concurrently migrate internal users to the new Event Delivery Infrastructure while building it.</figcaption></figure></div>



<h2>Just-in-time optimizations</h2>



<p>Prematurely optimizing is generally a bad idea without motivating metrics. We always want to be as efficient as possible, but we had to prioritize and make trade-offs. Part of the challenge was to find a good balance between the desired efficiency of our infrastructure and the features we absolutely needed to release in order to accomplish our goals.</p>



<p>We learned from the first EDI that we needed to design for our targeted service availability from the start. Since transactional data collection was not required, there was no need for 100% delivery. We instead had to determine what level of service availability was acceptable and understand the trade-offs associated with that.</p>



<h2>The design changes and decisions</h2>



<p>We have two main interfaces to the EDI. The control plane is the starting point where internal users declare their events, design their schemas, and bind them to specific SDKs. The data plane receives events sent by those SDKs, divides them by event type, and makes them available as batch datasets or data-streaming topics. The events go through several other components that deduplicate, translate to our well-designed data model, and pseudonymize personal information. The output data is reliably stored for Spotify’s data community to consume and build data pipelines. Due to the evolving needs of internal users, as well as operational overhead and scalability concerns, we needed to make changes between the old and new infrastructure.</p>



<h3>Client re-sends and new deduplication</h3>



<p>In order to reduce event loss and improve reliability, we implemented client resends. Due to connection stability and offline mode, these resends may happen immediately, within a few minutes, or maybe several days later. They may never happen! It’s actually impossible to tell if an event has been lost in transport, or if a user has used Spotify and then dropped their phone in the ocean, causing data loss. The combination of resend strategies and flaky network connections complicates things and introduces duplicate events.</p>



<p>In the old infrastructure, we only deduplicate events within a small window of hours. However, due to the significant increase of duplicates, we hit some bottlenecks and decided to redesign the job. The biggest changes in the new job are the introduction of event message identifiers, and the adoption of Google’s Dataflow processing service instead of Hadoop. The event message identifiers were used to generate lookup indices and remove duplicates. This new strategy allowed us to look back across multiple weeks.</p>



<h3>Receiver service — offline to online</h3>



<p>The legacy EDI used files on disk to store events before they were sent to a receiver service.  Spotify’s access point or other backend services would have their own availability guarantees, and we would read the data from disk eventually. In the new EDI, our receiver service needs its own availability guarantees, which was a paradigm shift in our infrastructure and for our team as <a href="https://sre.google/" target="_blank" rel="noreferrer noopener">SREs</a>. Furthermore, those files on disk were a blocker for Spotify to leverage auto-scaling fleets.</p>



<p>In the new EDI, we have the receiver service as a highly available API used by SDKs to send events. In case of a receiver service outage, events would be temporarily stored on clients and, eventually, re-sent according to a predefined retry policy.</p>



<h3>Managed Dataflow</h3>



<p>We wanted the new EDI to leverage cloud-managed services as much as possible. By rebuilding the architecture to run in the cloud, we can offload management responsibilities to Google, and our team can focus on providing additional value.</p>



<p>When building the legacy EDI, we needed to migrate a heavy Hadoop job from our on-premise cluster to the cloud. The easiest way was to run the same job on Google’s managed Hadoop solution, Dataproc, so that’s what we did. In the new EDI, the new implementation of that job uses <a href="https://spotify.github.io/scio/" target="_blank" rel="noreferrer noopener">Scio</a> (Scala API for Apache Beam) and runs on Google’s Dataflow instead. We considered Spark or Flink, but those had to run over Hadoop, which goes against our strategy to save us operational burden and cost.</p>



<p>By using Dataflow, we no longer needed to keep long-lived Hadoop clusters to execute our jobs. These clusters had to be big enough to process the largest job without issues, and were overkill for almost everything else. Maintaining these clusters was incredibly expensive. Conversely, Dataflow recycles clusters for every job and supports auto-scaling, allowing us to use and pay only for the resources we need.</p>



<h2>Wrap-up</h2>



<p>Once we decided to redesign our EDI, we evaluated new technologies and adopted new paradigms available to us in the cloud. We had been operating our old infrastructure for years, and that helped us to understand the main pain points and fragilities. We made decisions based on the technical direction of the company, the industry state of the art, and the known scalability issues with existing components. </p>



<p>We started by first designing new components for the new EDI, which we hacked together into a proof of concept and quickly evolved to a more robust prototype that could be used in production. Shipping as soon as possible was critical to validate the infrastructure end to end and catch issues fast. Having the internal users onboarded early was an important forcing function to keep quality and operational maturity high. Next, we solidified the interfaces to the prototype infrastructure and scaled up traffic by onboarding many noncritical event types. With the interfaces stable, we could improve or change out the internals without friction. This approach decoupled the mass migration from actually rebuilding the infrastructure and reduced wall-clock project time significantly.</p>



<p>As we neared the end of the migration, we had thrown out nearly all the old, obsolete infrastructure in favor of the state of the art. We successfully changed the wheels of the moving bus, and gave Spotify’s data community a smooth ride.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration&#xA;</title>
      <link>https://engineering.atspotify.com/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/</link>
      <description>At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infra</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 20, 2021</span>
                <span>
                    Published by Flavio Santos (Data Infrastructure Engineer) and Robert Stephenson (Senior Product Manager)                </span>
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/" title="Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-768x381.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-2048x1015.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-120x59.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infrastructure (EDI). Throughout this blog post we make a distinction between the internal users of the EDI, who are Spotify Engineers, Data Scientists, PMs and squads, and end users, who use Spotify as a service and audio platform.</p>



<p>In 2016, we redesigned the EDI in Google Cloud Platform (GCP) when Spotify migrated to the cloud, and we documented the journey in three blog posts (<a href="https://engineering.atspotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/" target="_blank" rel="noreferrer noopener">Part I</a>, <a href="https://engineering.atspotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/" target="_blank" rel="noreferrer noopener">Part II</a>, and <a href="https://engineering.atspotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/" target="_blank" rel="noreferrer noopener">Part III</a>). Not everything went as planned, and we wrote about our learnings from operating our cloud-native EDI in <a href="https://engineering.atspotify.com/2019/11/12/spotifys-event-delivery-life-in-the-cloud/" target="_blank" rel="noreferrer noopener">Part IV</a>. Our design was optimized to make it quick and easy for internal developers to instrument and log the data they needed. We then extended it to adapt to the General Data Protection Regulation (GDPR), we introduced streaming event delivery in addition to batch, and we brought BigQuery to our data community. We also improved operational stability and the quality of life of our on-call engineers. The peak traffic increased from 1.5M events per second to nearly 8M, and we were ready for that massive scale increase. This increased the total volume of data which we ingested daily to nearly 70TB! (Figure 1).</p>



<div><figure><img loading="lazy" width="700" height="436" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-768x478.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1.png 1480w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: Average total volume (TB) of events stored daily by our <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Extract,_transform,_load" target="_blank">ETL process</a> (after compression).</figcaption></figure></div>



<p>However, with that high adoption and traffic increase we discovered some bottlenecks. Our internal users had feature requests and needed more from the system. Now our incomplete and low-quality data was degrading the productivity of the Spotify data community. Whoops!</p>



<h2>What was hurting us?</h2>



<p>When we designed and built the initial EDI, our team had the mission statement to “provide infrastructure for teams at Spotify to reliably collect data, and make it available, safely and efficiently.” The use cases we focused on were well supported, such as music streaming and application monitoring. As other use cases started to appear, the assumptions we made when building the system had to be revisited. During three years of operating and scaling the existing EDI, we gathered a lot of feedback from our internal users and learned a lot about our limitations.</p>



<h3>Data loss</h3>



<p>Most events generated on mobile clients were sent in a fire-and-forget fashion. This might seem surprising, but because end users can enjoy Spotify while offline, there are some complications around deduplication of data that is re-sent. For example, if we detect that we are missing a data point, we don’t necessarily know if it is actually lost, or just has not arrived yet due to the user being offline, in a tunnel, or maybe having a flaky network connection. This leads to a small percentage of data loss for nearly all the data we collect, which is not acceptable for some types of data. Furthermore, this problem is compounded for datasets generated from a combination of multiple event types in order to “connect the dots” in user journeys where, for example, a single lost event can compromise the whole journey. While we had some specific client code and algorithms to reliably deliver business-critical data exactly once, it was not done in a way that we could extend to all 600+ event types that we had at that time.</p>



<h3>Control plane UX</h3>



<p>The workflow for a customer to progress from “instrumentation to insights” took far too long. Under normal circumstances it would take a customer a week to go through this workflow and get their data. One issue was that multiple components in the EDI had to be schema aware. For example, the receiver service, which is the entry point of the infrastructure, uses the schemas to validate that incoming data is well formed. Due to some tech debt, it took a few hours to propagate the schemas for this validation. This was an eternity in terms of iteration time. Since this process was so painful, some teams tried to instrument their features or services, but then gave up. Some other teams would shoehorn their data into existing data events. This led to gaps in what was instrumented, and a data-quality nightmare.</p>



<h3>Backwards compatible? Or stuck in the past?</h3>



<p>For strategic reasons, it was critical, in 2016, that we build the EDI in GCP and migrate over as quickly as possible. A key decision we took to make this happen was to stay backwards compatible to minimize the migration time. That meant we had to stick with some historical design choices that we would not have if we had built this EDI from scratch. For example:</p>



<p><strong>Tab-separated values (TSV):</strong> All data events were sent as TSV strings. The schemas were parsed and converted to Avro with a Python library created in 2007. The schema-aware tooling for parsing the TSV data was the main cause for the painful control plane UX mentioned earlier.</p>



<p><strong>Stateful services:</strong> Data events were first stored on disk and then forwarded to the EDI. This made us resilient to crashes, but made us vulnerable to data loss if a machine was taken down. Furthermore, Spotify could not take advantage of auto-scaling mechanisms or Kubernetes (without difficult workarounds) because the EDI made our service ecosystem stateful.</p>



<p><strong>Legacy perimeter:</strong> Since data events were forwarded from disk to our EDI, all events triggered by Spotify clients needed to be emitted from our perimeter servers. These servers had to keep events on disk and were tightly coupled to our legacy logging mechanism. This caused some pain to perimeter administrators and hindered architectural innovations. Besides the additional complexity in the perimeter, the shared ownership of different teams with different goals caused alignment problems.</p>



<h2>The situation</h2>



<p>We had hundreds of services sending events through a legacy EDI by logging data to disk. After being ingested by the infrastructure, events were consumed by hundreds of downstream data pipelines to produce derivative datasets (Figure 2). Our goal was to build a platform that takes advantage of the modern landscape in the cloud while also enabling legacy event types to be migrated easily. The workflow to create new events should be frictionless, while still following our data governance principles and applicable privacy laws.</p>



<div><figure><img loading="lazy" width="591" height="302" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png 591w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-250x128.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-120x61.png 120w" sizes="(max-width: 591px) 100vw, 591px"/><figcaption>Figure 2: Events produced by our internal services go through the legacy EDI and are consumed by hundreds of data pipelines.</figcaption></figure></div>



<p>Transitioning event logging to a new infrastructure would need to take into consideration the long tail that mobile app updates have. A new version of our mobile apps takes several months to gain adoption from a high percentage of Spotify end users. We knew that we would have traffic coming to both the old and new EDIs for quite some time. Moreover, events emitted from embedded devices, such as TVs and speakers, would need special treatment as some of these devices are unlikely to ever be upgraded. We call this challenge “The Long Tail Problem”.</p>



<h2>The strategy</h2>



<p>We partially solved “The Long Tail Problem” by designing a data transformation pipeline that reads events from legacy clients, converts them, and feeds them from the legacy EDI into the new infrastructure (Figure 3). Since we were breaking backwards compatibility, we took the opportunity to update our data model. The transformation to the new data model would not have all the necessary information available, so missing or inaccurate fields were expected occasionally. But since this transformation only applied to legacy clients, it would decrease as end users upgraded to the latest version of Spotify. This traffic would become negligible, eventually.</p>



<div><figure><img loading="lazy" width="700" height="355" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3.png 1286w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 3: To handle clients which had not yet upgraded to the latest version, we implemented a job to export legacy data to the new EDI and transform it to our new data model.</figcaption></figure></div>



<p>We gave data producers two options to adopt the new EDI: either redesign their instrumentation using the new data model, or stick with what they have and turn on exporting data from the legacy EDI to the new EDI. After producers onboarded, event consumers would migrate to read data from the new EDI. If producers and consumers agree to use the exporter, they would first need to update any downstream pipelines to read from the new infrastructure before making client-side changes.</p>



<h2>Get in production with real use cases ASAP</h2>



<p>In order to validate our decisions, we had to find early adopters to start producing events with the new infrastructure. We presented the advantages and explained the limitations of our alpha product to potential interested teams. It was important to be able to experiment, break, and fix issues fast and safely without worrying about affecting critical production systems or data. Setting expectations with our internal users was important so we could make breaking changes when our assumptions were wrong.</p>



<p>Next, we found at least one real use case to migrate. We were looking for something specific, since different event types have different levels of importance, timeliness requirements, and downstream dependencies. We reached out to event owners to understand how their data was being used and how we could help them migrate.</p>



<p>Given a set of eligible event types, we identified use cases that were satisfied by the limited features we had built so far. Learning which features our internal users were missing also helped prioritize our roadmap. The more features we added to the new EDI, the more event types we could onboard. We periodically revisited our design decisions and assumptions in order to identify potential problems in the new infrastructure as quickly as possible.</p>



<p>Once we had a prototype that was working with real production traffic, we solidified the interfaces and data model and helped the alpha internal users adapt to the changes (Figure 4). This enabled us to decouple the significant work of migrating the 600+ event types which were running on the legacy infrastructure, and actually building the new EDI behind the abstractions.</p>



<div><figure><img loading="lazy" width="700" height="187" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-250x67.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-768x206.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-1536x411.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-120x32.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 4: New interfaces for the prototype infrastructure, so we could concurrently migrate internal users to the new Event Delivery Infrastructure while building it.</figcaption></figure></div>



<h2>Just-in-time optimizations</h2>



<p>Prematurely optimizing is generally a bad idea without motivating metrics. We always want to be as efficient as possible, but we had to prioritize and make trade-offs. Part of the challenge was to find a good balance between the desired efficiency of our infrastructure and the features we absolutely needed to release in order to accomplish our goals.</p>



<p>We learned from the first EDI that we needed to design for our targeted service availability from the start. Since transactional data collection was not required, there was no need for 100% delivery. We instead had to determine what level of service availability was acceptable and understand the trade-offs associated with that.</p>



<h2>The design changes and decisions</h2>



<p>We have two main interfaces to the EDI. The control plane is the starting point where internal users declare their events, design their schemas, and bind them to specific SDKs. The data plane receives events sent by those SDKs, divides them by event type, and makes them available as batch datasets or data-streaming topics. The events go through several other components that deduplicate, translate to our well-designed data model, and pseudonymize personal information. The output data is reliably stored for Spotify’s data community to consume and build data pipelines. Due to the evolving needs of internal users, as well as operational overhead and scalability concerns, we needed to make changes between the old and new infrastructure.</p>



<h3>Client re-sends and new deduplication</h3>



<p>In order to reduce event loss and improve reliability, we implemented client resends. Due to connection stability and offline mode, these resends may happen immediately, within a few minutes, or maybe several days later. They may never happen! It’s actually impossible to tell if an event has been lost in transport, or if a user has used Spotify and then dropped their phone in the ocean, causing data loss. The combination of resend strategies and flaky network connections complicates things and introduces duplicate events.</p>



<p>In the old infrastructure, we only deduplicate events within a small window of hours. However, due to the significant increase of duplicates, we hit some bottlenecks and decided to redesign the job. The biggest changes in the new job are the introduction of event message identifiers, and the adoption of Google’s Dataflow processing service instead of Hadoop. The event message identifiers were used to generate lookup indices and remove duplicates. This new strategy allowed us to look back across multiple weeks.</p>



<h3>Receiver service — offline to online</h3>



<p>The legacy EDI used files on disk to store events before they were sent to a receiver service.  Spotify’s access point or other backend services would have their own availability guarantees, and we would read the data from disk eventually. In the new EDI, our receiver service needs its own availability guarantees, which was a paradigm shift in our infrastructure and for our team as <a href="https://sre.google/" target="_blank" rel="noreferrer noopener">SREs</a>. Furthermore, those files on disk were a blocker for Spotify to leverage auto-scaling fleets.</p>



<p>In the new EDI, we have the receiver service as a highly available API used by SDKs to send events. In case of a receiver service outage, events would be temporarily stored on clients and, eventually, re-sent according to a predefined retry policy.</p>



<h3>Managed Dataflow</h3>



<p>We wanted the new EDI to leverage cloud-managed services as much as possible. By rebuilding the architecture to run in the cloud, we can offload management responsibilities to Google, and our team can focus on providing additional value.</p>



<p>When building the legacy EDI, we needed to migrate a heavy Hadoop job from our on-premise cluster to the cloud. The easiest way was to run the same job on Google’s managed Hadoop solution, Dataproc, so that’s what we did. In the new EDI, the new implementation of that job uses <a href="https://spotify.github.io/scio/" target="_blank" rel="noreferrer noopener">Scio</a> (Scala API for Apache Beam) and runs on Google’s Dataflow instead. We considered Spark or Flink, but those had to run over Hadoop, which goes against our strategy to save us operational burden and cost.</p>



<p>By using Dataflow, we no longer needed to keep long-lived Hadoop clusters to execute our jobs. These clusters had to be big enough to process the largest job without issues, and were overkill for almost everything else. Maintaining these clusters was incredibly expensive. Conversely, Dataflow recycles clusters for every job and supports auto-scaling, allowing us to use and pay only for the resources we need.</p>



<h2>Wrap-up</h2>



<p>Once we decided to redesign our EDI, we evaluated new technologies and adopted new paradigms available to us in the cloud. We had been operating our old infrastructure for years, and that helped us to understand the main pain points and fragilities. We made decisions based on the technical direction of the company, the industry state of the art, and the known scalability issues with existing components. </p>



<p>We started by first designing new components for the new EDI, which we hacked together into a proof of concept and quickly evolved to a more robust prototype that could be used in production. Shipping as soon as possible was critical to validate the infrastructure end to end and catch issues fast. Having the internal users onboarded early was an important forcing function to keep quality and operational maturity high. Next, we solidified the interfaces to the prototype infrastructure and scaled up traffic by onboarding many noncritical event types. With the interfaces stable, we could improve or change out the internals without friction. This approach decoupled the mass migration from actually rebuilding the infrastructure and reduced wall-clock project time significantly.</p>



<p>As we neared the end of the migration, we had thrown out nearly all the old, obsolete infrastructure in favor of the state of the art. We successfully changed the wheels of the moving bus, and gave Spotify’s data community a smooth ride.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration&#xA;</title>
      <link>https://engineering.atspotify.com/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/</link>
      <description>At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infra</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 20, 2021</span>
                <span>
                    Published by Flavio Santos (Data Infrastructure Engineer) and Robert Stephenson (Senior Product Manager)                </span>
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/" title="Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-768x381.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-2048x1015.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-120x59.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infrastructure (EDI). Throughout this blog post we make a distinction between the internal users of the EDI, who are Spotify Engineers, Data Scientists, PMs and squads, and end users, who use Spotify as a service and audio platform.</p>



<p>In 2016, we redesigned the EDI in Google Cloud Platform (GCP) when Spotify migrated to the cloud, and we documented the journey in three blog posts (<a href="https://engineering.atspotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/" target="_blank" rel="noreferrer noopener">Part I</a>, <a href="https://engineering.atspotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/" target="_blank" rel="noreferrer noopener">Part II</a>, and <a href="https://engineering.atspotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/" target="_blank" rel="noreferrer noopener">Part III</a>). Not everything went as planned, and we wrote about our learnings from operating our cloud-native EDI in <a href="https://engineering.atspotify.com/2019/11/12/spotifys-event-delivery-life-in-the-cloud/" target="_blank" rel="noreferrer noopener">Part IV</a>. Our design was optimized to make it quick and easy for internal developers to instrument and log the data they needed. We then extended it to adapt to the General Data Protection Regulation (GDPR), we introduced streaming event delivery in addition to batch, and we brought BigQuery to our data community. We also improved operational stability and the quality of life of our on-call engineers. The peak traffic increased from 1.5M events per second to nearly 8M, and we were ready for that massive scale increase. This increased the total volume of data which we ingested daily to nearly 70TB! (Figure 1).</p>



<div><figure><img loading="lazy" width="700" height="436" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-768x478.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1.png 1480w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: Average total volume (TB) of events stored daily by our <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Extract,_transform,_load" target="_blank">ETL process</a> (after compression).</figcaption></figure></div>



<p>However, with that high adoption and traffic increase we discovered some bottlenecks. Our internal users had feature requests and needed more from the system. Now our incomplete and low-quality data was degrading the productivity of the Spotify data community. Whoops!</p>



<h2>What was hurting us?</h2>



<p>When we designed and built the initial EDI, our team had the mission statement to “provide infrastructure for teams at Spotify to reliably collect data, and make it available, safely and efficiently.” The use cases we focused on were well supported, such as music streaming and application monitoring. As other use cases started to appear, the assumptions we made when building the system had to be revisited. During three years of operating and scaling the existing EDI, we gathered a lot of feedback from our internal users and learned a lot about our limitations.</p>



<h3>Data loss</h3>



<p>Most events generated on mobile clients were sent in a fire-and-forget fashion. This might seem surprising, but because end users can enjoy Spotify while offline, there are some complications around deduplication of data that is re-sent. For example, if we detect that we are missing a data point, we don’t necessarily know if it is actually lost, or just has not arrived yet due to the user being offline, in a tunnel, or maybe having a flaky network connection. This leads to a small percentage of data loss for nearly all the data we collect, which is not acceptable for some types of data. Furthermore, this problem is compounded for datasets generated from a combination of multiple event types in order to “connect the dots” in user journeys where, for example, a single lost event can compromise the whole journey. While we had some specific client code and algorithms to reliably deliver business-critical data exactly once, it was not done in a way that we could extend to all 600+ event types that we had at that time.</p>



<h3>Control plane UX</h3>



<p>The workflow for a customer to progress from “instrumentation to insights” took far too long. Under normal circumstances it would take a customer a week to go through this workflow and get their data. One issue was that multiple components in the EDI had to be schema aware. For example, the receiver service, which is the entry point of the infrastructure, uses the schemas to validate that incoming data is well formed. Due to some tech debt, it took a few hours to propagate the schemas for this validation. This was an eternity in terms of iteration time. Since this process was so painful, some teams tried to instrument their features or services, but then gave up. Some other teams would shoehorn their data into existing data events. This led to gaps in what was instrumented, and a data-quality nightmare.</p>



<h3>Backwards compatible? Or stuck in the past?</h3>



<p>For strategic reasons, it was critical, in 2016, that we build the EDI in GCP and migrate over as quickly as possible. A key decision we took to make this happen was to stay backwards compatible to minimize the migration time. That meant we had to stick with some historical design choices that we would not have if we had built this EDI from scratch. For example:</p>



<p><strong>Tab-separated values (TSV):</strong> All data events were sent as TSV strings. The schemas were parsed and converted to Avro with a Python library created in 2007. The schema-aware tooling for parsing the TSV data was the main cause for the painful control plane UX mentioned earlier.</p>



<p><strong>Stateful services:</strong> Data events were first stored on disk and then forwarded to the EDI. This made us resilient to crashes, but made us vulnerable to data loss if a machine was taken down. Furthermore, Spotify could not take advantage of auto-scaling mechanisms or Kubernetes (without difficult workarounds) because the EDI made our service ecosystem stateful.</p>



<p><strong>Legacy perimeter:</strong> Since data events were forwarded from disk to our EDI, all events triggered by Spotify clients needed to be emitted from our perimeter servers. These servers had to keep events on disk and were tightly coupled to our legacy logging mechanism. This caused some pain to perimeter administrators and hindered architectural innovations. Besides the additional complexity in the perimeter, the shared ownership of different teams with different goals caused alignment problems.</p>



<h2>The situation</h2>



<p>We had hundreds of services sending events through a legacy EDI by logging data to disk. After being ingested by the infrastructure, events were consumed by hundreds of downstream data pipelines to produce derivative datasets (Figure 2). Our goal was to build a platform that takes advantage of the modern landscape in the cloud while also enabling legacy event types to be migrated easily. The workflow to create new events should be frictionless, while still following our data governance principles and applicable privacy laws.</p>



<div><figure><img loading="lazy" width="591" height="302" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png 591w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-250x128.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-120x61.png 120w" sizes="(max-width: 591px) 100vw, 591px"/><figcaption>Figure 2: Events produced by our internal services go through the legacy EDI and are consumed by hundreds of data pipelines.</figcaption></figure></div>



<p>Transitioning event logging to a new infrastructure would need to take into consideration the long tail that mobile app updates have. A new version of our mobile apps takes several months to gain adoption from a high percentage of Spotify end users. We knew that we would have traffic coming to both the old and new EDIs for quite some time. Moreover, events emitted from embedded devices, such as TVs and speakers, would need special treatment as some of these devices are unlikely to ever be upgraded. We call this challenge “The Long Tail Problem”.</p>



<h2>The strategy</h2>



<p>We partially solved “The Long Tail Problem” by designing a data transformation pipeline that reads events from legacy clients, converts them, and feeds them from the legacy EDI into the new infrastructure (Figure 3). Since we were breaking backwards compatibility, we took the opportunity to update our data model. The transformation to the new data model would not have all the necessary information available, so missing or inaccurate fields were expected occasionally. But since this transformation only applied to legacy clients, it would decrease as end users upgraded to the latest version of Spotify. This traffic would become negligible, eventually.</p>



<div><figure><img loading="lazy" width="700" height="355" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3.png 1286w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 3: To handle clients which had not yet upgraded to the latest version, we implemented a job to export legacy data to the new EDI and transform it to our new data model.</figcaption></figure></div>



<p>We gave data producers two options to adopt the new EDI: either redesign their instrumentation using the new data model, or stick with what they have and turn on exporting data from the legacy EDI to the new EDI. After producers onboarded, event consumers would migrate to read data from the new EDI. If producers and consumers agree to use the exporter, they would first need to update any downstream pipelines to read from the new infrastructure before making client-side changes.</p>



<h2>Get in production with real use cases ASAP</h2>



<p>In order to validate our decisions, we had to find early adopters to start producing events with the new infrastructure. We presented the advantages and explained the limitations of our alpha product to potential interested teams. It was important to be able to experiment, break, and fix issues fast and safely without worrying about affecting critical production systems or data. Setting expectations with our internal users was important so we could make breaking changes when our assumptions were wrong.</p>



<p>Next, we found at least one real use case to migrate. We were looking for something specific, since different event types have different levels of importance, timeliness requirements, and downstream dependencies. We reached out to event owners to understand how their data was being used and how we could help them migrate.</p>



<p>Given a set of eligible event types, we identified use cases that were satisfied by the limited features we had built so far. Learning which features our internal users were missing also helped prioritize our roadmap. The more features we added to the new EDI, the more event types we could onboard. We periodically revisited our design decisions and assumptions in order to identify potential problems in the new infrastructure as quickly as possible.</p>



<p>Once we had a prototype that was working with real production traffic, we solidified the interfaces and data model and helped the alpha internal users adapt to the changes (Figure 4). This enabled us to decouple the significant work of migrating the 600+ event types which were running on the legacy infrastructure, and actually building the new EDI behind the abstractions.</p>



<div><figure><img loading="lazy" width="700" height="187" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-250x67.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-768x206.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-1536x411.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-120x32.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 4: New interfaces for the prototype infrastructure, so we could concurrently migrate internal users to the new Event Delivery Infrastructure while building it.</figcaption></figure></div>



<h2>Just-in-time optimizations</h2>



<p>Prematurely optimizing is generally a bad idea without motivating metrics. We always want to be as efficient as possible, but we had to prioritize and make trade-offs. Part of the challenge was to find a good balance between the desired efficiency of our infrastructure and the features we absolutely needed to release in order to accomplish our goals.</p>



<p>We learned from the first EDI that we needed to design for our targeted service availability from the start. Since transactional data collection was not required, there was no need for 100% delivery. We instead had to determine what level of service availability was acceptable and understand the trade-offs associated with that.</p>



<h2>The design changes and decisions</h2>



<p>We have two main interfaces to the EDI. The control plane is the starting point where internal users declare their events, design their schemas, and bind them to specific SDKs. The data plane receives events sent by those SDKs, divides them by event type, and makes them available as batch datasets or data-streaming topics. The events go through several other components that deduplicate, translate to our well-designed data model, and pseudonymize personal information. The output data is reliably stored for Spotify’s data community to consume and build data pipelines. Due to the evolving needs of internal users, as well as operational overhead and scalability concerns, we needed to make changes between the old and new infrastructure.</p>



<h3>Client re-sends and new deduplication</h3>



<p>In order to reduce event loss and improve reliability, we implemented client resends. Due to connection stability and offline mode, these resends may happen immediately, within a few minutes, or maybe several days later. They may never happen! It’s actually impossible to tell if an event has been lost in transport, or if a user has used Spotify and then dropped their phone in the ocean, causing data loss. The combination of resend strategies and flaky network connections complicates things and introduces duplicate events.</p>



<p>In the old infrastructure, we only deduplicate events within a small window of hours. However, due to the significant increase of duplicates, we hit some bottlenecks and decided to redesign the job. The biggest changes in the new job are the introduction of event message identifiers, and the adoption of Google’s Dataflow processing service instead of Hadoop. The event message identifiers were used to generate lookup indices and remove duplicates. This new strategy allowed us to look back across multiple weeks.</p>



<h3>Receiver service — offline to online</h3>



<p>The legacy EDI used files on disk to store events before they were sent to a receiver service.  Spotify’s access point or other backend services would have their own availability guarantees, and we would read the data from disk eventually. In the new EDI, our receiver service needs its own availability guarantees, which was a paradigm shift in our infrastructure and for our team as <a href="https://sre.google/" target="_blank" rel="noreferrer noopener">SREs</a>. Furthermore, those files on disk were a blocker for Spotify to leverage auto-scaling fleets.</p>



<p>In the new EDI, we have the receiver service as a highly available API used by SDKs to send events. In case of a receiver service outage, events would be temporarily stored on clients and, eventually, re-sent according to a predefined retry policy.</p>



<h3>Managed Dataflow</h3>



<p>We wanted the new EDI to leverage cloud-managed services as much as possible. By rebuilding the architecture to run in the cloud, we can offload management responsibilities to Google, and our team can focus on providing additional value.</p>



<p>When building the legacy EDI, we needed to migrate a heavy Hadoop job from our on-premise cluster to the cloud. The easiest way was to run the same job on Google’s managed Hadoop solution, Dataproc, so that’s what we did. In the new EDI, the new implementation of that job uses <a href="https://spotify.github.io/scio/" target="_blank" rel="noreferrer noopener">Scio</a> (Scala API for Apache Beam) and runs on Google’s Dataflow instead. We considered Spark or Flink, but those had to run over Hadoop, which goes against our strategy to save us operational burden and cost.</p>



<p>By using Dataflow, we no longer needed to keep long-lived Hadoop clusters to execute our jobs. These clusters had to be big enough to process the largest job without issues, and were overkill for almost everything else. Maintaining these clusters was incredibly expensive. Conversely, Dataflow recycles clusters for every job and supports auto-scaling, allowing us to use and pay only for the resources we need.</p>



<h2>Wrap-up</h2>



<p>Once we decided to redesign our EDI, we evaluated new technologies and adopted new paradigms available to us in the cloud. We had been operating our old infrastructure for years, and that helped us to understand the main pain points and fragilities. We made decisions based on the technical direction of the company, the industry state of the art, and the known scalability issues with existing components. </p>



<p>We started by first designing new components for the new EDI, which we hacked together into a proof of concept and quickly evolved to a more robust prototype that could be used in production. Shipping as soon as possible was critical to validate the infrastructure end to end and catch issues fast. Having the internal users onboarded early was an important forcing function to keep quality and operational maturity high. Next, we solidified the interfaces to the prototype infrastructure and scaled up traffic by onboarding many noncritical event types. With the interfaces stable, we could improve or change out the internals without friction. This approach decoupled the mass migration from actually rebuilding the infrastructure and reduced wall-clock project time significantly.</p>



<p>As we neared the end of the migration, we had thrown out nearly all the old, obsolete infrastructure in favor of the state of the art. We successfully changed the wheels of the moving bus, and gave Spotify’s data community a smooth ride.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: Three Lessons We Learned from Developing the Mobile App&#xA;</title>
      <link>https://engineering.atspotify.com/2021/10/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/</link>
      <description>TL;DR Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we trave</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 5, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/10/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/" title="A Product Story: Three Lessons We Learned from Developing the Mobile App">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we travel with our music forever.</p>



<p>In <a href="https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3" target="_blank" rel="noreferrer noopener">Episode 02</a> of our podcast series, <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener"><em>Spotify: A Product Story</em></a><em>,</em> host and Chief R&amp;D Officer Gustav Söderström chats with the engineers, executives, and other Spotifiers who helped make the mobile streaming revolution possible. Why was it so difficult to create Spotify for mobile even after we created the desktop app? Why were Spotify’s licensing deals such a game changer for us? Keep reading to find out the answers, and don’t forget to <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">listen to the podcast</a> itself to learn even more.</p>



<h2><strong>What’s user research really for?</strong></h2>



<p>When we first released the Spotify desktop app in Sweden in 2008, the idea of creating an on-the-go version of that same experience seemed out of the question. After all, Spotify didn’t have access to iPods or the other dedicated digital music players, and besides, at the time, most phones were equipped with only enough memory to hold a handful of songs.</p>



<p>Then something happened. Smartphones happened. The iPhone happened. Suddenly the idea of creating a piece of software that people could carry around with them everywhere didn’t seem so far-fetched.</p>



<p>Though it was theoretically possible to create a mobile edition of Spotify, there was still one major issue. A free, offline music app, one that could truly compete with the already-offline MP3 players, just wasn’t in the cards. First off, it couldn’t be free — you can’t click on ads without an internet connection — so it would have to be a paid service. Plus, our listeners already had access to all the music they wanted through piracy. So, once again, we had to ask ourselves something: Why would anyone pay for Spotify? How do you charge for nothing? </p>



<p>And indeed, when we surveyed our users, we discovered that $10 per month — the number we settled on after months of licensing negotiations with labels (more on those in a bit) — was a number literally no one said they would pay.</p>



<p>Well, at least they <em>said</em> that. See, user research is great, but you have to know what it’s really there for. That was our <strong>first lesson:</strong> <strong>user research is for understanding what people </strong><strong><em>think</em></strong><strong> they will do, not what they will actually do.</strong></p>



<p>There’s a reason for that, as Gustav points out.</p>



<blockquote><p><strong>Gustav:</strong> Humans are not very good at predicting our own future behavior, especially when it comes to what we are prepared to pay for something like convenience. We just can’t imagine our future selves being that lazy.</p></blockquote>



<p>Exactly. In other words, just like we discovered while creating the desktop app, convenience is key. Of course people don’t think they would pay for something they were already getting for free — but they just might if it’s a lot more convenient than what they’re using. At least, that’s what we were betting on.</p>



<p>We just needed to find another magic trick.</p>



<h2><strong>Beware your optimization bias</strong></h2>



<p>That was easier said than done, though. Setting aside the business side of things, actually building a mobile experience convenient enough to convert music pirates was a tall order, one without an obvious solution.</p>



<p>Back then, in the early days of Spotify, the desktop app may have technically been able to run on the iPhone, but that didn’t mean it was built for it. When Gustav asked Mattias Arrelid, one of Spotify’s early Mac developers, what the challenges were in translating the desktop app into a great mobile product, two things sprang immediately to Mattias’ mind:</p>



<ol><li>“One [challenge] is that … a phone doesn’t have a constant power supply. For example, it doesn’t have a constant high-bandwidth network connection. Spotify was, at that time, at least leveraging peer to peer … If you do that on the phone, you basically force it to keep the radio on quite a bit. And that is draining in terms of battery.”</li></ol>



<ol start="2"><li>“The other one is data. [When] you had Spotify running on the desktop, you relied on some kind of data connection that [you] don’t really need to care about … On the phone, you had many contracts that had limited bandwidth included in the monthly allowance. So you really wanted to be careful about your consumption of that.”</li></ol>



<p>It turns out by developing the desktop app to be as efficient as possible, we were actually developing something that was <em>only</em> efficient as a desktop app. Concerns like bandwidth and power consumption never factored into the equation, since you don’t need to worry about them when you’re a desktop user. And that’s where <strong>the second lesson</strong> comes in: <strong>whether you realize it or not, you’re always optimizing for something.</strong> This isn’t always a bad thing — after all, you’re still optimizing — but you are unwittingly creating blind spots for you and your product, ones that can hamper your growth down the road.</p>



<p>One of the solutions Mattias and the engineering team came up with to improve battery performance involved changing up the codec we used to encode audio. (<a href="https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3" target="_blank" rel="noreferrer noopener">Dive into the full episode</a> to hear all the details about why.) But changing up our codec didn’t just have technical ramifications — it had business implications as well, implications that dovetail nicely with our final lesson.</p>



<h2><strong>Your tech is only half of the story</strong></h2>



<p>Spotify didn’t invent streaming, and it was only a matter of time until other companies could start to pull off a service similar to what we were offering. What we needed, in other words, was a breakthrough in our business strategy as well.</p>



<p>Enter licensing. Contrary to what you may expect, there isn’t a one-size-fits-all codec license for streaming songs. Different licenses are required for almost every use case — say, streaming on desktop versus streaming on mobile. This meant that hammering out licensing deals with record labels could end up being a lengthy, complex process. Rather than dealing with these complications, we saw plenty of other companies kick that sort of nitty-gritty work down the road, focusing instead on shipping code and getting to market quicker.</p>



<p>But to us, this wasn’t a sustainable strategy. So rather than take the same route, Spotify slowed things down, sweating the details in the negotiations for months on end, making sure that we had licenses that no one else had. That way, we could guarantee that we hit the ground running when we introduced a paid mobile tier. Here’s Spotify’s first general counsel, Petra Hansson, on the matter:</p>



<blockquote><p><strong>Petra:</strong> We spent a lot of time getting those licenses right. Because, you know, back in the day, the norm was more to just sign any licenses and then, you know, try and flip the companies, so it became someone else’s problem or companies would go belly up. So we were sort of hell-bent. And that’s one of the reasons why I joined [Spotify], because what I really liked about the company was that [co-founder and CEO Daniel Ek]’s vision was always to build something long-term and sustainable.</p></blockquote>



<p>As it so happens, truly great product development is more than just a cool new feature or even a breakthrough new technology. <strong>Our third lesson is that</strong> <strong>truly great product development, in short, almost always combines technological innovation with business innovation.</strong></p>



<p>That’s another three lessons down, but there are plenty more to go. The podcast series <em>Spotify: A Product Story</em> shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join podcast host and Chief R&amp;D Officer Gustav Söderström, and <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">check out all the episodes right here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: Three Lessons We Learned from Developing the Mobile App&#xA;</title>
      <link>https://engineering.atspotify.com/2021/10/05/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/</link>
      <description>TL;DR Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we trave</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 5, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/10/05/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/" title="A Product Story: Three Lessons We Learned from Developing the Mobile App">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png" alt="" loading="lazy" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we travel with our music forever.</p>



<p>In <a href="https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3" target="_blank" rel="noreferrer noopener">Episode 02</a> of our podcast series, <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener"><em>Spotify: A Product Story</em></a><em>,</em> host and Chief R&amp;D Officer Gustav Söderström chats with the engineers, executives, and other Spotifiers who helped make the mobile streaming revolution possible. Why was it so difficult to create Spotify for mobile even after we created the desktop app? Why were Spotify’s licensing deals such a game changer for us? Keep reading to find out the answers, and don’t forget to <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">listen to the podcast</a> itself to learn even more.</p>



<h2><strong>What’s user research really for?</strong></h2>



<p>When we first released the Spotify desktop app in Sweden in 2008, the idea of creating an on-the-go version of that same experience seemed out of the question. After all, Spotify didn’t have access to iPods or the other dedicated digital music players, and besides, at the time, most phones were equipped with only enough memory to hold a handful of songs.</p>



<p>Then something happened. Smartphones happened. The iPhone happened. Suddenly the idea of creating a piece of software that people could carry around with them everywhere didn’t seem so far-fetched.</p>



<p>Though it was theoretically possible to create a mobile edition of Spotify, there was still one major issue. A free, offline music app, one that could truly compete with the already-offline MP3 players, just wasn’t in the cards. First off, it couldn’t be free — you can’t click on ads without an internet connection — so it would have to be a paid service. Plus, our listeners already had access to all the music they wanted through piracy. So, once again, we had to ask ourselves something: Why would anyone pay for Spotify? How do you charge for nothing? </p>



<p>And indeed, when we surveyed our users, we discovered that $10 per month — the number we settled on after months of licensing negotiations with labels (more on those in a bit) — was a number literally no one said they would pay.</p>



<p>Well, at least they <em>said</em> that. See, user research is great, but you have to know what it’s really there for. That was our <strong>first lesson:</strong> <strong>user research is for understanding what people </strong><strong><em>think</em></strong><strong> they will do, not what they will actually do.</strong></p>



<p>There’s a reason for that, as Gustav points out.</p>



<blockquote><p><strong>Gustav:</strong> Humans are not very good at predicting our own future behavior, especially when it comes to what we are prepared to pay for something like convenience. We just can’t imagine our future selves being that lazy.</p></blockquote>



<p>Exactly. In other words, just like we discovered while creating the desktop app, convenience is key. Of course people don’t think they would pay for something they were already getting for free — but they just might if it’s a lot more convenient than what they’re using. At least, that’s what we were betting on.</p>



<p>We just needed to find another magic trick.</p>



<h2><strong>Beware your optimization bias</strong></h2>



<p>That was easier said than done, though. Setting aside the business side of things, actually building a mobile experience convenient enough to convert music pirates was a tall order, one without an obvious solution.</p>



<p>Back then, in the early days of Spotify, the desktop app may have technically been able to run on the iPhone, but that didn’t mean it was built for it. When Gustav asked Mattias Arrelid, one of Spotify’s early Mac developers, what the challenges were in translating the desktop app into a great mobile product, two things sprang immediately to Mattias’ mind:</p>



<ol><li>“One [challenge] is that … a phone doesn’t have a constant power supply. For example, it doesn’t have a constant high-bandwidth network connection. Spotify was, at that time, at least leveraging peer to peer … If you do that on the phone, you basically force it to keep the radio on quite a bit. And that is draining in terms of battery.”</li></ol>



<ol start="2"><li>“The other one is data. [When] you had Spotify running on the desktop, you relied on some kind of data connection that [you] don’t really need to care about … On the phone, you had many contracts that had limited bandwidth included in the monthly allowance. So you really wanted to be careful about your consumption of that.”</li></ol>



<p>It turns out by developing the desktop app to be as efficient as possible, we were actually developing something that was <em>only</em> efficient as a desktop app. Concerns like bandwidth and power consumption never factored into the equation, since you don’t need to worry about them when you’re a desktop user. And that’s where <strong>the second lesson</strong> comes in: <strong>whether you realize it or not, you’re always optimizing for something.</strong> This isn’t always a bad thing — after all, you’re still optimizing — but you are unwittingly creating blind spots for you and your product, ones that can hamper your growth down the road.</p>



<p>One of the solutions Mattias and the engineering team came up with to improve battery performance involved changing up the codec we used to encode audio. (<a href="https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3" target="_blank" rel="noreferrer noopener">Dive into the full episode</a> to hear all the details about why.) But changing up our codec didn’t just have technical ramifications — it had business implications as well, implications that dovetail nicely with our final lesson.</p>



<h2><strong>Your tech is only half of the story</strong></h2>



<p>Spotify didn’t invent streaming, and it was only a matter of time until other companies could start to pull off a service similar to what we were offering. What we needed, in other words, was a breakthrough in our business strategy as well.</p>



<p>Enter licensing. Contrary to what you may expect, there isn’t a one-size-fits-all codec license for streaming songs. Different licenses are required for almost every use case — say, streaming on desktop versus streaming on mobile. This meant that hammering out licensing deals with record labels could end up being a lengthy, complex process. Rather than dealing with these complications, we saw plenty of other companies kick that sort of nitty-gritty work down the road, focusing instead on shipping code and getting to market quicker.</p>



<p>But to us, this wasn’t a sustainable strategy. So rather than take the same route, Spotify slowed things down, sweating the details in the negotiations for months on end, making sure that we had licenses that no one else had. That way, we could guarantee that we hit the ground running when we introduced a paid mobile tier. Here’s Spotify’s first general counsel, Petra Hansson, on the matter:</p>



<blockquote><p><strong>Petra:</strong> We spent a lot of time getting those licenses right. Because, you know, back in the day, the norm was more to just sign any licenses and then, you know, try and flip the companies, so it became someone else’s problem or companies would go belly up. So we were sort of hell-bent. And that’s one of the reasons why I joined [Spotify], because what I really liked about the company was that [co-founder and CEO Daniel Ek]’s vision was always to build something long-term and sustainable.</p></blockquote>



<p>As it so happens, truly great product development is more than just a cool new feature or even a breakthrough new technology. <strong>Our third lesson is that</strong> <strong>truly great product development, in short, almost always combines technological innovation with business innovation.</strong></p>



<p>That’s another three lessons down, but there are plenty more to go. The podcast series <em>Spotify: A Product Story</em> shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join podcast host and Chief R&amp;D Officer Gustav Söderström, and <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">check out all the episodes right here</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: Three Lessons We Learned from Developing the Mobile App&#xA;</title>
      <link>https://engineering.atspotify.com/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/</link>
      <description>TL;DR Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we trave</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 5, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/" title="A Product Story: Three Lessons We Learned from Developing the Mobile App">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we travel with our music forever.</p>



<p>In <a href="https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3" target="_blank" rel="noreferrer noopener">Episode 02</a> of our podcast series, <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener"><em>Spotify: A Product Story</em></a><em>,</em> host and Chief R&amp;D Officer Gustav Söderström chats with the engineers, executives, and other Spotifiers who helped make the mobile streaming revolution possible. Why was it so difficult to create Spotify for mobile even after we created the desktop app? Why were Spotify’s licensing deals such a game changer for us? Keep reading to find out the answers, and don’t forget to <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">listen to the podcast</a> itself to learn even more.</p>



<h2><strong>What’s user research really for?</strong></h2>



<p>When we first released the Spotify desktop app in Sweden in 2008, the idea of creating an on-the-go version of that same experience seemed out of the question. After all, Spotify didn’t have access to iPods or the other dedicated digital music players, and besides, at the time, most phones were equipped with only enough memory to hold a handful of songs.</p>



<p>Then something happened. Smartphones happened. The iPhone happened. Suddenly the idea of creating a piece of software that people could carry around with them everywhere didn’t seem so far-fetched.</p>



<p>Though it was theoretically possible to create a mobile edition of Spotify, there was still one major issue. A free, offline music app, one that could truly compete with the already-offline MP3 players, just wasn’t in the cards. First off, it couldn’t be free — you can’t click on ads without an internet connection — so it would have to be a paid service. Plus, our listeners already had access to all the music they wanted through piracy. So, once again, we had to ask ourselves something: Why would anyone pay for Spotify? How do you charge for nothing? </p>



<p>And indeed, when we surveyed our users, we discovered that $10 per month — the number we settled on after months of licensing negotiations with labels (more on those in a bit) — was a number literally no one said they would pay.</p>



<p>Well, at least they <em>said</em> that. See, user research is great, but you have to know what it’s really there for. That was our <strong>first lesson:</strong> <strong>user research is for understanding what people </strong><strong><em>think</em></strong><strong> they will do, not what they will actually do.</strong></p>



<p>There’s a reason for that, as Gustav points out.</p>



<blockquote><p><strong>Gustav:</strong> Humans are not very good at predicting our own future behavior, especially when it comes to what we are prepared to pay for something like convenience. We just can’t imagine our future selves being that lazy.</p></blockquote>



<p>Exactly. In other words, just like we discovered while creating the desktop app, convenience is key. Of course people don’t think they would pay for something they were already getting for free — but they just might if it’s a lot more convenient than what they’re using. At least, that’s what we were betting on.</p>



<p>We just needed to find another magic trick.</p>



<h2><strong>Beware your optimization bias</strong></h2>



<p>That was easier said than done, though. Setting aside the business side of things, actually building a mobile experience convenient enough to convert music pirates was a tall order, one without an obvious solution.</p>



<p>Back then, in the early days of Spotify, the desktop app may have technically been able to run on the iPhone, but that didn’t mean it was built for it. When Gustav asked Mattias Arrelid, one of Spotify’s early Mac developers, what the challenges were in translating the desktop app into a great mobile product, two things sprang immediately to Mattias’ mind:</p>



<ol><li>“One [challenge] is that … a phone doesn’t have a constant power supply. For example, it doesn’t have a constant high-bandwidth network connection. Spotify was, at that time, at least leveraging peer to peer … If you do that on the phone, you basically force it to keep the radio on quite a bit. And that is draining in terms of battery.”</li></ol>



<ol start="2"><li>“The other one is data. [When] you had Spotify running on the desktop, you relied on some kind of data connection that [you] don’t really need to care about … On the phone, you had many contracts that had limited bandwidth included in the monthly allowance. So you really wanted to be careful about your consumption of that.”</li></ol>



<p>It turns out by developing the desktop app to be as efficient as possible, we were actually developing something that was <em>only</em> efficient as a desktop app. Concerns like bandwidth and power consumption never factored into the equation, since you don’t need to worry about them when you’re a desktop user. And that’s where <strong>the second lesson</strong> comes in: <strong>whether you realize it or not, you’re always optimizing for something.</strong> This isn’t always a bad thing — after all, you’re still optimizing — but you are unwittingly creating blind spots for you and your product, ones that can hamper your growth down the road.</p>



<p>One of the solutions Mattias and the engineering team came up with to improve battery performance involved changing up the codec we used to encode audio. (<a href="https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3" target="_blank" rel="noreferrer noopener">Dive into the full episode</a> to hear all the details about why.) But changing up our codec didn’t just have technical ramifications — it had business implications as well, implications that dovetail nicely with our final lesson.</p>



<h2><strong>Your tech is only half of the story</strong></h2>



<p>Spotify didn’t invent streaming, and it was only a matter of time until other companies could start to pull off a service similar to what we were offering. What we needed, in other words, was a breakthrough in our business strategy as well.</p>



<p>Enter licensing. Contrary to what you may expect, there isn’t a one-size-fits-all codec license for streaming songs. Different licenses are required for almost every use case — say, streaming on desktop versus streaming on mobile. This meant that hammering out licensing deals with record labels could end up being a lengthy, complex process. Rather than dealing with these complications, we saw plenty of other companies kick that sort of nitty-gritty work down the road, focusing instead on shipping code and getting to market quicker.</p>



<p>But to us, this wasn’t a sustainable strategy. So rather than take the same route, Spotify slowed things down, sweating the details in the negotiations for months on end, making sure that we had licenses that no one else had. That way, we could guarantee that we hit the ground running when we introduced a paid mobile tier. Here’s Spotify’s first general counsel, Petra Hansson, on the matter:</p>



<blockquote><p><strong>Petra:</strong> We spent a lot of time getting those licenses right. Because, you know, back in the day, the norm was more to just sign any licenses and then, you know, try and flip the companies, so it became someone else’s problem or companies would go belly up. So we were sort of hell-bent. And that’s one of the reasons why I joined [Spotify], because what I really liked about the company was that [co-founder and CEO Daniel Ek]’s vision was always to build something long-term and sustainable.</p></blockquote>



<p>As it so happens, truly great product development is more than just a cool new feature or even a breakthrough new technology. <strong>Our third lesson is that</strong> <strong>truly great product development, in short, almost always combines technological innovation with business innovation.</strong></p>



<p>That’s another three lessons down, but there are plenty more to go. The podcast series <em>Spotify: A Product Story</em> shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join podcast host and Chief R&amp;D Officer Gustav Söderström, and <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">check out all the episodes right here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: Three Lessons We Learned from Developing the Mobile App&#xA;</title>
      <link>https://engineering.atspotify.com/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/</link>
      <description>TL;DR Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we trave</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 5, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/" title="A Product Story: Three Lessons We Learned from Developing the Mobile App">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we travel with our music forever.</p>



<p>In <a href="https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3" target="_blank" rel="noreferrer noopener">Episode 02</a> of our podcast series, <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener"><em>Spotify: A Product Story</em></a><em>,</em> host and Chief R&amp;D Officer Gustav Söderström chats with the engineers, executives, and other Spotifiers who helped make the mobile streaming revolution possible. Why was it so difficult to create Spotify for mobile even after we created the desktop app? Why were Spotify’s licensing deals such a game changer for us? Keep reading to find out the answers, and don’t forget to <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">listen to the podcast</a> itself to learn even more.</p>



<h2><strong>What’s user research really for?</strong></h2>



<p>When we first released the Spotify desktop app in Sweden in 2008, the idea of creating an on-the-go version of that same experience seemed out of the question. After all, Spotify didn’t have access to iPods or the other dedicated digital music players, and besides, at the time, most phones were equipped with only enough memory to hold a handful of songs.</p>



<p>Then something happened. Smartphones happened. The iPhone happened. Suddenly the idea of creating a piece of software that people could carry around with them everywhere didn’t seem so far-fetched.</p>



<p>Though it was theoretically possible to create a mobile edition of Spotify, there was still one major issue. A free, offline music app, one that could truly compete with the already-offline MP3 players, just wasn’t in the cards. First off, it couldn’t be free — you can’t click on ads without an internet connection — so it would have to be a paid service. Plus, our listeners already had access to all the music they wanted through piracy. So, once again, we had to ask ourselves something: Why would anyone pay for Spotify? How do you charge for nothing? </p>



<p>And indeed, when we surveyed our users, we discovered that $10 per month — the number we settled on after months of licensing negotiations with labels (more on those in a bit) — was a number literally no one said they would pay.</p>



<p>Well, at least they <em>said</em> that. See, user research is great, but you have to know what it’s really there for. That was our <strong>first lesson:</strong> <strong>user research is for understanding what people </strong><strong><em>think</em></strong><strong> they will do, not what they will actually do.</strong></p>



<p>There’s a reason for that, as Gustav points out.</p>



<blockquote><p><strong>Gustav:</strong> Humans are not very good at predicting our own future behavior, especially when it comes to what we are prepared to pay for something like convenience. We just can’t imagine our future selves being that lazy.</p></blockquote>



<p>Exactly. In other words, just like we discovered while creating the desktop app, convenience is key. Of course people don’t think they would pay for something they were already getting for free — but they just might if it’s a lot more convenient than what they’re using. At least, that’s what we were betting on.</p>



<p>We just needed to find another magic trick.</p>



<h2><strong>Beware your optimization bias</strong></h2>



<p>That was easier said than done, though. Setting aside the business side of things, actually building a mobile experience convenient enough to convert music pirates was a tall order, one without an obvious solution.</p>



<p>Back then, in the early days of Spotify, the desktop app may have technically been able to run on the iPhone, but that didn’t mean it was built for it. When Gustav asked Mattias Arrelid, one of Spotify’s early Mac developers, what the challenges were in translating the desktop app into a great mobile product, two things sprang immediately to Mattias’ mind:</p>



<ol><li>“One [challenge] is that … a phone doesn’t have a constant power supply. For example, it doesn’t have a constant high-bandwidth network connection. Spotify was, at that time, at least leveraging peer to peer … If you do that on the phone, you basically force it to keep the radio on quite a bit. And that is draining in terms of battery.”</li></ol>



<ol start="2"><li>“The other one is data. [When] you had Spotify running on the desktop, you relied on some kind of data connection that [you] don’t really need to care about … On the phone, you had many contracts that had limited bandwidth included in the monthly allowance. So you really wanted to be careful about your consumption of that.”</li></ol>



<p>It turns out by developing the desktop app to be as efficient as possible, we were actually developing something that was <em>only</em> efficient as a desktop app. Concerns like bandwidth and power consumption never factored into the equation, since you don’t need to worry about them when you’re a desktop user. And that’s where <strong>the second lesson</strong> comes in: <strong>whether you realize it or not, you’re always optimizing for something.</strong> This isn’t always a bad thing — after all, you’re still optimizing — but you are unwittingly creating blind spots for you and your product, ones that can hamper your growth down the road.</p>



<p>One of the solutions Mattias and the engineering team came up with to improve battery performance involved changing up the codec we used to encode audio. (<a href="https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3" target="_blank" rel="noreferrer noopener">Dive into the full episode</a> to hear all the details about why.) But changing up our codec didn’t just have technical ramifications — it had business implications as well, implications that dovetail nicely with our final lesson.</p>



<h2><strong>Your tech is only half of the story</strong></h2>



<p>Spotify didn’t invent streaming, and it was only a matter of time until other companies could start to pull off a service similar to what we were offering. What we needed, in other words, was a breakthrough in our business strategy as well.</p>



<p>Enter licensing. Contrary to what you may expect, there isn’t a one-size-fits-all codec license for streaming songs. Different licenses are required for almost every use case — say, streaming on desktop versus streaming on mobile. This meant that hammering out licensing deals with record labels could end up being a lengthy, complex process. Rather than dealing with these complications, we saw plenty of other companies kick that sort of nitty-gritty work down the road, focusing instead on shipping code and getting to market quicker.</p>



<p>But to us, this wasn’t a sustainable strategy. So rather than take the same route, Spotify slowed things down, sweating the details in the negotiations for months on end, making sure that we had licenses that no one else had. That way, we could guarantee that we hit the ground running when we introduced a paid mobile tier. Here’s Spotify’s first general counsel, Petra Hansson, on the matter:</p>



<blockquote><p><strong>Petra:</strong> We spent a lot of time getting those licenses right. Because, you know, back in the day, the norm was more to just sign any licenses and then, you know, try and flip the companies, so it became someone else’s problem or companies would go belly up. So we were sort of hell-bent. And that’s one of the reasons why I joined [Spotify], because what I really liked about the company was that [co-founder and CEO Daniel Ek]’s vision was always to build something long-term and sustainable.</p></blockquote>



<p>As it so happens, truly great product development is more than just a cool new feature or even a breakthrough new technology. <strong>Our third lesson is that</strong> <strong>truly great product development, in short, almost always combines technological innovation with business innovation.</strong></p>



<p>That’s another three lessons down, but there are plenty more to go. The podcast series <em>Spotify: A Product Story</em> shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join podcast host and Chief R&amp;D Officer Gustav Söderström, and <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">check out all the episodes right here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too&#xA;</title>
      <link>https://engineering.atspotify.com/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/</link>
      <description>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/" title="How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w" sizes="(max-width: 512px) 100vw, 512px"/>                    </a>
                        
        </p>

        

        
<p>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?</p>



<p>Instead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?</p>



<p>Pia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener"><em>Thoughtworks</em> podcast</a>: What types of problems do Spotify engineers face? And why did we create <a href="http://backstage.spotify.com" target="_blank" rel="noreferrer noopener">Backstage</a> to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.</p>



<h2>Growing pains</h2>



<p>As Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.</p>



<p>So we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?</p>



<p>Pia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:</p>



<ol><li>“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”<br/></li><li>“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”</li></ol>



<p>Considering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.</p>



<p>But that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.</p>



<p>What else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.</p>



<h2>Backstage: a platform for your platforms</h2>



<p>As Pia notes, Spotify developed <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">Backstage</a> to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.</p>



<p>Where our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.</p>



<p>Now, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our <a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">Golden Paths</a>.</p>



<p>Because of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because <a href="https://github.com/backstage/backstage" target="_blank" rel="noreferrer noopener">Backstage is a rapidly growing open source tool</a>, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.</p>



<p>So, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” </p>



<p>Interested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener">Thoughtworks podcast episode</a>. And if you’re curious about how to get started with Backstage, read more about that <a href="https://backstage.spotify.com/blog/getting-started-with-backstage/" target="_blank" rel="noreferrer noopener">here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too&#xA;</title>
      <link>https://engineering.atspotify.com/2021/09/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/</link>
      <description>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/09/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/" title="How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w" sizes="(max-width: 512px) 100vw, 512px"/>                    </a>
                        
        </p>

        

        
<p>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?</p>



<p>Instead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?</p>



<p>Pia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener"><em>Thoughtworks</em> podcast</a>: What types of problems do Spotify engineers face? And why did we create <a href="http://backstage.spotify.com" target="_blank" rel="noreferrer noopener">Backstage</a> to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.</p>



<h2>Growing pains</h2>



<p>As Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.</p>



<p>So we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?</p>



<p>Pia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:</p>



<ol><li>“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”<br/></li><li>“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”</li></ol>



<p>Considering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.</p>



<p>But that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.</p>



<p>What else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.</p>



<h2>Backstage: a platform for your platforms</h2>



<p>As Pia notes, Spotify developed <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">Backstage</a> to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.</p>



<p>Where our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.</p>



<p>Now, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our <a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">Golden Paths</a>.</p>



<p>Because of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because <a href="https://github.com/backstage/backstage" target="_blank" rel="noreferrer noopener">Backstage is a rapidly growing open source tool</a>, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.</p>



<p>So, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” </p>



<p>Interested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener">Thoughtworks podcast episode</a>. And if you’re curious about how to get started with Backstage, read more about that <a href="https://backstage.spotify.com/blog/getting-started-with-backstage/" target="_blank" rel="noreferrer noopener">here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too&#xA;</title>
      <link>https://engineering.atspotify.com/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/</link>
      <description>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/" title="How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w" sizes="(max-width: 512px) 100vw, 512px"/>                    </a>
                        
        </p>

        

        
<p>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?</p>



<p>Instead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?</p>



<p>Pia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener"><em>Thoughtworks</em> podcast</a>: What types of problems do Spotify engineers face? And why did we create <a href="http://backstage.spotify.com" target="_blank" rel="noreferrer noopener">Backstage</a> to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.</p>



<h2>Growing pains</h2>



<p>As Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.</p>



<p>So we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?</p>



<p>Pia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:</p>



<ol><li>“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”<br/></li><li>“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”</li></ol>



<p>Considering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.</p>



<p>But that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.</p>



<p>What else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.</p>



<h2>Backstage: a platform for your platforms</h2>



<p>As Pia notes, Spotify developed <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">Backstage</a> to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.</p>



<p>Where our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.</p>



<p>Now, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our <a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">Golden Paths</a>.</p>



<p>Because of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because <a href="https://github.com/backstage/backstage" target="_blank" rel="noreferrer noopener">Backstage is a rapidly growing open source tool</a>, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.</p>



<p>So, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” </p>



<p>Interested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener">Thoughtworks podcast episode</a>. And if you’re curious about how to get started with Backstage, read more about that <a href="https://backstage.spotify.com/blog/getting-started-with-backstage/" target="_blank" rel="noreferrer noopener">here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too&#xA;</title>
      <link>https://engineering.atspotify.com/2021/09/23/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/</link>
      <description>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/09/23/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/" title="How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w" sizes="(max-width: 512px) 100vw, 512px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/09/Backstage_Developers_Header.png"/>                    </a>
                        
        </p>

        

        
<p>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?</p>



<p>Instead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?</p>



<p>Pia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener"><em>Thoughtworks</em> podcast</a>: What types of problems do Spotify engineers face? And why did we create <a href="http://backstage.spotify.com" target="_blank" rel="noreferrer noopener">Backstage</a> to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.</p>



<h2>Growing pains</h2>



<p>As Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.</p>



<p>So we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?</p>



<p>Pia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:</p>



<ol><li>“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”<br/></li><li>“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”</li></ol>



<p>Considering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.</p>



<p>But that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.</p>



<p>What else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.</p>



<h2>Backstage: a platform for your platforms</h2>



<p>As Pia notes, Spotify developed <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">Backstage</a> to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.</p>



<p>Where our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.</p>



<p>Now, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our <a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">Golden Paths</a>.</p>



<p>Because of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because <a href="https://github.com/backstage/backstage" target="_blank" rel="noreferrer noopener">Backstage is a rapidly growing open source tool</a>, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.</p>



<p>So, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” </p>



<p>Interested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener">Thoughtworks podcast episode</a>. And if you’re curious about how to get started with Backstage, read more about that <a href="https://backstage.spotify.com/blog/getting-started-with-backstage/" target="_blank" rel="noreferrer noopener">here</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python&#xA;</title>
      <link>https://engineering.atspotify.com/2021/09/introducing-pedalboard-spotifys-audio-effects-library-for-python/</link>
      <description>We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4787">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/09/introducing-pedalboard-spotifys-audio-effects-library-for-python/" title="Introducing Pedalboard: Spotify’s Audio Effects Library for Python">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p>We’ve just open sourced <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">Pedalboard</a>, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).</p>



<p>If you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro<sup>®</sup>, or Pro Tools<sup>®</sup>, or newer, more accessible tools like <a rel="noreferrer noopener" href="https://www.soundtrap.com/" target="_blank">Soundtrap</a> or <a rel="noreferrer noopener" href="https://anchor.fm/" target="_blank">Anchor</a>. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.</p>



<p>This ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at <a href="https://research.atspotify.com/audio-intelligence/" target="_blank" rel="noreferrer noopener">Spotify’s Audio Intelligence Lab</a> found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter <em>Pedalboard</em>, a new Python package.</p>



<figure><img loading="lazy" width="700" height="182" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Pedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of <a rel="noreferrer noopener" href="https://juce.com/" target="_blank">JUCE</a>, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3<sup>® </sup>and Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.</p>



<p>Similar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3<sup>® </sup>or Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.</p>



<figure><img loading="lazy" width="700" height="538" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>We’ve found a number of great uses for Pedalboard at Spotify so far, including:</p>



<ul><li><strong>Machine Learning (ML):</strong> Pedalboard makes the process of <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Data_augmentation" target="_blank">data augmentation</a> for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.</li></ul>







<ul><li><strong>Content Creation: </strong>Pedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3<sup>®</sup> or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.</li></ul>







<ul><li><strong>Creativity:</strong> Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.</li></ul>



<p>Spotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.</p>



<p>If you’re interested in trying out Pedalboard, it’s ready now. You can find its <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">code and documentation on GitHub</a>, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!</p>



<p>—</p>



<p><em>VST is a registered trademark of Steinberg Media Technologies GmbH.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a>, <a href="https://engineering.atspotify.com/tag/tech-research/" rel="tag">tech research</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python&#xA;</title>
      <link>https://engineering.atspotify.com/introducing-pedalboard-spotifys-audio-effects-library-for-python/</link>
      <description>We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4787">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/introducing-pedalboard-spotifys-audio-effects-library-for-python/" title="Introducing Pedalboard: Spotify’s Audio Effects Library for Python">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p>We’ve just open sourced <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">Pedalboard</a>, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).</p>



<p>If you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro<sup>®</sup>, or Pro Tools<sup>®</sup>, or newer, more accessible tools like <a rel="noreferrer noopener" href="https://www.soundtrap.com/" target="_blank">Soundtrap</a> or <a rel="noreferrer noopener" href="https://anchor.fm/" target="_blank">Anchor</a>. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.</p>



<p>This ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at <a href="https://research.atspotify.com/audio-intelligence/" target="_blank" rel="noreferrer noopener">Spotify’s Audio Intelligence Lab</a> found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter <em>Pedalboard</em>, a new Python package.</p>



<figure><img loading="lazy" width="700" height="182" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Pedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of <a rel="noreferrer noopener" href="https://juce.com/" target="_blank">JUCE</a>, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3<sup>® </sup>and Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.</p>



<p>Similar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3<sup>® </sup>or Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.</p>



<figure><img loading="lazy" width="700" height="538" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>We’ve found a number of great uses for Pedalboard at Spotify so far, including:</p>



<ul><li><strong>Machine Learning (ML):</strong> Pedalboard makes the process of <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Data_augmentation" target="_blank">data augmentation</a> for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.</li></ul>







<ul><li><strong>Content Creation: </strong>Pedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3<sup>®</sup> or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.</li></ul>







<ul><li><strong>Creativity:</strong> Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.</li></ul>



<p>Spotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.</p>



<p>If you’re interested in trying out Pedalboard, it’s ready now. You can find its <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">code and documentation on GitHub</a>, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!</p>



<p>—</p>



<p><em>VST is a registered trademark of Steinberg Media Technologies GmbH.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a>, <a href="https://engineering.atspotify.com/tag/tech-research/" rel="tag">tech research</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python&#xA;</title>
      <link>https://engineering.atspotify.com/introducing-pedalboard-spotifys-audio-effects-library-for-python/</link>
      <description>We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4787">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/introducing-pedalboard-spotifys-audio-effects-library-for-python/" title="Introducing Pedalboard: Spotify’s Audio Effects Library for Python">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p>We’ve just open sourced <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">Pedalboard</a>, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).</p>



<p>If you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro<sup>®</sup>, or Pro Tools<sup>®</sup>, or newer, more accessible tools like <a rel="noreferrer noopener" href="https://www.soundtrap.com/" target="_blank">Soundtrap</a> or <a rel="noreferrer noopener" href="https://anchor.fm/" target="_blank">Anchor</a>. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.</p>



<p>This ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at <a href="https://research.atspotify.com/audio-intelligence/" target="_blank" rel="noreferrer noopener">Spotify’s Audio Intelligence Lab</a> found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter <em>Pedalboard</em>, a new Python package.</p>



<figure><img loading="lazy" width="700" height="182" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Pedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of <a rel="noreferrer noopener" href="https://juce.com/" target="_blank">JUCE</a>, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3<sup>® </sup>and Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.</p>



<p>Similar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3<sup>® </sup>or Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.</p>



<figure><img loading="lazy" width="700" height="538" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>We’ve found a number of great uses for Pedalboard at Spotify so far, including:</p>



<ul><li><strong>Machine Learning (ML):</strong> Pedalboard makes the process of <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Data_augmentation" target="_blank">data augmentation</a> for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.</li></ul>







<ul><li><strong>Content Creation: </strong>Pedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3<sup>®</sup> or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.</li></ul>







<ul><li><strong>Creativity:</strong> Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.</li></ul>



<p>Spotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.</p>



<p>If you’re interested in trying out Pedalboard, it’s ready now. You can find its <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">code and documentation on GitHub</a>, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!</p>



<p>—</p>



<p><em>VST is a registered trademark of Steinberg Media Technologies GmbH.</em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a>, <a href="https://engineering.atspotify.com/tag/tech-research/" rel="tag">tech research</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python&#xA;</title>
      <link>https://engineering.atspotify.com/2021/09/07/introducing-pedalboard-spotifys-audio-effects-library-for-python/</link>
      <description>We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4787">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/09/07/introducing-pedalboard-spotifys-audio-effects-library-for-python/" title="Introducing Pedalboard: Spotify’s Audio Effects Library for Python">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_header.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/09/Pedalboard_header.gif"/>                    </a>
                        
        </p>

        

        
<p>We’ve just open sourced <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">Pedalboard</a>, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).</p>



<p>If you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro<sup>®</sup>, or Pro Tools<sup>®</sup>, or newer, more accessible tools like <a rel="noreferrer noopener" href="https://www.soundtrap.com/" target="_blank">Soundtrap</a> or <a rel="noreferrer noopener" href="https://anchor.fm/" target="_blank">Anchor</a>. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.</p>



<p>This ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at <a href="https://research.atspotify.com/audio-intelligence/" target="_blank" rel="noreferrer noopener">Spotify’s Audio Intelligence Lab</a> found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter <em>Pedalboard</em>, a new Python package.</p>



<figure><img loading="lazy" width="700" height="182" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Pedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of <a rel="noreferrer noopener" href="https://juce.com/" target="_blank">JUCE</a>, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3<sup>® </sup>and Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.</p>



<p>Similar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3<sup>® </sup>or Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.</p>



<figure><img loading="lazy" width="700" height="538" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>We’ve found a number of great uses for Pedalboard at Spotify so far, including:</p>



<ul><li><strong>Machine Learning (ML):</strong> Pedalboard makes the process of <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Data_augmentation" target="_blank">data augmentation</a> for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.</li></ul>







<ul><li><strong>Content Creation: </strong>Pedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3<sup>®</sup> or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.</li></ul>







<ul><li><strong>Creativity:</strong> Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.</li></ul>



<p>Spotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.</p>



<p>If you’re interested in trying out Pedalboard, it’s ready now. You can find its <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">code and documentation on GitHub</a>, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!</p>



<p>—</p>



<p><em>VST is a registered trademark of Steinberg Media Technologies GmbH.</em></p>
        <br/>

        
        

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_header.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Four Lessons We Learned from Creating Spotify’s Desktop App&#xA;</title>
      <link>https://engineering.atspotify.com/2021/08/04/four-lessons-we-learned-from-creating-spotifys-desktop-app/</link>
      <description>TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>August 4, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/08/04/four-lessons-we-learned-from-creating-spotifys-desktop-app/" title="Four Lessons We Learned from Creating Spotify’s Desktop App">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to <a href="https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/" target="_blank" rel="noreferrer noopener">web players</a> to <a href="https://carthing.spotify.com/" target="_blank" rel="noreferrer noopener">car things</a>. But sitting at the core is our flagship product, the one that started it all: the desktop app. In <a href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank" rel="noreferrer noopener">the first episode</a> of our podcast series, “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, host and Chief R&amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">check out the podcast</a> yourself to hear even more about how Spotify became, well, Spotify.</p>



<h2>How do you steal from a pirate?</h2>



<p>Let’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. </p>



<p>Sure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?</p>



<p>In the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:</p>



<blockquote><p><strong>Daniel: </strong>I guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.</p></blockquote>



<p>In other words, when it comes to guiding your product strategy, <strong>our</strong> <strong>first lesson is an important rule to live by: convenience trumps everything. </strong>The appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.</p>



<h2>Go big or go home</h2>



<p>But how exactly could we make a user experience <em>that</em> good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.</p>



<p>Luckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.</p>



<p>Little surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” </p>



<p>Bad news for other companies, but great news for us. You see, it’s always good to keep in mind <strong>our second lesson:</strong> <strong>great ambition attracts great talent. This is why companies always need to keep moving the goalposts.</strong> When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.</p>



<h2>The rules are only suggestions</h2>



<p>And it wasn’t long before he identified Spotify’s problem — and the solution.</p>



<blockquote><p><strong>Ludde: </strong>Doing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.</p></blockquote>



<p>Essentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. </p>



<p>Fortunately, Ludde already knew <strong>our</strong> <strong>third product lesson:</strong> <strong>don’t be afraid to break the rules. </strong>The problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.</p>



<p>Not to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (<a rel="noreferrer noopener" href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank">Check out Episode 01, “How do you steal from a pirate?”</a>, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …</p>



<h2>Do you want to see a magic trick?</h2>



<p>All of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?</p>



<p>When Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.</p>



<blockquote><p><strong>Michelle</strong>: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”</p></blockquote>



<p><strong>That’s our fourth and final lesson:</strong> <strong>differentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.</strong> Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.</p>



<p>So, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R&amp;D Officer Gustav Söderström and <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">check out all the episodes right here</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Four Lessons We Learned from Creating Spotify’s Desktop App&#xA;</title>
      <link>https://engineering.atspotify.com/four-lessons-we-learned-from-creating-spotifys-desktop-app/</link>
      <description>TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>August 4, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/four-lessons-we-learned-from-creating-spotifys-desktop-app/" title="Four Lessons We Learned from Creating Spotify’s Desktop App">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to <a href="https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/" target="_blank" rel="noreferrer noopener">web players</a> to <a href="https://carthing.spotify.com/" target="_blank" rel="noreferrer noopener">car things</a>. But sitting at the core is our flagship product, the one that started it all: the desktop app. In <a href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank" rel="noreferrer noopener">the first episode</a> of our podcast series, “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, host and Chief R&amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">check out the podcast</a> yourself to hear even more about how Spotify became, well, Spotify.</p>



<h2>How do you steal from a pirate?</h2>



<p>Let’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. </p>



<p>Sure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?</p>



<p>In the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:</p>



<blockquote><p><strong>Daniel: </strong>I guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.</p></blockquote>



<p>In other words, when it comes to guiding your product strategy, <strong>our</strong> <strong>first lesson is an important rule to live by: convenience trumps everything. </strong>The appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.</p>



<h2>Go big or go home</h2>



<p>But how exactly could we make a user experience <em>that</em> good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.</p>



<p>Luckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.</p>



<p>Little surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” </p>



<p>Bad news for other companies, but great news for us. You see, it’s always good to keep in mind <strong>our second lesson:</strong> <strong>great ambition attracts great talent. This is why companies always need to keep moving the goalposts.</strong> When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.</p>



<h2>The rules are only suggestions</h2>



<p>And it wasn’t long before he identified Spotify’s problem — and the solution.</p>



<blockquote><p><strong>Ludde: </strong>Doing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.</p></blockquote>



<p>Essentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. </p>



<p>Fortunately, Ludde already knew <strong>our</strong> <strong>third product lesson:</strong> <strong>don’t be afraid to break the rules. </strong>The problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.</p>



<p>Not to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (<a rel="noreferrer noopener" href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank">Check out Episode 01, “How do you steal from a pirate?”</a>, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …</p>



<h2>Do you want to see a magic trick?</h2>



<p>All of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?</p>



<p>When Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.</p>



<blockquote><p><strong>Michelle</strong>: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”</p></blockquote>



<p><strong>That’s our fourth and final lesson:</strong> <strong>differentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.</strong> Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.</p>



<p>So, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R&amp;D Officer Gustav Söderström and <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">check out all the episodes right here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Four Lessons We Learned from Creating Spotify’s Desktop App&#xA;</title>
      <link>https://engineering.atspotify.com/2021/08/four-lessons-we-learned-from-creating-spotifys-desktop-app/</link>
      <description>TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>August 4, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/08/four-lessons-we-learned-from-creating-spotifys-desktop-app/" title="Four Lessons We Learned from Creating Spotify’s Desktop App">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to <a href="https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/" target="_blank" rel="noreferrer noopener">web players</a> to <a href="https://carthing.spotify.com/" target="_blank" rel="noreferrer noopener">car things</a>. But sitting at the core is our flagship product, the one that started it all: the desktop app. In <a href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank" rel="noreferrer noopener">the first episode</a> of our podcast series, “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, host and Chief R&amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">check out the podcast</a> yourself to hear even more about how Spotify became, well, Spotify.</p>



<h2>How do you steal from a pirate?</h2>



<p>Let’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. </p>



<p>Sure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?</p>



<p>In the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:</p>



<blockquote><p><strong>Daniel: </strong>I guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.</p></blockquote>



<p>In other words, when it comes to guiding your product strategy, <strong>our</strong> <strong>first lesson is an important rule to live by: convenience trumps everything. </strong>The appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.</p>



<h2>Go big or go home</h2>



<p>But how exactly could we make a user experience <em>that</em> good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.</p>



<p>Luckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.</p>



<p>Little surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” </p>



<p>Bad news for other companies, but great news for us. You see, it’s always good to keep in mind <strong>our second lesson:</strong> <strong>great ambition attracts great talent. This is why companies always need to keep moving the goalposts.</strong> When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.</p>



<h2>The rules are only suggestions</h2>



<p>And it wasn’t long before he identified Spotify’s problem — and the solution.</p>



<blockquote><p><strong>Ludde: </strong>Doing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.</p></blockquote>



<p>Essentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. </p>



<p>Fortunately, Ludde already knew <strong>our</strong> <strong>third product lesson:</strong> <strong>don’t be afraid to break the rules. </strong>The problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.</p>



<p>Not to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (<a rel="noreferrer noopener" href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank">Check out Episode 01, “How do you steal from a pirate?”</a>, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …</p>



<h2>Do you want to see a magic trick?</h2>



<p>All of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?</p>



<p>When Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.</p>



<blockquote><p><strong>Michelle</strong>: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”</p></blockquote>



<p><strong>That’s our fourth and final lesson:</strong> <strong>differentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.</strong> Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.</p>



<p>So, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R&amp;D Officer Gustav Söderström and <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">check out all the episodes right here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Four Lessons We Learned from Creating Spotify’s Desktop App&#xA;</title>
      <link>https://engineering.atspotify.com/four-lessons-we-learned-from-creating-spotifys-desktop-app/</link>
      <description>TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>August 4, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/four-lessons-we-learned-from-creating-spotifys-desktop-app/" title="Four Lessons We Learned from Creating Spotify’s Desktop App">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to <a href="https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/" target="_blank" rel="noreferrer noopener">web players</a> to <a href="https://carthing.spotify.com/" target="_blank" rel="noreferrer noopener">car things</a>. But sitting at the core is our flagship product, the one that started it all: the desktop app. In <a href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank" rel="noreferrer noopener">the first episode</a> of our podcast series, “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, host and Chief R&amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">check out the podcast</a> yourself to hear even more about how Spotify became, well, Spotify.</p>



<h2>How do you steal from a pirate?</h2>



<p>Let’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. </p>



<p>Sure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?</p>



<p>In the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:</p>



<blockquote><p><strong>Daniel: </strong>I guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.</p></blockquote>



<p>In other words, when it comes to guiding your product strategy, <strong>our</strong> <strong>first lesson is an important rule to live by: convenience trumps everything. </strong>The appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.</p>



<h2>Go big or go home</h2>



<p>But how exactly could we make a user experience <em>that</em> good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.</p>



<p>Luckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.</p>



<p>Little surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” </p>



<p>Bad news for other companies, but great news for us. You see, it’s always good to keep in mind <strong>our second lesson:</strong> <strong>great ambition attracts great talent. This is why companies always need to keep moving the goalposts.</strong> When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.</p>



<h2>The rules are only suggestions</h2>



<p>And it wasn’t long before he identified Spotify’s problem — and the solution.</p>



<blockquote><p><strong>Ludde: </strong>Doing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.</p></blockquote>



<p>Essentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. </p>



<p>Fortunately, Ludde already knew <strong>our</strong> <strong>third product lesson:</strong> <strong>don’t be afraid to break the rules. </strong>The problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.</p>



<p>Not to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (<a rel="noreferrer noopener" href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank">Check out Episode 01, “How do you steal from a pirate?”</a>, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …</p>



<h2>Do you want to see a magic trick?</h2>



<p>All of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?</p>



<p>When Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.</p>



<blockquote><p><strong>Michelle</strong>: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”</p></blockquote>



<p><strong>That’s our fourth and final lesson:</strong> <strong>differentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.</strong> Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.</p>



<p>So, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R&amp;D Officer Gustav Söderström and <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">check out all the episodes right here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Patrick Balestra: Senior Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2021/06/04/patrick-balestra-senior-engineer/</link>
      <description>8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4729">
     <div>
         
         
         
         <div>
             <p><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w" sizes="(max-width: 693px) 100vw, 693px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/06/Patrick-Balestra_Header-Image.png"/>
                                  
             </p>
             <p><b>Originally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.</b></p>
         </div>

         


         

         
<blockquote><p>8:30am</p></blockquote>



<p>I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!</p>



<blockquote><p>10:00am</p></blockquote>



<p>After breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. </p>



<p>My team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>One way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. </p>



<blockquote><p>2:00pm</p></blockquote>



<p>After lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source <a rel="noreferrer noopener" href="https://xcmetrics.io/" target="_blank">XCMetrics</a> — <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/" target="_blank">a tool developed at Spotify</a> for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.</p>



<blockquote><p>7:00pm</p></blockquote>



<p>I tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…</p>







<figure><img loading="lazy" width="700" height="111" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="557" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w" sizes="(max-width: 700px) 100vw, 700px"/></figure>

         
         

         <p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Patrick Balestra: Senior Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2021/06/patrick-balestra-senior-engineer/</link>
      <description>8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4729">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w" sizes="(max-width: 693px) 100vw, 693px"/>
                                  
             </p>
             <div>
             
                 <p><b>Originally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.</b></p>
             </div>
         </div>

         


         

         
<blockquote><p>8:30am</p></blockquote>



<p>I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!</p>



<blockquote><p>10:00am</p></blockquote>



<p>After breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. </p>



<p>My team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>One way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. </p>



<blockquote><p>2:00pm</p></blockquote>



<p>After lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source <a rel="noreferrer noopener" href="https://xcmetrics.io/" target="_blank">XCMetrics</a> — <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/" target="_blank">a tool developed at Spotify</a> for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.</p>



<blockquote><p>7:00pm</p></blockquote>



<p>I tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="557" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w" sizes="(max-width: 700px) 100vw, 700px"/></figure><p>

         Tags: <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Patrick Balestra: Senior Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/patrick-balestra-senior-engineer/</link>
      <description>8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4729">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w" sizes="(max-width: 693px) 100vw, 693px"/>
                                  
             </p>
             <div>
             
                 <p><b>Originally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.</b></p>
             </div>
         </div>

         


         

         
<blockquote><p>8:30am</p></blockquote>



<p>I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!</p>



<blockquote><p>10:00am</p></blockquote>



<p>After breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. </p>



<p>My team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>One way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. </p>



<blockquote><p>2:00pm</p></blockquote>



<p>After lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source <a rel="noreferrer noopener" href="https://xcmetrics.io/" target="_blank">XCMetrics</a> — <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/" target="_blank">a tool developed at Spotify</a> for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.</p>



<blockquote><p>7:00pm</p></blockquote>



<p>I tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="557" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w" sizes="(max-width: 700px) 100vw, 700px"/></figure><p>

         Tags: <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Patrick Balestra: Senior Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/patrick-balestra-senior-engineer/</link>
      <description>8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4729">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w" sizes="(max-width: 693px) 100vw, 693px"/>
                                  
             </p>
             <div>
             
                 <p><b>Originally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.</b></p>
             </div>
         </div>

         


         

         
<blockquote><p>8:30am</p></blockquote>



<p>I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!</p>



<blockquote><p>10:00am</p></blockquote>



<p>After breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. </p>



<p>My team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>One way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. </p>



<blockquote><p>2:00pm</p></blockquote>



<p>After lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source <a rel="noreferrer noopener" href="https://xcmetrics.io/" target="_blank">XCMetrics</a> — <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/" target="_blank">a tool developed at Spotify</a> for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.</p>



<blockquote><p>7:00pm</p></blockquote>



<p>I tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="557" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w" sizes="(max-width: 700px) 100vw, 700px"/></figure><p>

         Tags: <a href="https://engineering.atspotify.com/tag/mobile/" rel="tag">Mobile</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Achieving Team Purpose and Pride with Scrum&#xA;</title>
      <link>https://engineering.atspotify.com/achieving-team-purpose-and-pride-with-scrum/</link>
      <description>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 27, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/achieving-team-purpose-and-pride-with-scrum/" title="Achieving Team Purpose and Pride with Scrum">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.</p>



<p>At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.</p>



<p>While this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.</p>



<h2>Our problem</h2>



<p>Our squad had long been following a process comprising bits and pieces from the <a rel="noreferrer noopener" href="https://scrumguides.org/scrum-guide.html" target="_blank">Scrum</a> framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.</p>



<p>The goals we ultimately wanted our process to achieve were:</p>



<ol><li><strong>Continuous improvement: </strong>We wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.</li></ol>



<ol start="2"><li><strong>Shared understanding and transparency: </strong>We wanted everyone on the team to know at any given time what work was happening, and what it entailed.</li></ol>



<ol start="3"><li><strong>Confidence: </strong>We wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.</li></ol>



<h2>Our approach</h2>



<p>To help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. </p>



<h3>Backlog refinement</h3>



<p><strong>Goal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.</strong></p>



<p>Before these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.</p>



<p>With the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually <em>point </em>those tickets.</p>



<p>This system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.</p>



<h3>Sprint planning</h3>



<p><strong>Goal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.</strong></p>



<p>Previously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.</p>



<p>Now, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.</p>



<h3>Sprint review</h3>



<p><strong>Goal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.</strong></p>



<p>While we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.</p>



<p>In a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:</p>



<ul><li><em>What work did we complete?</em></li><li><em>Is there anything we need to extend or add to what we’ve done?</em></li><li><em>Did we discover any tech debt?</em></li><li><em>Are we on track to meet our longer-term goals?</em></li></ul>



<p>This allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.</p>



<h3>Retro</h3>



<p><strong>Goal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.</strong></p>



<p>In previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.</p>



<p>Now, we continue to create those notes, but then vote on a <em>single issue </em>to spend the majority of the retro discussing and ideating to solve.</p>



<figure><img loading="lazy" width="746" height="787" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w" sizes="(max-width: 746px) 100vw, 746px"/><figcaption><em>Now, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.</em></figcaption></figure>



<p>By the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.</p>



<h3>Stand-ups</h3>



<p><strong>Goal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.</strong></p>



<p>Incorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.</p>



<h2>Recommendations</h2>



<p>With simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:</p>



<p><strong>1. Try out a system holistically before making adjustments. </strong></p>



<p>Agile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.</p>



<p><strong>2. Ask the “stand-up question”.</strong></p>



<p>Asking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.</p>



<p><strong>3. Focus on a single issue in retros.</strong></p>



<p>Allow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.</p>



<p><strong>4. Plan sprints you can finish, and commit to finishing them.</strong></p>



<p>Create multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.</p>



<p>These changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers <a href="https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/" target="_blank" rel="noreferrer noopener">became more T-shaped</a> to finish the sprint’s work in time. </p>



<p>Additionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  </p>



<p>My team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.</p>



<h2>Acknowledgments</h2>



<p>Many thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Sophia Ciocca, Web Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Achieving Team Purpose and Pride with Scrum&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/27/achieving-team-purpose-and-pride-with-scrum/</link>
      <description>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 27, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/27/achieving-team-purpose-and-pride-with-scrum/" title="Achieving Team Purpose and Pride with Scrum">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/05/Scrum-Post_Header.png"/>                    </a>
                        
        </p>

        

        
<p>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.</p>



<p>At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.</p>



<p>While this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.</p>



<h2>Our problem</h2>



<p>Our squad had long been following a process comprising bits and pieces from the <a rel="noreferrer noopener" href="https://scrumguides.org/scrum-guide.html" target="_blank">Scrum</a> framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.</p>



<p>The goals we ultimately wanted our process to achieve were:</p>



<ol><li><strong>Continuous improvement: </strong>We wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.</li></ol>



<ol start="2"><li><strong>Shared understanding and transparency: </strong>We wanted everyone on the team to know at any given time what work was happening, and what it entailed.</li></ol>



<ol start="3"><li><strong>Confidence: </strong>We wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.</li></ol>



<h2>Our approach</h2>



<p>To help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. </p>



<h3>Backlog refinement</h3>



<p><strong>Goal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.</strong></p>



<p>Before these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.</p>



<p>With the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually <em>point </em>those tickets.</p>



<p>This system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.</p>



<h3>Sprint planning</h3>



<p><strong>Goal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.</strong></p>



<p>Previously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.</p>



<p>Now, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.</p>



<h3>Sprint review</h3>



<p><strong>Goal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.</strong></p>



<p>While we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.</p>



<p>In a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:</p>



<ul><li><em>What work did we complete?</em></li><li><em>Is there anything we need to extend or add to what we’ve done?</em></li><li><em>Did we discover any tech debt?</em></li><li><em>Are we on track to meet our longer-term goals?</em></li></ul>



<p>This allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.</p>



<h3>Retro</h3>



<p><strong>Goal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.</strong></p>



<p>In previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.</p>



<p>Now, we continue to create those notes, but then vote on a <em>single issue </em>to spend the majority of the retro discussing and ideating to solve.</p>



<figure><img loading="lazy" width="746" height="787" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w" sizes="(max-width: 746px) 100vw, 746px"/><figcaption><em>Now, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.</em></figcaption></figure>



<p>By the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.</p>



<h3>Stand-ups</h3>



<p><strong>Goal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.</strong></p>



<p>Incorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.</p>



<h2>Recommendations</h2>



<p>With simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:</p>



<p><strong>1. Try out a system holistically before making adjustments. </strong></p>



<p>Agile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.</p>



<p><strong>2. Ask the “stand-up question”.</strong></p>



<p>Asking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.</p>



<p><strong>3. Focus on a single issue in retros.</strong></p>



<p>Allow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.</p>



<p><strong>4. Plan sprints you can finish, and commit to finishing them.</strong></p>



<p>Create multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.</p>



<p>These changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers <a href="https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/" target="_blank" rel="noreferrer noopener">became more T-shaped</a> to finish the sprint’s work in time. </p>



<p>Additionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  </p>



<p>My team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.</p>



<h2>Acknowledgments</h2>



<p>Many thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Sophia Ciocca, Web Engineer</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Achieving Team Purpose and Pride with Scrum&#xA;</title>
      <link>https://engineering.atspotify.com/achieving-team-purpose-and-pride-with-scrum/</link>
      <description>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 27, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/achieving-team-purpose-and-pride-with-scrum/" title="Achieving Team Purpose and Pride with Scrum">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.</p>



<p>At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.</p>



<p>While this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.</p>



<h2>Our problem</h2>



<p>Our squad had long been following a process comprising bits and pieces from the <a rel="noreferrer noopener" href="https://scrumguides.org/scrum-guide.html" target="_blank">Scrum</a> framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.</p>



<p>The goals we ultimately wanted our process to achieve were:</p>



<ol><li><strong>Continuous improvement: </strong>We wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.</li></ol>



<ol start="2"><li><strong>Shared understanding and transparency: </strong>We wanted everyone on the team to know at any given time what work was happening, and what it entailed.</li></ol>



<ol start="3"><li><strong>Confidence: </strong>We wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.</li></ol>



<h2>Our approach</h2>



<p>To help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. </p>



<h3>Backlog refinement</h3>



<p><strong>Goal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.</strong></p>



<p>Before these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.</p>



<p>With the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually <em>point </em>those tickets.</p>



<p>This system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.</p>



<h3>Sprint planning</h3>



<p><strong>Goal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.</strong></p>



<p>Previously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.</p>



<p>Now, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.</p>



<h3>Sprint review</h3>



<p><strong>Goal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.</strong></p>



<p>While we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.</p>



<p>In a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:</p>



<ul><li><em>What work did we complete?</em></li><li><em>Is there anything we need to extend or add to what we’ve done?</em></li><li><em>Did we discover any tech debt?</em></li><li><em>Are we on track to meet our longer-term goals?</em></li></ul>



<p>This allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.</p>



<h3>Retro</h3>



<p><strong>Goal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.</strong></p>



<p>In previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.</p>



<p>Now, we continue to create those notes, but then vote on a <em>single issue </em>to spend the majority of the retro discussing and ideating to solve.</p>



<figure><img loading="lazy" width="746" height="787" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w" sizes="(max-width: 746px) 100vw, 746px"/><figcaption><em>Now, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.</em></figcaption></figure>



<p>By the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.</p>



<h3>Stand-ups</h3>



<p><strong>Goal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.</strong></p>



<p>Incorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.</p>



<h2>Recommendations</h2>



<p>With simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:</p>



<p><strong>1. Try out a system holistically before making adjustments. </strong></p>



<p>Agile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.</p>



<p><strong>2. Ask the “stand-up question”.</strong></p>



<p>Asking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.</p>



<p><strong>3. Focus on a single issue in retros.</strong></p>



<p>Allow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.</p>



<p><strong>4. Plan sprints you can finish, and commit to finishing them.</strong></p>



<p>Create multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.</p>



<p>These changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers <a href="https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/" target="_blank" rel="noreferrer noopener">became more T-shaped</a> to finish the sprint’s work in time. </p>



<p>Additionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  </p>



<p>My team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.</p>



<h2>Acknowledgments</h2>



<p>Many thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Sophia Ciocca, Web Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Achieving Team Purpose and Pride with Scrum&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/achieving-team-purpose-and-pride-with-scrum/</link>
      <description>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 27, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/achieving-team-purpose-and-pride-with-scrum/" title="Achieving Team Purpose and Pride with Scrum">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.</p>



<p>At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.</p>



<p>While this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.</p>



<h2>Our problem</h2>



<p>Our squad had long been following a process comprising bits and pieces from the <a rel="noreferrer noopener" href="https://scrumguides.org/scrum-guide.html" target="_blank">Scrum</a> framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.</p>



<p>The goals we ultimately wanted our process to achieve were:</p>



<ol><li><strong>Continuous improvement: </strong>We wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.</li></ol>



<ol start="2"><li><strong>Shared understanding and transparency: </strong>We wanted everyone on the team to know at any given time what work was happening, and what it entailed.</li></ol>



<ol start="3"><li><strong>Confidence: </strong>We wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.</li></ol>



<h2>Our approach</h2>



<p>To help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. </p>



<h3>Backlog refinement</h3>



<p><strong>Goal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.</strong></p>



<p>Before these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.</p>



<p>With the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually <em>point </em>those tickets.</p>



<p>This system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.</p>



<h3>Sprint planning</h3>



<p><strong>Goal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.</strong></p>



<p>Previously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.</p>



<p>Now, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.</p>



<h3>Sprint review</h3>



<p><strong>Goal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.</strong></p>



<p>While we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.</p>



<p>In a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:</p>



<ul><li><em>What work did we complete?</em></li><li><em>Is there anything we need to extend or add to what we’ve done?</em></li><li><em>Did we discover any tech debt?</em></li><li><em>Are we on track to meet our longer-term goals?</em></li></ul>



<p>This allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.</p>



<h3>Retro</h3>



<p><strong>Goal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.</strong></p>



<p>In previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.</p>



<p>Now, we continue to create those notes, but then vote on a <em>single issue </em>to spend the majority of the retro discussing and ideating to solve.</p>



<figure><img loading="lazy" width="746" height="787" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w" sizes="(max-width: 746px) 100vw, 746px"/><figcaption><em>Now, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.</em></figcaption></figure>



<p>By the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.</p>



<h3>Stand-ups</h3>



<p><strong>Goal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.</strong></p>



<p>Incorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.</p>



<h2>Recommendations</h2>



<p>With simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:</p>



<p><strong>1. Try out a system holistically before making adjustments. </strong></p>



<p>Agile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.</p>



<p><strong>2. Ask the “stand-up question”.</strong></p>



<p>Asking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.</p>



<p><strong>3. Focus on a single issue in retros.</strong></p>



<p>Allow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.</p>



<p><strong>4. Plan sprints you can finish, and commit to finishing them.</strong></p>



<p>Create multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.</p>



<p>These changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers <a href="https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/" target="_blank" rel="noreferrer noopener">became more T-shaped</a> to finish the sprint’s work in time. </p>



<p>Additionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  </p>



<p>My team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.</p>



<h2>Acknowledgments</h2>



<p>Many thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Sophia Ciocca, Web Engineer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture&#xA;</title>
      <link>https://engineering.atspotify.com/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/</link>
      <description>TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/" title="A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR;</strong> In episode 08 of our podcast series “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, we share stories and lessons from building and open sourcing <a href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank" rel="noreferrer noopener">Backstage</a>, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. <a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">Listen to the episode now</a> and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.</p>



<h2>How it started: “Like a cold shower”</h2>



<p>The story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. </p>



<p>As Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? </p>



<p>The answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. </p>



<p>But the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R&amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:</p>



<blockquote><p><strong>Gustav</strong>: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?</p><p><strong>Pia</strong>: Having spent 15 years as an engineer at other companies, it was like a cold shower.</p></blockquote>



<p>Brrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?</p>



<h2>Engineers are users, too</h2>



<p>When it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? </p>



<p>But the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. </p>



<p>According to Pia, two issues emerged as common causes for declining productivity:</p>



<ol><li><strong>Context switching</strong>: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” </li></ol>



<ol start="2"><li><strong>Discoverability</strong>: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many <em>almost</em> duplications — not pure duplications — because people are very smart and they would recognize that.” </li></ol>



<p>There would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. </p>



<p>In a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “<a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">rumour-driven development</a>”.</p>



<p>And as Spotify continued to grow, the problem only got worse.</p>



<h2>Speed, scale, autonomy… pick two?</h2>



<p>Now that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?</p>



<p>Turns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:</p>



<blockquote><p>“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”</p></blockquote>



<figure></figure>



<p>What made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:</p>



<p>“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”</p>



<p>Spotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.</p>



<h2>How it’s going: Not just adopted, but embraced</h2>



<p>So if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. </p>



<p>Even more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.</p>



<p>People can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. </p>



<p>For a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.</p>



<p>Want to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?</p>



<div><figure><a href="https://backstage.io/"><img loading="lazy" width="350" height="350" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/backstage-service-catalog-icon-4.gif" alt=""/></a></figure></div>







<p>Listen to episode 08 — “<a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">When to build vs buy — and when to open source</a>” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!</p>



<figure></figure>



<p>Want to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. </p>



<p>In each episode, Spotify’s Chief R&amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from <a href="https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD" target="_blank" rel="noreferrer noopener">Metallica’s Lars Ulrich and Napster’s Sean Parker</a>, to <a href="https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp" target="_blank" rel="noreferrer noopener">ML legend Andrew Ng</a>. </p>



<p>How did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" target="_blank" rel="noreferrer noopener">Google Cloud’s largest Dataflow jobs ever</a>? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">You can find all the podcast episodes here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture&#xA;</title>
      <link>https://engineering.atspotify.com/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/</link>
      <description>TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/" title="A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR;</strong> In episode 08 of our podcast series “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, we share stories and lessons from building and open sourcing <a href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank" rel="noreferrer noopener">Backstage</a>, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. <a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">Listen to the episode now</a> and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.</p>



<h2>How it started: “Like a cold shower”</h2>



<p>The story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. </p>



<p>As Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? </p>



<p>The answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. </p>



<p>But the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R&amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:</p>



<blockquote><p><strong>Gustav</strong>: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?</p><p><strong>Pia</strong>: Having spent 15 years as an engineer at other companies, it was like a cold shower.</p></blockquote>



<p>Brrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?</p>



<h2>Engineers are users, too</h2>



<p>When it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? </p>



<p>But the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. </p>



<p>According to Pia, two issues emerged as common causes for declining productivity:</p>



<ol><li><strong>Context switching</strong>: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” </li></ol>



<ol start="2"><li><strong>Discoverability</strong>: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many <em>almost</em> duplications — not pure duplications — because people are very smart and they would recognize that.” </li></ol>



<p>There would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. </p>



<p>In a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “<a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">rumour-driven development</a>”.</p>



<p>And as Spotify continued to grow, the problem only got worse.</p>



<h2>Speed, scale, autonomy… pick two?</h2>



<p>Now that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?</p>



<p>Turns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:</p>



<blockquote><p>“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”</p></blockquote>



<figure></figure>



<p>What made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:</p>



<p>“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”</p>



<p>Spotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.</p>



<h2>How it’s going: Not just adopted, but embraced</h2>



<p>So if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. </p>



<p>Even more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.</p>



<p>People can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. </p>



<p>For a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.</p>



<p>Want to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?</p>



<div><figure><a href="https://backstage.io/"><img loading="lazy" width="350" height="350" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/backstage-service-catalog-icon-4.gif" alt=""/></a></figure></div>







<p>Listen to episode 08 — “<a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">When to build vs buy — and when to open source</a>” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!</p>



<figure></figure>



<p>Want to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. </p>



<p>In each episode, Spotify’s Chief R&amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from <a href="https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD" target="_blank" rel="noreferrer noopener">Metallica’s Lars Ulrich and Napster’s Sean Parker</a>, to <a href="https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp" target="_blank" rel="noreferrer noopener">ML legend Andrew Ng</a>. </p>



<p>How did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" target="_blank" rel="noreferrer noopener">Google Cloud’s largest Dataflow jobs ever</a>? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">You can find all the podcast episodes here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/18/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/</link>
      <description>TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/18/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/" title="A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/a-product-story-backstage-1.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/05/a-product-story-backstage-1.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR;</strong> In episode 08 of our podcast series “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, we share stories and lessons from building and open sourcing <a href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank" rel="noreferrer noopener">Backstage</a>, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. <a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">Listen to the episode now</a> and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.</p>



<h2>How it started: “Like a cold shower”</h2>



<p>The story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. </p>



<p>As Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? </p>



<p>The answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. </p>



<p>But the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R&amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:</p>



<blockquote><p><strong>Gustav</strong>: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?</p><p><strong>Pia</strong>: Having spent 15 years as an engineer at other companies, it was like a cold shower.</p></blockquote>



<p>Brrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?</p>



<h2>Engineers are users, too</h2>



<p>When it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? </p>



<p>But the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. </p>



<p>According to Pia, two issues emerged as common causes for declining productivity:</p>



<ol><li><strong>Context switching</strong>: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” </li></ol>



<ol start="2"><li><strong>Discoverability</strong>: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many <em>almost</em> duplications — not pure duplications — because people are very smart and they would recognize that.” </li></ol>



<p>There would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. </p>



<p>In a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “<a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">rumour-driven development</a>”.</p>



<p>And as Spotify continued to grow, the problem only got worse.</p>



<h2>Speed, scale, autonomy… pick two?</h2>



<p>Now that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?</p>



<p>Turns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:</p>



<blockquote><p>“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”</p></blockquote>



<figure></figure>



<p>What made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:</p>



<p>“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”</p>



<p>Spotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.</p>



<h2>How it’s going: Not just adopted, but embraced</h2>



<p>So if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. </p>



<p>Even more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.</p>



<p>People can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. </p>



<p>For a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.</p>



<p>Want to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?</p>



<div><figure><a href="https://backstage.io/"><img loading="lazy" width="350" height="350" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/backstage-service-catalog-icon-4.gif" alt=""/></a></figure></div>







<p>Listen to episode 08 — “<a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">When to build vs buy — and when to open source</a>” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!</p>



<figure></figure>



<p>Want to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. </p>



<p>In each episode, Spotify’s Chief R&amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from <a href="https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD" target="_blank" rel="noreferrer noopener">Metallica’s Lars Ulrich and Napster’s Sean Parker</a>, to <a href="https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp" target="_blank" rel="noreferrer noopener">ML legend Andrew Ng</a>. </p>



<p>How did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" target="_blank" rel="noreferrer noopener">Google Cloud’s largest Dataflow jobs ever</a>? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">You can find all the podcast episodes here</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/a-product-story-backstage-1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/</link>
      <description>TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/" title="A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR;</strong> In episode 08 of our podcast series “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, we share stories and lessons from building and open sourcing <a href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank" rel="noreferrer noopener">Backstage</a>, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. <a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">Listen to the episode now</a> and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.</p>



<h2>How it started: “Like a cold shower”</h2>



<p>The story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. </p>



<p>As Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? </p>



<p>The answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. </p>



<p>But the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R&amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:</p>



<blockquote><p><strong>Gustav</strong>: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?</p><p><strong>Pia</strong>: Having spent 15 years as an engineer at other companies, it was like a cold shower.</p></blockquote>



<p>Brrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?</p>



<h2>Engineers are users, too</h2>



<p>When it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? </p>



<p>But the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. </p>



<p>According to Pia, two issues emerged as common causes for declining productivity:</p>



<ol><li><strong>Context switching</strong>: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” </li></ol>



<ol start="2"><li><strong>Discoverability</strong>: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many <em>almost</em> duplications — not pure duplications — because people are very smart and they would recognize that.” </li></ol>



<p>There would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. </p>



<p>In a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “<a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">rumour-driven development</a>”.</p>



<p>And as Spotify continued to grow, the problem only got worse.</p>



<h2>Speed, scale, autonomy… pick two?</h2>



<p>Now that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?</p>



<p>Turns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:</p>



<blockquote><p>“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”</p></blockquote>



<figure></figure>



<p>What made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:</p>



<p>“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”</p>



<p>Spotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.</p>



<h2>How it’s going: Not just adopted, but embraced</h2>



<p>So if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. </p>



<p>Even more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.</p>



<p>People can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. </p>



<p>For a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.</p>



<p>Want to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?</p>



<div><figure><a href="https://backstage.io/"><img loading="lazy" width="350" height="350" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/backstage-service-catalog-icon-4.gif" alt=""/></a></figure></div>







<p>Listen to episode 08 — “<a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">When to build vs buy — and when to open source</a>” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!</p>



<figure></figure>



<p>Want to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. </p>



<p>In each episode, Spotify’s Chief R&amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from <a href="https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD" target="_blank" rel="noreferrer noopener">Metallica’s Lars Ulrich and Napster’s Sean Parker</a>, to <a href="https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp" target="_blank" rel="noreferrer noopener">ML legend Andrew Ng</a>. </p>



<p>How did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" target="_blank" rel="noreferrer noopener">Google Cloud’s largest Dataflow jobs ever</a>? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">You can find all the podcast episodes here</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a>, <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It&#xA;</title>
      <link>https://engineering.atspotify.com/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/</link>
      <description>Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4585">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 11, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/" title="Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>Last week, Spotify won an award — and we’re not playing it cool. We <a href="https://twitter.com/SpotifyEng/status/1389988765725245441" target="_blank" rel="noreferrer noopener">tweeted</a>. We bragged <a href="https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs" target="_blank" rel="noreferrer noopener">on LinkedIn</a>. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.</p>



<p>The award is <a rel="noreferrer noopener" href="https://www.cncf.io/" target="_blank">Cloud Native Computing Foundation</a>’s Top End User Award, <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">announced</a> at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the <a rel="noreferrer noopener" href="https://www.cncf.io/enduser/" target="_blank">End User Community</a>, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).<br/></p>



<p>We joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like <a rel="noreferrer noopener" href="https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/" target="_blank">Kubernetes</a>, <a rel="noreferrer noopener" href="https://www.youtube.com/watch?v=fMq3IpPE3TU" target="_blank">gRPC</a>, and <a href="https://www.youtube.com/watch?v=HfRU414cjjQ">Envoy</a>), our leadership in CNCF forums and meetups (<a rel="noreferrer noopener" href="https://community.cncf.io/stockholm/" target="_blank">look out for the next Stockholm one here</a>), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank">Backstage</a>, which is now in <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank">the CNCF Sandbox</a>.</p>



<figure></figure>



<p>But in the end, this is a win for all of Spotify R&amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.</p>



<p>You can read more about the award on the <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">CNCF website</a>. And to learn more about what got us here, listen to our podcast series “<a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">Spotify: A Product Story</a>”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:</p>



<div>
<figure><figcaption>Hear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers</figcaption></figure>



<figure><figcaption>Hear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.</figcaption></figure>
</div>




        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Dave Zolotusky, Principal Engineer &amp; CNCF Technical Oversight Committee Representative</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/11/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/</link>
      <description>Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4585">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 11, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/11/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/" title="Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png"/>                    </a>
                        
        </p>

        

        
<p>Last week, Spotify won an award — and we’re not playing it cool. We <a href="https://twitter.com/SpotifyEng/status/1389988765725245441" target="_blank" rel="noreferrer noopener">tweeted</a>. We bragged <a href="https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs" target="_blank" rel="noreferrer noopener">on LinkedIn</a>. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.</p>



<p>The award is <a rel="noreferrer noopener" href="https://www.cncf.io/" target="_blank">Cloud Native Computing Foundation</a>’s Top End User Award, <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">announced</a> at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the <a rel="noreferrer noopener" href="https://www.cncf.io/enduser/" target="_blank">End User Community</a>, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).<br/></p>



<p>We joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like <a rel="noreferrer noopener" href="https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/" target="_blank">Kubernetes</a>, <a rel="noreferrer noopener" href="https://www.youtube.com/watch?v=fMq3IpPE3TU" target="_blank">gRPC</a>, and <a href="https://www.youtube.com/watch?v=HfRU414cjjQ">Envoy</a>), our leadership in CNCF forums and meetups (<a rel="noreferrer noopener" href="https://community.cncf.io/stockholm/" target="_blank">look out for the next Stockholm one here</a>), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank">Backstage</a>, which is now in <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank">the CNCF Sandbox</a>.</p>



<figure></figure>



<p>But in the end, this is a win for all of Spotify R&amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.</p>



<p>You can read more about the award on the <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">CNCF website</a>. And to learn more about what got us here, listen to our podcast series “<a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">Spotify: A Product Story</a>”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:</p>



<div>
<figure><figcaption>Hear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers</figcaption></figure>



<figure><figcaption>Hear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.</figcaption></figure>
</div>




        <br/>

        
        

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Dave Zolotusky, Principal Engineer &amp; CNCF Technical Oversight Committee Representative</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It&#xA;</title>
      <link>https://engineering.atspotify.com/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/</link>
      <description>Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4585">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 11, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/" title="Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>Last week, Spotify won an award — and we’re not playing it cool. We <a href="https://twitter.com/SpotifyEng/status/1389988765725245441" target="_blank" rel="noreferrer noopener">tweeted</a>. We bragged <a href="https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs" target="_blank" rel="noreferrer noopener">on LinkedIn</a>. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.</p>



<p>The award is <a rel="noreferrer noopener" href="https://www.cncf.io/" target="_blank">Cloud Native Computing Foundation</a>’s Top End User Award, <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">announced</a> at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the <a rel="noreferrer noopener" href="https://www.cncf.io/enduser/" target="_blank">End User Community</a>, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).<br/></p>



<p>We joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like <a rel="noreferrer noopener" href="https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/" target="_blank">Kubernetes</a>, <a rel="noreferrer noopener" href="https://www.youtube.com/watch?v=fMq3IpPE3TU" target="_blank">gRPC</a>, and <a href="https://www.youtube.com/watch?v=HfRU414cjjQ">Envoy</a>), our leadership in CNCF forums and meetups (<a rel="noreferrer noopener" href="https://community.cncf.io/stockholm/" target="_blank">look out for the next Stockholm one here</a>), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank">Backstage</a>, which is now in <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank">the CNCF Sandbox</a>.</p>



<figure></figure>



<p>But in the end, this is a win for all of Spotify R&amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.</p>



<p>You can read more about the award on the <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">CNCF website</a>. And to learn more about what got us here, listen to our podcast series “<a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">Spotify: A Product Story</a>”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:</p>



<div>
<figure><figcaption>Hear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers</figcaption></figure>



<figure><figcaption>Hear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.</figcaption></figure>
</div>




        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Dave Zolotusky, Principal Engineer &amp; CNCF Technical Oversight Committee Representative</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/</link>
      <description>Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4585">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 11, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/" title="Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>Last week, Spotify won an award — and we’re not playing it cool. We <a href="https://twitter.com/SpotifyEng/status/1389988765725245441" target="_blank" rel="noreferrer noopener">tweeted</a>. We bragged <a href="https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs" target="_blank" rel="noreferrer noopener">on LinkedIn</a>. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.</p>



<p>The award is <a rel="noreferrer noopener" href="https://www.cncf.io/" target="_blank">Cloud Native Computing Foundation</a>’s Top End User Award, <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">announced</a> at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the <a rel="noreferrer noopener" href="https://www.cncf.io/enduser/" target="_blank">End User Community</a>, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).<br/></p>



<p>We joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like <a rel="noreferrer noopener" href="https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/" target="_blank">Kubernetes</a>, <a rel="noreferrer noopener" href="https://www.youtube.com/watch?v=fMq3IpPE3TU" target="_blank">gRPC</a>, and <a href="https://www.youtube.com/watch?v=HfRU414cjjQ">Envoy</a>), our leadership in CNCF forums and meetups (<a rel="noreferrer noopener" href="https://community.cncf.io/stockholm/" target="_blank">look out for the next Stockholm one here</a>), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank">Backstage</a>, which is now in <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank">the CNCF Sandbox</a>.</p>



<figure></figure>



<p>But in the end, this is a win for all of Spotify R&amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.</p>



<p>You can read more about the award on the <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">CNCF website</a>. And to learn more about what got us here, listen to our podcast series “<a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">Spotify: A Product Story</a>”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:</p>



<div>
<figure><figcaption>Hear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers</figcaption></figure>



<figure><figcaption>Hear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.</figcaption></figure>
</div>




        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Dave Zolotusky, Principal Engineer &amp; CNCF Technical Oversight Committee Representative</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Customization vs. Configuration in Evolving Design Systems&#xA;</title>
      <link>https://engineering.atspotify.com/customization-vs-configuration-in-evolving-design-systems/</link>
      <description>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 28, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/customization-vs-configuration-in-evolving-design-systems/" title="Customization vs. Configuration in Evolving Design Systems">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.</p>



<p>As the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.</p>



<p>How do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?</p>



<p>As a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.</p>



<p>When working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.</p>



<p>In this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    </p>



<h2>Abstraction</h2>



<p>So what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. </p>



<p><em>For a more thorough view of abstraction in software, and in life, check out </em><a href="https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af" target="_blank" rel="noreferrer noopener"><em>Levels of Abstraction, A Key Concept in Systems Design</em></a><em> by Daniel Jhin Yoo. </em></p>



<p>In this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.</p>



<h2>Current landscape</h2>



<p>Now that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:</p>



<ul><li><em>Customization — </em>Custom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.</li></ul>



<ul><li><em>Configuration — </em>The original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.</li></ul>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-768x433.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-1536x865.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1.png 1672w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Some highlights of our available options:</p>



<ul><li><strong>Powerhouse definitions</strong><strong>:</strong> By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. </li></ul>



<ul><li><strong>Prepacked guidelines</strong><strong>:</strong> Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.</li></ul>



<ul><li><strong>Property passthroughs</strong><strong>:</strong> These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.</li></ul>



<ul><li><strong>Direct overrides</strong><strong>:</strong> These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.</li></ul>



<h2>Customization vs. configuration</h2>



<p>With the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.</p>



<h3>Customization</h3>



<p><strong>Pros: Autonomy, speed, innovation</strong></p>



<p>The greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.</p>



<p><strong>Cons: Lack of coherency, loss of maintainability, potential duplication</strong></p>



<p>A local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.</p>



<h3>Configuration</h3>



<p><strong>Pros: Consistency, contribution, maintainability</strong> </p>



<p>If emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.</p>



<p><strong>Cons: Can become a bottleneck, rigidness, vocabulary awareness</strong></p>



<p>The other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.</p>



<h2>How to decide which approach to use</h2>



<p>With both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:</p>



<ul><li><strong>Feature maturity</strong><strong>:</strong> If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.</li></ul>



<ul><li><strong>Product maturity</strong><strong>:</strong> As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.</li></ul>



<ul><li><strong>Timeline</strong><strong>:</strong> While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?</li></ul>



<ul><li><strong>Reusability</strong><strong>:</strong> If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.</li></ul>



<h2>Key takeaways</h2>



<p>When evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.</p>



<p>The more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.</p>



<p>Lastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.</p>



<p>While there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.</p>



<p>—</p>



<p><em>A huge shout out to Krist Wongsuphasawat and his article </em><a href="https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7" target="_blank" rel="noreferrer noopener"><em>Navigating the Wide World of Data Visualization Libraries</em></a><em>. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. </em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/web/" rel="tag">web</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Charlie Backus</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Customization vs. Configuration in Evolving Design Systems&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/28/customization-vs-configuration-in-evolving-design-systems/</link>
      <description>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 28, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/28/customization-vs-configuration-in-evolving-design-systems/" title="Customization vs. Configuration in Evolving Design Systems">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/04/image3.png"/>                    </a>
                        
        </p>

        

        
<p>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.</p>



<p>As the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.</p>



<p>How do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?</p>



<p>As a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.</p>



<p>When working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.</p>



<p>In this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    </p>



<h2>Abstraction</h2>



<p>So what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. </p>



<p><em>For a more thorough view of abstraction in software, and in life, check out </em><a href="https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af" target="_blank" rel="noreferrer noopener"><em>Levels of Abstraction, A Key Concept in Systems Design</em></a><em> by Daniel Jhin Yoo. </em></p>



<p>In this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.</p>



<h2>Current landscape</h2>



<p>Now that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:</p>



<ul><li><em>Customization — </em>Custom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.</li></ul>



<ul><li><em>Configuration — </em>The original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.</li></ul>



<div><figure><img loading="lazy" width="700" height="394" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x394.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x394.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-250x141.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-768x433.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-1536x865.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-120x68.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1.png 1672w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Some highlights of our available options:</p>



<ul><li><strong>Powerhouse definitions</strong><strong>:</strong> By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. </li></ul>



<ul><li><strong>Prepacked guidelines</strong><strong>:</strong> Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.</li></ul>



<ul><li><strong>Property passthroughs</strong><strong>:</strong> These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.</li></ul>



<ul><li><strong>Direct overrides</strong><strong>:</strong> These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.</li></ul>



<h2>Customization vs. configuration</h2>



<p>With the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.</p>



<h3>Customization</h3>



<p><strong>Pros: Autonomy, speed, innovation</strong></p>



<p>The greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.</p>



<p><strong>Cons: Lack of coherency, loss of maintainability, potential duplication</strong></p>



<p>A local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.</p>



<h3>Configuration</h3>



<p><strong>Pros: Consistency, contribution, maintainability</strong> </p>



<p>If emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.</p>



<p><strong>Cons: Can become a bottleneck, rigidness, vocabulary awareness</strong></p>



<p>The other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.</p>



<h2>How to decide which approach to use</h2>



<p>With both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:</p>



<ul><li><strong>Feature maturity</strong><strong>:</strong> If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.</li></ul>



<ul><li><strong>Product maturity</strong><strong>:</strong> As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.</li></ul>



<ul><li><strong>Timeline</strong><strong>:</strong> While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?</li></ul>



<ul><li><strong>Reusability</strong><strong>:</strong> If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.</li></ul>



<h2>Key takeaways</h2>



<p>When evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.</p>



<p>The more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.</p>



<p>Lastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.</p>



<p>While there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.</p>



<p>—</p>



<p><em>A huge shout out to Krist Wongsuphasawat and his article </em><a href="https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7" target="_blank" rel="noreferrer noopener"><em>Navigating the Wide World of Data Visualization Libraries</em></a><em>. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. </em></p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Charlie Backus</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Customization vs. Configuration in Evolving Design Systems&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/customization-vs-configuration-in-evolving-design-systems/</link>
      <description>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 28, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/customization-vs-configuration-in-evolving-design-systems/" title="Customization vs. Configuration in Evolving Design Systems">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.</p>



<p>As the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.</p>



<p>How do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?</p>



<p>As a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.</p>



<p>When working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.</p>



<p>In this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    </p>



<h2>Abstraction</h2>



<p>So what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. </p>



<p><em>For a more thorough view of abstraction in software, and in life, check out </em><a href="https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af" target="_blank" rel="noreferrer noopener"><em>Levels of Abstraction, A Key Concept in Systems Design</em></a><em> by Daniel Jhin Yoo. </em></p>



<p>In this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.</p>



<h2>Current landscape</h2>



<p>Now that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:</p>



<ul><li><em>Customization — </em>Custom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.</li></ul>



<ul><li><em>Configuration — </em>The original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.</li></ul>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-768x433.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-1536x865.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1.png 1672w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Some highlights of our available options:</p>



<ul><li><strong>Powerhouse definitions</strong><strong>:</strong> By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. </li></ul>



<ul><li><strong>Prepacked guidelines</strong><strong>:</strong> Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.</li></ul>



<ul><li><strong>Property passthroughs</strong><strong>:</strong> These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.</li></ul>



<ul><li><strong>Direct overrides</strong><strong>:</strong> These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.</li></ul>



<h2>Customization vs. configuration</h2>



<p>With the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.</p>



<h3>Customization</h3>



<p><strong>Pros: Autonomy, speed, innovation</strong></p>



<p>The greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.</p>



<p><strong>Cons: Lack of coherency, loss of maintainability, potential duplication</strong></p>



<p>A local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.</p>



<h3>Configuration</h3>



<p><strong>Pros: Consistency, contribution, maintainability</strong> </p>



<p>If emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.</p>



<p><strong>Cons: Can become a bottleneck, rigidness, vocabulary awareness</strong></p>



<p>The other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.</p>



<h2>How to decide which approach to use</h2>



<p>With both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:</p>



<ul><li><strong>Feature maturity</strong><strong>:</strong> If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.</li></ul>



<ul><li><strong>Product maturity</strong><strong>:</strong> As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.</li></ul>



<ul><li><strong>Timeline</strong><strong>:</strong> While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?</li></ul>



<ul><li><strong>Reusability</strong><strong>:</strong> If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.</li></ul>



<h2>Key takeaways</h2>



<p>When evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.</p>



<p>The more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.</p>



<p>Lastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.</p>



<p>While there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.</p>



<p>—</p>



<p><em>A huge shout out to Krist Wongsuphasawat and his article </em><a href="https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7" target="_blank" rel="noreferrer noopener"><em>Navigating the Wide World of Data Visualization Libraries</em></a><em>. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. </em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/web/" rel="tag">web</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Charlie Backus</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Customization vs. Configuration in Evolving Design Systems&#xA;</title>
      <link>https://engineering.atspotify.com/customization-vs-configuration-in-evolving-design-systems/</link>
      <description>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 28, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/customization-vs-configuration-in-evolving-design-systems/" title="Customization vs. Configuration in Evolving Design Systems">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.</p>



<p>As the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.</p>



<p>How do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?</p>



<p>As a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.</p>



<p>When working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.</p>



<p>In this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    </p>



<h2>Abstraction</h2>



<p>So what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. </p>



<p><em>For a more thorough view of abstraction in software, and in life, check out </em><a href="https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af" target="_blank" rel="noreferrer noopener"><em>Levels of Abstraction, A Key Concept in Systems Design</em></a><em> by Daniel Jhin Yoo. </em></p>



<p>In this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.</p>



<h2>Current landscape</h2>



<p>Now that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:</p>



<ul><li><em>Customization — </em>Custom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.</li></ul>



<ul><li><em>Configuration — </em>The original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.</li></ul>



<div><figure><img loading="lazy" width="700" height="394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-768x433.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-1536x865.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1.png 1672w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Some highlights of our available options:</p>



<ul><li><strong>Powerhouse definitions</strong><strong>:</strong> By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. </li></ul>



<ul><li><strong>Prepacked guidelines</strong><strong>:</strong> Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.</li></ul>



<ul><li><strong>Property passthroughs</strong><strong>:</strong> These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.</li></ul>



<ul><li><strong>Direct overrides</strong><strong>:</strong> These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.</li></ul>



<h2>Customization vs. configuration</h2>



<p>With the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.</p>



<h3>Customization</h3>



<p><strong>Pros: Autonomy, speed, innovation</strong></p>



<p>The greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.</p>



<p><strong>Cons: Lack of coherency, loss of maintainability, potential duplication</strong></p>



<p>A local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.</p>



<h3>Configuration</h3>



<p><strong>Pros: Consistency, contribution, maintainability</strong> </p>



<p>If emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.</p>



<p><strong>Cons: Can become a bottleneck, rigidness, vocabulary awareness</strong></p>



<p>The other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.</p>



<h2>How to decide which approach to use</h2>



<p>With both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:</p>



<ul><li><strong>Feature maturity</strong><strong>:</strong> If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.</li></ul>



<ul><li><strong>Product maturity</strong><strong>:</strong> As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.</li></ul>



<ul><li><strong>Timeline</strong><strong>:</strong> While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?</li></ul>



<ul><li><strong>Reusability</strong><strong>:</strong> If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.</li></ul>



<h2>Key takeaways</h2>



<p>When evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.</p>



<p>The more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.</p>



<p>Lastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.</p>



<p>While there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.</p>



<p>—</p>



<p><em>A huge shout out to Krist Wongsuphasawat and his article </em><a href="https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7" target="_blank" rel="noreferrer noopener"><em>Navigating the Wide World of Data Visualization Libraries</em></a><em>. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. </em></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/web/" rel="tag">web</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Charlie Backus</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Nour Daoud Bösing: Security Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/nour-daoud-bosing-security-engineer/</link>
      <description>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4531">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-120x69.png 120w" sizes="(max-width: 1200px) 100vw, 1200px"/>
                                  
             </p>
             <div>
             
                 <p><b>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. </b></p>
             </div>
         </div>

         


         

         
<blockquote><p>6:00am</p></blockquote>



<p>These days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. </p>



<blockquote><p>7:30am</p></blockquote>



<p>I work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  </p>



<p>Things are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Twice a week, my team has a virtual <em>fika</em> (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>My afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.</p>



<blockquote><p>4:00pm</p></blockquote>



<p>I finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! </p>



<p>On nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to <a href="https://jusoorsyria.com/">Jussor</a>, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" width="700" height="583" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div><p>

         Tags: <a href="https://engineering.atspotify.com/tag/security/" rel="tag">security</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Nour Daoud Bösing: Security Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/26/nour-daoud-bosing-security-engineer/</link>
      <description>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4531">
     <div>
         
         
         
         <div>
             <p><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png 1200w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-120x69.png 120w" sizes="(max-width: 1200px) 100vw, 1200px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/04/Nour-edit-1.png"/>
                                  
             </p>
             <p><b>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. </b></p>
         </div>

         


         

         
<blockquote><p>6:00am</p></blockquote>



<p>These days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. </p>



<blockquote><p>7:30am</p></blockquote>



<p>I work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  </p>



<p>Things are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Twice a week, my team has a virtual <em>fika</em> (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>My afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.</p>



<blockquote><p>4:00pm</p></blockquote>



<p>I finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! </p>



<p>On nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to <a href="https://jusoorsyria.com/">Jussor</a>, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.</p>







<figure><img loading="lazy" width="700" height="111" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" width="700" height="583" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>

         
         

         <p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Nour Daoud Bösing: Security Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/nour-daoud-bosing-security-engineer/</link>
      <description>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4531">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-120x69.png 120w" sizes="(max-width: 1200px) 100vw, 1200px"/>
                                  
             </p>
             <div>
             
                 <p><b>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. </b></p>
             </div>
         </div>

         


         

         
<blockquote><p>6:00am</p></blockquote>



<p>These days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. </p>



<blockquote><p>7:30am</p></blockquote>



<p>I work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  </p>



<p>Things are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Twice a week, my team has a virtual <em>fika</em> (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>My afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.</p>



<blockquote><p>4:00pm</p></blockquote>



<p>I finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! </p>



<p>On nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to <a href="https://jusoorsyria.com/">Jussor</a>, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" width="700" height="583" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div><p>

         Tags: <a href="https://engineering.atspotify.com/tag/security/" rel="tag">security</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Nour Daoud Bösing: Security Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/nour-daoud-bosing-security-engineer/</link>
      <description>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4531">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-120x69.png 120w" sizes="(max-width: 1200px) 100vw, 1200px"/>
                                  
             </p>
             <div>
             
                 <p><b>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. </b></p>
             </div>
         </div>

         


         

         
<blockquote><p>6:00am</p></blockquote>



<p>These days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. </p>



<blockquote><p>7:30am</p></blockquote>



<p>I work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  </p>



<p>Things are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Twice a week, my team has a virtual <em>fika</em> (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>My afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.</p>



<blockquote><p>4:00pm</p></blockquote>



<p>I finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! </p>



<p>On nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to <a href="https://jusoorsyria.com/">Jussor</a>, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" width="700" height="583" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div><p>

         Tags: <a href="https://engineering.atspotify.com/tag/security/" rel="tag">security</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Rethinking Spotify Search&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/15/rethinking-spotify-search/</link>
      <description>Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/15/rethinking-spotify-search/" title="Rethinking Spotify Search">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Search-gif.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/04/Search-gif.gif"/>                    </a>
                        
        </p>

        

        
<h2>Search @ Spotify</h2>



<div><p>Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.</p><p>Since Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.</p></div>



<h2>When more doesn’t mean more</h2>



<p>In the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-700x352.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “<em>If each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”</em> That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-700x352.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>By the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.</p>



<p>We were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. </p>



<div><figure><img loading="lazy" width="2101" height="879" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15.jpg 2101w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-120x50.jpg 120w" sizes="(max-width: 2101px) 100vw, 2101px"/></figure></div>



<p>In the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. </p>



<h2>Search as a platform</h2>



<p>Great — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.</p>



<h2>From user obsession to developer satisfaction</h2>



<p>With insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>While these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “<em>[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.</em>” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.</p>



<h2>Conclusion</h2>



<p>Katarina Berg, Chief HR Officer, says “<em>Growth is our mantra</em>” and “<em>Change is our constant.”</em> This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.</p>



<p><strong>References</strong></p>



<p>Ang Li et al., “Search Mindsets:<em> </em>Understanding Focused and Non-Focused Information Seeking  in<em> </em>Music Search,” <a href="https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/" target="_blank" rel="noreferrer noopener">publication</a> <em>WWW ’19: The World Wide Web Conference</em> (May 2019): 2971–2977. <a href="https://doi.org/10.1145/3308558.3313627" target="_blank" rel="noreferrer noopener">https://doi.org/10.1145/3308558.3313627</a></p>



<p>“Shareholder Letter Q4 2020” (February 3, 2021) <a href="https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf" target="_blank" rel="noreferrer noopener">https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf</a></p>



<p>“Spotify — Company Info,” For the Record, <a href="https://newsroom.spotify.com/company-info/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/company-info/</a></p>



<p>“The Band Manifesto.” <a href="https://www.spotifyjobs.com/culture/the-band-manifesto">https://www.spotifyjobs.com/culture/the-band-manifesto</a></p>



<p>“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).</p>



<p>“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) <a href="https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/</a></p>



<p> “SPOTIFY PODCASTS DATASET.” <a href="https://podcastsdataset.byspotify.com/" target="_blank" rel="noreferrer noopener">https://podcastsdataset.byspotify.com/</a></p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Hugo Galvão and Daniel Doro</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Search-gif.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Rethinking Spotify Search&#xA;</title>
      <link>https://engineering.atspotify.com/rethinking-spotify-search/</link>
      <description>Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/rethinking-spotify-search/" title="Rethinking Spotify Search">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif" alt=""/>                    </a>
                        
        </p>

        

        
<h2>Search @ Spotify</h2>



<div><p>Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.</p><p>Since Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.</p></div>



<h2>When more doesn’t mean more</h2>



<p>In the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “<em>If each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”</em> That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>By the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.</p>



<p>We were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. </p>



<div><figure><img loading="lazy" width="2101" height="879" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg 2101w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-120x50.jpg 120w" sizes="(max-width: 2101px) 100vw, 2101px"/></figure></div>



<p>In the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. </p>



<h2>Search as a platform</h2>



<p>Great — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.</p>



<h2>From user obsession to developer satisfaction</h2>



<p>With insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>While these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “<em>[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.</em>” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.</p>



<h2>Conclusion</h2>



<p>Katarina Berg, Chief HR Officer, says “<em>Growth is our mantra</em>” and “<em>Change is our constant.”</em> This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.</p>



<p><strong>References</strong></p>



<p>Ang Li et al., “Search Mindsets:<em> </em>Understanding Focused and Non-Focused Information Seeking  in<em> </em>Music Search,” <a href="https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/" target="_blank" rel="noreferrer noopener">publication</a> <em>WWW ’19: The World Wide Web Conference</em> (May 2019): 2971–2977. <a href="https://doi.org/10.1145/3308558.3313627" target="_blank" rel="noreferrer noopener">https://doi.org/10.1145/3308558.3313627</a></p>



<p>“Shareholder Letter Q4 2020” (February 3, 2021) <a href="https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf" target="_blank" rel="noreferrer noopener">https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf</a></p>



<p>“Spotify — Company Info,” For the Record, <a href="https://newsroom.spotify.com/company-info/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/company-info/</a></p>



<p>“The Band Manifesto.” <a href="https://www.spotifyjobs.com/culture/the-band-manifesto">https://www.spotifyjobs.com/culture/the-band-manifesto</a></p>



<p>“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).</p>



<p>“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) <a href="https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/</a></p>



<p> “SPOTIFY PODCASTS DATASET.” <a href="https://podcastsdataset.byspotify.com/" target="_blank" rel="noreferrer noopener">https://podcastsdataset.byspotify.com/</a></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Hugo Galvão and Daniel Doro</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Rethinking Spotify Search&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/rethinking-spotify-search/</link>
      <description>Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/rethinking-spotify-search/" title="Rethinking Spotify Search">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif" alt=""/>                    </a>
                        
        </p>

        

        
<h2>Search @ Spotify</h2>



<div><p>Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.</p><p>Since Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.</p></div>



<h2>When more doesn’t mean more</h2>



<p>In the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “<em>If each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”</em> That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>By the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.</p>



<p>We were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. </p>



<div><figure><img loading="lazy" width="2101" height="879" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg 2101w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-120x50.jpg 120w" sizes="(max-width: 2101px) 100vw, 2101px"/></figure></div>



<p>In the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. </p>



<h2>Search as a platform</h2>



<p>Great — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.</p>



<h2>From user obsession to developer satisfaction</h2>



<p>With insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>While these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “<em>[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.</em>” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.</p>



<h2>Conclusion</h2>



<p>Katarina Berg, Chief HR Officer, says “<em>Growth is our mantra</em>” and “<em>Change is our constant.”</em> This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.</p>



<p><strong>References</strong></p>



<p>Ang Li et al., “Search Mindsets:<em> </em>Understanding Focused and Non-Focused Information Seeking  in<em> </em>Music Search,” <a href="https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/" target="_blank" rel="noreferrer noopener">publication</a> <em>WWW ’19: The World Wide Web Conference</em> (May 2019): 2971–2977. <a href="https://doi.org/10.1145/3308558.3313627" target="_blank" rel="noreferrer noopener">https://doi.org/10.1145/3308558.3313627</a></p>



<p>“Shareholder Letter Q4 2020” (February 3, 2021) <a href="https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf" target="_blank" rel="noreferrer noopener">https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf</a></p>



<p>“Spotify — Company Info,” For the Record, <a href="https://newsroom.spotify.com/company-info/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/company-info/</a></p>



<p>“The Band Manifesto.” <a href="https://www.spotifyjobs.com/culture/the-band-manifesto">https://www.spotifyjobs.com/culture/the-band-manifesto</a></p>



<p>“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).</p>



<p>“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) <a href="https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/</a></p>



<p> “SPOTIFY PODCASTS DATASET.” <a href="https://podcastsdataset.byspotify.com/" target="_blank" rel="noreferrer noopener">https://podcastsdataset.byspotify.com/</a></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Hugo Galvão and Daniel Doro</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Rethinking Spotify Search&#xA;</title>
      <link>https://engineering.atspotify.com/rethinking-spotify-search/</link>
      <description>Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/rethinking-spotify-search/" title="Rethinking Spotify Search">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif" alt=""/>                    </a>
                        
        </p>

        

        
<h2>Search @ Spotify</h2>



<div><p>Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.</p><p>Since Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.</p></div>



<h2>When more doesn’t mean more</h2>



<p>In the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “<em>If each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”</em> That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>By the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.</p>



<p>We were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. </p>



<div><figure><img loading="lazy" width="2101" height="879" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg 2101w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-120x50.jpg 120w" sizes="(max-width: 2101px) 100vw, 2101px"/></figure></div>



<p>In the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. </p>



<h2>Search as a platform</h2>



<p>Great — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.</p>



<h2>From user obsession to developer satisfaction</h2>



<p>With insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>While these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “<em>[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.</em>” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.</p>



<h2>Conclusion</h2>



<p>Katarina Berg, Chief HR Officer, says “<em>Growth is our mantra</em>” and “<em>Change is our constant.”</em> This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.</p>



<p><strong>References</strong></p>



<p>Ang Li et al., “Search Mindsets:<em> </em>Understanding Focused and Non-Focused Information Seeking  in<em> </em>Music Search,” <a href="https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/" target="_blank" rel="noreferrer noopener">publication</a> <em>WWW ’19: The World Wide Web Conference</em> (May 2019): 2971–2977. <a href="https://doi.org/10.1145/3308558.3313627" target="_blank" rel="noreferrer noopener">https://doi.org/10.1145/3308558.3313627</a></p>



<p>“Shareholder Letter Q4 2020” (February 3, 2021) <a href="https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf" target="_blank" rel="noreferrer noopener">https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf</a></p>



<p>“Spotify — Company Info,” For the Record, <a href="https://newsroom.spotify.com/company-info/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/company-info/</a></p>



<p>“The Band Manifesto.” <a href="https://www.spotifyjobs.com/culture/the-band-manifesto">https://www.spotifyjobs.com/culture/the-band-manifesto</a></p>



<p>“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).</p>



<p>“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) <a href="https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/</a></p>



<p> “SPOTIFY PODCASTS DATASET.” <a href="https://podcastsdataset.byspotify.com/" target="_blank" rel="noreferrer noopener">https://podcastsdataset.byspotify.com/</a></p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/backend/" rel="tag">backend</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Hugo Galvão and Daniel Doro</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Building the Future of Our Desktop Apps&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/building-the-future-of-our-desktop-apps/</link>
      <description>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/building-the-future-of-our-desktop-apps/" title="Building the Future of Our Desktop Apps">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.</p>



<p>We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</p>



<h2>In the beginning, there were two clients</h2>



<div><p>Towards the end of 2018, our team was the owner of a recently built <a href="https://open.spotify.com/" target="_blank" rel="noreferrer noopener">Web Player</a>, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.</p><p>Because the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to <a href="https://en.wikipedia.org/wiki/Conway%27s_law" target="_blank" rel="noreferrer noopener">Conway’s law</a>). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.</p></div>



<div><p>In addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.</p><p>We had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.</p></div>



<p>We knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.</p>



<p>There were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.</p>



<p>Now, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.</p>



<p>Let’s talk more in detail about how we turned the idea into reality.</p>



<div><figure><img loading="lazy" width="700" height="259" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-250x93.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-768x285.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-1536x569.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-120x44.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>One UI, multiple containers</h2>



<p>The Spotify Desktop client is a Windows and Mac native application that uses CEF (<a href="https://bitbucket.org/chromiumembedded/cef/" target="_blank" rel="noreferrer noopener">Chromium Embedded Framework</a>) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.</p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png" alt="" width="450" height="451" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-250x250.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-150x150.png 150w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-768x769.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-120x120.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7.png 1200w" sizes="(max-width: 450px) 100vw, 450px"/><figcaption>Previous architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.</figcaption></figure></div>



<p>The previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow <a href="http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf" target="_blank" rel="noreferrer noopener">playback to begin as soon as a listener clicked</a>. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.</p>



<div><p>The Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.</p><p>The Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.</p></div>



<p>This concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.</p>



<p>While work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.</p>



<p>The final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.</p>



<figure><img loading="lazy" width="700" height="375" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-250x134.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-768x412.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-120x64.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879.png 1199w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>The new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.</figcaption></figure>



<p>With this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.</p>



<div><figure><img loading="lazy" width="700" height="387" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-250x138.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-768x425.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-120x66.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><figure><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x483.png" alt=""/><figcaption>Old vs New: the Web Player UI has come a long way since the project started.</figcaption></figure></div>



<h2>Solving the organizational challenge</h2>



<p>From the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?</p>



<p>There was also a large information gap we had to solve. What features in the existing Desktop application <em>had</em> to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.</p>



<p>At the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.</p>



<p>After three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. </p>



<p>In reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.</p>



<h2>Evaluating success</h2>



<p>We had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?</p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" alt="" width="500" height="500"/></figure></div>



<h3>1. <strong>Reusability</strong></h3>



<p>Reusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.</p>



<h3>2. <strong>Unification</strong></h3>



<p>User experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.</p>



<p>Critically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. </p>



<h3>3. <strong>Speed</strong></h3>



<p>An important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.</p>



<h3>4. <strong>Satisfaction: Meeting Desktop and Web Player user needs</strong></h3>



<p>The new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.</p>



<p>We are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.</p>



<h2>What does all this mean for you as a user?</h2>



<div><p>As a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.</p><p>As time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.</p></div>



<h2>Is this your jam? Join us!</h2>



<p>Want to join the band and build the future of Spotify? Head over to our <a href="https://www.spotifyjobs.com/" target="_blank" rel="noreferrer noopener">job board</a> and see if anything catches your eye. We’ve just announced our <a href="https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/" target="_blank" rel="noreferrer noopener">Working From Anywhere</a> policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.</p>



<p><em>A shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian</em>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/web/" rel="tag">web</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Building the Future of Our Desktop Apps&#xA;</title>
      <link>https://engineering.atspotify.com/building-the-future-of-our-desktop-apps/</link>
      <description>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/building-the-future-of-our-desktop-apps/" title="Building the Future of Our Desktop Apps">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.</p>



<p>We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</p>



<h2>In the beginning, there were two clients</h2>



<div><p>Towards the end of 2018, our team was the owner of a recently built <a href="https://open.spotify.com/" target="_blank" rel="noreferrer noopener">Web Player</a>, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.</p><p>Because the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to <a href="https://en.wikipedia.org/wiki/Conway%27s_law" target="_blank" rel="noreferrer noopener">Conway’s law</a>). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.</p></div>



<div><p>In addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.</p><p>We had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.</p></div>



<p>We knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.</p>



<p>There were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.</p>



<p>Now, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.</p>



<p>Let’s talk more in detail about how we turned the idea into reality.</p>



<div><figure><img loading="lazy" width="700" height="259" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-250x93.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-768x285.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-1536x569.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-120x44.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>One UI, multiple containers</h2>



<p>The Spotify Desktop client is a Windows and Mac native application that uses CEF (<a href="https://bitbucket.org/chromiumembedded/cef/" target="_blank" rel="noreferrer noopener">Chromium Embedded Framework</a>) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.</p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png" alt="" width="450" height="451" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-250x250.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-150x150.png 150w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-768x769.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-120x120.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7.png 1200w" sizes="(max-width: 450px) 100vw, 450px"/><figcaption>Previous architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.</figcaption></figure></div>



<p>The previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow <a href="http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf" target="_blank" rel="noreferrer noopener">playback to begin as soon as a listener clicked</a>. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.</p>



<div><p>The Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.</p><p>The Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.</p></div>



<p>This concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.</p>



<p>While work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.</p>



<p>The final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.</p>



<figure><img loading="lazy" width="700" height="375" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-250x134.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-768x412.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-120x64.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879.png 1199w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>The new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.</figcaption></figure>



<p>With this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.</p>



<div><figure><img loading="lazy" width="700" height="387" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-250x138.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-768x425.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-120x66.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><figure><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x483.png" alt=""/><figcaption>Old vs New: the Web Player UI has come a long way since the project started.</figcaption></figure></div>



<h2>Solving the organizational challenge</h2>



<p>From the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?</p>



<p>There was also a large information gap we had to solve. What features in the existing Desktop application <em>had</em> to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.</p>



<p>At the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.</p>



<p>After three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. </p>



<p>In reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.</p>



<h2>Evaluating success</h2>



<p>We had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?</p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" alt="" width="500" height="500"/></figure></div>



<h3>1. <strong>Reusability</strong></h3>



<p>Reusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.</p>



<h3>2. <strong>Unification</strong></h3>



<p>User experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.</p>



<p>Critically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. </p>



<h3>3. <strong>Speed</strong></h3>



<p>An important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.</p>



<h3>4. <strong>Satisfaction: Meeting Desktop and Web Player user needs</strong></h3>



<p>The new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.</p>



<p>We are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.</p>



<h2>What does all this mean for you as a user?</h2>



<div><p>As a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.</p><p>As time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.</p></div>



<h2>Is this your jam? Join us!</h2>



<p>Want to join the band and build the future of Spotify? Head over to our <a href="https://www.spotifyjobs.com/" target="_blank" rel="noreferrer noopener">job board</a> and see if anything catches your eye. We’ve just announced our <a href="https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/" target="_blank" rel="noreferrer noopener">Working From Anywhere</a> policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.</p>



<p><em>A shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian</em>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/web/" rel="tag">web</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Building the Future of Our Desktop Apps&#xA;</title>
      <link>https://engineering.atspotify.com/building-the-future-of-our-desktop-apps/</link>
      <description>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/building-the-future-of-our-desktop-apps/" title="Building the Future of Our Desktop Apps">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/>                    </a>
                        
        </p>

        

        
<p>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.</p>



<p>We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</p>



<h2>In the beginning, there were two clients</h2>



<div><p>Towards the end of 2018, our team was the owner of a recently built <a href="https://open.spotify.com/" target="_blank" rel="noreferrer noopener">Web Player</a>, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.</p><p>Because the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to <a href="https://en.wikipedia.org/wiki/Conway%27s_law" target="_blank" rel="noreferrer noopener">Conway’s law</a>). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.</p></div>



<div><p>In addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.</p><p>We had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.</p></div>



<p>We knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.</p>



<p>There were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.</p>



<p>Now, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.</p>



<p>Let’s talk more in detail about how we turned the idea into reality.</p>



<div><figure><img loading="lazy" width="700" height="259" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-250x93.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-768x285.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-1536x569.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-120x44.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>One UI, multiple containers</h2>



<p>The Spotify Desktop client is a Windows and Mac native application that uses CEF (<a href="https://bitbucket.org/chromiumembedded/cef/" target="_blank" rel="noreferrer noopener">Chromium Embedded Framework</a>) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.</p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png" alt="" width="450" height="451" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-250x250.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-150x150.png 150w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-768x769.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-120x120.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7.png 1200w" sizes="(max-width: 450px) 100vw, 450px"/><figcaption>Previous architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.</figcaption></figure></div>



<p>The previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow <a href="http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf" target="_blank" rel="noreferrer noopener">playback to begin as soon as a listener clicked</a>. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.</p>



<div><p>The Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.</p><p>The Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.</p></div>



<p>This concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.</p>



<p>While work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.</p>



<p>The final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.</p>



<figure><img loading="lazy" width="700" height="375" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-250x134.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-768x412.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-120x64.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879.png 1199w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>The new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.</figcaption></figure>



<p>With this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.</p>



<div><figure><img loading="lazy" width="700" height="387" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-250x138.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-768x425.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-120x66.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><figure><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x483.png" alt=""/><figcaption>Old vs New: the Web Player UI has come a long way since the project started.</figcaption></figure></div>



<h2>Solving the organizational challenge</h2>



<p>From the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?</p>



<p>There was also a large information gap we had to solve. What features in the existing Desktop application <em>had</em> to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.</p>



<p>At the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.</p>



<p>After three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. </p>



<p>In reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.</p>



<h2>Evaluating success</h2>



<p>We had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?</p>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png" alt="" width="500" height="500"/></figure></div>



<h3>1. <strong>Reusability</strong></h3>



<p>Reusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.</p>



<h3>2. <strong>Unification</strong></h3>



<p>User experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.</p>



<p>Critically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. </p>



<h3>3. <strong>Speed</strong></h3>



<p>An important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.</p>



<h3>4. <strong>Satisfaction: Meeting Desktop and Web Player user needs</strong></h3>



<p>The new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.</p>



<p>We are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.</p>



<h2>What does all this mean for you as a user?</h2>



<div><p>As a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.</p><p>As time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.</p></div>



<h2>Is this your jam? Join us!</h2>



<p>Want to join the band and build the future of Spotify? Head over to our <a href="https://www.spotifyjobs.com/" target="_blank" rel="noreferrer noopener">job board</a> and see if anything catches your eye. We’ve just announced our <a href="https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/" target="_blank" rel="noreferrer noopener">Working From Anywhere</a> policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.</p>



<p><em>A shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian</em>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/web/" rel="tag">web</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Building the Future of Our Desktop Apps&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/</link>
      <description>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/" title="Building the Future of Our Desktop Apps">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/04/ClientX.png"/>                    </a>
                        
        </p>

        

        
<p>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.</p>



<p>We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</p>



<h2>In the beginning, there were two clients</h2>



<div><p>Towards the end of 2018, our team was the owner of a recently built <a href="https://open.spotify.com/" target="_blank" rel="noreferrer noopener">Web Player</a>, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.</p><p>Because the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to <a href="https://en.wikipedia.org/wiki/Conway%27s_law" target="_blank" rel="noreferrer noopener">Conway’s law</a>). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.</p></div>



<div><p>In addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.</p><p>We had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.</p></div>



<p>We knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.</p>



<p>There were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.</p>



<p>Now, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.</p>



<p>Let’s talk more in detail about how we turned the idea into reality.</p>



<div><figure><img loading="lazy" width="700" height="259" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-700x259.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-700x259.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-250x93.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-768x285.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-1536x569.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-120x44.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>One UI, multiple containers</h2>



<p>The Spotify Desktop client is a Windows and Mac native application that uses CEF (<a href="https://bitbucket.org/chromiumembedded/cef/" target="_blank" rel="noreferrer noopener">Chromium Embedded Framework</a>) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.</p>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-700x701.png" alt="" width="450" height="451" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-700x701.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-250x250.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-150x150.png 150w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-768x769.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-120x120.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7.png 1200w" sizes="(max-width: 450px) 100vw, 450px"/><figcaption>Previous architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.</figcaption></figure></div>



<p>The previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow <a href="http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf" target="_blank" rel="noreferrer noopener">playback to begin as soon as a listener clicked</a>. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.</p>



<div><p>The Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.</p><p>The Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.</p></div>



<p>This concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.</p>



<p>While work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.</p>



<p>The final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.</p>



<figure><img loading="lazy" width="700" height="375" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-700x375.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-700x375.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-250x134.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-768x412.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-120x64.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879.png 1199w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>The new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.</figcaption></figure>



<p>With this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.</p>



<div><figure><img loading="lazy" width="700" height="387" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-700x387.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-700x387.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-250x138.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-768x425.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-120x66.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><figure><img loading="lazy" width="700" height="483" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x483.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x483.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-250x172.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-768x529.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-1536x1059.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-120x83.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Old vs New: the Web Player UI has come a long way since the project started.</figcaption></figure></div>



<h2>Solving the organizational challenge</h2>



<p>From the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?</p>



<p>There was also a large information gap we had to solve. What features in the existing Desktop application <em>had</em> to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.</p>



<p>At the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.</p>



<p>After three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. </p>



<p>In reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.</p>



<h2>Evaluating success</h2>



<p>We had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?</p>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png" alt="" width="500" height="500" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x250.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x700.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-150x150.png 150w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x768.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x1536.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x120.png 120w" sizes="(max-width: 500px) 100vw, 500px"/></figure></div>



<h3>1. <strong>Reusability</strong></h3>



<p>Reusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.</p>



<h3>2. <strong>Unification</strong></h3>



<p>User experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.</p>



<p>Critically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. </p>



<h3>3. <strong>Speed</strong></h3>



<p>An important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.</p>



<h3>4. <strong>Satisfaction: Meeting Desktop and Web Player user needs</strong></h3>



<p>The new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.</p>



<p>We are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.</p>



<h2>What does all this mean for you as a user?</h2>



<div><p>As a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.</p><p>As time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.</p></div>



<h2>Is this your jam? Join us!</h2>



<p>Want to join the band and build the future of Spotify? Head over to our <a href="https://www.spotifyjobs.com/" target="_blank" rel="noreferrer noopener">job board</a> and see if anything catches your eye. We’ve just announced our <a href="https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/" target="_blank" rel="noreferrer noopener">Working From Anywhere</a> policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.</p>



<p><em>A shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian</em>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            David Riordan: Product Manager&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/my-beat-david-riordan/</link>
      <description>5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4054">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w" sizes="(max-width: 500px) 100vw, 500px"/>
                                  
             </p>
             <div>
             
                 <p><b>David is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  </b></p>
             </div>
         </div>

         


         

         
<blockquote><p>5:00 am </p></blockquote>



<p>My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. </p>



<blockquote><p>9:00 am</p></blockquote>



<p>Back home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. </p>



<p>As a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. </p>



<p>For the past year, I’ve been part of a project called <a href="https://klio.io/">Klio</a> – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  </p>



<p>Klio has been a long time in the making, so <a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/">it was amazing to finally open-source it</a> last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. </p>



<blockquote><p>11:30 am</p></blockquote>



<p>Working on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Time to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.</p>



<blockquote><p>1:00 pm</p></blockquote>



<p>Whilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. </p>



<blockquote><p>6:00 pm</p></blockquote>



<p>We’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png" alt="" width="700" height="685" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>

         Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            David Riordan: Product Manager&#xA;</title>
      <link>https://engineering.atspotify.com/my-beat-david-riordan/</link>
      <description>5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4054">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w" sizes="(max-width: 500px) 100vw, 500px"/>
                                  
             </p>
             <div>
             
                 <p><b>David is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  </b></p>
             </div>
         </div>

         


         

         
<blockquote><p>5:00 am </p></blockquote>



<p>My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. </p>



<blockquote><p>9:00 am</p></blockquote>



<p>Back home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. </p>



<p>As a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. </p>



<p>For the past year, I’ve been part of a project called <a href="https://klio.io/">Klio</a> – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  </p>



<p>Klio has been a long time in the making, so <a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/">it was amazing to finally open-source it</a> last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. </p>



<blockquote><p>11:30 am</p></blockquote>



<p>Working on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Time to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.</p>



<blockquote><p>1:00 pm</p></blockquote>



<p>Whilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. </p>



<blockquote><p>6:00 pm</p></blockquote>



<p>We’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png" alt="" width="700" height="685" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>

         Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            David Riordan: Product Manager&#xA;</title>
      <link>https://engineering.atspotify.com/my-beat-david-riordan/</link>
      <description>5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4054">
     <div>
         
         
        
         <div>
            <div>
            <h2>Putting the Spotlight on our technical employees</h2>
    <p>My beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.</p>
            </div>
 
             <p><img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w" sizes="(max-width: 500px) 100vw, 500px"/>
                                  
             </p>
             <div>
             
                 <p><b>David is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  </b></p>
             </div>
         </div>

         


         

         
<blockquote><p>5:00 am </p></blockquote>



<p>My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. </p>



<blockquote><p>9:00 am</p></blockquote>



<p>Back home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. </p>



<p>As a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. </p>



<p>For the past year, I’ve been part of a project called <a href="https://klio.io/">Klio</a> – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  </p>



<p>Klio has been a long time in the making, so <a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/">it was amazing to finally open-source it</a> last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. </p>



<blockquote><p>11:30 am</p></blockquote>



<p>Working on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Time to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.</p>



<blockquote><p>1:00 pm</p></blockquote>



<p>Whilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. </p>



<blockquote><p>6:00 pm</p></blockquote>



<p>We’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?</p>







<figure><img loading="lazy" width="700" height="111" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png" alt="" width="700" height="685" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>

         Tags: <a href="https://engineering.atspotify.com/tag/machine-learning/" rel="tag">machine learning</a></p><p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            David Riordan: Product Manager&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/30/my-beat-david-riordan/</link>
      <description>5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4054">
     <div>
         
         
         
         <div>
             <p><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w" sizes="(max-width: 500px) 100vw, 500px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/11/MyBeat_David-Riordan.png"/>
                                  
             </p>
             <p><b>David is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  </b></p>
         </div>

         


         

         
<blockquote><p>5:00 am </p></blockquote>



<p>My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. </p>



<blockquote><p>9:00 am</p></blockquote>



<p>Back home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. </p>



<p>As a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. </p>



<p>For the past year, I’ve been part of a project called <a href="https://klio.io/">Klio</a> – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  </p>



<p>Klio has been a long time in the making, so <a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/">it was amazing to finally open-source it</a> last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. </p>



<blockquote><p>11:30 am</p></blockquote>



<p>Working on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Time to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.</p>



<blockquote><p>1:00 pm</p></blockquote>



<p>Whilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. </p>



<blockquote><p>6:00 pm</p></blockquote>



<p>We’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?</p>







<figure><img loading="lazy" width="700" height="111" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png" alt="" width="700" height="685" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>





         
         

         <p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Why You Should Pair with Non-Engineers&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/</link>
      <description>TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn&#39;t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/" title="Why You Should Pair with Non-Engineers">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/Pairing_02.png"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Spotify encourages engineers to become <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and <a href="https://www.youtube.com/watch?v=4jckqGVtyAA" target="_blank" rel="noreferrer noopener">learning from peers</a> outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. </p>



<p>Prior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.</p>



<h2><strong>Broaden scope of knowledge (T shape)</strong></h2>



<p>Engineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.</p>



<p>Within web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.</p>



<p>Additionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.</p>



<p>When I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.</p>



<h2><strong>Stress-test documentation</strong></h2>



<p>I had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had <em>never</em> been used for writing code for the web.</p>



<p>This meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.</p>



<p>Stress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.</p>



<h2><strong>Build empathy between disciplines</strong></h2>



<p>Understanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.</p>



<p>The next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. </p>



<p>I once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an <a href="https://en.wikipedia.org/wiki/Integrated_development_environment" target="_blank" rel="noreferrer noopener">IDE</a> and also display in <a href="http://www.figma.com" target="_blank" rel="noreferrer noopener">Figma</a>.</p>



<p>You might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.</p>



<h2><strong>Shared language</strong></h2>



<p>Many workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This <em>can</em> be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.</p>



<p>Something we use extensively across Spotify is SEMVER (<a href="https://semver.org/" target="_blank" rel="noreferrer noopener">semantic versioning</a>), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. </p>



<p>Doing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.</p>



<h2><strong>Summary</strong></h2>



<p>We made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.</p>



<p>Share more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Tyce Clee</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Why You Should Pair with Non-Engineers&#xA;</title>
      <link>https://engineering.atspotify.com/why-you-should-pair-with-non-engineers/</link>
      <description>TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn&#39;t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/why-you-should-pair-with-non-engineers/" title="Why You Should Pair with Non-Engineers">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Spotify encourages engineers to become <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and <a href="https://www.youtube.com/watch?v=4jckqGVtyAA" target="_blank" rel="noreferrer noopener">learning from peers</a> outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. </p>



<p>Prior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.</p>



<h2><strong>Broaden scope of knowledge (T shape)</strong></h2>



<p>Engineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.</p>



<p>Within web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.</p>



<p>Additionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.</p>



<p>When I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.</p>



<h2><strong>Stress-test documentation</strong></h2>



<p>I had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had <em>never</em> been used for writing code for the web.</p>



<p>This meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.</p>



<p>Stress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.</p>



<h2><strong>Build empathy between disciplines</strong></h2>



<p>Understanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.</p>



<p>The next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. </p>



<p>I once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an <a href="https://en.wikipedia.org/wiki/Integrated_development_environment" target="_blank" rel="noreferrer noopener">IDE</a> and also display in <a href="http://www.figma.com" target="_blank" rel="noreferrer noopener">Figma</a>.</p>



<p>You might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.</p>



<h2><strong>Shared language</strong></h2>



<p>Many workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This <em>can</em> be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.</p>



<p>Something we use extensively across Spotify is SEMVER (<a href="https://semver.org/" target="_blank" rel="noreferrer noopener">semantic versioning</a>), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. </p>



<p>Doing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.</p>



<h2><strong>Summary</strong></h2>



<p>We made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.</p>



<p>Share more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Tyce Clee</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Why You Should Pair with Non-Engineers&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/why-you-should-pair-with-non-engineers/</link>
      <description>TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn&#39;t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/why-you-should-pair-with-non-engineers/" title="Why You Should Pair with Non-Engineers">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Spotify encourages engineers to become <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and <a href="https://www.youtube.com/watch?v=4jckqGVtyAA" target="_blank" rel="noreferrer noopener">learning from peers</a> outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. </p>



<p>Prior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.</p>



<h2><strong>Broaden scope of knowledge (T shape)</strong></h2>



<p>Engineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.</p>



<p>Within web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.</p>



<p>Additionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.</p>



<p>When I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.</p>



<h2><strong>Stress-test documentation</strong></h2>



<p>I had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had <em>never</em> been used for writing code for the web.</p>



<p>This meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.</p>



<p>Stress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.</p>



<h2><strong>Build empathy between disciplines</strong></h2>



<p>Understanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.</p>



<p>The next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. </p>



<p>I once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an <a href="https://en.wikipedia.org/wiki/Integrated_development_environment" target="_blank" rel="noreferrer noopener">IDE</a> and also display in <a href="http://www.figma.com" target="_blank" rel="noreferrer noopener">Figma</a>.</p>



<p>You might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.</p>



<h2><strong>Shared language</strong></h2>



<p>Many workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This <em>can</em> be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.</p>



<p>Something we use extensively across Spotify is SEMVER (<a href="https://semver.org/" target="_blank" rel="noreferrer noopener">semantic versioning</a>), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. </p>



<p>Doing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.</p>



<h2><strong>Summary</strong></h2>



<p>We made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.</p>



<p>Share more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Tyce Clee</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Why You Should Pair with Non-Engineers&#xA;</title>
      <link>https://engineering.atspotify.com/why-you-should-pair-with-non-engineers/</link>
      <description>TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn&#39;t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/why-you-should-pair-with-non-engineers/" title="Why You Should Pair with Non-Engineers">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Spotify encourages engineers to become <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and <a href="https://www.youtube.com/watch?v=4jckqGVtyAA" target="_blank" rel="noreferrer noopener">learning from peers</a> outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. </p>



<p>Prior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.</p>



<h2><strong>Broaden scope of knowledge (T shape)</strong></h2>



<p>Engineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.</p>



<p>Within web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.</p>



<p>Additionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.</p>



<p>When I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.</p>



<h2><strong>Stress-test documentation</strong></h2>



<p>I had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had <em>never</em> been used for writing code for the web.</p>



<p>This meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.</p>



<p>Stress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.</p>



<h2><strong>Build empathy between disciplines</strong></h2>



<p>Understanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.</p>



<p>The next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. </p>



<p>I once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an <a href="https://en.wikipedia.org/wiki/Integrated_development_environment" target="_blank" rel="noreferrer noopener">IDE</a> and also display in <a href="http://www.figma.com" target="_blank" rel="noreferrer noopener">Figma</a>.</p>



<p>You might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.</p>



<h2><strong>Shared language</strong></h2>



<p>Many workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This <em>can</em> be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.</p>



<p>Something we use extensively across Spotify is SEMVER (<a href="https://semver.org/" target="_blank" rel="noreferrer noopener">semantic versioning</a>), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. </p>



<p>Doing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.</p>



<h2><strong>Summary</strong></h2>



<p>We made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.</p>



<p>Share more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Tyce Clee</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/</link>
      <description>TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" title="Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR:</strong> As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. </p>



<h2>From Hack Week hunch to CNCF Sandbox</h2>



<p>Last year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? </p>



<p>The team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a> was out in the wild — making its official open source debut <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">one year ago today</a>. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at <a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank" rel="noreferrer noopener">the CNCF</a> (also home to Kubernetes, Envoy, and Helm).</p>



<p>Looking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. </p>



<figure><img loading="lazy" width="1999" height="1016" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-700x356.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-1536x781.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-120x61.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/><figcaption><em>“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” </em><br/><em>— Expedia Group Developer Experience Team</em></figcaption></figure>



<p>Companies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. </p>



<h2>Why a developer portal?</h2>



<p>To understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard <a href="https://backstage.io/docs/overview/background" target="_blank" rel="noreferrer noopener">this story</a> before, feel free to skip ahead.) </p>



<p>In March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. </p>



<p>Our small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. </p>



<p>The faster we grew, the more this fragmentation slowed us down again. </p>



<h2>A single pane of glass</h2>



<p>Designed first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.</p>



<p>It became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.</p>



<p>With Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. </p>



<p>Speed was the key. We saw firsthand that faster developers aren’t just <a href="https://martinfowler.com/articles/developer-effectiveness.html" target="_blank" rel="noreferrer noopener">more productive developers</a>, they’re happier developers.</p>



<h2>From internal portal to open platform</h2>



<p>What’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.</p>



<p>Unlike <a href="https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/" target="_blank" rel="noreferrer noopener">the internal version of Backstage</a>, which has more than 120 different <a href="https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage" target="_blank" rel="noreferrer noopener">plugins</a> built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. </p>



<p>Since that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:</p>



<ul><li>We introduced four core features: the <a href="https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha" target="_blank" rel="noreferrer noopener">Service Catalog</a>, <a href="https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates" target="_blank" rel="noreferrer noopener">Software Templates</a>, <a href="https://backstage.io/blog/2020/09/08/announcing-tech-docs" target="_blank" rel="noreferrer noopener">TechDocs</a>, and our <a href="https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/" target="_blank" rel="noreferrer noopener">new Kubernetes monitoring tool</a>. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.</li></ul>



<ul><li>We launched the <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">Plugin Marketplace</a>: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like <a href="https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md" target="_blank" rel="noreferrer noopener">Snyk</a>. </li></ul>



<ul><li>We created the <a href="https://backstage.io/blog/2020/09/30/backstage-design-system" target="_blank" rel="noreferrer noopener">Backstage Design System</a>: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.</li></ul>



<h2>Stabilizing the core</h2>



<p>The work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: <a href="https://backstage.io/blog/2020/12/22/stability-index" target="_blank" rel="noreferrer noopener">stabilizing those features and APIs</a> so that more companies can adopt the platform for production use. </p>



<p>In the coming weeks, our team will:</p>



<ul><li>Bring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.</li></ul>



<ul><li>Create an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.</li></ul>



<ul><li>Update other parts of the core app — notably, improving search and incorporating GraphQL systemwide.</li></ul>



<p>You can learn more in <a href="https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md" target="_blank" rel="noreferrer noopener">the project roadmap</a>.</p>



<h2>Adopters: Backstage in the wild!</h2>



<p>Beyond the <a href="https://github.com/backstage/backstage/blob/master/ADOPTERS.md" target="_blank" rel="noreferrer noopener">official adopters list</a>, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.</p>



<ul><li>Early adopters Zalando and SDA SE shared <a href="https://youtu.be/4-VX9tDdJYY?t=1756" target="_blank" rel="noreferrer noopener">their adoption experiences</a> last month at our first community session. </li></ul>



<ul><li>Expedia has a team dedicated to rolling out Backstage.</li></ul>







<ul><li>American Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:<p>“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines</p></li></ul>



<ul><li>DoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. <p>“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash</p></li></ul>



<h2>A world of contributors grows into a community</h2>



<p>Of course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged <a href="https://twitter.com/SpotifyEng/status/1341376341636239364" target="_blank" rel="noreferrer noopener">two new contributors a week</a>. </p>



<figure></figure>



<p>This year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the <a href="https://github.com/backstage/community" target="_blank" rel="noreferrer noopener">Backstage Community</a> page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in <a href="https://github.com/backstage/backstage/" target="_blank" rel="noreferrer noopener">the main repo</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Tyson Singer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast&#xA;</title>
      <link>https://engineering.atspotify.com/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/</link>
      <description>TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" title="Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR:</strong> As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. </p>



<h2>From Hack Week hunch to CNCF Sandbox</h2>



<p>Last year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? </p>



<p>The team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a> was out in the wild — making its official open source debut <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">one year ago today</a>. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at <a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank" rel="noreferrer noopener">the CNCF</a> (also home to Kubernetes, Envoy, and Helm).</p>



<p>Looking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. </p>



<figure><img loading="lazy" width="1999" height="1016" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-700x356.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-1536x781.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-120x61.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/><figcaption><em>“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” </em><br/><em>— Expedia Group Developer Experience Team</em></figcaption></figure>



<p>Companies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. </p>



<h2>Why a developer portal?</h2>



<p>To understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard <a href="https://backstage.io/docs/overview/background" target="_blank" rel="noreferrer noopener">this story</a> before, feel free to skip ahead.) </p>



<p>In March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. </p>



<p>Our small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. </p>



<p>The faster we grew, the more this fragmentation slowed us down again. </p>



<h2>A single pane of glass</h2>



<p>Designed first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.</p>



<p>It became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.</p>



<p>With Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. </p>



<p>Speed was the key. We saw firsthand that faster developers aren’t just <a href="https://martinfowler.com/articles/developer-effectiveness.html" target="_blank" rel="noreferrer noopener">more productive developers</a>, they’re happier developers.</p>



<h2>From internal portal to open platform</h2>



<p>What’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.</p>



<p>Unlike <a href="https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/" target="_blank" rel="noreferrer noopener">the internal version of Backstage</a>, which has more than 120 different <a href="https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage" target="_blank" rel="noreferrer noopener">plugins</a> built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. </p>



<p>Since that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:</p>



<ul><li>We introduced four core features: the <a href="https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha" target="_blank" rel="noreferrer noopener">Service Catalog</a>, <a href="https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates" target="_blank" rel="noreferrer noopener">Software Templates</a>, <a href="https://backstage.io/blog/2020/09/08/announcing-tech-docs" target="_blank" rel="noreferrer noopener">TechDocs</a>, and our <a href="https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/" target="_blank" rel="noreferrer noopener">new Kubernetes monitoring tool</a>. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.</li></ul>



<ul><li>We launched the <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">Plugin Marketplace</a>: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like <a href="https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md" target="_blank" rel="noreferrer noopener">Snyk</a>. </li></ul>



<ul><li>We created the <a href="https://backstage.io/blog/2020/09/30/backstage-design-system" target="_blank" rel="noreferrer noopener">Backstage Design System</a>: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.</li></ul>



<h2>Stabilizing the core</h2>



<p>The work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: <a href="https://backstage.io/blog/2020/12/22/stability-index" target="_blank" rel="noreferrer noopener">stabilizing those features and APIs</a> so that more companies can adopt the platform for production use. </p>



<p>In the coming weeks, our team will:</p>



<ul><li>Bring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.</li></ul>



<ul><li>Create an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.</li></ul>



<ul><li>Update other parts of the core app — notably, improving search and incorporating GraphQL systemwide.</li></ul>



<p>You can learn more in <a href="https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md" target="_blank" rel="noreferrer noopener">the project roadmap</a>.</p>



<h2>Adopters: Backstage in the wild!</h2>



<p>Beyond the <a href="https://github.com/backstage/backstage/blob/master/ADOPTERS.md" target="_blank" rel="noreferrer noopener">official adopters list</a>, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.</p>



<ul><li>Early adopters Zalando and SDA SE shared <a href="https://youtu.be/4-VX9tDdJYY?t=1756" target="_blank" rel="noreferrer noopener">their adoption experiences</a> last month at our first community session. </li></ul>



<ul><li>Expedia has a team dedicated to rolling out Backstage.</li></ul>







<ul><li>American Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:<p>“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines</p></li></ul>



<ul><li>DoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. <p>“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash</p></li></ul>



<h2>A world of contributors grows into a community</h2>



<p>Of course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged <a href="https://twitter.com/SpotifyEng/status/1341376341636239364" target="_blank" rel="noreferrer noopener">two new contributors a week</a>. </p>



<figure></figure>



<p>This year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the <a href="https://github.com/backstage/community" target="_blank" rel="noreferrer noopener">Backstage Community</a> page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in <a href="https://github.com/backstage/backstage/" target="_blank" rel="noreferrer noopener">the main repo</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Tyson Singer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast&#xA;</title>
      <link>https://engineering.atspotify.com/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/</link>
      <description>TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" title="Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" alt=""/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR:</strong> As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. </p>



<h2>From Hack Week hunch to CNCF Sandbox</h2>



<p>Last year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? </p>



<p>The team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a> was out in the wild — making its official open source debut <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">one year ago today</a>. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at <a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank" rel="noreferrer noopener">the CNCF</a> (also home to Kubernetes, Envoy, and Helm).</p>



<p>Looking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. </p>



<figure><img loading="lazy" width="1999" height="1016" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-700x356.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-1536x781.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-120x61.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/><figcaption><em>“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” </em><br/><em>— Expedia Group Developer Experience Team</em></figcaption></figure>



<p>Companies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. </p>



<h2>Why a developer portal?</h2>



<p>To understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard <a href="https://backstage.io/docs/overview/background" target="_blank" rel="noreferrer noopener">this story</a> before, feel free to skip ahead.) </p>



<p>In March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. </p>



<p>Our small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. </p>



<p>The faster we grew, the more this fragmentation slowed us down again. </p>



<h2>A single pane of glass</h2>



<p>Designed first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.</p>



<p>It became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.</p>



<p>With Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. </p>



<p>Speed was the key. We saw firsthand that faster developers aren’t just <a href="https://martinfowler.com/articles/developer-effectiveness.html" target="_blank" rel="noreferrer noopener">more productive developers</a>, they’re happier developers.</p>



<h2>From internal portal to open platform</h2>



<p>What’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.</p>



<p>Unlike <a href="https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/" target="_blank" rel="noreferrer noopener">the internal version of Backstage</a>, which has more than 120 different <a href="https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage" target="_blank" rel="noreferrer noopener">plugins</a> built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. </p>



<p>Since that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:</p>



<ul><li>We introduced four core features: the <a href="https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha" target="_blank" rel="noreferrer noopener">Service Catalog</a>, <a href="https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates" target="_blank" rel="noreferrer noopener">Software Templates</a>, <a href="https://backstage.io/blog/2020/09/08/announcing-tech-docs" target="_blank" rel="noreferrer noopener">TechDocs</a>, and our <a href="https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/" target="_blank" rel="noreferrer noopener">new Kubernetes monitoring tool</a>. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.</li></ul>



<ul><li>We launched the <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">Plugin Marketplace</a>: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like <a href="https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md" target="_blank" rel="noreferrer noopener">Snyk</a>. </li></ul>



<ul><li>We created the <a href="https://backstage.io/blog/2020/09/30/backstage-design-system" target="_blank" rel="noreferrer noopener">Backstage Design System</a>: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.</li></ul>



<h2>Stabilizing the core</h2>



<p>The work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: <a href="https://backstage.io/blog/2020/12/22/stability-index" target="_blank" rel="noreferrer noopener">stabilizing those features and APIs</a> so that more companies can adopt the platform for production use. </p>



<p>In the coming weeks, our team will:</p>



<ul><li>Bring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.</li></ul>



<ul><li>Create an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.</li></ul>



<ul><li>Update other parts of the core app — notably, improving search and incorporating GraphQL systemwide.</li></ul>



<p>You can learn more in <a href="https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md" target="_blank" rel="noreferrer noopener">the project roadmap</a>.</p>



<h2>Adopters: Backstage in the wild!</h2>



<p>Beyond the <a href="https://github.com/backstage/backstage/blob/master/ADOPTERS.md" target="_blank" rel="noreferrer noopener">official adopters list</a>, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.</p>



<ul><li>Early adopters Zalando and SDA SE shared <a href="https://youtu.be/4-VX9tDdJYY?t=1756" target="_blank" rel="noreferrer noopener">their adoption experiences</a> last month at our first community session. </li></ul>



<ul><li>Expedia has a team dedicated to rolling out Backstage.</li></ul>







<ul><li>American Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:<p>“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines</p></li></ul>



<ul><li>DoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. <p>“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash</p></li></ul>



<h2>A world of contributors grows into a community</h2>



<p>Of course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged <a href="https://twitter.com/SpotifyEng/status/1341376341636239364" target="_blank" rel="noreferrer noopener">two new contributors a week</a>. </p>



<figure></figure>



<p>This year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the <a href="https://github.com/backstage/community" target="_blank" rel="noreferrer noopener">Backstage Community</a> page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in <a href="https://github.com/backstage/backstage/" target="_blank" rel="noreferrer noopener">the main repo</a>.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/engineering-leadership/" rel="tag">engineering leadership</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Tyson Singer</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/</link>
      <description>TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" title="Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/Backstage-BDay-Blog_v002.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR:</strong> As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. </p>



<h2>From Hack Week hunch to CNCF Sandbox</h2>



<p>Last year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? </p>



<p>The team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a> was out in the wild — making its official open source debut <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">one year ago today</a>. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at <a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank" rel="noreferrer noopener">the CNCF</a> (also home to Kubernetes, Envoy, and Helm).</p>



<p>Looking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. </p>



<figure><img loading="lazy" width="1999" height="1016" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-250x127.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-700x356.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-768x390.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-1536x781.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-120x61.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/><figcaption><em>“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” </em><br/><em>— Expedia Group Developer Experience Team</em></figcaption></figure>



<p>Companies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. </p>



<h2>Why a developer portal?</h2>



<p>To understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard <a href="https://backstage.io/docs/overview/background" target="_blank" rel="noreferrer noopener">this story</a> before, feel free to skip ahead.) </p>



<p>In March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. </p>



<p>Our small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. </p>



<p>The faster we grew, the more this fragmentation slowed us down again. </p>



<h2>A single pane of glass</h2>



<p>Designed first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.</p>



<p>It became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.</p>



<p>With Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. </p>



<p>Speed was the key. We saw firsthand that faster developers aren’t just <a href="https://martinfowler.com/articles/developer-effectiveness.html" target="_blank" rel="noreferrer noopener">more productive developers</a>, they’re happier developers.</p>



<h2>From internal portal to open platform</h2>



<p>What’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.</p>



<p>Unlike <a href="https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/" target="_blank" rel="noreferrer noopener">the internal version of Backstage</a>, which has more than 120 different <a href="https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage" target="_blank" rel="noreferrer noopener">plugins</a> built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. </p>



<p>Since that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:</p>



<ul><li>We introduced four core features: the <a href="https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha" target="_blank" rel="noreferrer noopener">Service Catalog</a>, <a href="https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates" target="_blank" rel="noreferrer noopener">Software Templates</a>, <a href="https://backstage.io/blog/2020/09/08/announcing-tech-docs" target="_blank" rel="noreferrer noopener">TechDocs</a>, and our <a href="https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/" target="_blank" rel="noreferrer noopener">new Kubernetes monitoring tool</a>. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.</li></ul>



<ul><li>We launched the <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">Plugin Marketplace</a>: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like <a href="https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md" target="_blank" rel="noreferrer noopener">Snyk</a>. </li></ul>



<ul><li>We created the <a href="https://backstage.io/blog/2020/09/30/backstage-design-system" target="_blank" rel="noreferrer noopener">Backstage Design System</a>: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.</li></ul>



<h2>Stabilizing the core</h2>



<p>The work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: <a href="https://backstage.io/blog/2020/12/22/stability-index" target="_blank" rel="noreferrer noopener">stabilizing those features and APIs</a> so that more companies can adopt the platform for production use. </p>



<p>In the coming weeks, our team will:</p>



<ul><li>Bring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.</li></ul>



<ul><li>Create an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.</li></ul>



<ul><li>Update other parts of the core app — notably, improving search and incorporating GraphQL systemwide.</li></ul>



<p>You can learn more in <a href="https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md" target="_blank" rel="noreferrer noopener">the project roadmap</a>.</p>



<h2>Adopters: Backstage in the wild!</h2>



<p>Beyond the <a href="https://github.com/backstage/backstage/blob/master/ADOPTERS.md" target="_blank" rel="noreferrer noopener">official adopters list</a>, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.</p>



<ul><li>Early adopters Zalando and SDA SE shared <a href="https://youtu.be/4-VX9tDdJYY?t=1756" target="_blank" rel="noreferrer noopener">their adoption experiences</a> last month at our first community session. </li></ul>



<ul><li>Expedia has a team dedicated to rolling out Backstage.</li></ul>







<ul><li>American Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:<p>“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines</p></li></ul>



<ul><li>DoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. <p>“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash</p></li></ul>



<h2>A world of contributors grows into a community</h2>



<p>Of course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged <a href="https://twitter.com/SpotifyEng/status/1341376341636239364" target="_blank" rel="noreferrer noopener">two new contributors a week</a>. </p>



<figure></figure>



<p>This year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the <a href="https://github.com/backstage/community" target="_blank" rel="noreferrer noopener">Backstage Community</a> page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in <a href="https://github.com/backstage/backstage/" target="_blank" rel="noreferrer noopener">the main repo</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Tyson Singer</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify’s New Experimentation Coordination Strategy&#xA;</title>
      <link>https://engineering.atspotify.com/spotifys-new-experimentation-coordination-strategy/</link>
      <description>At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 10, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/spotifys-new-experimentation-coordination-strategy/" title="Spotify’s New Experimentation Coordination Strategy">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time. These are well-known problems among tech companies — take, for example, Google’s solution in Tang et al (2010). But the statistical implications of different solutions have not been properly investigated. In a recent paper (<a href="https://link.springer.com/chapter/10.1007/978-3-030-89906-6_50" target="_blank" rel="noreferrer noopener">M. Schultzberg, O. Kjellin, and J. Rydberg, 2020</a>), we investigate important statistical properties of a common technical solution to the coordination — called “Bucket Reuse”. In this blog post we highlight some interesting results and present some details about how Spotify will coordinate experiments from now on.</p>



<h2>What is Bucket Reuse?</h2>



<p>Bucket Reuse is a simple idea utilizing the power of hashing. Essentially the steps are as follows: decide on a number of buckets (B). Take the unique user ID and hash it together with a  random salt into B “buckets” such that all users hash into one and only one bucket. Once the hash map is established, all sampling is performed on the bucket level. This implies that a bucket either is or is not in a sample at any given time point. If we want to sample N number of users, we sample the number of buckets that contain the number of users closest to the desired number N. If, e.g., the desired N is 20 and each bucket contains 3 users, we would sample 7 buckets and end up with 21 users. Note that a bucket is simply a logical group of units to which we assign a certain user by a fixed hash map. Figure 1 illustrates such a map. The second part of the name Bucket <em>Reuse</em> comes from the fact that we reuse the same buckets over and over again in the sampling for all experiments. That is, the random salt for the hashing is selected only once; after that, the hash map and the number of buckets is fixed. <strong>Importantly,</strong> when we talk about Bucket Reuse for experimentation, we always mean the following: the random sampling is performed on the bucket level; the random treatment allocation is performed at the user level on the users in the sample. </p>



<figure><img loading="lazy" width="700" height="625" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-700x625.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-700x625.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-250x223.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-768x686.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-120x107.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4.png 1366w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: <em>Schematic illustration of a hash map. A user ID is hashed together with a random salt to map each user to a unique bucket. </em></figcaption></figure>



<h2>What is experiment coordination?</h2>



<p>To get into the interesting parts of experiment coordination, we need to establish some key concepts. Figure 2 illustrates the concepts of exclusive and nonexclusive experiments. That two or more experiments “are exclusive” to each other simply means that they are run on distinct sets of users. Experiments that are nonexclusive are nonexclusive to <em>all</em> experiments at Spotify, meaning that they all randomly overlap in terms of users.</p>



<figure><img loading="lazy" width="700" height="383" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-700x383.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-700x383.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-250x137.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-768x420.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-120x66.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5.png 1498w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: <em>Illustration of exclusive and nonexclusive experiments. Exclusive experiments never overlap with each other in terms of users; nonexclusive experiments randomly overlap with exclusive experiments and other nonexclusive experiments. Note that the allocations in this figure were selected for illustration; in a true random sample we would expect exclusive experiments to also be spread out uniformly.  </em></figcaption></figure>



<p>The most critical challenge from a statistical perspective is imposed by what we call <em>programs of exclusive experiments</em>. A program of exclusive experiments is a set of experiments run over time where all simultaneous experiments are exclusive to each other. That is, a unit is in at most one, and only one, of the experiments in the program at any given time point. At Spotify we have such programs for several surfaces in the app, for example Search, the Home screen, and certain parts of the backend code base. To better understand the limitations imposed on the sampling by running programs of exclusive experiments, it is helpful to introduce the concepts of paths. A path is simply a sequence of experiments that a unit can be in. Figure 3 illustrates a program of exclusive experiments containing 5 experiments over time. Below the experiments, their possible paths are displayed. For example, it is not possible to go through both Experiments 3 and 4 as they overlap in time and are exclusive, and must therefore be run on distinct users. The number of unique possible paths explodes combinatorially after a relatively short time period in most programs, and only a small partition of the possible paths can be taken by any unit regardless of the sample strategies discussed in this post.</p>



<figure><img loading="lazy" width="700" height="806" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-700x806.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-700x806.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-250x288.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-768x885.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-1334x1536.png 1334w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-120x138.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3.png 1370w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 3: <em>Paths of experiences possible during a hypothetical program of 5 exclusive experiments.</em></figcaption></figure>



<h2>Highlights from <a href="https://link.springer.com/chapter/10.1007/978-3-030-89906-6_50" target="_blank" rel="noreferrer noopener">Schultzberg et al.</a></h2>



<p>Schultzberg et al. presents two key results:</p>



<ol><li>Under unrestricted sampling of buckets, i.e., Bucket Reuse in nonexclusive experiments, the properties of the difference-in-means estimator of the average treatment effect is approximately equivalent to the properties under random sampling of units. In other words, standard t-tests can be used for inference. The key to this remarkable finding is connecting Bucket Reuse to the existing literature on randomized experiments embedded in complex sampling designs (Horvitz and Thompson, 1952;  van den Brakel and Renssen, 2005). </li></ol>



<ol start="2"><li>The bias imposed by the restricted sampling of buckets implied by programs of exclusive experiments is derived. It is shown that this bias is often restricted to the history right before the experiment. Moreover, the length of the window of the history that affects the bias can be estimated for any empirical experimentation program. One way to phrase this finding is that the sample at a time point T is not random with respect to the last D days leading up to time T, but random with respect to everything that happened before the time point T-D. If things happened during the last D days that make the set of buckets available for sampling at time T different from the population with respect to the treatment effect, the estimator is biased. This insight makes it possible for experimenters to evaluate the risk for biases by checking what experiments have been run in the program over the last D days, and if those risks for biases are likely to affect the average treatment effect in the experiment that is about to be started. </li></ol>



<h2>Spotify’s new experimentation coordination strategy</h2>



<p>We have migrated our experimentation platform to using Bucket Reuse for all experiments at Spotify. There are a few key reasons why we prefer Bucket Reuse over other solutions:</p>



<ul><li>It is simple to implement and understand. </li><li>It is a technically feasible solution that allows us to do complex coordination without losing speed in our systems. As new users come into Spotify, they are uniformly hashed into the existing buckets — that is, the system scales as Spotify’s user base grows.</li><li>Using one company-global bucket structure makes it easy to coordinate experiments arbitrarily. For example, two programs that have been run independently can easily be merged into one program of exclusive experiments at any time point for any period of time. And, a sample from a previous experiment with a broken experience can easily be quarantined and avoided in any future experiments, as the sampling units are always the same over time.  </li></ul>



<p>At Spotify we have chosen Bucket Reuse with 1M (1,000,000) buckets to coordinate all experiments. That is, all users are hashed into 1M buckets, and these buckets are used for all experiments in all experimentation programs, exclusive and nonexclusive. Although Schultzberg et al.  establishes that smaller numbers of buckets can have statistical properties enabling straight forward inference, it should be clear that the larger the number of buckets, the better. Even though the inference for the average treatment effect is unaffected by the bucket sampling, it is well known that the effect of cluster sampling on other estimands decreases when the number of buckets increases (Kumar Pradhan). That is, imposing a bucket structure is not preferred from a statistical perspective, but it is a technical necessity. The choice of 1M buckets was made because it is close to the largest number of buckets we can have while still keeping the selected buckets within an executable script stored in a database without having to resort to <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Binary_large_object" target="_blank">BLOB</a> storage. </p>



<p>We are not planning to have any full stops or to reshuffle users into new buckets. However, there are naturally occurring periods of low experimentation that will help decrease the dependency between the samples in programs of exclusive experiments. For example, many programs run fewer general product experiments over the winter holidays due to unusual listening behaviors. These natural pauses effectively reset the programs in terms of dependencies.  </p>



<p>To help experimenters running programs of exclusive experiments, we are implementing a few tools to keep track of the short-term dependencies. For each program, we will estimate the length of the history that can bias the results. Moreover, we are also implementing a tool to see the history of the available buckets at any time point. Figure 4 displays a prototype of this tool. It allows experimenters to evaluate if the experiments that the available user came out of are likely to bias the estimator in their experiment. It also provides information about the size of the overlap with previous experiments. </p>



<figure><img loading="lazy" width="1600" height="1394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/2020-10-23-10.58.56.gif" alt=""/><figcaption>Figure 4: <em>Dependency graph that shows the experimenter where the free space, and thereby their sample, will come from in terms of previous experiments in the exclusive program. Each rectangle corresponds to a previously run experiment. The numbers in the yellow circles indicate the percentage points of the population that went from one experiment into another, and finally into the proportion of the population that is available (“free space”) for sampling right now. The experimenter can see what effects the previous experiments had on the metrics of interest in their experiment. </em><br/><em>Figure above for illustrative purposes only.</em><br/></figcaption></figure>







<p>Statistical validity, derived from proper random sampling and random treatment allocation, is the cornerstone of a successful experimentation program. However, implementing systems that can serve hundreds of experiments to hundreds of millions users — while retaining the ability to conveniently coordinate experiments to be exclusive, without overlap — requires compromises between technical feasibility and statistical properties. At Spotify, we have migrated the internal experimentation platform to rely fully on <em>bucket reuse</em>, a technically desirable solution that provides speed, simplicity, and flexibility. In this post we establish the statistical properties under bucket reuse and conclude that the validity is unaffected. This migration enables more experiments of higher quality at Spotify.  </p>



<p><strong>Experimentation Platform team, Spotify</strong></p>



<p><strong>References</strong></p>



<p>Brakel, Jan van den, and Robbert H. Renssen. “Analysis of Experiments Embedded in Complex Sampling Designs.” <em>Survey Methodology </em> 31, no. 1 (2005): 23–40.</p>



<p>Horvitz, D. G. and D. J. Thompson. “A Generalization of Sampling Without Replacement from a Finite Universe.” <em>Journal of the American Statistical Association</em> 47, no. 260 (1952): 663–685. https://doi.org/10.2307/2280784</p>



<p>Kumar Pradhan, Bijoy. “On efficiency of cluster sampling on sampling on two occasions.” <em>Statistica</em> 64, no. 1 (2004): 183–191. <a href="https://doi.org/10.6092/issn.1973-2201/31">https://doi.org/10.6092/issn.1973-2201/31</a></p>



<p>Schultzberg, Mårten, Oskar Kjellin, and Johan Rydberg. “Statistical Properties of Exclusive and Non-exclusive Online Randomized Experiments using Bucket Reuse.” <em>arXiv preprint arXiv:2012.10202 </em>(2020).</p>



<p>Tang, Diane, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. “Overlapping experiment infrastructure: more, better, faster experimentation.” <em>KDD</em> <em>’10: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, (July 2010): 17–26.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Mårten Schultzberg, Oskar Kjellin, and Johan Rydberg</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify’s New Experimentation Coordination Strategy&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/</link>
      <description>At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 10, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/" title="Spotify’s New Experimentation Coordination Strategy">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/Bucket-Reuse_01-1.png"/>                    </a>
                        
        </p>

        

        
<p>At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time. These are well-known problems among tech companies — take, for example, Google’s solution in Tang et al (2010). But the statistical implications of different solutions have not been properly investigated. In a recent paper (<a rel="noreferrer noopener" href="https://arxiv.org/abs/2012.10202" target="_blank">M. Schultzberg, O. Kjellin, and J. Rydberg, 2020</a>), we investigate important statistical properties of a common technical solution to the coordination — called “Bucket Reuse”. In this blog post we highlight some interesting results and present some details about how Spotify will coordinate experiments from now on.</p>



<h2>What is Bucket Reuse?</h2>



<p>Bucket Reuse is a simple idea utilizing the power of hashing. Essentially the steps are as follows: decide on a number of buckets (B). Take the unique user ID and hash it together with a  random salt into B “buckets” such that all users hash into one and only one bucket. Once the hash map is established, all sampling is performed on the bucket level. This implies that a bucket either is or is not in a sample at any given time point. If we want to sample N number of users, we sample the number of buckets that contain the number of users closest to the desired number N. If, e.g., the desired N is 20 and each bucket contains 3 users, we would sample 7 buckets and end up with 21 users. Note that a bucket is simply a logical group of units to which we assign a certain user by a fixed hash map. Figure 1 illustrates such a map. The second part of the name Bucket <em>Reuse</em> comes from the fact that we reuse the same buckets over and over again in the sampling for all experiments. That is, the random salt for the hashing is selected only once; after that, the hash map and the number of buckets is fixed. <strong>Importantly,</strong> when we talk about Bucket Reuse for experimentation, we always mean the following: the random sampling is performed on the bucket level; the random treatment allocation is performed at the user level on the users in the sample. </p>



<figure><img loading="lazy" width="700" height="625" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x625.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x625.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-250x223.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-768x686.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-120x107.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4.png 1366w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: <em>Schematic illustration of a hash map. A user ID is hashed together with a random salt to map each user to a unique bucket. </em></figcaption></figure>



<h2>What is experiment coordination?</h2>



<p>To get into the interesting parts of experiment coordination, we need to establish some key concepts. Figure 2 illustrates the concepts of exclusive and nonexclusive experiments. That two or more experiments “are exclusive” to each other simply means that they are run on distinct sets of users. Experiments that are nonexclusive are nonexclusive to <em>all</em> experiments at Spotify, meaning that they all randomly overlap in terms of users.</p>



<figure><img loading="lazy" width="700" height="383" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-700x383.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-700x383.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-250x137.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-768x420.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-120x66.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5.png 1498w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: <em>Illustration of exclusive and nonexclusive experiments. Exclusive experiments never overlap with each other in terms of users; nonexclusive experiments randomly overlap with exclusive experiments and other nonexclusive experiments. Note that the allocations in this figure were selected for illustration; in a true random sample we would expect exclusive experiments to also be spread out uniformly.  </em></figcaption></figure>



<p>The most critical challenge from a statistical perspective is imposed by what we call <em>programs of exclusive experiments</em>. A program of exclusive experiments is a set of experiments run over time where all simultaneous experiments are exclusive to each other. That is, a unit is in at most one, and only one, of the experiments in the program at any given time point. At Spotify we have such programs for several surfaces in the app, for example Search, the Home screen, and certain parts of the backend code base. To better understand the limitations imposed on the sampling by running programs of exclusive experiments, it is helpful to introduce the concepts of paths. A path is simply a sequence of experiments that a unit can be in. Figure 3 illustrates a program of exclusive experiments containing 5 experiments over time. Below the experiments, their possible paths are displayed. For example, it is not possible to go through both Experiments 3 and 4 as they overlap in time and are exclusive, and must therefore be run on distinct users. The number of unique possible paths explodes combinatorially after a relatively short time period in most programs, and only a small partition of the possible paths can be taken by any unit regardless of the sample strategies discussed in this post.</p>



<figure><img loading="lazy" width="700" height="806" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-700x806.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-700x806.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-250x288.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-768x885.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-1334x1536.png 1334w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-120x138.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3.png 1370w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 3: <em>Paths of experiences possible during a hypothetical program of 5 exclusive experiments.</em></figcaption></figure>



<h2>Highlights from <a rel="noreferrer noopener" href="https://arxiv.org/abs/2012.10202" target="_blank">Schultzberg et al.</a></h2>



<p>Schultzberg et al. presents two key results:</p>



<ol><li>Under unrestricted sampling of buckets, i.e., Bucket Reuse in nonexclusive experiments, the properties of the difference-in-means estimator of the average treatment effect is approximately equivalent to the properties under random sampling of units. In other words, standard t-tests can be used for inference. The key to this remarkable finding is connecting Bucket Reuse to the existing literature on randomized experiments embedded in complex sampling designs (Horvitz and Thompson, 1952;  van den Brakel and Renssen, 2005). </li></ol>



<ol start="2"><li>The bias imposed by the restricted sampling of buckets implied by programs of exclusive experiments is derived. It is shown that this bias is often restricted to the history right before the experiment. Moreover, the length of the window of the history that affects the bias can be estimated for any empirical experimentation program. One way to phrase this finding is that the sample at a time point T is not random with respect to the last D days leading up to time T, but random with respect to everything that happened before the time point T-D. If things happened during the last D days that make the set of buckets available for sampling at time T different from the population with respect to the treatment effect, the estimator is biased. This insight makes it possible for experimenters to evaluate the risk for biases by checking what experiments have been run in the program over the last D days, and if those risks for biases are likely to affect the average treatment effect in the experiment that is about to be started. </li></ol>



<h2>Spotify’s new experimentation coordination strategy</h2>



<p>We have migrated our experimentation platform to using Bucket Reuse for all experiments at Spotify. There are a few key reasons why we prefer Bucket Reuse over other solutions:</p>



<ul><li>It is simple to implement and understand. </li><li>It is a technically feasible solution that allows us to do complex coordination without losing speed in our systems. As new users come into Spotify, they are uniformly hashed into the existing buckets — that is, the system scales as Spotify’s user base grows.</li><li>Using one company-global bucket structure makes it easy to coordinate experiments arbitrarily. For example, two programs that have been run independently can easily be merged into one program of exclusive experiments at any time point for any period of time. And, a sample from a previous experiment with a broken experience can easily be quarantined and avoided in any future experiments, as the sampling units are always the same over time.  </li></ul>



<p>At Spotify we have chosen Bucket Reuse with 1M (1,000,000) buckets to coordinate all experiments. That is, all users are hashed into 1M buckets, and these buckets are used for all experiments in all experimentation programs, exclusive and nonexclusive. Although Schultzberg et al.  establishes that smaller numbers of buckets can have statistical properties enabling straight forward inference, it should be clear that the larger the number of buckets, the better. Even though the inference for the average treatment effect is unaffected by the bucket sampling, it is well known that the effect of cluster sampling on other estimands decreases when the number of buckets increases (Kumar Pradhan). That is, imposing a bucket structure is not preferred from a statistical perspective, but it is a technical necessity. The choice of 1M buckets was made because it is close to the largest number of buckets we can have while still keeping the selected buckets within an executable script stored in a database without having to resort to <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Binary_large_object" target="_blank">BLOB</a> storage. </p>



<p>We are not planning to have any full stops or to reshuffle users into new buckets. However, there are naturally occurring periods of low experimentation that will help decrease the dependency between the samples in programs of exclusive experiments. For example, many programs run fewer general product experiments over the winter holidays due to unusual listening behaviors. These natural pauses effectively reset the programs in terms of dependencies.  </p>



<p>To help experimenters running programs of exclusive experiments, we are implementing a few tools to keep track of the short-term dependencies. For each program, we will estimate the length of the history that can bias the results. Moreover, we are also implementing a tool to see the history of the available buckets at any time point. Figure 4 displays a prototype of this tool. It allows experimenters to evaluate if the experiments that the available user came out of are likely to bias the estimator in their experiment. It also provides information about the size of the overlap with previous experiments. </p>



<figure><img loading="lazy" width="1600" height="1394" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-10-23-10.58.56.gif" alt=""/><figcaption>Figure 4: <em>Dependency graph that shows the experimenter where the free space, and thereby their sample, will come from in terms of previous experiments in the exclusive program. Each rectangle corresponds to a previously run experiment. The numbers in the yellow circles indicate the percentage points of the population that went from one experiment into another, and finally into the proportion of the population that is available (“free space”) for sampling right now. The experimenter can see what effects the previous experiments had on the metrics of interest in their experiment. </em><br/><em>Figure above for illustrative purposes only.</em><br/></figcaption></figure>







<p>Statistical validity, derived from proper random sampling and random treatment allocation, is the cornerstone of a successful experimentation program. However, implementing systems that can serve hundreds of experiments to hundreds of millions users — while retaining the ability to conveniently coordinate experiments to be exclusive, without overlap — requires compromises between technical feasibility and statistical properties. At Spotify, we have migrated the internal experimentation platform to rely fully on <em>bucket reuse</em>, a technically desirable solution that provides speed, simplicity, and flexibility. In this post we establish the statistical properties under bucket reuse and conclude that the validity is unaffected. This migration enables more experiments of higher quality at Spotify.  </p>



<p><strong>Experimentation Platform team, Spotify</strong></p>



<p><strong>References</strong></p>



<p>Brakel, Jan van den, and Robbert H. Renssen. “Analysis of Experiments Embedded in Complex Sampling Designs.” <em>Survey Methodology </em> 31, no. 1 (2005): 23–40.</p>



<p>Horvitz, D. G. and D. J. Thompson. “A Generalization of Sampling Without Replacement from a Finite Universe.” <em>Journal of the American Statistical Association</em> 47, no. 260 (1952): 663–685. https://doi.org/10.2307/2280784</p>



<p>Kumar Pradhan, Bijoy. “On efficiency of cluster sampling on sampling on two occasions.” <em>Statistica</em> 64, no. 1 (2004): 183–191. <a href="https://doi.org/10.6092/issn.1973-2201/31">https://doi.org/10.6092/issn.1973-2201/31</a></p>



<p>Schultzberg, Mårten, Oskar Kjellin, and Johan Rydberg. “Statistical Properties of Exclusive and Non-exclusive Online Randomized Experiments using Bucket Reuse.” <em>arXiv preprint arXiv:2012.10202 </em>(2020).</p>



<p>Tang, Diane, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. “Overlapping experiment infrastructure: more, better, faster experimentation.” <em>KDD</em> <em>’10: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, (July 2010): 17–26.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Mårten Schultzberg, Oskar Kjellin, and Johan Rydberg</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify’s New Experimentation Coordination Strategy&#xA;</title>
      <link>https://engineering.atspotify.com/spotifys-new-experimentation-coordination-strategy/</link>
      <description>At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 10, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/spotifys-new-experimentation-coordination-strategy/" title="Spotify’s New Experimentation Coordination Strategy">
                        <img src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px"/>                    </a>
                        
        </p>

        

        
<p>At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time. These are well-known problems among tech companies — take, for example, Google’s solution in Tang et al (2010). But the statistical implications of different solutions have not been properly investigated. In a recent paper (<a href="https://link.springer.com/chapter/10.1007/978-3-030-89906-6_50" target="_blank" rel="noreferrer noopener">M. Schultzberg, O. Kjellin, and J. Rydberg, 2020</a>), we investigate important statistical properties of a common technical solution to the coordination — called “Bucket Reuse”. In this blog post we highlight some interesting results and present some details about how Spotify will coordinate experiments from now on.</p>



<h2>What is Bucket Reuse?</h2>



<p>Bucket Reuse is a simple idea utilizing the power of hashing. Essentially the steps are as follows: decide on a number of buckets (B). Take the unique user ID and hash it together with a  random salt into B “buckets” such that all users hash into one and only one bucket. Once the hash map is established, all sampling is performed on the bucket level. This implies that a bucket either is or is not in a sample at any given time point. If we want to sample N number of users, we sample the number of buckets that contain the number of users closest to the desired number N. If, e.g., the desired N is 20 and each bucket contains 3 users, we would sample 7 buckets and end up with 21 users. Note that a bucket is simply a logical group of units to which we assign a certain user by a fixed hash map. Figure 1 illustrates such a map. The second part of the name Bucket <em>Reuse</em> comes from the fact that we reuse the same buckets over and over again in the sampling for all experiments. That is, the random salt for the hashing is selected only once; after that, the hash map and the number of buckets is fixed. <strong>Importantly,</strong> when we talk about Bucket Reuse for experimentation, we always mean the following: the random sampling is performed on the bucket level; the random treatment allocation is performed at the user level on the users in the sample. </p>



<figure><img loading="lazy" width="700" height="625" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-700x625.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-700x625.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-250x223.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-768x686.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-120x107.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4.png 1366w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: <em>Schematic illustration of a hash map. A user ID is hashed together with a random salt to map each user to a unique bucket. </em></figcaption></figure>



<h2>What is experiment coordination?</h2>



<p>To get into the interesting parts of experiment coordination, we need to establish some key concepts. Figure 2 illustrates the concepts of exclusive and nonexclusive experiments. That two or more experiments “are exclusive” to each other simply means that they are run on distinct sets of users. Experiments that are nonexclusive are nonexclusive to <em>all</em> experiments at Spotify, meaning that they all randomly overlap in terms of users.</p>



<figure><img loading="lazy" width="700" height="383" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-700x383.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-700x383.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-250x137.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-768x420.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-120x66.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5.png 1498w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: <em>Illustration of exclusive and nonexclusive experiments. Exclusive experiments never overlap with each other in terms of users; nonexclusive experiments randomly overlap with exclusive experiments and other nonexclusive experiments. Note that the allocations in this figure were selected for illustration; in a true random sample we would expect exclusive experiments to also be spread out uniformly.  </em></figcaption></figure>



<p>The most critical challenge from a statistical perspective is imposed by what we call <em>programs of exclusive experiments</em>. A program of exclusive experiments is a set of experiments run over time where all simultaneous experiments are exclusive to each other. That is, a unit is in at most one, and only one, of the experiments in the program at any given time point. At Spotify we have such programs for several surfaces in the app, for example Search, the Home screen, and certain parts of the backend code base. To better understand the limitations imposed on the sampling by running programs of exclusive experiments, it is helpful to introduce the concepts of paths. A path is simply a sequence of experiments that a unit can be in. Figure 3 illustrates a program of exclusive experiments containing 5 experiments over time. Below the experiments, their possible paths are displayed. For example, it is not possible to go through both Experiments 3 and 4 as they overlap in time and are exclusive, and must therefore be run on distinct users. The number of unique possible paths explodes combinatorially after a relatively short time period in most programs, and only a small partition of the possible paths can be taken by any unit regardless of the sample strategies discussed in this post.</p>



<figure><img loading="lazy" width="700" height="806" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-700x806.png" alt="" srcset="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-700x806.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-250x288.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-768x885.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-1334x1536.png 1334w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-120x138.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3.png 1370w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 3: <em>Paths of experiences possible during a hypothetical program of 5 exclusive experiments.</em></figcaption></figure>



<h2>Highlights from <a href="https://link.springer.com/chapter/10.1007/978-3-030-89906-6_50" target="_blank" rel="noreferrer noopener">Schultzberg et al.</a></h2>



<p>Schultzberg et al. presents two key results:</p>



<ol><li>Under unrestricted sampling of buckets, i.e., Bucket Reuse in nonexclusive experiments, the properties of the difference-in-means estimator of the average treatment effect is approximately equivalent to the properties under random sampling of units. In other words, standard t-tests can be used for inference. The key to this remarkable finding is connecting Bucket Reuse to the existing literature on randomized experiments embedded in complex sampling designs (Horvitz and Thompson, 1952;  van den Brakel and Renssen, 2005). </li></ol>



<ol start="2"><li>The bias imposed by the restricted sampling of buckets implied by programs of exclusive experiments is derived. It is shown that this bias is often restricted to the history right before the experiment. Moreover, the length of the window of the history that affects the bias can be estimated for any empirical experimentation program. One way to phrase this finding is that the sample at a time point T is not random with respect to the last D days leading up to time T, but random with respect to everything that happened before the time point T-D. If things happened during the last D days that make the set of buckets available for sampling at time T different from the population with respect to the treatment effect, the estimator is biased. This insight makes it possible for experimenters to evaluate the risk for biases by checking what experiments have been run in the program over the last D days, and if those risks for biases are likely to affect the average treatment effect in the experiment that is about to be started. </li></ol>



<h2>Spotify’s new experimentation coordination strategy</h2>



<p>We have migrated our experimentation platform to using Bucket Reuse for all experiments at Spotify. There are a few key reasons why we prefer Bucket Reuse over other solutions:</p>



<ul><li>It is simple to implement and understand. </li><li>It is a technically feasible solution that allows us to do complex coordination without losing speed in our systems. As new users come into Spotify, they are uniformly hashed into the existing buckets — that is, the system scales as Spotify’s user base grows.</li><li>Using one company-global bucket structure makes it easy to coordinate experiments arbitrarily. For example, two programs that have been run independently can easily be merged into one program of exclusive experiments at any time point for any period of time. And, a sample from a previous experiment with a broken experience can easily be quarantined and avoided in any future experiments, as the sampling units are always the same over time.  </li></ul>



<p>At Spotify we have chosen Bucket Reuse with 1M (1,000,000) buckets to coordinate all experiments. That is, all users are hashed into 1M buckets, and these buckets are used for all experiments in all experimentation programs, exclusive and nonexclusive. Although Schultzberg et al.  establishes that smaller numbers of buckets can have statistical properties enabling straight forward inference, it should be clear that the larger the number of buckets, the better. Even though the inference for the average treatment effect is unaffected by the bucket sampling, it is well known that the effect of cluster sampling on other estimands decreases when the number of buckets increases (Kumar Pradhan). That is, imposing a bucket structure is not preferred from a statistical perspective, but it is a technical necessity. The choice of 1M buckets was made because it is close to the largest number of buckets we can have while still keeping the selected buckets within an executable script stored in a database without having to resort to <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Binary_large_object" target="_blank">BLOB</a> storage. </p>



<p>We are not planning to have any full stops or to reshuffle users into new buckets. However, there are naturally occurring periods of low experimentation that will help decrease the dependency between the samples in programs of exclusive experiments. For example, many programs run fewer general product experiments over the winter holidays due to unusual listening behaviors. These natural pauses effectively reset the programs in terms of dependencies.  </p>



<p>To help experimenters running programs of exclusive experiments, we are implementing a few tools to keep track of the short-term dependencies. For each program, we will estimate the length of the history that can bias the results. Moreover, we are also implementing a tool to see the history of the available buckets at any time point. Figure 4 displays a prototype of this tool. It allows experimenters to evaluate if the experiments that the available user came out of are likely to bias the estimator in their experiment. It also provides information about the size of the overlap with previous experiments. </p>



<figure><img loading="lazy" width="1600" height="1394" src="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/2020-10-23-10.58.56.gif" alt=""/><figcaption>Figure 4: <em>Dependency graph that shows the experimenter where the free space, and thereby their sample, will come from in terms of previous experiments in the exclusive program. Each rectangle corresponds to a previously run experiment. The numbers in the yellow circles indicate the percentage points of the population that went from one experiment into another, and finally into the proportion of the population that is available (“free space”) for sampling right now. The experimenter can see what effects the previous experiments had on the metrics of interest in their experiment. </em><br/><em>Figure above for illustrative purposes only.</em><br/></figcaption></figure>







<p>Statistical validity, derived from proper random sampling and random treatment allocation, is the cornerstone of a successful experimentation program. However, implementing systems that can serve hundreds of experiments to hundreds of millions users — while retaining the ability to conveniently coordinate experiments to be exclusive, without overlap — requires compromises between technical feasibility and statistical properties. At Spotify, we have migrated the internal experimentation platform to rely fully on <em>bucket reuse</em>, a technically desirable solution that provides speed, simplicity, and flexibility. In this post we establish the statistical properties under bucket reuse and conclude that the validity is unaffected. This migration enables more experiments of higher quality at Spotify.  </p>



<p><strong>Experimentation Platform team, Spotify</strong></p>



<p><strong>References</strong></p>



<p>Brakel, Jan van den, and Robbert H. Renssen. “Analysis of Experiments Embedded in Complex Sampling Designs.” <em>Survey Methodology </em> 31, no. 1 (2005): 23–40.</p>



<p>Horvitz, D. G. and D. J. Thompson. “A Generalization of Sampling Without Replacement from a Finite Universe.” <em>Journal of the American Statistical Association</em> 47, no. 260 (1952): 663–685. https://doi.org/10.2307/2280784</p>



<p>Kumar Pradhan, Bijoy. “On efficiency of cluster sampling on sampling on two occasions.” <em>Statistica</em> 64, no. 1 (2004): 183–191. <a href="https://doi.org/10.6092/issn.1973-2201/31">https://doi.org/10.6092/issn.1973-2201/31</a></p>



<p>Schultzberg, Mårten, Oskar Kjellin, and Johan Rydberg. “Statistical Properties of Exclusive and Non-exclusive Online Randomized Experiments using Bucket Reuse.” <em>arXiv preprint arXiv:2012.10202 </em>(2020).</p>



<p>Tang, Diane, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. “Overlapping experiment infrastructure: more, better, faster experimentation.” <em>KDD</em> <em>’10: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, (July 2010): 17–26.</p>
        <p>

        Tags: <a href="https://engineering.atspotify.com/tag/data/" rel="tag">Data</a><br/>        
            </p></div></div>]]></content:encoded>
      <author>Published by Mårten Schultzberg, Oskar Kjellin, and Johan Rydberg</author>
      <enclosure url="https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Matt Clarke: Senior Backend Infrastructure Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/09/my-beat-matt-clarke/</link>
      <description>Matt is a Senior Backend Infrastructure Engineer and has been at Spotify for two-and-a-half years. This time last year, he was living and working in London – but that’s all changed since the start of the pandemic…</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4139">
     <div>
         
         
         
         <div>
             <p><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg" alt="Matt Clarke" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg 800w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-250x192.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-700x537.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-768x589.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-120x92.jpg 120w" sizes="(max-width: 800px) 100vw, 800px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/02/Matt-Clarke-edit.jpg"/>
                                  
             </p>
             <p><strong>Matt is a Senior Backend Infrastructure Engineer and has been at Spotify for two-and-a-half years. This time last year, he was living and working in London – but that’s all changed since the start of the pandemic… </strong></p>
         </div>

         


         

         
<blockquote><p>8:00am</p></blockquote>



<p>At the moment, I’m living with my wife and step daughter in London and working from home like most people. Which means I get up around 8am, drink lots of coffee (way too much!) and log onto my computer an hour later – ready to check my messages and start getting my brain into thinking mode. </p>



<blockquote><p>10:00am</p></blockquote>



<p>My team is actually based in New York, so I tend to work 10–6 to overlap as much as possible with their hours. But the time difference means my mornings are fairly quiet and meeting-free – a chance for me to get down to focussed, individual tasks, without too many interruptions. </p>



<p>My work is mostly to do with Kubernetes – the technology we use to deploy our services at Spotify – and a lot of my time is spent helping other engineers get to grips with the system, debugging their issues and developing our infrastructure services, so they can deploy more easily and reliably. </p>



<p>Recently, I’ve been working on something called the k8s plug in, which vastly simplifies the Kubernetes experience for developers and means they can operate without a huge understanding of the platform under the hood. Earlier this year, we open-sourced this plug in, which felt like a really great moment – it’s amazing to think it’s now available to everyone in the tech community worldwide and can benefit so many people outside our organization. That to me is the magical thing about open-source. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>I try to grab a bite to eat around 1pm, although I’m really bad at taking breaks – I get so sucked into what I’m doing that I forget the time, especially if I’m coding. It used to happen when I worked in the office too, even though there was an awesome canteen and a table tennis table to tempt me away from my desk!</p>



<blockquote><p>2:00pm</p></blockquote>



<p>This is when New York starts to wake up, so my work becomes more team-based – I often pair remotely with one of the developers out there, which means jumping on a hangout and sharing our screens, so we can collaborate on a piece of code. Alternatively, we might work together to write documents like RFCs, debug production issues or help other developers with their infrastructure issues. It’s a bit of a mix, really, </p>



<p>Weirdly, I’ve only met one of team-mates in real life, when I first joined up and spent two weeks in the New York office as part of an embed. But we still all work together really well – it’s friendly and we joke around a lot. I think there are some rules you need to learn for remote working and being in different zones – you need to be a bit flexible and not always expect to get your answers straight away. But once you’ve got used to that, things are surprisingly easy – it’s really not a big deal at all. </p>



<blockquote><p>6:00pm </p></blockquote>



<p>I usually finish up at 6ish, although I’m terrible for checking my emails in the evening. To try and switch off, I watch TV, listen to podcasts or play video games – I also started up an engineering book club at the start of the first lockdown. And I’m really into cooking at the moment – there’s something about following a recipe and going through a series of orchestrated steps that reminds me of coding. Although, as the old joke goes, at least your potato peeler never turns out to be ten versions out of date… </p>







<figure><img loading="lazy" width="700" height="111" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="557" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-700x557.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-700x557.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-250x199.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-768x611.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-120x96.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph.png 1520w" sizes="(max-width: 700px) 100vw, 700px"/></figure>

         
         

         <p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            2020 Unwrapped: The people behind the numbers&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/01/2020-unwrapped-the-people-behind-the-numbers/</link>
      <description>2020 Wrapped is a story of gratitude and resilience. And we’re grateful for the people and teams behind the curtain who built this product experience (👏🏽Give them a hand!). The effort behind Wrapped spans the entire company and is founded on communication and collaboration. With the shift to wor</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 1, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/01/2020-unwrapped-the-people-behind-the-numbers/" title="2020 Unwrapped: The people behind the numbers">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg 1510w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-250x122.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-700x341.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-768x374.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-120x58.jpg 120w" sizes="(max-width: 1510px) 100vw, 1510px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/2020-Wrapped.jpg"/>                    </a>
                        
        </p>

        

        
<p>2020 Wrapped is a story of gratitude and resilience. And we’re grateful for the people and teams behind the curtain who built this product experience (👏🏽Give them a hand!). </p>



<p>The effort behind Wrapped spans the entire company and is founded on communication and collaboration. With the shift to working from home, we needed to create a structure that enabled us to collaborate and communicate remotely, and prioritized asynchronous communication over synchronous communication when possible. We were able to adapt to unprecedented challenges and join forces to deliver a personalized product experience for our listeners. Now, let’s take a look at the numbers behind the 2020 Wrapped experience.</p>



<h2><strong>Distributed execution</strong></h2>



<p>2020 Wrapped, like the Wrapped campaigns before it, was a company-wide project that involved collaboration among hundreds of Spotifiers. Over a span of 4 months, two key groups dedicated themselves to building the product experience — <strong>Personalization</strong> <strong>(PZN)</strong>, the minds behind the <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" target="_blank" rel="noreferrer noopener">data</a> magic, and <strong>Edison</strong>, drivers of the rich end-user experience. Around twenty engineers from 4 different squads within Edison and 6 engineers from other squads joined as embeds, all working as a virtual team covering 4 different time zones. </p>



<p>We adapted our ways of working to allow team members to contribute, communicate and collaborate regardless of their location. The team asynchronously worked on 10 technical specification documents, 8 RFC documents, 4 project planners, and more than 15 slide decks to facilitate information sharing.</p>



<figure><ul><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-700x1245.png" alt="" data-id="4235" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer.png" data-link="https://engineering.atspotify.com/?attachment_id=4235" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li><li><figure><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3-700x1245.png" alt="" data-id="4225" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.png" data-link="https://engineering.atspotify.com/?attachment_id=4225"/></figure></li></ul></figure>



<p>With a distributed-first mindset, Slack became our place to build traceability and transparency, allowing us to easily retrace our steps and find documentation that led to business decisions and actions. By the end of the project,  we had created 11 Slack channels each dedicated to brief messages that required quick answers or a brief team discussion around specific topics. One particular channel existed solely for the purpose of sharing memes — 8,116 and counting. 😬</p>



<figure><ul><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-700x1245.png" alt="" data-id="4237" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack.png" data-link="https://engineering.atspotify.com/?attachment_id=4237" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-700x1245.png" alt="" data-id="4238" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison.png" data-link="https://engineering.atspotify.com/?attachment_id=4238" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li></ul></figure>



<p>Ultimately, we kept the team connected and created team spirit by holding brainstorm meetings and building on ideas we generated as a group. It was especially helpful when complex problems arose and we needed to get people on the same page.</p>



<h2><strong>Collaboration</strong></h2>



<p>The core <a href="https://spotify.design/article/how-we-brought-2020-wrapped-to-life-in-the-mobile-app" target="_blank" rel="noreferrer noopener">mobile experience</a> was built by a few teams, but to deliver the product experience, it took the effort of 8 additional squads working simultaneously. We defaulted to open communication and, in most cases, preferred to overshare than undershare. Given the number of teams, oversharing ensured a successful flow of information. </p>



<p>We focused a great deal on expanding the mobile experience for our listeners in 2019, and we were able to reuse some of the components and apply some of the <a href="https://engineering.atspotify.com/2020/09/21/spotify-unwrapped-2019-how-we-built-an-in-app-experience-just-for-you/" target="_blank" rel="noreferrer noopener">lessons from 2019 Wrapped</a> to the 2020 experience. Compared to 2019, we were able to reduce our engineering effort by 50% for Android development, reduce our timeline for iOS development by 30%, and repeat with 60 engineering weeks for Backend. For Web, we increased our scope with an ambitious vision and made a significant investment in our engineering effort compared to 2019.</p>



<p>On the day of launch, individuals from multiple functions — Marketing, Brand + Creative, R&amp;D, Customer Support, Local Marketing, Public Relations, and Localization — gathered in a war room (aka a virtual meeting) testing, troubleshooting, and tackling live issues with tight coordination across 8 hours.</p>



<h2><strong>Wrapped around the world</strong></h2>



<p>With 12 new markets and 4 new languages, 2020 Wrapped was the most global iteration of the project to date 🌎🌍🌏. We served 26 total languages, 8 different fonts, and 87,343 words total — approximately 4,000 words per language in both left-to-right and right-to-left text orientations. Though a challenging task, it was absolutely critical that we embed localization into our process to improve the listening experience for our users across the globe. </p>



<figure><ul><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-700x1245.png" alt="" data-id="4240" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily.png" data-link="https://engineering.atspotify.com/?attachment_id=4240" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-700x1245.png" alt="" data-id="4241" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts.png" data-link="https://engineering.atspotify.com/?attachment_id=4241" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li></ul></figure>



<p>Shareable cards, like the ones above, hold a number of different corner cases to test, and quality remains top of mind. Because we weren’t physically in the office to pair and review pieces, it became imperative that we find creative solutions to allow for effective and efficient collaboration. For Localization testing and the Design Review process, we created a dedicated tool to reduce the feedback loop and enable the team to use it asynchronously. Using this tool, we were able to increase productivity in our distributed work environment, while respecting flexible work schedules and work/life boundaries.</p>



<figure><img loading="lazy" width="700" height="358" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-700x358.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-700x358.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-250x128.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-768x393.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-1536x786.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-120x61.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8.png 1749w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="359" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-700x359.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-700x359.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-250x128.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-768x393.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-1536x787.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-120x61.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton.png 1743w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h2><strong>Closing words from the Wrapped fellowship</strong></h2>



<p>In the shift to working from home on a project that spanned the company, we needed to (quickly) set up a structure to keep productivity high and collaboration and communication flowing, while ensuring we were taking care of ourselves, our family and friends, and our team members. </p>



<p>We hope you enjoyed reading about the people behind the numbers, and hope that Wrapped made the end of your 2020 just a little bit better.</p>



<p>Thank you to everyone who made another year of Wrapped.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Designing a Better Kubernetes Experience for Developers&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/</link>
      <description>TLDR; If you’re deploying a service with Kubernetes, you shouldn’t have to use all of your cluster management skills just to perform everyday developer tasks (like seeing which pods are experiencing errors or checking autoscaler limits). Backstage Kubernetes simplifies your deployment workflow by co</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 1, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/" title="Designing a Better Kubernetes Experience for Developers">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/image3.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR;</strong> If you’re deploying a service with Kubernetes, you shouldn’t have to use all of your cluster management skills just to perform everyday developer tasks (like seeing which pods are experiencing errors or checking autoscaler limits). Backstage Kubernetes simplifies your deployment workflow by connecting to your existing Kubernetes implementation and aggregating the status of all your deployments into a single view — even across multiple clusters in multiple regions. </p>



<h2>Navigating the complexity of Kubernetes</h2>



<p>If you’re building a service today, you’re likely deploying it as a container, which is inside a pod, which is inside a cluster (alongside a bunch of other services that don’t belong to you), with deployments on different clusters spinning up and down all around the world. It can be hard to keep track of everything.</p>



<p>But despite widespread adoption of Kubernetes, all the tools for navigating this complexity have been focussed on the needs of cluster admins. This can make something as simple as checking the health of your service somewhat complicated. </p>



<p>That’s why we built a Kubernetes monitoring tool focussed on the needs of service owners and made it a core feature of <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a>, our open platform for building developer portals. We wanted to make the experience of managing services deployed on Kubernetes easier for all developers. </p>



<p>But first, how did we get here?</p>



<h2>The rise of Kubernetes and DevOps</h2>



<p>Since its release in 2014, Kubernetes has become one of the most widely adopted and important open source projects. Capabilities like autoscaling and cost optimisation through container scheduling used to be time-consuming and tricky to get right — now they’ve been democratised. </p>



<p>At the same time, the concept of DevOps has become mainstream. Developers now regularly perform tasks that were traditionally the domain of operations experts. </p>



<p>So, while everyday engineers can do more than ever before, their new powers have also come along with a new set of responsibilities.</p>



<h2>New powers, shifting roles</h2>



<p>When I first started using Kubernetes, cluster admins and service owners were one and the same: the people who built a cluster were usually the same people who owned the services that ran in the cluster. That’s not how it is today. As Kubernetes has achieved widespread adoption there has been a shift in Kubernetes usage as well as a shift in how Kubernetes is managed at the organisation level. </p>



<p>Now organisations tend to have a separate infrastructure team (sometimes not-so-ironically called the “DevOps” team) who build and maintain clusters for the feature developers and service owners. As the teams have become more specialized, the setups have become more advanced. For instance, the infrastructure team might set up Kubernetes clusters in multiple geographic regions in order to reduce end-user latency, wherever the user is in the world. </p>



<p>This is a better experience for the user, and it’s an optimization you might not have considered before Kubernetes existed or without a dedicated infrastructure team. But it also comes with productivity costs for the developer.</p>



<h2>Frustration also scales</h2>



<p>When your deployment environment reaches this kind of complexity and scale, the maintenance overhead for service owners increases. It forces them to use multiple kubectl contexts or multiple UIs just to get an overall view of their system. </p>



<p>It’s a small overhead — but adds up over time — and multiplies as service owners build more services and deploy them to more regions. Just checking the status of a service first requires hunting for it across multiple clusters. This can reduce productivity (and patience) company-wide.</p>



<h2>Better tools for the job</h2>



<p>We believed we could solve the problem through developer tooling. But we soon discovered the available tools weren’t suitable, because they:</p>



<ul><li>Don’t cater well for deploying to multiple Kubernetes clusters,</li><li>Usually require that users have clusterwide permissions, or</li><li>Display everything on a cluster and aren’t focused on the service the user cares about.</li></ul>



<p>As we often do when we want to <a href="https://backstage.io/blog/2020/10/22/cost-insights-plugin">solve a problem involving infrastructure complexity</a>, we wondered, why not build a custom plugin for Backstage, our homegrown developer portal?</p>



<h2>Backstage Kubernetes: Manage your services, not clusters</h2>



<p>Backstage provides vital information from Kubernetes — specifically focussed on the developer’s service. At a glance, the developer can see:</p>



<ul><li>The current status of their systems running in Kubernetes<ul><li>Including information aggregated from multiple clusters/regions</li></ul></li><li>Any errors reported by Kubernetes</li><li>How close the system is to its autoscaling limits</li><li>Container restarts</li></ul>



<figure><img loading="lazy" width="700" height="419" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-700x419.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-700x419.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-250x150.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-768x459.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-1536x919.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-120x72.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption><em>A default Kubernetes UI provides a cluster-centric view, including info about software you don’t own. </em><br/><em>(Source: </em><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/"><em>kubernetes.io</em></a><em>)</em></figcaption></figure>



<figure><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x394.png" alt=""/><figcaption><em>The Backstage Kubernetes UI provides a service-centric view, showing you the status of your service no matter how many clusters it’s been deployed to. </em><br/><em>Figures above are for illustrative purposes.</em></figcaption></figure>



<figure><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-700x382.png" alt=""/><figcaption><em>More detail about your deployments is just a click away. You can see autoscaler limits, errors, and the status of individual pods — all at a glance — and without a trip to the CLI.</em><br/><em>Figures above are for illustrative purposes.</em></figcaption></figure>



<p>Instead of spending 20 minutes in a CLI trying to track down which clusters your service has been deployed to, you get all the information you need to know at a glance. You can learn more about these features on the <a href="https://backstage.io/blog/2021/01/12/new-backstage-feature-kubernetes-for-service-owners" target="_blank" rel="noreferrer noopener">Backstage blog</a> — or watch the demo video below to get an overview.</p>



<figure><p>
<iframe loading="lazy" title="How to monitor your services on Kubernetes with Backstage (Demo)" width="900" height="506" src="https://www.youtube.com/embed/VivuOxn3VQ8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>



<h2>Everything about your service in one place</h2>



<p>As a standalone monitoring tool, we think Backstage Kubernetes can improve the experience of any developer who deploys to Kubernetes. Combined with the other features of Backstage, developers get a complete solution for building and managing their services.</p>



<p>At the core of Backstage is its <a href="https://backstage.io/blog/2020/05/22/phase-2-service-catalog" target="_blank" rel="noreferrer noopener">service catalog</a>, which aggregates information about software systems together so you have a consistent UI and one tool for developers to use. For years, Backstage has provided one place for Spotify’s developers to see everything they need to know about their services (APIs, documentation, ownership, etc.). Now that includes the current status of their service, regardless of how many Kubernetes clusters they deploy to.</p>



<p>Now that Backstage is open source, we want to improve on what we have built internally and provide Kubernetes as a core component of Backstage for anyone to contribute to and benefit from. </p>



<h2>Future Iteration</h2>



<p>As we continue to grow and develop Kubernetes in Backstage with the community, we hope to offer support for Kubernetes resources beyond Deployments and Custom Resource Definitions. </p>



<p>Although at Spotify we currently use GKE extensively, Kubernetes in Backstage communicates directly with the Kubernetes API and is cloud agnostic, accordingly. It will work with other cloud providers, including AWS and Azure, as well as managed Kubernetes services, like Red Hat OpenShift.</p>



<p>To contribute or get more information on Kubernetes in Backstage, <a href="https://discord.gg/MUpMjP2" target="_blank" rel="noreferrer noopener">join the discussion on Discord</a>!</p>



<p><em>Ask us anything: Matthew and the Backstage team will be hosting a Reddit AMA on March 3 at 4:00pm GMT. Send questions in <a href="https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/" target="_blank" rel="noreferrer noopener">r/kubernetes</a> starting March 2.</em></p>



<p><em>A version of this article first appeared on </em><a href="https://thenewstack.io" target="_blank" rel="noreferrer noopener"><em>The New Stack</em></a><em>.</em></p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Matthew Clarke, Senior Engineer</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020&#xA;</title>
      <link>https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/</link>
      <description>In this post we&#39;ll discuss how Spotify optimized and sped up elements from our largest Dataflow job, Wrapped 2019, for Wrapped 2020 using a technique called Sort Merge Bucket (SMB) join. We&#39;ll present the design and implementation of SMB and how we incorporated it into our data pipelines. Introdu</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>February 11, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" title="How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image1.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/02/image1.gif"/>                    </a>
                        
        </p>

        

        
<p>In this post we’ll discuss how Spotify optimized and sped up elements from our largest Dataflow job, <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/02/18/spotify-unwrapped-how-we-brought-you-a-decade-of-data/" target="_blank">Wrapped 2019</a>, for <a href="https://open.spotify.com/genre/2020-page">Wrapped 2020</a> using a technique called Sort Merge Bucket (SMB) join. We’ll present the design and implementation of SMB and how we incorporated it into our data pipelines.</p>



<h2>Introduction</h2>



<p>Shuffle is the core building block for many big data transforms, such as a join, GroupByKey, or other reduce operations. Unfortunately, it’s also one of the most expensive steps in many pipelines. Sort Merge Bucket is an optimization that reduces shuffle by doing work up front on the producer side. The intuition is that for datasets commonly and frequently joined on a known key, e.g., user events with user metadata on a user ID, we can write them in bucket files with records bucketed and sorted by that key. By knowing which files contain a subset of keys and in what order, shuffle becomes a matter of merge-sorting values from matching bucket files, completely eliminating costly disk and network I/O of moving key–value pairs around. Andrea Nardelli carried out the original investigation on Sort Merge Buckets for his <a href="http://kth.diva-portal.org/smash/get/diva2:1334587/FULLTEXT01.pdf">2018 master’s thesis</a>, and we started looking into generalizing the idea as a <a rel="noreferrer noopener" href="https://spotify.github.io/scio/extras/Sort-Merge-Bucket.html" target="_blank">Scio module</a> afterwards.</p>



<h2>Design and Implementation</h2>



<p>The majority of the data pipelines at Spotify are written in <a rel="noreferrer noopener" href="https://github.com/spotify/scio" target="_blank">Scio</a>, a Scala API for <a href="https://beam.apache.org/">Apache Beam</a>, and run on the <a href="https://cloud.google.com/dataflow">Google Cloud Dataflow</a> service. We implemented SMB in Java to be closer to the native Beam SDK (and even wrote and collaborated on a <a href="https://docs.google.com/document/d/1AQlonN8t4YJrARcWzepyP7mWHTxHAd6WIECwk1s3LQQ/edit?usp=sharing">design document with the Beam community</a>), and provide Scala syntactic sugar in Scio like many other I/Os. The design is modularized into the main components listed below — we’ll start with the two top-level SMB <a href="https://beam.apache.org/documentation/programming-guide/#transforms" target="_blank" rel="noreferrer noopener">PTransforms</a> — the write and read operations SortedBucketSink and SortedBucketSource.</p>



<h3>SortedBucketSink</h3>



<p>This transform writes a <a rel="noreferrer noopener" href="https://beam.apache.org/documentation/programming-guide/#pcollections" target="_blank">PCollection</a>&lt;T&gt; (where T has a corresponding <a href="https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/FileOperations.java" target="_blank" rel="noreferrer noopener">FileOperations&lt;T&gt;</a> instance) in SMB format. It first extracts keys and assigns bucket IDs using logic provided by <a href="https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/BucketMetadata.java" target="_blank" rel="noreferrer noopener">BucketMetadata</a>, groups key–values by the ID, sorts all values, and then writes them into files corresponding to bucket IDs using the FileOperations instance.</p>



<p>In addition to the bucket files, a JSON file is also written to the output directory representing the information from BucketMetadata that’s necessary to read the source: the number of buckets, the hashing scheme, and the instructions to extract the key from each record (for example, for Avro records we can encode this instruction with the name of the GenericRecord field containing the key).</p>



<figure><img loading="lazy" width="700" height="255" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-250x91.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-768x280.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-120x44.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5.png 1180w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h3>SortedBucketSource</h3>



<p>This transform reads from one or more sources written in SMB format with the same key and hashing scheme. It opens file handles for corresponding buckets from each source (using FileOperations&lt;T&gt; for that input type) and merges them while maintaining sorted order. Results are emitted as <a rel="noreferrer noopener" href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java" target="_blank">CoGbkResult</a> objects per key group, the same class Beam uses for regular Cogroup operations, so the user can extract the results per source with the correct parameterized type.</p>



<figure><img loading="lazy" width="700" height="365" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-250x130.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-768x400.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-120x63.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7.png 1067w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h3>FileOperations</h3>



<p>FileOperations abstracts away the reading and writing of individual bucket files. Since we need fine-grained control over the exact elements and their order in every file, we cannot leverage the existing Beam file I/Os, which operate on a PCollection level and abstract away the locality and order of elements. Instead, SMB file operations happen at a lower level of BoundedSource for input and ParDo for output. Currently Avro, BigQuery TableRow JSON, and TensorFlow TFRecord/Example records are supported. We plan to add other formats like Parquet as well.</p>



<h3>BucketMetadata</h3>



<p>This class abstracts the keying and bucketing of elements, and includes information such as key field, class, number of buckets, shards, and hash function. The metadata is serialized as a JSON file alongside data files when writing, and used to check compatibility when reading SMB sources.</p>



<h3>Optimizations and Variants</h3>



<p>Over the last year and a half we’ve been adopting SMB at Spotify for various use cases, and accumulated many improvements to handle the scale and complexity of our data pipelines.</p>



<ul><li><strong>Date partitioning:</strong> At Spotify, event data is written to Google Cloud Services (GCS) in hourly or daily partitions. A common data engineering use case is to read many partitions in a single pipeline — for example, to compute stream count over the last seven days. For a non-SMB read, this can be easily done in a single PTransform using wildcard file patterns to match files across multiple directories. However, unlike most File I/Os in Beam, the SMB Read API requires the input to be specified as a directory, rather than a file pattern (this is because we need to check the directory’s metadata.json file as well as the actual record files). Additionally, it must match up bucket files across partitions as well as across different sources, while ensuring that the CoGbkResult output correctly groups data from all partitions of a source into the same TupleTag key. We evolved the SMB Read API to accept one or more directories <em>per source</em>. </li></ul>



<ul><li><strong>Sharding:</strong> Although the Murmur class of hash functions we use during bucket assignment usually ensures an even distribution of records across buckets, in some instances one or more buckets may be disproportionately large if the key space is skewed, creating possible OOM errors when grouping and sorting records. In this case, we allow users to specify a number of <em>shards</em> to further split each bucket file. During the bucket assignment step, a value between [0, numShards) is generated randomly <a href="https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence"><em>per bundle</em></a>. Since this value is computed completely orthogonally to the bucket ID, it can break up large key groups across files. Since each shard is still written in sorted order, they can simply be merged together at read time.</li></ul>



<ul><li><strong>Parallelism:</strong> Since the number of buckets in an SMB sink is always a power of 2, we can come up with a joining scheme across sources with different numbers of buckets based off of a desired level of parallelism specified by the user. For example, if the user wants to join Source 1 with 4 buckets and Source 2 with 2 buckets, they can specify either:<ul><li><strong>Minimum parallelism,</strong> or “Merge Greatest Buckets” strategy: 2 parallel readers will be created. Each reader will read 2 buckets from source A and 1 from source B, merging them together. Because bucket IDs are assigned by taking the integer hash value of the key modulo the desired number of buckets, mathematically we know that the key spaces of the merged buckets overlap.</li><li><strong>Maximum parallelism,</strong> or “Least Bucket Replication” strategy: 4 parallel readers will be created. Each reader will read 1 bucket from Source A and 1 from Source B. After merging each key group, the reader will have to rehash the key modulo the greatest number of buckets, to avoid emitting duplicate values. Therefore, even though this strategy achieves a higher level of parallelism, there is some overhead of computing duplicate values and rehashing to eliminate them.</li><li><strong>Auto parallelism:</strong> Creates a number of readers between minimal and maximal amounts, based on a desired split size value provided by the Runner at runtime.</li></ul></li></ul>



<figure><img loading="lazy" width="700" height="459" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-250x164.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-768x504.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-120x79.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3.png 1115w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<ul><li><strong>SortedBucketTransform:</strong> A common usage pattern is for pipelines to enrich an existing dataset by joining it with one or more other sources, then writing it to an output location. We decided to specifically support this in SMB with a unique PTransform that reads, transforms, and writes output using the same keying and bucketing scheme. By doing the read/transform/write logic per bucket on the same worker, we can avoid having to reshuffle the data and recompute buckets — since the key is the same, we know that the transformed elements from bucket M of the inputs also correspond to bucket M in the output, in the same sorted order as they were read from.</li></ul>



<figure><img loading="lazy" width="700" height="320" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-250x114.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-768x351.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-120x55.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4.png 902w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<ul><li><strong>External Sort:</strong> We made a number of improvements to Beam’s <a href="https://github.com/apache/beam/tree/master/sdks/java/extensions/sorter">external sorter extension</a>, including replacing the Hadoop sequence file with the native file I/O, removing the 2GB memory limit, and reducing disk usage and coder overhead.</li></ul>



<h2>Adoption — Core Data Producers</h2>



<p>Since SMB requires data to be bucketed and sorted in a specific fashion, the adoption naturally starts from the producer of that data. A majority of the Spotify data processing relies on a few core data sets that act as single sources of truth for various business domains like streaming activities, user metadata and streaming context. We worked with the maintainer of these data sets to convert a year’s worth of data to SMB format.</p>



<p>Implementation was straightforward since SortedBucketSink is mostly a drop-in replacement for the vanilla Avro sink with some extra settings. We were using Avro sink with the sharding option to control the number and size of output files. After migrating to SMB, we did not notice any major bump in terms of vCPU, vRAM, or wall time since sharding requires a full shuffle similar to the additional cost of SMB sinks. A few other settings we have since had to tweak:</p>



<ul><li>Agree on user_id as a hexadecimal string as bucket and sort key, since we need the same key type and semantic across all SMB datasets.</li><li>Set compression to DEFLATE with level 6 to be consistent with the default Avro sink in Scio. As a nice side effect of data being bucketed and sorted by key, we observed ~50% reduction in storage from better compression due to collocation of similar records.</li><li>Make sure output files are backwards compatible. SMB output files have “bucket-X-shard-Y” in their names but otherwise contain the same records with the same schema. So existing pipelines can consume them without any code change; they just do not leverage the speedup in certain join cases.</li></ul>



<h2>Adoption — Wrapped 2020</h2>



<p>Once the core datasets were available in SMB format, we started Wrapped 2020, building off the work left from the Wrapped 2019 campaign. The architecture was meant to be reusable and was a great place to start. However, the source of data was a large, expensive Bigtable cluster that had to be scaled further up to handle the load of Wrapped jobs. We wanted to save cost and time by moving from Bigtable to SMB sources. This year we also needed to handle new complex requirements for filtering and aggregating streams. This required us to join a large dataset containing stream contextual information to the user’s listening history. This would have been nearly impossible or at the very least extremely expensive because of the considerable size of each of these joins. Instead we tried using SMB to eliminate that join completely and avoid using Bigtable as our listening history source.</p>



<p>To compute Wrapped 2020, we had to read from three main data sources for streaming activity, user metadata and streaming context. These three sources had all the data we needed to generate each person’s Wrapped while filtering based on listening context. Previously, the Bigtable had 5 years’ worth of listening history already keyed by user_id. Now, we are able to read data already keyed by user_id from these three sources through SMB. We then aggregated a year’s worth of data per key to calculate each user’s Wrapped.</p>



<p>Because 1 of the 3 main sources are partitioned hourly while the other 2 are partitioned daily, it would be problematic to read a year’s worth of data in one job due to the excessive number of concurrent reads from the hourly partitioned source. Instead, we first ran smaller jobs that would aggregate a week’s or day’s worth of play counts, msPlayed, and other information on each user. From there, we then aggregated all these smaller partitions to a singular partition of data that would hold a year’s worth of data. </p>



<figure><img loading="lazy" width="700" height="218" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-700x218.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-700x218.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-250x78.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-768x240.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-1536x479.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-120x37.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2.png 1692w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>SMB made this relatively easy. We used sortMergeTransform to combine our three sources of data, read each one keyed by user_id, and write our Wrapped output (play counts, ms played, play context, etc.) in SMB format. </p>



<figure><img loading="lazy" width="700" height="353" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-700x353.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-700x353.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-768x387.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-120x60.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM.png 1246w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Finally, we ran our aggregate job that uses sortMergeGroupByKey to read all Wrapped weekly partitions of SMB, combine a year’s worth of data, and write the output so later jobs can calculate the rest of Wrapped. A key point of flexibility here is that the aggregate job can take any mix of weekly and daily partitions, which is incredibly helpful logistically when running these jobs. The end result in practice looks something like this:</p>



<figure><img loading="lazy" width="700" height="218" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-700x218.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-700x218.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-250x78.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-768x240.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-1536x479.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-120x37.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6.png 1692w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>This ended up being a huge cost savings for us in this year’s Wrapped project. By leveraging SMB, we managed to join roughly a total of 1PB data without using conventional shuffle or Bigtable. We estimate around a 50% decrease in Dataflow costs this year compared to previous years’ Bigtable-based approach. Additionally, we avoided scaling the Bigtable cluster up two to three times its normal capacity (up to around 1,500 nodes at peak) to support the heavy Wrapped jobs. This was a huge win in this year’s campaign as we were able to bring a wonderful experience in a more cost effective way than ever before.</p>



<h2>Conclusion</h2>



<p>By adopting SMB, we were able to perform extremely large joins that were previously either unfeasible or cost-prohibitive, or that required custom workarounds like Bigtable. We achieved significant cost savings and opened up more ways of optimizing our workflows. There’s still much work to be done. We look forward to migrating more workflows to SMB, while handling more edge cases like data skew, composite keys, and more file formats.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Neville Li, Claire McGinty, Sahith Nallapareddy, &amp; Joel Östlund</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing XCMetrics: Our All-in-One Tool for Tracking Xcode Build Metrics&#xA;</title>
      <link>https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/</link>
      <description>TL;DR We just open sourced XCMetrics — a tool for Apple’s developer software, Xcode, that lets you collect, display, and track the valuable metrics hiding inside your team’s Xcode build logs. Are your build times improving or regressing? Which version of Xcode is slowest? Which hardware setup is fas</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>January 20, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/" title="Introducing XCMetrics: Our All-in-One Tool for Tracking Xcode Build Metrics">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR</strong> We just open sourced <a href="https://xcmetrics.io" target="_blank" rel="noreferrer noopener">XCMetrics</a> — a tool for Apple’s developer software, Xcode, that lets you collect, display, and track the valuable metrics hiding inside your team’s Xcode build logs. Are your build times improving or regressing? Which version of Xcode is slowest? Which hardware setup is fastest? XCMetrics makes it easy to find out all this and more. Made for iOS engineers, by iOS engineers, the tool is written completely in Swift, so it’s easy to customize. Use it to track the metrics you want — and get insights that can help improve both developer experience and productivity.</p>



<h2>The problem: Where do you get good Xcode build data?</h2>



<p>You’ve read before on this blog about how our infrastructure teams are always finding new and innovative ways to make <a href="https://engineering.atspotify.com/2020/07/22/leveraging-mobile-infrastructure-with-data-driven-decisions/" target="_blank" rel="noreferrer noopener">data-driven decisions</a>. But what if you don’t have access to good data in the first place? This is especially challenging for iOS engineers given that most of that platform’s tools are closed source, making it especially tricky to customize them to your needs.</p>



<p>For example, when we first introduced Swift to our music app, a requirement that we set for ourselves was not to worsen the developer experience. One metric for that is build time: is adopting Swift slowing our Xcode build times down or speeding them up? And how do we accurately measure that (without using a stopwatch every time we hit run)?</p>



<h2>Our first solution: Parse the data from Xcode’s log files</h2>



<p>Whenever you run a build in Xcode, whether it’s a test build or a continuous integration build in production, xcodebuild produces a log file called xcactivitylog. Many developers don’t know that this file exists or that it’s useful for inspecting warnings, errors, and other data from past builds, like build times. So, over a year ago we developed and released an open source tool called <a href="https://github.com/spotify/XCLogParser" target="_blank" rel="noreferrer noopener">XCLogParser</a> — which parses those xcactivitylog files and makes all that build data more accessible to developers.</p>



<p>XCLogParser was created for a simple purpose: unearth the data buried in Xcode’s build logs and make it more human readable. But one piece of feedback we received from various teams after open sourcing XCLogParser is that it still requires substantial time to build the infrastructure for continuously collecting those build logs and maintaining them over time. </p>



<p>It was time for us to build a more full-featured tool — one that could integrate with a production environment composed of distributed teams, and provide better insights over time. A collector and a tracker, not just a parser. And that’s how XCMetrics was born.</p>



<h2>A complete solution: Collect, parse, store, track, repeat</h2>



<p>We’ve been developing and testing XCMetrics over the last year, building a whole suite of tools in order to create a complete solution for tracking Xcode build metrics. <strong>Since introducing this system at Spotify, the tools have been used to collect over one million builds and billions of compilation steps — producing over 10TB of data. </strong></p>



<p>With this amount of data, we’ve been able to answer complex questions for our developer teams, such as:</p>



<ul><li>Which function takes the longest to typecheck in our project every day? </li><li>Which pull requests introduce a specific warning or compilation failure?</li><li>How should we configure our engineers’ machines in order to maximize their productivity (hardware specs, installed software, etc.)?</li></ul>



<p>We’ve used these insights to improve the everyday experience and productivity of our developers, and we think other organizations will find these kinds of insights valuable, as well. So we are happy to open source XCMetrics with the world — we’re especially excited to see and learn from the insights other teams uncover.</p>



<h2>Architectural overview: Designed for scale and customization</h2>



<p>XCMetrics is an all-in-one tool that tracks Xcode build metrics for teams of all sizes. We built it with a flexible and extensible architecture in order to fit as many requirements as possible into its plugin system, allowing for customization of the information collected in every build. </p>



<p>XCMetrics is made up of the following components:</p>



<ul><li><strong>A Swift CLI tool</strong> that should be invoked in a post-scheme action after every build completes, whose task is to cache and upload build metrics.</li><li><strong>A backend service</strong> written in Swift receives the log and attaches metadata via a multipart request. The data can be parsed and saved synchronously or asynchronously.<ul><li>If the configuration specifies parsing logs asynchronously, they are enqueued for processing in a Redis instance.</li></ul></li><li><strong>A PostgreSQL database</strong> — once the log is parsed, the data for each build is inserted into the database, partitioned by day, for easy retrieval and historical analysis.</li></ul>



<figure><img loading="lazy" width="700" height="280" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-700x280.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-700x280.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-250x100.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-768x307.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-1536x615.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-120x48.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3.png 1837w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h2>Getting started: Which metrics do you want to track?</h2>



<p>We did our best to make XCMetrics as generic and customizable as possible. The only decisions you have to make are where the backend service should be deployed — and, more interestingly, what type of data you would like to collect.</p>



<h3>Standard metrics</h3>



<p>XCMetrics is distributed as an executable from our GitHub releases page. You can follow the <a href="https://xcmetrics.io/docs/getting-started.html" target="_blank" rel="noreferrer noopener">Getting Started guide</a> to learn how to get XCMetrics on your developer’s machine and execute it in a post-action scheme. Once that’s done, the default set of build metrics will be collected and uploaded to your service. You can check out the default set of collected metrics here.</p>



<h3>Custom metrics</h3>



<p>If you would like to collect even more metrics, you can wrap the XCMetrics Swift Package in your own package in order to invoke it manually. By doing so, you’ll be able to provide even more metrics to be attached to every build. Some examples are:</p>



<ul><li>Anonymized version control information to correlate build times with dirty checkout state</li><li>Thermal throttling of the machine that could affect build times</li><li>Project configuration information that could affect build metrics</li></ul>



<p>This is the minimal example of a XCMetrics plugin that collects the thermal throttling state of the machine and attaches it to each build.</p>



<figure><img loading="lazy" width="700" height="593" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-700x593.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-700x593.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-250x212.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-768x651.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-1536x1302.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-120x102.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2.png 1930w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>The main method forwards the arguments parsing to <em>XCMetrics</em>. You proceed to create a <em>XCMetricsConfiguration</em> and add <em>XCMetricsPlugin</em> to it. Each plugin takes a dictionary of the environment variables passed to the post-action scheme environment and returns a dictionary of the metrics to be collected. You would then distribute your own custom version of XCMetrics and execute it with the same arguments to upload the logs with the new metrics attached.</p>



<h3>Service deployment</h3>



<p>We provide a <a href="https://hub.docker.com/r/spotify/xcmetrics" target="_blank" rel="noreferrer noopener">Docker image</a> that has everything needed to deploy the XCMetrics backend in any infrastructure. We also support a one-click deployment to Google Cloud via Google Cloud Run. Our documentation also contains examples on how to deploy to Kubernetes, if you fancy that.</p>



<p>Needless to say, you don’t need a complex DevOps team to deploy and run XCMetrics. It’s made by iOS engineers, for iOS engineers, so simplicity is at its heart.</p>



<h2>Using XCMetrics at Spotify</h2>



<p>XCMetrics has been in use in production at Spotify for over one year, and it has allowed us to make more informed decisions in regards to our project structure and investments. We have data pipelines and dashboards that are used every day to monitor the state of our codebase and tools. </p>



<figure><img loading="lazy" width="700" height="424" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-700x424.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-700x424.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-250x151.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-768x465.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-1536x930.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-120x73.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4.png 1792w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figures above are for illustrative purposes only.</figcaption></figure>



<p>We hope XCMetrics will inspire and help other teams keep track of their build metrics and improve their developer experience.</p>



<p>You can learn more and watch a demo at <a href="https://xcmetrics.io">XCMetrics.io</a>. We are happy to receive bug fixes and improvements on <a href="https://github.com/spotify/XCMetrics/">GitHub</a>. And make sure to check out our <a href="https://github.com/spotify/XCMetrics/blob/master/CONTRIBUTING.md">contribution guide</a>, which explains more advanced concepts of the project.</p>











<p><em>Xcode is a trademark of Apple Inc., registered in the U.S. and other countries.</em></p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Patrick Balestra, Sr. Engineer</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How We Built It: Spotify Lite, One Year Later&#xA;</title>
      <link>https://engineering.atspotify.com/2020/12/03/how-we-built-it-spotify-lite-one-year-later/</link>
      <description>What if, for some users, the very best Spotify is a little less Spotify? Spotify Lite started as an experiment that had to be proven, both from a technical and a product-market fit perspective. In 2017, we found that a significant portion of registrations in some of our fastest-growing markets were</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 3, 2020</span>
                <span>
                    Published by Erik Ghonyan (Senior Engineer), Slava Savitskiy (Senior Engineer), and Tommy Tynjä (Engineering Manager)                </span>
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/12/03/how-we-built-it-spotify-lite-one-year-later/" title="How We Built It: Spotify Lite, One Year Later">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/12/Spotify-Lite_B.png"/>                    </a>
                        
        </p>

        

        
<p>What if, for some users, the very best Spotify is a little less Spotify? Spotify Lite started as an experiment that had to be proven, both from a technical and a product-market fit perspective. In 2017, we found that a significant portion of registrations in some of our fastest-growing markets were happening on Android devices much older than what we were used to seeing in North American and European markets. Because of storage constraints, many of our potential users couldn’t install Spotify, and the ones that could weren’t getting the full “Spotify experience”.</p>



<p>Our mission was clear: we needed to make Spotify accessible to users with constrained resources, i.e., unreliable networks or phones with limited storage, memory, and low-resolution screens. Now we needed a team.</p>



<h2>A flexible, Lite team</h2>



<p>The main Spotify Music Android client is divided into multiple features, all owned by separate teams. But for Spotify Lite, we formed a single, autonomous team to fully own the entire process of designing, developing, and releasing the app. This allowed us to roll out an MVP product in record time.</p>



<p>Before building the new app, the Spotify Lite team — a cross-functional mix of insights, design, product, and engineering — travelled to a number of locations where Lite would be available in order to experience the network and device constraints firsthand. It was absolutely critical to design Spotify Lite with our users in mind, and to experiment and iterate on the streaming experience for cases when devices have poor connectivity or are completely offline. Only then were we able to come up with an optimal, performant solution.</p>



<p>It should be noted that we wouldn’t have achieved success had it not been for the existing tooling that we were able to reuse — tools for enabling recommendations, playback, search, browsing, and instrumentation. We were building on all the work, experience, and knowledge that came before us, giving us the ability to focus on finding solutions for our users.</p>



<h2>Spotify Lite: Spotify’s first separate app</h2>



<p>Creating a more performant and smaller version of the Spotify app proved to be more challenging than we liked, as the codebase hadn’t been modularized. With these challenges in mind, we decided to build a new separate app from scratch, giving us the ability to quickly iterate, obtain feedback, and innovate freely.</p>



<p>Spotify Lite was initially built on an entirely different playback stack than the regular Android app. This allowed Lite to be as small as possible, with minimal memory and network data usage. Having a separate app enabled us to test new performance ideas and to gain insights, such as understanding how application size impacts the new user funnel. We no longer use the initial playback stack, and have evolved towards a tailored setup that guarantees stability and playback quality on unreliable networks.</p>



<p>Building Lite was a lot like packing a backpack for your travels. With limited space, you have to be selective in what you bring. Only the most crucial and necessary components were carried forward.</p>



<h2>A balancing act</h2>



<p>Shrinking the original Spotify app to create Spotify Lite brought up two crucial questions: What key elements of the original Spotify should remain intact to ensure listeners still get the “Spotify experience”? And what sacrifices do we need to make to ensure Spotify Lite does, in fact, remain light? </p>



<p>In answering the first question, we knew that keeping the brand look and feel was absolutely critical to giving listeners a Spotify they could recognize. So, we used the same design philosophy as the original Spotify Android app. However, given a range of constraints (smaller screens, quick/performant interactions), we had to adapt some of our design choices. For example, information density has to be reviewed with smaller screen sizes and lower resolutions in mind, as well as whether information is still readable on a broken or scratched screen (these phones have been around for a while!). We’ve recently added our heuristics for how to design for these constraints to the overall design strategy so that this is kept in mind for other apps and surfaces, as well.</p>



<figure><img loading="lazy" width="700" height="444" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-700x444.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-700x444.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-250x159.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-768x487.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-1536x975.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-2048x1300.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-120x76.png 120w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Along with streamlining the design, we also had to shrink things down under the hood. The team put a lot of work into implementing all the known techniques for binary size reduction, as well as making tradeoffs when selecting features. As the app includes a native shared library for playback, we have experimented with many compiler and linker flags to prioritize a small app size. This includes, for instance, switching to the <em>lld</em> linker, employing link time code optimizations, and disabling certain language features like RTTI. </p>



<p>On the Android side, it was the use of App Bundles for application publishing, optimizing our R8 shrinking, carefully choosing dependency libraries, and stripping unused translations. We made an effort to reduce the install size, too. We store the shared library unpacked in the APK without copying it to the install folder, and allow users to store both the app and its cache and downloads separately on the SD card.</p>



<p>After the initial larger gains, it became harder and harder to reduce the app size. It was a constant balance between keeping it small while adding additional requested features. Along with monitoring the app download and install sizes in the Google Play Store, we added checks to our continuous delivery pipelines to prevent size bloat.</p>



<h2>Lite is different</h2>



<p>Because Lite was a brand-new concept, some of our work went beyond the app itself, leading to improvements to Spotify systems that other teams could benefit from, too.</p>



<p>Before Lite, developers could safely assume there was only one Spotify app for any given platform — the Android platform and the Android app were considered one and the same. Backend services — including those providing application views and deciding which features are enabled — were built with that assumption in mind. Some of these assumptions cascaded through many different parts of our internal systems. </p>



<p>When we added Lite to the mix, developers needed to know exactly which app a user was using, not just what platform they were on. We generalized that issue beyond our own app and built ways to identify all the apps in the Spotify ecosystem. That work paid off again each time anyone introduced a new Spotify app to the Android platform, including our sister apps <a rel="noreferrer noopener" href="https://www.spotify.com/us/kids/?utm_source=us-en_brand_contextual_text&amp;utm_medium=paidsearch&amp;utm_campaign=alwayson_ucanz_us_premiumbusiness_kids_brand+contextual-desktop+text+exact+us-en+google&amp;gclid=CjwKCAiA8Jf-BRB-EiwAWDtEGnamKsxw1Yx_w3KgzFDyJ1g4NKVvIUkc9jRA8fBFdlHCkR8pD4iHmBoCMLAQAvD_BwE&amp;gclsrc=aw.ds" target="_blank">Spotify Kids</a>, <a rel="noreferrer noopener" href="https://www.spotify.com/us/stations/" target="_blank">Spotify Stations</a>, and <a href="https://spotify-everywhere.com/collections/car-audio/products/polestar" target="_blank" rel="noreferrer noopener">Android Automotive</a>.</p>



<p>We also had to redesign parts of Spotify’s playback library with Lite constraints in mind — taking into account smaller download and installation sizes, memory usage, and the reduced feature set. Similar considerations have been applied to Spotify’s music and image transcoding services.</p>



<h2>Making Lite a big deal</h2>



<p>Our ambition is to be the best-in-class Lite app. We are constantly modifying and updating the app to adapt to our users and their ever-evolving needs. As we’ve seen positive adoption of Spotify Lite since launch, we’ve invested in performance improvements, quality, and resilience. We recently rolled out an overhaul of our client architecture to cater to our growing user base and to reduce playback latencies.</p>



<p>The birth of Spotify Lite has given us flexible solutions that our other apps have benefited from. One such example is our backend service that scales down images to use less network traffic. Another is the support for App Bundles, which has allowed us to reduce the app size significantly so that users only download the assets needed for their particular device. Creating a separate app was a first for our build system — one that laid the groundwork for building native dependencies, sharing code components, and setting up crash and ANR reporting for tracking app quality.</p>



<p>We are continuing our work to lower the barrier for people to access Spotify. We have our backlog full of ideas and performance improvements we want to keep investing in, not only for Lite but also for our other apps to benefit from.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            It’s All Just Wiggly Air: Building Infrastructure to Support Audio Research&#xA;</title>
      <link>https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/</link>
      <description>TL;DR We just open sourced Klio — our framework for building smarter data pipelines for audio and other media processing. Based on Python and Apache Beam, Klio helps our teams process Spotify’s massive catalog of music and podcasts, faster and more efficiently. We think Klio’s ease of use — and its</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 4, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/" title="It’s All Just Wiggly Air: Building Infrastructure to Support Audio Research">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio-Blog2.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/10/Klio-Blog2.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong><a href="https://venturebeat.com/2020/10/13/spotify-open-sources-klio-a-framework-for-ai-audio-research/" target="_blank" rel="noreferrer noopener">We just open sourced Klio</a> — our framework for building smarter data pipelines for audio and other media processing. Based on Python and Apache Beam, <a href="https://klio.io" target="_blank" rel="noreferrer noopener">Klio</a> helps our teams process Spotify’s massive catalog of music and podcasts, faster and more efficiently. We think Klio’s <a href="https://docs.klio.io" target="_blank" rel="noreferrer noopener">ease of use</a> — and its ability to let anyone leverage modern cloud infrastructure and tooling — has the potential to unlock new possibilities in media and ML research everywhere, from big tech companies to universities and libraries. </p>



<p>But now we’re getting ahead of ourselves. What exactly is Klio and what does it do? Let’s start with the problem of audio itself.</p>



<h2>Audio is hard </h2>



<p>Really, sound is just wiggly air. At a basic level, every violin concerto, love song, dog bark, and knock-knock joke is the result of air compressing and vibrating, which we sense as it moves bones and hair in our ears. Sound is an invisible force that reaches us in ways that we can’t see, but can feel. And that’s what also makes audio so difficult for machines to parse: Humans can tell the difference between a swooning vocal, a danceable beat, and a buzzing bee. Can we teach machines to hear those differences, too? </p>



<p>Machine listening, the field of research focused on getting computers to understand audio, combines expertise and methods from signal processing, music information retrieval, and machine learning — so that all those vibrations in the air result in data that makes a bit more sense for an engineer to work with. When encoded, compressed, and stored on a computer, you’re left with ones and zeroes packed into relatively large binary files. At a glance, a guitar solo can look just like a yodel. So, how do we begin to make sense of it all? And at scale?</p>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_01-Which-is-which-01-1.gif" alt="" width="580" height="525"/><figcaption><em>One is a </em><a href="https://open.spotify.com/show/71mvGXupfKcmO6jlmOJQTP?si=6vu_5jg7TUaxQmySGfzliA" target="_blank" rel="noreferrer noopener"><em>popular podcast</em></a><em>, one is </em><a href="https://open.spotify.com/track/3Zb3SXZdtyNA0Cdq0DWLeC" target="_blank" rel="noreferrer noopener"><em>acoustic guitar</em></a><em>. It’s all just wiggly air. Software can help process the audio — identify voices, find beats per minute, analyze frequencies. But all at once? And 60+ million tracks at a time?</em></figcaption></figure></div>



<h2>One problem multiplied 60 million times</h2>



<p>Processing massive amounts of large binary files: It was a problem that was only getting bigger at Spotify. We’re adding about 40,000 songs a day and are processing our music catalog — about 60 million songs — on a regular basis, with multiple teams around the world doing work at the same time. Besides the problem of engineering that kind of scale and parallelization, we also wanted a way to tie the processing jobs more closely with the work our audio and ML research teams were doing.</p>



<p>We were already building sophisticated data pipelines that supported AI and ML jobs using <a href="https://spotify.github.io/scio" target="_blank" rel="noreferrer noopener">Scio</a>, a precursor to Klio. Scio proved to be a flexible, scalable framework that any team could use to <a href="https://engineering.atspotify.com/tag/scio/" target="_blank" rel="noreferrer noopener">build smarter data pipelines at scale</a>. By tying together large database queries, map-filter-reduce operations, natural language processing, and ML models, teams could create better, more personalized playlists, like Discover Weekly, Release Radar, and dozens of others. </p>



<p>So, Scio created a platform for processing massive amounts of data about the audio. But what about processing the audio itself? </p>



<h2>A uniquely Spotify problem, a uniquely Spotify solution</h2>



<p>While processing metadata for the libraries of 299+ million users is impressive, it’s not the same as processing the content itself — those tens of millions of binary audio files that Spotify hosts and serves all over the world. On top of that, Java-based languages weren’t interfacing well with our Python-based research tools for audio and ML.</p>



<p>We knew that if we could build data pipelines that supported large-scale audio processing, there were untold features and personalizations waiting to be unlocked. We just needed a framework that supported it — and that worked as well with our research tools as our engineering tools. </p>



<p>In 2019, an ad hoc team of data engineers, ML researchers, and audio experts outlined the requirements for creating a framework designed especially for processing media. Scio was a model of success, but still just a starting point. This new framework would need to support:</p>



<ul><li><strong>Large-file input/output: </strong>We wanted to transform audio, videos, images — all kinds of heavy-duty binary media files — in dozens of ways, with both streaming and batch processing.</li></ul>







<ul><li><strong>Scalability, reproducibility, efficiency: </strong>When you’re working with a dataset as large as the world’s music, as well as a burgeoning ecosystem of podcasts, you don’t want to have to redo your work over and over again.</li></ul>







<ul><li><strong>Closer collaboration between researchers and engineers:</strong> This translated into support for both Python (the lingua franca of both audio processing and ML) as well as non-Python dependencies (e.g., libsndfile, ffmpeg, etc.).</li></ul>



<p>In short, we needed a framework that could production-ize audio processing. This wasn’t just about creating data pipelines for media. It was about doing it at Spotify scale and with support for the latest audio and ML research. Let’s dig into that last requirement first.</p>



<h2>Researchers, engineers, and Python: The importance of speaking a common language</h2>



<p>Around this time, we noticed that both our researchers and engineers were beginning to get a little tired of the roadblocks preventing their audio work from getting adopted. Audio researchers were making promising breakthroughs, but the cost of getting new approaches integrated into shipping products was becoming increasingly high. </p>



<p>As much as their counterparts in data and ML engineering wanted to help, those engineers were spending much of their time looking after several distinct, bespoke systems for production audio processing, all built and customized for individual teams. In other words, we had smart people all over the company working on audio, but our <a href="https://research.atspotify.com" target="_blank" rel="noreferrer noopener">world-class researchers</a> and engineers couldn’t work together, until most of the research was rewritten by the engineers. And even then, all that work and effort was siloed.</p>



<p>The solution was simple: Python. It’s the native language of research and well-suited for the engineering problems at hand. Most importantly, allowing everyone to speak without a translation layer puts everyone in a position to focus on what they excel at. Audio and ML researchers get to focus on experimentation and building cutting-edge research tools. Engineers get to focus on building clean, reliable code.</p>



<h2>What is Klio?</h2>



<div><figure><img loading="lazy" width="2409" height="1868" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_02-What-is-02-B.gif" alt=""/></figure></div>



<p>Klio is a framework for building smarter data pipelines for audio and other binary files, enabling you to production-ize media processing at scale.</p>



<ul><li><strong>Streamlined Apache Beam</strong> for a more ergonomic, Python-native experience for researchers and engineers</li></ul>



<ul><li><strong>Open graph of job dependencies</strong> with support for top-down and bottom-up executions</li></ul>



<ul><li><strong>Integration with cloud processing engines</strong> for managed resources and autoscaling production pipelines</li></ul>



<ul><li><strong>Containerization of custom dependencies</strong> for simplified development and easily  reproducible deployment</li></ul>



<ul><li><strong>Batch and streaming pipelines</strong> for continuous processing</li></ul>



<h2>Apache Beam under the hood, Klio in the driver’s seat</h2>



<p>It’s no surprise then that Klio is built on top of <a href="https://beam.apache.org/" target="_blank" rel="noreferrer noopener">Apache Beam</a> for Python, while also aiming to be a more Pythonic experience of Beam. Additionally, Klio offers several advantages over traditional Python Beam for media processing — providing a substantial reduction in boilerplate code (an average of 60%), a focus on heavy file I/O, and standards for connecting multiple streaming jobs together in a jobs dependency graph (with top-down and bottom-up execution). This allows teams to immediately focus on writing new pipelines, with the knowledge that they can easily be extended and connected later. </p>



<p>This ease of use and streamlining of Apache Beam means we can get our state-of-the-art audio research into people’s hands and ears, faster. And while Klio offers this more opinionated way to use Apache Beam for common media processing use cases by default, it also allows the use of core Python Beam at any time if Klio’s opinions don’t fit your use case.</p>



<h2>Efficiency, <s>efficiency,</s> <s>efficiency</s> (DRY: Don’t Repeat Yourself)</h2>



<p>When we were developing Klio, we decided to test it by downsampling every track in Spotify’s 60-million song catalog — amounting to well over 100 million audio files in all (including multiple releases of the same song). Downsampling is often the first step of audio analysis, so it’s a great benchmark of what real-world performance might look like. Previously, the fastest we had accomplished this at Spotify was about three or four weeks. With Klio, we did it in six days, and reduced costs by four times. When you think about the number of songs in our catalog, and our quickly growing podcast library, Klio can have a tremendous impact on our teams and our business.</p>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-700x409.png" alt="" width="580" height="338" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-700x409.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-250x146.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-768x449.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-1536x898.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-2048x1197.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-120x70.png 120w" sizes="(max-width: 580px) 100vw, 580px"/><figcaption><em>With Klio’s streamlined framework, pipelines are more efficient and reliable. We can do in days what took weeks. And since jobs don’t have to be repeated (missing dependencies can be recursively created), you don’t have to run files through the whole pipeline again just to apply one more transformation at the end. </em></figcaption></figure></div>



<p>You’ll find these kinds of optimizations throughout Klio’s implementation. Klio pipelines improve processing time and costs by avoiding duplicate work on already processed audio. And the framework is opinionated — encouraging engineers and researchers to write a pipeline focused on one thing, like finding the timestamps of all the beats to a song or measuring a song’s loudness. By creating reusable building blocks, Klio allows for researchers to build more easily on top of previous research and create graphs of pipelines, leading to features like infinite playlists optimized for your current mood, internal tools that help automate the review of new content, and powerful data that personalizes the Spotify experience for each user.</p>



<h2>Scale, reproducibility, and clouds. No infra team required.</h2>



<p>Klio can be run locally, but it really shines in the cloud — and is ready-made for it. In order to achieve the large-scale processing and reproducibility that we require at Spotify, Klio leverages the best parts of modern cloud infrastructures (like managed resources to autoscale production pipelines) and tooling (like containerization for easier deployments).</p>



<p>Klio was designed to be cloud agnostic, and the underlying Apache Beam project is designed to run workloads across any data workflow engine. Right now, it’s configured to work with Google Cloud Platform, but we welcome <a href="https://docs.klio.io/en/latest/contributors.html" target="_blank" rel="noreferrer noopener">contributions</a> to help get Klio running on AWS, Azure, or another infrastructure. </p>



<p>One thing to note: Current limitations to Beam Python prevent all of its features from being used on every engine, but we expect increased compatibility with Apache Flink and Apache Spark as Apache Beam extends its underlying compatibility with these engines. Preliminary work has also been done testing Klio on Amazon AWS and S3 using Klio’s Direct Runner.</p>



<p>We think this cloud integration (infrastructure as a service) can unlock production bottlenecks, as well as encourage experimentation. Engineering teams can rely on Klio to standardize media processing — using data processing and monitoring tools they’re already familiar with — rather than creating architectures from the ground up. Klio’s ability to autoscale production pipelines to handle variable workloads lets engineers focus on the next thing, rather than constantly tuning workloads.</p>



<h2>From Sing Along to dolphin songs: Open and the great unknown</h2>



<p>Klio began as a proof of concept a little less than two years ago. It was invented out of necessity — to overcome challenges we were facing internally. But even from the very beginning, it was built with the intention of being free and open source software. </p>



<p>As we’ve seen with <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a>, our open platform for building developer portals, <a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank" rel="noreferrer noopener">Spotify is committed to open source and developer experience</a>. We want to make the lives of engineers easier, so they can focus on building amazing things. So we’re excited to see not only how Klio can help others and advance audio/media research, but also what we can learn from others’ contributions and how Klio can evolve as a result. </p>



<p>Before and after Klio, Spotify has been doing this kind of large-scale audio analysis for nearly a decade, extracting and transforming tracks in our catalog on a weekly, daily, and streaming basis. Audio analysis algorithms power our <a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" target="_blank" rel="noreferrer noopener">Audio Features API</a> for fingerprinting songs by their unique attributes (illustrated in this interactive <a href="https://www.nytimes.com/interactive/2018/08/09/opinion/do-songs-of-the-summer-sound-the-same.html" target="_blank" rel="noreferrer noopener">New York Times</a> article), in-house tools, like our automated content review screener; and market-specific features, like our Sing Along feature in Japan — which <a href="https://research.atspotify.com/making-sense-of-music-by-extracting-and-analyzing-individual-instruments-in-a-song/" target="_blank" rel="noreferrer noopener">separates the vocals from the instruments</a> as songs are uploaded to the catalog to create interactive versions that people can sing along with.</p>



<p>But as we saw when we open sourced Backstage, the open source community will come up with use cases we never dreamed of. And since Klio enables anyone to do this kind of heavy-duty media processing at scale (not just big tech companies), we’re particularly curious to see what academics and research institutions will build with it. (<a href="https://twitter.com/tomncooper/status/1316071741131759617" target="_blank" rel="noreferrer noopener">Dolphin speech, anyone</a>?)</p>



<p>So, thank you to the Klio team and to everyone who’s ever used Klio or contributed to its development over the years (including its sibling framework, Scio). And thank you to all those reading this right now and who will contribute to its development in the future. It’s a product that only Spotify could have built. But we’re even more proud now that it’s out there for the world to share. Now let’s <a href="https://docs.klio.io" target="_blank" rel="noreferrer noopener">get started</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by David Riordan and Lynn Root</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio-Blog2.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify’s New Experimentation Platform (Part 2)&#xA;</title>
      <link>https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/</link>
      <description>So you’ve read Part I of our two-part series about the new Experimentation Platform we’ve built at Spotify, and now know why we decided to invest in a new platform. In Part II, you’ll get a more detailed look at how we assigned users to experiments, how we analyze results and ensure test integrity.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 2, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/" title="Spotify’s New Experimentation Platform (Part 2)">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/11/Experimentation-Platform_Part-II_A.png"/>                    </a>
                        
        </p>

        

        
<p>So you’ve read <a href="https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/" target="_blank" rel="noreferrer noopener">Part I</a> of our two-part series about the new Experimentation Platform we’ve built at Spotify, and now know why we decided to invest in a new platform. In Part II, you’ll get a more detailed look at how we assigned users to experiments, how we analyze results and ensure test integrity. </p>



<h2>Coordination, holdbacks, and exclusivity</h2>



<p>A lot of the experiments we run change some small aspect of the user experience in one of our prime surfaces and it’s important for teams to be aware of what other experiments are running at the same time, as well as what other experiments are running in their field of interest.</p>



<p>To accommodate for this, we allow experiments to be put into a “domain”. Domains roughly map different surfaces or systems in our service. Each domain has a timeline that shows what experiments have been running and what’s upcoming.</p>



<div><figure><img loading="lazy" width="700" height="486" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-700x486.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-700x486.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-250x174.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-768x533.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-1536x1067.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-120x83.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>For illustrative purposes only.</figcaption></figure></div>



<p>When a lot of teams experiment in the same proximity, there’s risk of interaction effects. For this reason many experiments need to run in an exclusive manner, where a user can only be in one of a set of experiments that can potentially impact each other. Currently only experiments in a single domain can be exclusive to each other. We’re planning to decouple exclusivity from the domain concept, to allow for experiments across domains to also be exclusive to each other. </p>



<p>We implement holdbacks (the practice of exempting a set of users from experiments and new features, in order to see long-term effects and combined evaluation) in domains. Each domain can have a set of holdbacks. Users in these holdbacks are exempt from the general experimentation that happens in the domain. </p>



<p>At Spotify we have established a pattern where at the start of a quarter, we create a new holdback. Experiments that run throughout the quarter will never be assigned to any of those users subject to the holdback. When the quarter ends, a single test is run on these users where the combined experience of all (successful) experiments is given to the treatment group. This way we can get a read for the compound effect of everything the team decided to ship during the quarter. Once this test is done, the holdback is released and these users will go into new experiments. </p>



<h2>The Salt Machine</h2>



<p>At Spotify, autonomous are teams free to move at schedules that fit them best. This means that they need to be able to start and stop experiments at any time. With requirements of exclusivity and holdbacks, assigning users to experiments gets quite complex if we do not want to compromise on randomization (and we do not want to).</p>



<p>We have developed something we call the “salt machine” that automatically reshuffles users without the need to stop all experiments. This is done by hashing users into buckets using a tree of “salts” (it’s worth noting that if two experiments are disjoint because of targeting, we do not have to use the same salt tree for them).</p>



<p>For this article, imagine that we split users into 8 buckets:</p>



<div><figure><img loading="lazy" width="700" height="126" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-700x126.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-700x126.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-250x45.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-768x138.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-1536x276.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-120x22.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>A user ends up in bucket 1 if HASH(user id, SALT) % 8 = 1 and so forth. We allocate buckets to experiments. In the image below, experiment E1 has been allocated buckets 0 and 1. Note that we also have a per-experiment salt to spread users from the allocated buckets over treatments, but for simplicity we omit that from the images in this article. </p>



<figure><img loading="lazy" width="700" height="146" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-700x146.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-700x146.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-250x52.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-768x160.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-1536x320.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-120x25.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Since we’re experimenting a lot, most buckets are always allocated to an experiment. So what happens when two experiments (E1and E1) end, releasing some space that can be allocated to a new experiment?</p>



<figure><img loading="lazy" width="700" height="289" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-700x289.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-700x289.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-250x103.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-768x317.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-1536x635.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-120x50.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Now 50% of the buckets are free and we want to start E4 that needs 25% of the population.  How can we allocate buckets safely without jeopardizing randomization? If we were to pick only bucket 0 and 1 we would have a 100% overlap with experiment E1 that just ended, which might lead to biased results due to carryover effects. Not good.</p>



<figure><img loading="lazy" width="700" height="122" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-700x122.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-700x122.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-250x43.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-768x133.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-1536x267.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-120x21.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>What if we shuffled the free users into new buckets using a new salt?</p>



<figure><img loading="lazy" width="700" height="279" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-700x279.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-700x279.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-250x100.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-768x306.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-1536x612.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-120x48.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Now we have eight free buckets we can use for experiments. But because of the dilution of bucket size (those new eight buckets only get 50% of the traffic), we need to allocate four of them to E4 to get 25% of the population. We call the amount of required overallocation of buckets the “compensation factor” — and in this case it’s 1/.50 = 2. The remaining four buckets can be allocated to some other experiment.</p>



<figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-700x302.png" alt="" width="579" height="250" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-700x302.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-250x108.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-768x332.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-1536x663.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-120x52.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8.png 1600w" sizes="(max-width: 579px) 100vw, 579px"/></figure>



<p>When experiment E3 ends we can completely get rid of salt 1 — but because we diluted the buckets the released space cannot be used until E4 finishes. In effect, we’re wasting 50% of the users. </p>



<p>The compensation factor changes all the time as experiments start and end. Over time we have learned that it’s good practice to not start new experiments if the compensation factor is higher than 5 (the higher the compensation factor, the more space is being wasted).</p>



<p>We’re currently working on the second iteration of our allocation scheme where we believe we waste less space but still maintain the benefits of randomization.</p>



<h2>Analysis</h2>



<p>To conduct a well-designed experiment we need to decide up front what we want to measure and test. The Experiment Planner asks that all necessary information be specified when an experiment is created.  A metric can have one of two roles in an experiment:</p>



<ul><li>Success metrics to find evidence for the hypothesis.</li><li>Guardrail metrics to find evidence that the experiment is not introducing any harmful side effects. </li></ul>



<p>For each success metric it is possible to choose either a one- or two-sided statistical test. </p>



<figure><img loading="lazy" width="700" height="417" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-700x417.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-700x417.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-250x149.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-768x457.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-1536x914.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-120x71.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>For success metrics, we perform superiority tests and require a relative minimum detectable effect (MDE) to be specified. This is used in power calculations in the result analysis and also in the sample size calculator. </p>



<p>For guardrails, we perform a non-inferiority test where a non-inferiority margin has to be specified so we know when a change is considered non-inferior or not. </p>



<figure><img loading="lazy" width="700" height="243" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-700x243.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-700x243.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-250x87.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-768x266.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-1536x533.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-120x42.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>If the experimenter wants to see results as the experiment is running, they need to choose sequential testing. If they decide to do a fixed horizon test, the results will only be available once the experiment is stopped. Regardless, to minimize weekday biases we recommend that tests are always run for the planned period and are only stopped early if harmful side effects are detected. We also have an optional (but highly recommended) gradual ramp-up assignment of the experiment over a time period to further minimize possible weekday effects. </p>



<p>With a potentially large number of metrics, targeting, different statistical tests, and many treatment groups, it’s not always easy to calculate an accurate required sample size. For this reason we’ve built a sample size calculator and put it into the platform (it’s optional to use for fixed horizon tests, but required for sequential testing).  </p>



<figure><img loading="lazy" width="700" height="266" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-700x266.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-700x266.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-250x95.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-768x291.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-1536x583.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-120x46.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>As can be seen above (numbers redacted), the sample size calculator shows how many users are needed to power the metric for the specified target population. The calculator automatically queries historical data for the specified target population to proxy the control group average and variance needed for the calculations.</p>



<h2>Validity checks</h2>



<p>Many things can go wrong when we run an experiment, and even subtle issues can have a big impact on the result. For this reason we’re continuously monitoring all running experiments for potential problems. If a problem is detected, we notify the owning team so that they can decide what to do.</p>



<p>We have the following checks in place:</p>



<ul><li><strong>Sample ratio mismatch:</strong> We make sure that the targeted proportion between the treatment groups align with exposure. If we see a statistically significant difference, we sound the alarm.</li><li><strong>Pre-exposure activity: </strong>We see if there’s any difference in activity between the groups prior to the experiment starting. </li><li><strong>Increases in crashes:</strong> We ensure that we do not see an increase in client crashes.</li><li><strong>Property collisions:</strong> If two experiments use the same Remote Configuration properties (and are not exclusive to each other), we will warn that the experiments might not get the exposure that was expected. </li></ul>



<p>For checks that require a statistical test, we deploy sequential testing and correct for multiple comparisons. </p>



<h2>Rollouts</h2>



<p>A use case supported by the Experimentation Platform, in addition to experimentation, is gradual rollouts. Once we learn that our change improves the user experience, we want to ship it, and with gradual rollouts we can do that while protecting against unexpected regressions. </p>



<p>There are two ways of doing rollouts: with or without statistical testing. If we select the latter, we will be able to select a set of guardrail metrics and deploy sequential testing so we can continuously monitor the progress. Every day we also provide one of three recommendations to the owning team:</p>



<ul><li>We cannot detect any harmful effects, so the recommendation is to continue the rollout.</li><li>We have statistical evidence of harmful effects, so we recommend aborting the rollout.</li><li>We do not know yet, and we recommend continuing with caution or wait until we have more data.</li></ul>



<p>The ability to get metrics for rollouts is fairly new so we’re still iterating on it, but we plan to make it the default option going forward.</p>



<h2>Summary</h2>



<p>We have spent the last two years rebuilding our experimentation capabilities at Spotify. The new platform is a step change in ease of use and capabilities, but we still feel it’s early for experimentation at Spotify.</p>



<p>We are constantly evolving our Experimentation Platform and practices. If you would like to know more, or if you’re interested in joining the team and contribute to our journey, do not hesitate to reach out.</p>



<p>Johan Rydberg, <a href="mailto:jrydberg@spotify.com" target="_blank" rel="noreferrer noopener">jrydberg@spotify.com</a> / @datamishap<br/>Experimentation Lead</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Johan Rydberg</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify’s New Experimentation Platform (Part 1)&#xA;</title>
      <link>https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/</link>
      <description>At Spotify we try to be as scientific as possible about how we build our products. Teams generate hypotheses that we test by running experiments — normally in the form of an A/B test — to learn what works and what doesn’t. The learnings give us insights and fuel new product ideas. Want to know wh</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 29, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/" title="Spotify’s New Experimentation Platform (Part 1)">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png 4209w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-120x60.png 120w" sizes="(max-width: 4209px) 100vw, 4209px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/10/Experimentation-Platform_01.png"/>                    </a>
                        
        </p>

        

        
<p>At Spotify we try to be as scientific as possible about how we build our products. Teams generate hypotheses that we test by running experiments — normally in the form of an A/B test — to learn what works and what doesn’t. The learnings give us insights and fuel new product ideas.</p>



<p>Want to know why Spotify decided to build a new Experimentation Platform and how it works? In this two-part series, we’ll share what led us to throw out our old A/B testing platform (called ABBA) and details around the new architecture we’ve chosen to substitute it.</p>



<h2>Early days</h2>



<p>Today almost all product decisions are made with some input from one or more A/B tests. But it hasn’t always been like that. Back when Spotify was a small startup in Sweden, a team, simply called Analytics, played around with various kinds of tests. </p>



<p>Over time, interest in A/B testing grew, and in 2013 we decided to spin up a team to take on building a more robust system. Thus was born <a href="https://open.spotify.com/track/2PzCOP5Aj9SABiBgNEZ52G" target="_blank" rel="noreferrer noopener">ABBA</a>, an A/B testing system that allowed us to (more) easily run experiments. Now we had a place to see what A/B tests were actually running, and a pipeline that computed results. The introduction of the system was a step change in productivity, and over time it was integrated into pretty much every aspect of Spotify — in our desktop clients and mobile clients, backend services and data pipelines, in-app messaging, and email campaigns. </p>



<p>ABBA as a system was quite simple. Each experiment (or rollout) mapped one to one to a feature flag, named after the experiment. When a client fetched the value of the feature flag it got back the name of the treatment group — e.g., “Control” or “Enabled” or “Sort according to color” — anything the user decided to name the group. (Fun trivia: some users of ABBA started encoding more elaborate configurations as JSON in the group names. Life finds a way). Every time a feature flag value was resolved, an event was logged, which fed into the exposure and results pipelines. For each experiment, only a small number of metrics were calculated. Many of these metrics were not very sensitive, leading to almost all analysis being performed manually in notebooks.</p>



<h2>Why we decided to build a new system</h2>



<p>Around 2017, the system began to reveal its limitations. We had a few big projects that required a lot of experimentation, and the sentiment at the company was that the system needed to improve.</p>



<p>At a hack week in late 2017, a few senior engineers gathered to sketch out a new system, which aimed to address the following challenges (as well as some others):</p>



<ul><li><strong>Reduce Time:</strong> The 1-1 mapping between an experiment and the feature flag led to some interesting side effects. If there were a problem with the experiment (and often there were) and it needed to be restarted, we simply couldn’t just … restart it. A new experiment would have to be created, and the software would need to be updated to use the new feature flag. The new system would have to reduce the time it took to complete this cycle.</li></ul>



<ul><li><strong>Produce less events</strong>: The volume of events that were logged by the A/B testing system had over time grown to almost 25% of our total event volume. This drove up the cost of processing, and the volume of events caused incidents in the event delivery system.</li></ul>



<ul><li><strong>Improved analysis:</strong> The metrics that ABBA provided out of the box were no way near enough for our analysis needs, and our data scientists were getting tired of performing analyses in notebooks. It was time consuming, and we also didn’t have any consistency across the company when it came to how experiments were analyzed. The new system would have to allow us to add custom metrics and we needed a solid analysis methodology.</li></ul>



<ul><li><strong>Sophisticated coordination:</strong> Over time our needs for how we allocated users to experiments changed, which was done manually by coordinating bucket ranges between teams. This was of course error prone — if someone ended up using the wrong buckets a whole slew of experiments would be impacted; the new system would have to address this.</li></ul>



<h2>The Experimentation Platform</h2>



<p>The new experimentation system, dubbed “The Experimentation Platform”, is composed of three parts:</p>



<ol><li><strong>Remote Configuration</strong> – replaces our feature-flagging service. Instead of “flags”, its model is based on “properties” — a configurable aspect of one of our clients or backend services. An example of a property could be the color of our buttons, or the number of tracks in the top list. </li></ol>



<ol start="2"><li><strong>Metrics Catalog</strong> – a managed environment for running SQL pipelines to ingest metrics into a data warehouse, from where data can be served with sub-second latency to UIs and notebooks. </li></ol>



<ol start="3"><li><strong>Experiment Planner</strong> – manages and orchestrates experiments. This is the part of the platform users interact with when they want to run an experiment.</li></ol>



<div><figure><img loading="lazy" width="700" height="505" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-700x505.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-700x505.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-250x180.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-768x554.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-1536x1109.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-120x87.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>Remote Configuration</h2>



<p>Remote Configuration is a way to change the experience a user receives. This is done through controlling the values of a set of “properties” of the client. A property is a variable with a type (enum or integer) and a default value, and can represent the appearance or behavior of pretty much anything. </p>



<div><figure><img loading="lazy" width="700" height="525" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-700x525.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-700x525.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-250x188.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-768x576.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-1536x1153.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-120x90.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>The example above shows an imaginary version of Spotify’s home page in our mobile apps. It’s made up of a set of shelves, and each shelf has a set of cards. With Remote Configuration properties we can control elements for any purpose, i.e. the number of shelves or font sizes on the home page for experiments, rollouts, or personalization or localization. </p>



<p>The properties are defined in a yaml file living next to the code that uses it. When the code is built, all properties and their default values are gathered and published via an API to the admin interface together with the ID of the client being built and the version number. </p>



<div><figure><img loading="lazy" width="700" height="351" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-700x351.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-250x125.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-768x385.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-120x60.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3.png 1420w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>The default value is critical. It allows us to have a programmatic understanding of what the end user experience will be if a client fails to fetch or apply property values. Also, we only have to transfer values to the client when they differ from the default, which saves a lot of time and data traffic when the client starts up. We know what defaults a client has since it identifies itself with the version number. </p>



<div><figure><img loading="lazy" width="700" height="234" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-700x234.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-700x234.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-250x84.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-768x257.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-1536x514.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-120x40.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>The way different users get different values for properties is through a set of policies that is evaluated when a client requests the configuration. The policy has a set of filtering criteria and a property-value mapping to apply if the filters match. The actual implementation of the policy is a <a href="https://facebook.github.io/planout/" target="_blank" rel="noreferrer noopener">PlanOut</a> script that the Remote Configuration service executes. </p>



<p>An important side effect of the fetching of property values is that two events are being logged:</p>



<ul><li><strong>Config Assigned,</strong> which lets us know that a user has fetched its values. Besides user information, this log message also identifies which policies were applied. This information is later used to determine which experiments a user was exposed to.</li></ul>



<ul><li><strong>Config Applied,</strong> which lets us know that the device has actually started using the property values. We use this event as the trigger event for exposure. </li></ul>



<p>Property values are re-fetched in the background at regular intervals, but are only applied when the app is relaunched. The main reason for this is that we do not want the user experience to change mid-session. </p>



<h2>Metrics Catalog</h2>



<p>The Metrics Catalog is where we manage, store, and serve metrics to the Experimentation Platform. </p>



<div><figure><img loading="lazy" width="700" height="336" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-700x336.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-700x336.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-250x120.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-768x369.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-1536x737.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-120x58.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>On a high level, raw metric data is fed into a pipeline where it’s joined with information on which experiment groups a user belongs to. This data is then aggregated into a OLAP cube and put into a data warehouse. In front of the data warehouse sits an API that allows other parties to query for information without knowing too much about the underlying storage.</p>



<p>Exposure is assembled from the Config Assigned and Config Applied messages from Remote Configuration.</p>



<figure><img loading="lazy" width="700" height="292" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-700x292.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-700x292.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-250x104.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-768x320.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-1536x640.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-120x50.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>A user is considered exposed to an experiment if the following is true:</p>



<ul><li>We have a Config Assigned event that assigns the user to one of the groups in the experiment,<p><strong>AND</strong></p></li></ul>



<ul><li>We have a Config Applied event that tells us that the user started using the configuration of the experiment.<p><strong>AND OPTIONALLY</strong></p></li><li>The user exists in one specified “custom exposure source”.</li></ul>



<p>The custom exposure sources allow us to define finer-grained exposure events, such as when a user visited a certain page in the mobile app.</p>



<h2>Experiment Planner</h2>



<p>The Experiment Planner sits as an orchestrating layer on top of Metrics Catalog and Remote Configuration. This is where we create, launch, and stop experiments, as well as analyze test results.<br/>The UI lives in <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">Backstage</a>, our developer portal. All of our internal teams have access to our internal instance of Backstage and are free to create as many experiments as they like.</p>



<div><figure><img loading="lazy" width="512" height="404" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7.png 512w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7-250x197.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7-120x95.png 120w" sizes="(max-width: 512px) 100vw, 512px"/><figcaption>For illustrative purposes only.</figcaption></figure></div>



<p>When creating an experiment, we have to define the test treatments, what experience users should get for each treatment (by specifying property values), and all the things that go into testing the hypothesis. Having programmatic understanding of available properties in Remote Configuration and their types helps this process and reduces configuration errors. It’s possible to define values for properties belonging to different systems in a single experiment. For example, if Android and iOS are implemented differently, we still can run a single experiment on both platforms.</p>



<h2>Summary</h2>



<p>We have spent the last two years rebuilding our experimentation capabilities at Spotify. The new platform is a step change in ease of use and capabilities, but we still feel it’s early for experimentation at Spotify.</p>



<p>We are constantly evolving our Experimentation Platform and practices. If you would like to know more, or if you’re interested in joining the team and contribute to our journey, do not hesitate to reach out.</p>



<p>Johan Rydberg, Experimentation Lead</p>



<p><a rel="noreferrer noopener" href="mailto:jrydberg@spotify.com" target="_blank">jrydberg@spotify.com</a> / <a rel="noreferrer noopener" href="https://twitter.com/datamishap" target="_blank">@datamishap</a></p>




        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Johan Rydberg</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Managing Clouds from the Ground Up: Cost Engineering at Spotify&#xA;</title>
      <link>https://engineering.atspotify.com/2020/09/29/managing-clouds-from-the-ground-up-cost-engineering-at-spotify/</link>
      <description>Like many of those in tech, we invest heavily in our cloud and data infrastructure. While seemingly routine, the ability to manage and scale our infrastructure to support our 299+ million listeners worldwide, 24/7, without missing a beat (or syllable) is crucial for the business and our brand. O</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-3958">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 29, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/09/29/managing-clouds-from-the-ground-up-cost-engineering-at-spotify/" title="Managing Clouds from the Ground Up: Cost Engineering at Spotify">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/09/Cost-Engineering_01C-1.png"/>                    </a>
                        
        </p>

        

        
<p>Like many of those in tech, we invest heavily in our cloud and data infrastructure. While seemingly routine, the ability to manage and scale our infrastructure to support our 299+ million listeners worldwide, 24/7, without missing a beat (or syllable) is crucial for the business and our brand. </p>



<p>On top of that, our infrastructure teams are resolute when it comes to upholding a highly valued cultural goal: enabling our autonomous engineering teams (called squads) to work as freely and quickly as they possibly can. Finish that off with the fact that we’re a growing public company, and we’ve created a challenging problem for our cost engineering team.</p>



<figure><img loading="lazy" width="700" height="493" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-700x493.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-700x493.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-250x176.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-768x541.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-1536x1082.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-120x85.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h2>Introducing Spotify’s Cost Insights tool</h2>



<p>Managing costs in our unique situation is no easy feat, but that certainly doesn’t stop us from innovating on the process. We’re leaving behind the days of reducing costs via top-down requests and moving on to finding fun and rewarding ways engineers can strengthen technology while improving the company’s bottom line. Our new <a href="https://github.com/spotify/backstage/tree/master/plugins/cost-insights" target="_blank" rel="noreferrer noopener">Cost Insights</a> product in <a href="https://backstage.io/" target="_blank" rel="noreferrer noopener">Backstage.io</a> explains cloud costs in a way our engineers can relate to and identifies optimizations that have resulted in some big wins for Spotify.  </p>



<figure><img loading="lazy" width="700" height="460" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-700x460.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-700x460.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-250x164.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-768x505.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-1536x1010.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-120x79.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption><em>Figure above for illustrative purposes only.</em></figcaption></figure>



<p>We’ve found that our engineers see these optimizations as an interesting challenge — they improve cost, performance, and reliability, turning our infrastructure into a lean, green execution machine. The Cost Insights tool brings those optimization opportunities to light, so engineers can move quickly in achieving those wins. </p>



<h3>Spotify’s unique take on cost management</h3>



<p>Many great cloud cost management tools are on the market, but we were looking for a way to better understand the relationship between a business’s cloud cost and its overall growth. Without broader context, it’s difficult to determine whether $10,000, for example, might be an appropriate amount to spend. </p>



<p>At Spotify, we believe showcasing cost and business data in a meaningful way will empower our engineers to understand where they are spending and optimize quickly. To encourage engineers to take action, their cloud cost tool should be located where other frequently used products and services are to increase productivity.</p>



<h3>Growing the business while being cost conscious</h3>



<p>Spotify continues to be heavily focused on growth, allowing teams that may be high spending on cloud costs to continue to do so if it results in growth opportunities for the business. We needed a tool that showcases costs and helps engineers, engineering managers, and product managers to be information driven when deciding between growth initiatives and worthwhile cost optimizations. Cost Insights is a solution that allows for engineering teams to:</p>



<ul><li>Become aware of their cloud spend and how it relates to their business unit’s growth.</li><li>Understand how cost optimizations should be prioritized compared to the goals for business growth.</li><li>Receive clear recommendations on how they can optimize or reduce their spend.</li></ul>



<h3>Cost Insights features</h3>



<p>Our goal was to launch the Cost Insights plugin with a strong foundation and a great potential for growth. The open source version includes three key features:</p>



<ul><li><strong>Cost vs. business graph:</strong> Users track how their team or a specific GCP project is trending compared to their company’s business growth.  </li><li><strong>Detailed product panels:</strong> Cost Insights currently includes detailed information on six cloud products. The product panels currently cater to GCP but can be configured to utilize other cloud providers. These panels help users understand the cost of their products down to the resource level and compare their growth over time.</li><li><strong>Project alerting:</strong> Teams are alerted when project costs exceed a chosen threshold, allowing them to deep dive into cost changes at the resource level.</li></ul>



<h3>Preventing over-optimizations</h3>



<p>Developing a cost tool in a growth-focused, autonomous culture can have severe consequences if executed incorrectly. The Cost Insights product is accessible by any Spotifier, so it became essential to be explicit with teams when there is a cost increase to review. We’ve set several thresholds to capture how much a team is spending and to track their growth trends. Only teams that are growing faster than Spotify’s business will be nudged to investigate their spending. </p>



<h3>Upcoming on the roadmap</h3>



<p>Cost Insights allows teams to determine for themselves if the time invested in an optimization is valuable compared to the costs saved. Now that it’s available in open source with <a href="http://backstage.io/" target="_blank" rel="noreferrer noopener">Backstage</a>, our focus will shift to open sourcing the backend, creating detailed cost breakdowns (SKU level), and delivering alert dismissals that incorporate user feedback.</p>



<p>For more information about Cost Insights, reach out to Janisa Anandamohan at <a href="mailto:janisa@spotify.com" target="_blank" rel="noreferrer noopener">janisa@spotify.com</a>.</p>
        <br/>

        
        

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Janisa Anandamohan</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Cloud Native Computing Foundation Accepts Backstage as a Sandbox Project&#xA;</title>
      <link>https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/</link>
      <description>If you hear faint whooping in the background of your playlists today, it’s just us celebrating a new milestone for Spotify’s open source efforts: The Cloud Native Computing Foundation (CNCF) has accepted Backstage, our open source developer portal, as an early stage project in the CNCF Sandbox. It’s</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-3947">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 24, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" title="Cloud Native Computing Foundation Accepts Backstage as a Sandbox Project">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png" alt="Cloud Native Computing Foundation Accepts Backstage" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png 753w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-250x121.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-700x337.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-120x58.png 120w" sizes="(max-width: 753px) 100vw, 753px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png"/>                    </a>
                        
        </p>

        

        
<p>If you hear faint whooping in the background of your playlists today, it’s just us celebrating a new milestone for Spotify’s open source efforts: <a href="https://www.cncf.io/" target="_blank" rel="noreferrer noopener">The Cloud Native Computing Foundation (CNCF)</a> has accepted <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a>, our open source developer portal, as an early stage project in the <a href="https://www.cncf.io/sandbox-projects/" target="_blank" rel="noreferrer noopener">CNCF Sandbox</a>. It’s just the first step in a longer journey with the CNCF, but it’s an important one for Spotify as it underlines our renewed commitment to open source — and developers everywhere.</p>



<h2>Backstage + CNCF = 🎉</h2>



<p>For those of you unfamiliar with the CNCF, you may recognize them as the home of such hits as Google’s Kubernetes and Lyft’s Envoy. With such a strong foundation watching over our community’s efforts and such an impressive roster of projects leading the way before us, we have high hopes for the future of Backstage — one of our most ambitious open source projects to date.</p>



<p>You’ve heard us talk about Backstage before on this blog — back in March when we <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">announced the open source project</a> and later when we shared <a href="https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/" target="_blank" rel="noreferrer noopener">how we use Backstage internally at Spotify</a>. In a nutshell: Backstage is an open platform for building developer portals. Built around a centralized service catalog, it’s designed to streamline your development environment from end to end. We built it to improve the everyday experience and productivity of developers — initially, our own developers, and then when we open sourced it, all developers, everywhere.</p>



<h2>Our commitment to improving developer experience</h2>



<p>At Spotify, Backstage enables us to scale safely and onboard quickly, helping us build and ship the product that hundreds of millions of people around the world use every day. We believe it has the potential to transform how all engineers work together, whether they’re in a 50-person startup or a Fortune 50. </p>



<p>Here’s what Backstage can do for companies and tech organizations, and how it improves developer experience:</p>



<ul><li><strong>Restore order to software ecosystems.</strong> For companies whose infrastructure has become a wilderness of competing technologies and orphaned dependencies hiding in the dark corners of their tech stack, the <a rel="noreferrer noopener" href="https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha" target="_blank">Backstage Service Catalog</a> brings back discoverability, accountability, and control — not to mention sanity. Instead of being overwhelmed by fragmentation and information sprawl, the Backstage Service Catalog creates a centralized system for tracking all your software — making it easy for teams to manage 10 services and making it possible for a company to manage thousands of them.</li></ul>



<ul><li><strong>Jumpstart productivity by standardizing software and tooling.</strong> With software templates, engineers can spin up a new software project in minutes instead of hours. <a rel="noreferrer noopener" href="https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates" target="_blank">Backstage Software Templates</a> are like automated getting started guides. After an engineer chooses a template, Backstage takes care of the rest — automatically setting up the repo, deploying the first build, and providing a Hello World project, all ready to go — with your organization’s best practices built right in, right from the start. By reducing the number of low-variance choices a developer is forced to consider when starting a project, templates remove friction and allow developers to spend more cycles solving problems higher up in the stack. Standards can set engineers free.</li></ul>



<ul><li><strong>Get unstuck with great technical documentation made easy. </strong>No one can ever find documentation when they need it — and if they do, it might not be that helpful because it hasn’t been kept up to date. Backstage solves both ends of the problem. With <a rel="noreferrer noopener" href="https://backstage.io/blog/2020/09/08/announcing-tech-docs" target="_blank">our “docs like code” approach</a>, engineers write their technical documentation in Markdown files right alongside their code. Whenever you create a new project in Backstage, a TechDocs site is automatically set up in the same repo — so you can update your code and your documentation with the same pull request. This integrated workflow and centralization makes great documentation easy. Easy to create and maintain. And easy to find and use.</li></ul>



<ul><li><strong>Customize and scale your infrastructure with a growing ecosystems of plugins.</strong> Every company has their own, homegrown infrastructure — Backstage’s plugin architecture makes it simple to make Backstage a perfect fit for yours. Integrating your custom, proprietary tooling is as simple as building an internal plugin for your installation of Backstage. You can also build open source plugins to share with the community. The open source <a rel="noreferrer noopener" href="https://backstage.io/plugins" target="_blank">plugin marketplace</a> for Backstage continues to grow, expanding Backstage’s functionality with each new plugin. It’s like an app store for your infrastructure.</li></ul>



<p>Backstage has already come a long way — and none of these features would be what they are today without contributions from the open source community.</p>



<h2>Our commitment to the open source community</h2>



<p>We were excited by the reception Backstage received when we first released it. But we’ve been even more gratified by how the community of contributors has grown since then, as they’ve built new <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">plugins</a> and added new functionality to the core product. Over <a href="https://github.com/spotify/backstage/graphs/contributors" target="_blank" rel="noreferrer noopener">130 people</a> have contributed to the project, and roughly 40% of pull requests are now coming from external, non-Spotify contributors.</p>



<p>As Principal Product Manager <a href="https://engineering.atspotify.com/2020/04/01/my-beat-stefan-alund/" target="_blank" rel="noreferrer noopener">Stefan Ålund</a> writes on the <a href="https://backstage.io/blog/2020/09/23/backstage-cncf-sandbox" target="_blank" rel="noreferrer noopener">Backstage blog</a>:</p>



<blockquote><div><p>We released the open source version of Backstage ‘early’. That was intentional. Because even though we’ve been using Backstage internally for years, we wanted the open source version to be developed with input and contributions from the community. And that’s exactly the product that’s going into the CNCF Sandbox today.</p><p>Backstage’s ability to simplify tooling and standardize engineering practices has attracted interest from other major tech companies, as well as airlines, auto manufacturers, investment firms, and global retailers. We know that Backstage solves a problem — infrastructure complexity — that’s common to a lot of large and growing companies today. But different companies work differently, use particular toolsets, and have unique use cases. By making Backstage open source, we can build it with people working inside a variety of engineering organizations all over the world. It makes for a better product that serves a wider group of users (beyond that of Spotify’s) and their needs.</p></div></blockquote>



<p>Thank you to everyone who has already contributed to this project, inside and outside of Spotify. And if you’ve been curious about Backstage, now is the perfect time to dive in. Visit <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage.io</a> to learn more and <a href="https://mailchi.mp/spotify/backstage-community" target="_blank" rel="noreferrer noopener">subscribe to our newsletter</a> for updates. Check out open issues on <a href="https://github.com/spotify/backstage/" target="_blank" rel="noreferrer noopener">GitHub</a> or get started building a <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">plugin</a> for your favorite tool or service. We look forward to seeing the community grow, and can’t wait to see where open source takes us all next.</p>



<p>We’ll give Remy DeCausemaker — Head of Spotify’s <a href="https://thenewstack.io/does-your-organization-need-an-open-source-program-office/" target="_blank" rel="noreferrer noopener">Open Source Program Office</a> (OSPO) — the last word: </p>



<blockquote><p>We’re excited to embark on this journey with the CNCF community. Backstage isn’t the first open source project Spotify has released, but it is the first one we felt was ready to dedicate to an upstream foundation, and we can’t wait to bring what we’ve learned to the next project. There’s so much great tech being built here, and it’s about time we share it to build even greater products, together.</p></blockquote>
        <br/>

        
        

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>