<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Spotify</title>
    <link>https://engineering.atspotify.com/</link>
    <description></description>
    <item>
      <title>&#xA;                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too&#xA;                                        </title>
      <link>https://engineering.atspotify.com/2021/09/23/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/</link>
      <description>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/09/23/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/" title="How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w" sizes="(max-width: 512px) 100vw, 512px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/09/Backstage_Developers_Header.png"/>                    </a>
                        
        </p>

        

        
<p>What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?</p>



<p>Instead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?</p>



<p>Pia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener"><em>Thoughtworks</em> podcast</a>: What types of problems do Spotify engineers face? And why did we create <a href="http://backstage.spotify.com" target="_blank" rel="noreferrer noopener">Backstage</a> to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.</p>



<h2>Growing pains</h2>



<p>As Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.</p>



<p>So we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?</p>



<p>Pia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:</p>



<ol><li>“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”<br/></li><li>“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”</li></ol>



<p>Considering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.</p>



<p>But that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.</p>



<p>What else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.</p>



<h2>Backstage: a platform for your platforms</h2>



<p>As Pia notes, Spotify developed <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">Backstage</a> to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.</p>



<p>Where our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.</p>



<p>Now, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our <a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">Golden Paths</a>.</p>



<p>Because of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because <a href="https://github.com/backstage/backstage" target="_blank" rel="noreferrer noopener">Backstage is a rapidly growing open source tool</a>, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.</p>



<p>So, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” </p>



<p>Interested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the <a href="https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness" target="_blank" rel="noreferrer noopener">Thoughtworks podcast episode</a>. And if you’re curious about how to get started with Backstage, read more about that <a href="https://backstage.spotify.com/blog/getting-started-with-backstage/" target="_blank" rel="noreferrer noopener">here</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python&#xA;</title>
      <link>https://engineering.atspotify.com/2021/09/07/introducing-pedalboard-spotifys-audio-effects-library-for-python/</link>
      <description>We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4787">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/09/07/introducing-pedalboard-spotifys-audio-effects-library-for-python/" title="Introducing Pedalboard: Spotify’s Audio Effects Library for Python">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_header.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/09/Pedalboard_header.gif"/>                    </a>
                        
        </p>

        

        
<p>We’ve just open sourced <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">Pedalboard</a>, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).</p>



<p>If you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro<sup>®</sup>, or Pro Tools<sup>®</sup>, or newer, more accessible tools like <a rel="noreferrer noopener" href="https://www.soundtrap.com/" target="_blank">Soundtrap</a> or <a rel="noreferrer noopener" href="https://anchor.fm/" target="_blank">Anchor</a>. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.</p>



<p>This ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at <a href="https://research.atspotify.com/audio-intelligence/" target="_blank" rel="noreferrer noopener">Spotify’s Audio Intelligence Lab</a> found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter <em>Pedalboard</em>, a new Python package.</p>



<figure><img loading="lazy" width="700" height="182" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Pedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of <a rel="noreferrer noopener" href="https://juce.com/" target="_blank">JUCE</a>, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3<sup>® </sup>and Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.</p>



<p>Similar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3<sup>® </sup>or Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.</p>



<figure><img loading="lazy" width="700" height="538" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>We’ve found a number of great uses for Pedalboard at Spotify so far, including:</p>



<ul><li><strong>Machine Learning (ML):</strong> Pedalboard makes the process of <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Data_augmentation" target="_blank">data augmentation</a> for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.</li></ul>







<ul><li><strong>Content Creation: </strong>Pedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3<sup>®</sup> or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.</li></ul>







<ul><li><strong>Creativity:</strong> Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.</li></ul>



<p>Spotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.</p>



<p>If you’re interested in trying out Pedalboard, it’s ready now. You can find its <a href="https://github.com/spotify/pedalboard" target="_blank" rel="noreferrer noopener">code and documentation on GitHub</a>, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!</p>



<p>—</p>



<p><em>VST is a registered trademark of Steinberg Media Technologies GmbH.</em></p>
        <br/>

        
        

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_header.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Four Lessons We Learned from Creating Spotify’s Desktop App&#xA;</title>
      <link>https://engineering.atspotify.com/2021/08/04/four-lessons-we-learned-from-creating-spotifys-desktop-app/</link>
      <description>TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>August 4, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/08/04/four-lessons-we-learned-from-creating-spotifys-desktop-app/" title="Four Lessons We Learned from Creating Spotify’s Desktop App">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w" sizes="(max-width: 1201px) 100vw, 1201px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to <a href="https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/" target="_blank" rel="noreferrer noopener">web players</a> to <a href="https://carthing.spotify.com/" target="_blank" rel="noreferrer noopener">car things</a>. But sitting at the core is our flagship product, the one that started it all: the desktop app. In <a href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank" rel="noreferrer noopener">the first episode</a> of our podcast series, “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, host and Chief R&amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please <a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA&amp;dl_branch=1" target="_blank" rel="noreferrer noopener">check out the podcast</a> yourself to hear even more about how Spotify became, well, Spotify.</p>



<h2>How do you steal from a pirate?</h2>



<p>Let’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. </p>



<p>Sure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?</p>



<p>In the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:</p>



<blockquote><p><strong>Daniel: </strong>I guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.</p></blockquote>



<p>In other words, when it comes to guiding your product strategy, <strong>our</strong> <strong>first lesson is an important rule to live by: convenience trumps everything. </strong>The appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.</p>



<h2>Go big or go home</h2>



<p>But how exactly could we make a user experience <em>that</em> good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.</p>



<p>Luckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.</p>



<p>Little surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” </p>



<p>Bad news for other companies, but great news for us. You see, it’s always good to keep in mind <strong>our second lesson:</strong> <strong>great ambition attracts great talent. This is why companies always need to keep moving the goalposts.</strong> When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.</p>



<h2>The rules are only suggestions</h2>



<p>And it wasn’t long before he identified Spotify’s problem — and the solution.</p>



<blockquote><p><strong>Ludde: </strong>Doing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.</p></blockquote>



<p>Essentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. </p>



<p>Fortunately, Ludde already knew <strong>our</strong> <strong>third product lesson:</strong> <strong>don’t be afraid to break the rules. </strong>The problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.</p>



<p>Not to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (<a rel="noreferrer noopener" href="https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c" target="_blank">Check out Episode 01, “How do you steal from a pirate?”</a>, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …</p>



<h2>Do you want to see a magic trick?</h2>



<p>All of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?</p>



<p>When Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.</p>



<blockquote><p><strong>Michelle</strong>: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”</p></blockquote>



<p><strong>That’s our fourth and final lesson:</strong> <strong>differentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.</strong> Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.</p>



<p>So, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R&amp;D Officer Gustav Söderström and <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">check out all the episodes right here</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Patrick Balestra: Senior Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2021/06/04/patrick-balestra-senior-engineer/</link>
      <description>8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4729">
     <div>
         
         
         
         <div>
             <p><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w" sizes="(max-width: 693px) 100vw, 693px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/06/Patrick-Balestra_Header-Image.png"/>
                                  
             </p>
             <p><b>Originally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.</b></p>
         </div>

         


         

         
<blockquote><p>8:30am</p></blockquote>



<p>I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!</p>



<blockquote><p>10:00am</p></blockquote>



<p>After breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. </p>



<p>My team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>One way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. </p>



<blockquote><p>2:00pm</p></blockquote>



<p>After lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source <a rel="noreferrer noopener" href="https://xcmetrics.io/" target="_blank">XCMetrics</a> — <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/" target="_blank">a tool developed at Spotify</a> for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.</p>



<blockquote><p>7:00pm</p></blockquote>



<p>I tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…</p>







<figure><img loading="lazy" width="700" height="111" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="557" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w" sizes="(max-width: 700px) 100vw, 700px"/></figure>

         
         

         <p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Achieving Team Purpose and Pride with Scrum&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/27/achieving-team-purpose-and-pride-with-scrum/</link>
      <description>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 27, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/27/achieving-team-purpose-and-pride-with-scrum/" title="Achieving Team Purpose and Pride with Scrum">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/05/Scrum-Post_Header.png"/>                    </a>
                        
        </p>

        

        
<p>Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.</p>



<p>At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.</p>



<p>While this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.</p>



<h2>Our problem</h2>



<p>Our squad had long been following a process comprising bits and pieces from the <a rel="noreferrer noopener" href="https://scrumguides.org/scrum-guide.html" target="_blank">Scrum</a> framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.</p>



<p>The goals we ultimately wanted our process to achieve were:</p>



<ol><li><strong>Continuous improvement: </strong>We wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.</li></ol>



<ol start="2"><li><strong>Shared understanding and transparency: </strong>We wanted everyone on the team to know at any given time what work was happening, and what it entailed.</li></ol>



<ol start="3"><li><strong>Confidence: </strong>We wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.</li></ol>



<h2>Our approach</h2>



<p>To help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. </p>



<h3>Backlog refinement</h3>



<p><strong>Goal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.</strong></p>



<p>Before these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.</p>



<p>With the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually <em>point </em>those tickets.</p>



<p>This system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.</p>



<h3>Sprint planning</h3>



<p><strong>Goal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.</strong></p>



<p>Previously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.</p>



<p>Now, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.</p>



<h3>Sprint review</h3>



<p><strong>Goal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.</strong></p>



<p>While we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.</p>



<p>In a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:</p>



<ul><li><em>What work did we complete?</em></li><li><em>Is there anything we need to extend or add to what we’ve done?</em></li><li><em>Did we discover any tech debt?</em></li><li><em>Are we on track to meet our longer-term goals?</em></li></ul>



<p>This allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.</p>



<h3>Retro</h3>



<p><strong>Goal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.</strong></p>



<p>In previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.</p>



<p>Now, we continue to create those notes, but then vote on a <em>single issue </em>to spend the majority of the retro discussing and ideating to solve.</p>



<figure><img loading="lazy" width="746" height="787" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w" sizes="(max-width: 746px) 100vw, 746px"/><figcaption><em>Now, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.</em></figcaption></figure>



<p>By the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.</p>



<h3>Stand-ups</h3>



<p><strong>Goal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.</strong></p>



<p>Incorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.</p>



<h2>Recommendations</h2>



<p>With simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:</p>



<p><strong>1. Try out a system holistically before making adjustments. </strong></p>



<p>Agile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.</p>



<p><strong>2. Ask the “stand-up question”.</strong></p>



<p>Asking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.</p>



<p><strong>3. Focus on a single issue in retros.</strong></p>



<p>Allow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.</p>



<p><strong>4. Plan sprints you can finish, and commit to finishing them.</strong></p>



<p>Create multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.</p>



<p>These changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers <a href="https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/" target="_blank" rel="noreferrer noopener">became more T-shaped</a> to finish the sprint’s work in time. </p>



<p>Additionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  </p>



<p>My team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.</p>



<h2>Acknowledgments</h2>



<p>Many thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Sophia Ciocca, Web Engineer</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/18/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/</link>
      <description>TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 18, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/18/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/" title="A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/a-product-story-backstage-1.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/05/a-product-story-backstage-1.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR;</strong> In episode 08 of our podcast series “<a href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank" rel="noreferrer noopener">Spotify: A Product Story</a>”, we share stories and lessons from building and open sourcing <a href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank" rel="noreferrer noopener">Backstage</a>, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. <a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">Listen to the episode now</a> and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.</p>



<h2>How it started: “Like a cold shower”</h2>



<p>The story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. </p>



<p>As Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? </p>



<p>The answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. </p>



<p>But the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R&amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:</p>



<blockquote><p><strong>Gustav</strong>: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?</p><p><strong>Pia</strong>: Having spent 15 years as an engineer at other companies, it was like a cold shower.</p></blockquote>



<p>Brrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?</p>



<h2>Engineers are users, too</h2>



<p>When it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? </p>



<p>But the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. </p>



<p>According to Pia, two issues emerged as common causes for declining productivity:</p>



<ol><li><strong>Context switching</strong>: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” </li></ol>



<ol start="2"><li><strong>Discoverability</strong>: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many <em>almost</em> duplications — not pure duplications — because people are very smart and they would recognize that.” </li></ol>



<p>There would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. </p>



<p>In a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “<a href="https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/" target="_blank" rel="noreferrer noopener">rumour-driven development</a>”.</p>



<p>And as Spotify continued to grow, the problem only got worse.</p>



<h2>Speed, scale, autonomy… pick two?</h2>



<p>Now that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?</p>



<p>Turns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:</p>



<blockquote><p>“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”</p></blockquote>



<figure></figure>



<p>What made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:</p>



<p>“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”</p>



<p>Spotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.</p>



<h2>How it’s going: Not just adopted, but embraced</h2>



<p>So if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. </p>



<p>Even more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.</p>



<p>People can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. </p>



<p>For a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.</p>



<p>Want to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?</p>



<div><figure><a href="https://backstage.io/"><img loading="lazy" width="350" height="350" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/backstage-service-catalog-icon-4.gif" alt=""/></a></figure></div>







<p>Listen to episode 08 — “<a href="https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl" target="_blank" rel="noreferrer noopener">When to build vs buy — and when to open source</a>” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!</p>



<figure></figure>



<p>Want to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. </p>



<p>In each episode, Spotify’s Chief R&amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from <a href="https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD" target="_blank" rel="noreferrer noopener">Metallica’s Lars Ulrich and Napster’s Sean Parker</a>, to <a href="https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp" target="_blank" rel="noreferrer noopener">ML legend Andrew Ng</a>. </p>



<p>How did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" target="_blank" rel="noreferrer noopener">Google Cloud’s largest Dataflow jobs ever</a>? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? <a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">You can find all the podcast episodes here</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/a-product-story-backstage-1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It&#xA;</title>
      <link>https://engineering.atspotify.com/2021/05/11/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/</link>
      <description>Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-4585">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>May 11, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/05/11/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/" title="Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png"/>                    </a>
                        
        </p>

        

        
<p>Last week, Spotify won an award — and we’re not playing it cool. We <a href="https://twitter.com/SpotifyEng/status/1389988765725245441" target="_blank" rel="noreferrer noopener">tweeted</a>. We bragged <a href="https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs" target="_blank" rel="noreferrer noopener">on LinkedIn</a>. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.</p>



<p>The award is <a rel="noreferrer noopener" href="https://www.cncf.io/" target="_blank">Cloud Native Computing Foundation</a>’s Top End User Award, <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">announced</a> at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the <a rel="noreferrer noopener" href="https://www.cncf.io/enduser/" target="_blank">End User Community</a>, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).<br/></p>



<p>We joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like <a rel="noreferrer noopener" href="https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/" target="_blank">Kubernetes</a>, <a rel="noreferrer noopener" href="https://www.youtube.com/watch?v=fMq3IpPE3TU" target="_blank">gRPC</a>, and <a href="https://www.youtube.com/watch?v=HfRU414cjjQ">Envoy</a>), our leadership in CNCF forums and meetups (<a rel="noreferrer noopener" href="https://community.cncf.io/stockholm/" target="_blank">look out for the next Stockholm one here</a>), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" target="_blank">Backstage</a>, which is now in <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank">the CNCF Sandbox</a>.</p>



<figure></figure>



<p>But in the end, this is a win for all of Spotify R&amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.</p>



<p>You can read more about the award on the <a rel="noreferrer noopener" href="https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/" target="_blank">CNCF website</a>. And to learn more about what got us here, listen to our podcast series “<a rel="noreferrer noopener" href="https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB" target="_blank">Spotify: A Product Story</a>”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:</p>



<div>
<figure><figcaption>Hear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers</figcaption></figure>



<figure><figcaption>Hear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.</figcaption></figure>
</div>




        <br/>

        
        

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Dave Zolotusky, Principal Engineer &amp; CNCF Technical Oversight Committee Representative</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 11 May 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Customization vs. Configuration in Evolving Design Systems&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/28/customization-vs-configuration-in-evolving-design-systems/</link>
      <description>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 28, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/28/customization-vs-configuration-in-evolving-design-systems/" title="Customization vs. Configuration in Evolving Design Systems">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/04/image3.png"/>                    </a>
                        
        </p>

        

        
<p>When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.</p>



<p>As the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.</p>



<p>How do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?</p>



<p>As a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.</p>



<p>When working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.</p>



<p>In this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    </p>



<h2>Abstraction</h2>



<p>So what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. </p>



<p><em>For a more thorough view of abstraction in software, and in life, check out </em><a href="https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af" target="_blank" rel="noreferrer noopener"><em>Levels of Abstraction, A Key Concept in Systems Design</em></a><em> by Daniel Jhin Yoo. </em></p>



<p>In this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.</p>



<h2>Current landscape</h2>



<p>Now that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:</p>



<ul><li><em>Customization — </em>Custom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.</li></ul>



<ul><li><em>Configuration — </em>The original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.</li></ul>



<div><figure><img loading="lazy" width="700" height="394" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x394.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x394.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-250x141.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-768x433.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-1536x865.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-120x68.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1.png 1672w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>Some highlights of our available options:</p>



<ul><li><strong>Powerhouse definitions</strong><strong>:</strong> By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. </li></ul>



<ul><li><strong>Prepacked guidelines</strong><strong>:</strong> Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.</li></ul>



<ul><li><strong>Property passthroughs</strong><strong>:</strong> These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.</li></ul>



<ul><li><strong>Direct overrides</strong><strong>:</strong> These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.</li></ul>



<h2>Customization vs. configuration</h2>



<p>With the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.</p>



<h3>Customization</h3>



<p><strong>Pros: Autonomy, speed, innovation</strong></p>



<p>The greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.</p>



<p><strong>Cons: Lack of coherency, loss of maintainability, potential duplication</strong></p>



<p>A local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.</p>



<h3>Configuration</h3>



<p><strong>Pros: Consistency, contribution, maintainability</strong> </p>



<p>If emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.</p>



<p><strong>Cons: Can become a bottleneck, rigidness, vocabulary awareness</strong></p>



<p>The other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.</p>



<h2>How to decide which approach to use</h2>



<p>With both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:</p>



<ul><li><strong>Feature maturity</strong><strong>:</strong> If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.</li></ul>



<ul><li><strong>Product maturity</strong><strong>:</strong> As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.</li></ul>



<ul><li><strong>Timeline</strong><strong>:</strong> While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?</li></ul>



<ul><li><strong>Reusability</strong><strong>:</strong> If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.</li></ul>



<h2>Key takeaways</h2>



<p>When evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.</p>



<p>The more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.</p>



<p>Lastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.</p>



<p>While there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.</p>



<p>—</p>



<p><em>A huge shout out to Krist Wongsuphasawat and his article </em><a href="https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7" target="_blank" rel="noreferrer noopener"><em>Navigating the Wide World of Data Visualization Libraries</em></a><em>. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. </em></p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Charlie Backus</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 28 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Nour Daoud Bösing: Security Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/26/nour-daoud-bosing-security-engineer/</link>
      <description>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4531">
     <div>
         
         
         
         <div>
             <p><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png 1200w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-120x69.png 120w" sizes="(max-width: 1200px) 100vw, 1200px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/04/Nour-edit-1.png"/>
                                  
             </p>
             <p><b>Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. </b></p>
         </div>

         


         

         
<blockquote><p>6:00am</p></blockquote>



<p>These days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. </p>



<blockquote><p>7:30am</p></blockquote>



<p>I work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  </p>



<p>Things are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Twice a week, my team has a virtual <em>fika</em> (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>My afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.</p>



<blockquote><p>4:00pm</p></blockquote>



<p>I finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! </p>



<p>On nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to <a href="https://jusoorsyria.com/">Jussor</a>, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.</p>







<figure><img loading="lazy" width="700" height="111" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" width="700" height="583" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>

         
         

         <p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Rethinking Spotify Search&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/15/rethinking-spotify-search/</link>
      <description>Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 15, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/15/rethinking-spotify-search/" title="Rethinking Spotify Search">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Search-gif.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/04/Search-gif.gif"/>                    </a>
                        
        </p>

        

        
<h2>Search @ Spotify</h2>



<div><p>Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.</p><p>Since Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.</p></div>



<h2>When more doesn’t mean more</h2>



<p>In the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-700x352.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “<em>If each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”</em> That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-700x352.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>By the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.</p>



<p>We were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. </p>



<div><figure><img loading="lazy" width="2101" height="879" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15.jpg 2101w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-120x50.jpg 120w" sizes="(max-width: 2101px) 100vw, 2101px"/></figure></div>



<p>In the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. </p>



<h2>Search as a platform</h2>



<p>Great — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.</p>



<h2>From user obsession to developer satisfaction</h2>



<p>With insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.</p>



<div><figure><img loading="lazy" width="700" height="352" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>While these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “<em>[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.</em>” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.</p>



<h2>Conclusion</h2>



<p>Katarina Berg, Chief HR Officer, says “<em>Growth is our mantra</em>” and “<em>Change is our constant.”</em> This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.</p>



<p><strong>References</strong></p>



<p>Ang Li et al., “Search Mindsets:<em> </em>Understanding Focused and Non-Focused Information Seeking  in<em> </em>Music Search,” <a href="https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/" target="_blank" rel="noreferrer noopener">publication</a> <em>WWW ’19: The World Wide Web Conference</em> (May 2019): 2971–2977. <a href="https://doi.org/10.1145/3308558.3313627" target="_blank" rel="noreferrer noopener">https://doi.org/10.1145/3308558.3313627</a></p>



<p>“Shareholder Letter Q4 2020” (February 3, 2021) <a href="https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf" target="_blank" rel="noreferrer noopener">https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf</a></p>



<p>“Spotify — Company Info,” For the Record, <a href="https://newsroom.spotify.com/company-info/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/company-info/</a></p>



<p>“The Band Manifesto.” <a href="https://www.spotifyjobs.com/culture/the-band-manifesto">https://www.spotifyjobs.com/culture/the-band-manifesto</a></p>



<p>“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).</p>



<p>“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) <a href="https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/" target="_blank" rel="noreferrer noopener">https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/</a></p>



<p> “SPOTIFY PODCASTS DATASET.” <a href="https://podcastsdataset.byspotify.com/" target="_blank" rel="noreferrer noopener">https://podcastsdataset.byspotify.com/</a></p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Hugo Galvão and Daniel Doro</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Search-gif.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Thu, 15 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Building the Future of Our Desktop Apps&#xA;</title>
      <link>https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/</link>
      <description>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>April 7, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/" title="Building the Future of Our Desktop Apps">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-120x60.png 120w" sizes="(max-width: 1999px) 100vw, 1999px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/04/ClientX.png"/>                    </a>
                        
        </p>

        

        
<p>For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.</p>



<p>We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.</p>



<h2>In the beginning, there were two clients</h2>



<div><p>Towards the end of 2018, our team was the owner of a recently built <a href="https://open.spotify.com/" target="_blank" rel="noreferrer noopener">Web Player</a>, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.</p><p>Because the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to <a href="https://en.wikipedia.org/wiki/Conway%27s_law" target="_blank" rel="noreferrer noopener">Conway’s law</a>). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.</p></div>



<div><p>In addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.</p><p>We had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.</p></div>



<p>We knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.</p>



<p>There were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.</p>



<p>Now, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.</p>



<p>Let’s talk more in detail about how we turned the idea into reality.</p>



<div><figure><img loading="lazy" width="700" height="259" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-700x259.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-700x259.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-250x93.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-768x285.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-1536x569.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-120x44.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>One UI, multiple containers</h2>



<p>The Spotify Desktop client is a Windows and Mac native application that uses CEF (<a href="https://bitbucket.org/chromiumembedded/cef/" target="_blank" rel="noreferrer noopener">Chromium Embedded Framework</a>) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.</p>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-700x701.png" alt="" width="450" height="451" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-700x701.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-250x250.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-150x150.png 150w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-768x769.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-120x120.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7.png 1200w" sizes="(max-width: 450px) 100vw, 450px"/><figcaption>Previous architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.</figcaption></figure></div>



<p>The previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow <a href="http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf" target="_blank" rel="noreferrer noopener">playback to begin as soon as a listener clicked</a>. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.</p>



<div><p>The Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.</p><p>The Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.</p></div>



<p>This concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.</p>



<p>While work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.</p>



<p>The final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.</p>



<figure><img loading="lazy" width="700" height="375" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-700x375.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-700x375.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-250x134.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-768x412.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-120x64.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879.png 1199w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>The new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.</figcaption></figure>



<p>With this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.</p>



<div><figure><img loading="lazy" width="700" height="387" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-700x387.jpg" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-700x387.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-250x138.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-768x425.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-120x66.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2.jpg 1999w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<div><figure><img loading="lazy" width="700" height="483" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x483.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x483.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-250x172.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-768x529.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-1536x1059.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-120x83.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Old vs New: the Web Player UI has come a long way since the project started.</figcaption></figure></div>



<h2>Solving the organizational challenge</h2>



<p>From the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?</p>



<p>There was also a large information gap we had to solve. What features in the existing Desktop application <em>had</em> to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.</p>



<p>At the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.</p>



<p>After three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. </p>



<p>In reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.</p>



<h2>Evaluating success</h2>



<p>We had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?</p>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png" alt="" width="500" height="500" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x250.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x700.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-150x150.png 150w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x768.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x1536.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x120.png 120w" sizes="(max-width: 500px) 100vw, 500px"/></figure></div>



<h3>1. <strong>Reusability</strong></h3>



<p>Reusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.</p>



<h3>2. <strong>Unification</strong></h3>



<p>User experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.</p>



<p>Critically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. </p>



<h3>3. <strong>Speed</strong></h3>



<p>An important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.</p>



<h3>4. <strong>Satisfaction: Meeting Desktop and Web Player user needs</strong></h3>



<p>The new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.</p>



<p>We are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.</p>



<h2>What does all this mean for you as a user?</h2>



<div><p>As a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.</p><p>As time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.</p></div>



<h2>Is this your jam? Join us!</h2>



<p>Want to join the band and build the future of Spotify? Head over to our <a href="https://www.spotifyjobs.com/" target="_blank" rel="noreferrer noopener">job board</a> and see if anything catches your eye. We’ve just announced our <a href="https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/" target="_blank" rel="noreferrer noopener">Working From Anywhere</a> policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.</p>



<p><em>A shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian</em>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            David Riordan: Product Manager&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/30/my-beat-david-riordan/</link>
      <description>5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4054">
     <div>
         
         
         
         <div>
             <p><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w" sizes="(max-width: 500px) 100vw, 500px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/11/MyBeat_David-Riordan.png"/>
                                  
             </p>
             <p><b>David is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  </b></p>
         </div>

         


         

         
<blockquote><p>5:00 am </p></blockquote>



<p>My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. </p>



<blockquote><p>9:00 am</p></blockquote>



<p>Back home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. </p>



<p>As a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. </p>



<p>For the past year, I’ve been part of a project called <a href="https://klio.io/">Klio</a> – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  </p>



<p>Klio has been a long time in the making, so <a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/">it was amazing to finally open-source it</a> last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. </p>



<blockquote><p>11:30 am</p></blockquote>



<p>Working on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. </p>



<blockquote><p>12:00 noon</p></blockquote>



<p>Time to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.</p>



<blockquote><p>1:00 pm</p></blockquote>



<p>Whilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. </p>



<blockquote><p>6:00 pm</p></blockquote>



<p>We’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?</p>







<figure><img loading="lazy" width="700" height="111" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png" alt="" width="700" height="685" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>





         
         

         <p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Why You Should Pair with Non-Engineers&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/</link>
      <description>TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn&#39;t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 23, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/" title="Why You Should Pair with Non-Engineers">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/Pairing_02.png"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong>Spotify encourages engineers to become <a href="https://jchyip.medium.com/why-t-shaped-people-e8706198e437" target="_blank" rel="noreferrer noopener">T-shaped</a> and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and <a href="https://www.youtube.com/watch?v=4jckqGVtyAA" target="_blank" rel="noreferrer noopener">learning from peers</a> outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. </p>



<p>Prior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.</p>



<h2><strong>Broaden scope of knowledge (T shape)</strong></h2>



<p>Engineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.</p>



<p>Within web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.</p>



<p>Additionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.</p>



<p>When I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.</p>



<h2><strong>Stress-test documentation</strong></h2>



<p>I had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had <em>never</em> been used for writing code for the web.</p>



<p>This meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.</p>



<p>Stress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.</p>



<h2><strong>Build empathy between disciplines</strong></h2>



<p>Understanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.</p>



<p>The next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. </p>



<p>I once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an <a href="https://en.wikipedia.org/wiki/Integrated_development_environment" target="_blank" rel="noreferrer noopener">IDE</a> and also display in <a href="http://www.figma.com" target="_blank" rel="noreferrer noopener">Figma</a>.</p>



<p>You might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.</p>



<h2><strong>Shared language</strong></h2>



<p>Many workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This <em>can</em> be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.</p>



<p>Something we use extensively across Spotify is SEMVER (<a href="https://semver.org/" target="_blank" rel="noreferrer noopener">semantic versioning</a>), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. </p>



<p>Doing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.</p>



<h2><strong>Summary</strong></h2>



<p>We made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.</p>



<p>Share more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Tyce Clee</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 23 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/</link>
      <description>TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 16, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/" title="Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/Backstage-BDay-Blog_v002.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR:</strong> As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. </p>



<h2>From Hack Week hunch to CNCF Sandbox</h2>



<p>Last year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? </p>



<p>The team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a> was out in the wild — making its official open source debut <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">one year ago today</a>. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at <a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank" rel="noreferrer noopener">the CNCF</a> (also home to Kubernetes, Envoy, and Helm).</p>



<p>Looking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. </p>



<figure><img loading="lazy" width="1999" height="1016" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-250x127.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-700x356.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-768x390.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-1536x781.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-120x61.png 120w" sizes="(max-width: 1999px) 100vw, 1999px"/><figcaption><em>“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” </em><br/><em>— Expedia Group Developer Experience Team</em></figcaption></figure>



<p>Companies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. </p>



<h2>Why a developer portal?</h2>



<p>To understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard <a href="https://backstage.io/docs/overview/background" target="_blank" rel="noreferrer noopener">this story</a> before, feel free to skip ahead.) </p>



<p>In March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. </p>



<p>Our small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. </p>



<p>The faster we grew, the more this fragmentation slowed us down again. </p>



<h2>A single pane of glass</h2>



<p>Designed first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.</p>



<p>It became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.</p>



<p>With Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. </p>



<p>Speed was the key. We saw firsthand that faster developers aren’t just <a href="https://martinfowler.com/articles/developer-effectiveness.html" target="_blank" rel="noreferrer noopener">more productive developers</a>, they’re happier developers.</p>



<h2>From internal portal to open platform</h2>



<p>What’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.</p>



<p>Unlike <a href="https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/" target="_blank" rel="noreferrer noopener">the internal version of Backstage</a>, which has more than 120 different <a href="https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage" target="_blank" rel="noreferrer noopener">plugins</a> built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. </p>



<p>Since that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:</p>



<ul><li>We introduced four core features: the <a href="https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha" target="_blank" rel="noreferrer noopener">Service Catalog</a>, <a href="https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates" target="_blank" rel="noreferrer noopener">Software Templates</a>, <a href="https://backstage.io/blog/2020/09/08/announcing-tech-docs" target="_blank" rel="noreferrer noopener">TechDocs</a>, and our <a href="https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/" target="_blank" rel="noreferrer noopener">new Kubernetes monitoring tool</a>. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.</li></ul>



<ul><li>We launched the <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">Plugin Marketplace</a>: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like <a href="https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md" target="_blank" rel="noreferrer noopener">Snyk</a>. </li></ul>



<ul><li>We created the <a href="https://backstage.io/blog/2020/09/30/backstage-design-system" target="_blank" rel="noreferrer noopener">Backstage Design System</a>: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.</li></ul>



<h2>Stabilizing the core</h2>



<p>The work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: <a href="https://backstage.io/blog/2020/12/22/stability-index" target="_blank" rel="noreferrer noopener">stabilizing those features and APIs</a> so that more companies can adopt the platform for production use. </p>



<p>In the coming weeks, our team will:</p>



<ul><li>Bring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.</li></ul>



<ul><li>Create an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.</li></ul>



<ul><li>Update other parts of the core app — notably, improving search and incorporating GraphQL systemwide.</li></ul>



<p>You can learn more in <a href="https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md" target="_blank" rel="noreferrer noopener">the project roadmap</a>.</p>



<h2>Adopters: Backstage in the wild!</h2>



<p>Beyond the <a href="https://github.com/backstage/backstage/blob/master/ADOPTERS.md" target="_blank" rel="noreferrer noopener">official adopters list</a>, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.</p>



<ul><li>Early adopters Zalando and SDA SE shared <a href="https://youtu.be/4-VX9tDdJYY?t=1756" target="_blank" rel="noreferrer noopener">their adoption experiences</a> last month at our first community session. </li></ul>



<ul><li>Expedia has a team dedicated to rolling out Backstage.</li></ul>







<ul><li>American Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:<p>“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines</p></li></ul>



<ul><li>DoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. <p>“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash</p></li></ul>



<h2>A world of contributors grows into a community</h2>



<p>Of course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged <a href="https://twitter.com/SpotifyEng/status/1341376341636239364" target="_blank" rel="noreferrer noopener">two new contributors a week</a>. </p>



<figure></figure>



<p>This year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the <a href="https://github.com/backstage/community" target="_blank" rel="noreferrer noopener">Backstage Community</a> page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in <a href="https://github.com/backstage/backstage/" target="_blank" rel="noreferrer noopener">the main repo</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Tyson Singer</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Backstage-BDay-Blog_v002.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify’s New Experimentation Coordination Strategy&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/</link>
      <description>At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 10, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/" title="Spotify’s New Experimentation Coordination Strategy">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/Bucket-Reuse_01-1.png"/>                    </a>
                        
        </p>

        

        
<p>At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time. These are well-known problems among tech companies — take, for example, Google’s solution in Tang et al (2010). But the statistical implications of different solutions have not been properly investigated. In a recent paper (<a rel="noreferrer noopener" href="https://arxiv.org/abs/2012.10202" target="_blank">M. Schultzberg, O. Kjellin, and J. Rydberg, 2020</a>), we investigate important statistical properties of a common technical solution to the coordination — called “Bucket Reuse”. In this blog post we highlight some interesting results and present some details about how Spotify will coordinate experiments from now on.</p>



<h2>What is Bucket Reuse?</h2>



<p>Bucket Reuse is a simple idea utilizing the power of hashing. Essentially the steps are as follows: decide on a number of buckets (B). Take the unique user ID and hash it together with a  random salt into B “buckets” such that all users hash into one and only one bucket. Once the hash map is established, all sampling is performed on the bucket level. This implies that a bucket either is or is not in a sample at any given time point. If we want to sample N number of users, we sample the number of buckets that contain the number of users closest to the desired number N. If, e.g., the desired N is 20 and each bucket contains 3 users, we would sample 7 buckets and end up with 21 users. Note that a bucket is simply a logical group of units to which we assign a certain user by a fixed hash map. Figure 1 illustrates such a map. The second part of the name Bucket <em>Reuse</em> comes from the fact that we reuse the same buckets over and over again in the sampling for all experiments. That is, the random salt for the hashing is selected only once; after that, the hash map and the number of buckets is fixed. <strong>Importantly,</strong> when we talk about Bucket Reuse for experimentation, we always mean the following: the random sampling is performed on the bucket level; the random treatment allocation is performed at the user level on the users in the sample. </p>



<figure><img loading="lazy" width="700" height="625" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x625.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x625.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-250x223.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-768x686.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-120x107.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4.png 1366w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 1: <em>Schematic illustration of a hash map. A user ID is hashed together with a random salt to map each user to a unique bucket. </em></figcaption></figure>



<h2>What is experiment coordination?</h2>



<p>To get into the interesting parts of experiment coordination, we need to establish some key concepts. Figure 2 illustrates the concepts of exclusive and nonexclusive experiments. That two or more experiments “are exclusive” to each other simply means that they are run on distinct sets of users. Experiments that are nonexclusive are nonexclusive to <em>all</em> experiments at Spotify, meaning that they all randomly overlap in terms of users.</p>



<figure><img loading="lazy" width="700" height="383" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-700x383.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-700x383.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-250x137.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-768x420.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-120x66.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5.png 1498w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 2: <em>Illustration of exclusive and nonexclusive experiments. Exclusive experiments never overlap with each other in terms of users; nonexclusive experiments randomly overlap with exclusive experiments and other nonexclusive experiments. Note that the allocations in this figure were selected for illustration; in a true random sample we would expect exclusive experiments to also be spread out uniformly.  </em></figcaption></figure>



<p>The most critical challenge from a statistical perspective is imposed by what we call <em>programs of exclusive experiments</em>. A program of exclusive experiments is a set of experiments run over time where all simultaneous experiments are exclusive to each other. That is, a unit is in at most one, and only one, of the experiments in the program at any given time point. At Spotify we have such programs for several surfaces in the app, for example Search, the Home screen, and certain parts of the backend code base. To better understand the limitations imposed on the sampling by running programs of exclusive experiments, it is helpful to introduce the concepts of paths. A path is simply a sequence of experiments that a unit can be in. Figure 3 illustrates a program of exclusive experiments containing 5 experiments over time. Below the experiments, their possible paths are displayed. For example, it is not possible to go through both Experiments 3 and 4 as they overlap in time and are exclusive, and must therefore be run on distinct users. The number of unique possible paths explodes combinatorially after a relatively short time period in most programs, and only a small partition of the possible paths can be taken by any unit regardless of the sample strategies discussed in this post.</p>



<figure><img loading="lazy" width="700" height="806" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-700x806.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-700x806.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-250x288.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-768x885.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-1334x1536.png 1334w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-120x138.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3.png 1370w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figure 3: <em>Paths of experiences possible during a hypothetical program of 5 exclusive experiments.</em></figcaption></figure>



<h2>Highlights from <a rel="noreferrer noopener" href="https://arxiv.org/abs/2012.10202" target="_blank">Schultzberg et al.</a></h2>



<p>Schultzberg et al. presents two key results:</p>



<ol><li>Under unrestricted sampling of buckets, i.e., Bucket Reuse in nonexclusive experiments, the properties of the difference-in-means estimator of the average treatment effect is approximately equivalent to the properties under random sampling of units. In other words, standard t-tests can be used for inference. The key to this remarkable finding is connecting Bucket Reuse to the existing literature on randomized experiments embedded in complex sampling designs (Horvitz and Thompson, 1952;  van den Brakel and Renssen, 2005). </li></ol>



<ol start="2"><li>The bias imposed by the restricted sampling of buckets implied by programs of exclusive experiments is derived. It is shown that this bias is often restricted to the history right before the experiment. Moreover, the length of the window of the history that affects the bias can be estimated for any empirical experimentation program. One way to phrase this finding is that the sample at a time point T is not random with respect to the last D days leading up to time T, but random with respect to everything that happened before the time point T-D. If things happened during the last D days that make the set of buckets available for sampling at time T different from the population with respect to the treatment effect, the estimator is biased. This insight makes it possible for experimenters to evaluate the risk for biases by checking what experiments have been run in the program over the last D days, and if those risks for biases are likely to affect the average treatment effect in the experiment that is about to be started. </li></ol>



<h2>Spotify’s new experimentation coordination strategy</h2>



<p>We have migrated our experimentation platform to using Bucket Reuse for all experiments at Spotify. There are a few key reasons why we prefer Bucket Reuse over other solutions:</p>



<ul><li>It is simple to implement and understand. </li><li>It is a technically feasible solution that allows us to do complex coordination without losing speed in our systems. As new users come into Spotify, they are uniformly hashed into the existing buckets — that is, the system scales as Spotify’s user base grows.</li><li>Using one company-global bucket structure makes it easy to coordinate experiments arbitrarily. For example, two programs that have been run independently can easily be merged into one program of exclusive experiments at any time point for any period of time. And, a sample from a previous experiment with a broken experience can easily be quarantined and avoided in any future experiments, as the sampling units are always the same over time.  </li></ul>



<p>At Spotify we have chosen Bucket Reuse with 1M (1,000,000) buckets to coordinate all experiments. That is, all users are hashed into 1M buckets, and these buckets are used for all experiments in all experimentation programs, exclusive and nonexclusive. Although Schultzberg et al.  establishes that smaller numbers of buckets can have statistical properties enabling straight forward inference, it should be clear that the larger the number of buckets, the better. Even though the inference for the average treatment effect is unaffected by the bucket sampling, it is well known that the effect of cluster sampling on other estimands decreases when the number of buckets increases (Kumar Pradhan). That is, imposing a bucket structure is not preferred from a statistical perspective, but it is a technical necessity. The choice of 1M buckets was made because it is close to the largest number of buckets we can have while still keeping the selected buckets within an executable script stored in a database without having to resort to <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Binary_large_object" target="_blank">BLOB</a> storage. </p>



<p>We are not planning to have any full stops or to reshuffle users into new buckets. However, there are naturally occurring periods of low experimentation that will help decrease the dependency between the samples in programs of exclusive experiments. For example, many programs run fewer general product experiments over the winter holidays due to unusual listening behaviors. These natural pauses effectively reset the programs in terms of dependencies.  </p>



<p>To help experimenters running programs of exclusive experiments, we are implementing a few tools to keep track of the short-term dependencies. For each program, we will estimate the length of the history that can bias the results. Moreover, we are also implementing a tool to see the history of the available buckets at any time point. Figure 4 displays a prototype of this tool. It allows experimenters to evaluate if the experiments that the available user came out of are likely to bias the estimator in their experiment. It also provides information about the size of the overlap with previous experiments. </p>



<figure><img loading="lazy" width="1600" height="1394" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-10-23-10.58.56.gif" alt=""/><figcaption>Figure 4: <em>Dependency graph that shows the experimenter where the free space, and thereby their sample, will come from in terms of previous experiments in the exclusive program. Each rectangle corresponds to a previously run experiment. The numbers in the yellow circles indicate the percentage points of the population that went from one experiment into another, and finally into the proportion of the population that is available (“free space”) for sampling right now. The experimenter can see what effects the previous experiments had on the metrics of interest in their experiment. </em><br/><em>Figure above for illustrative purposes only.</em><br/></figcaption></figure>







<p>Statistical validity, derived from proper random sampling and random treatment allocation, is the cornerstone of a successful experimentation program. However, implementing systems that can serve hundreds of experiments to hundreds of millions users — while retaining the ability to conveniently coordinate experiments to be exclusive, without overlap — requires compromises between technical feasibility and statistical properties. At Spotify, we have migrated the internal experimentation platform to rely fully on <em>bucket reuse</em>, a technically desirable solution that provides speed, simplicity, and flexibility. In this post we establish the statistical properties under bucket reuse and conclude that the validity is unaffected. This migration enables more experiments of higher quality at Spotify.  </p>



<p><strong>Experimentation Platform team, Spotify</strong></p>



<p><strong>References</strong></p>



<p>Brakel, Jan van den, and Robbert H. Renssen. “Analysis of Experiments Embedded in Complex Sampling Designs.” <em>Survey Methodology </em> 31, no. 1 (2005): 23–40.</p>



<p>Horvitz, D. G. and D. J. Thompson. “A Generalization of Sampling Without Replacement from a Finite Universe.” <em>Journal of the American Statistical Association</em> 47, no. 260 (1952): 663–685. https://doi.org/10.2307/2280784</p>



<p>Kumar Pradhan, Bijoy. “On efficiency of cluster sampling on sampling on two occasions.” <em>Statistica</em> 64, no. 1 (2004): 183–191. <a href="https://doi.org/10.6092/issn.1973-2201/31">https://doi.org/10.6092/issn.1973-2201/31</a></p>



<p>Schultzberg, Mårten, Oskar Kjellin, and Johan Rydberg. “Statistical Properties of Exclusive and Non-exclusive Online Randomized Experiments using Bucket Reuse.” <em>arXiv preprint arXiv:2012.10202 </em>(2020).</p>



<p>Tang, Diane, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. “Overlapping experiment infrastructure: more, better, faster experimentation.” <em>KDD</em> <em>’10: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</em>, (July 2010): 17–26.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Mårten Schultzberg, Oskar Kjellin, and Johan Rydberg</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Matt Clarke: Senior Backend Infrastructure Engineer&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/09/my-beat-matt-clarke/</link>
      <description>Matt is a Senior Backend Infrastructure Engineer and has been at Spotify for two-and-a-half years. This time last year, he was living and working in London – but that’s all changed since the start of the pandemic…</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                 
 <article id="post-4139">
     <div>
         
         
         
         <div>
             <p><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg" alt="Matt Clarke" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg 800w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-250x192.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-700x537.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-768x589.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-120x92.jpg 120w" sizes="(max-width: 800px) 100vw, 800px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/02/Matt-Clarke-edit.jpg"/>
                                  
             </p>
             <p><strong>Matt is a Senior Backend Infrastructure Engineer and has been at Spotify for two-and-a-half years. This time last year, he was living and working in London – but that’s all changed since the start of the pandemic… </strong></p>
         </div>

         


         

         
<blockquote><p>8:00am</p></blockquote>



<p>At the moment, I’m living with my wife and step daughter in London and working from home like most people. Which means I get up around 8am, drink lots of coffee (way too much!) and log onto my computer an hour later – ready to check my messages and start getting my brain into thinking mode. </p>



<blockquote><p>10:00am</p></blockquote>



<p>My team is actually based in New York, so I tend to work 10–6 to overlap as much as possible with their hours. But the time difference means my mornings are fairly quiet and meeting-free – a chance for me to get down to focussed, individual tasks, without too many interruptions. </p>



<p>My work is mostly to do with Kubernetes – the technology we use to deploy our services at Spotify – and a lot of my time is spent helping other engineers get to grips with the system, debugging their issues and developing our infrastructure services, so they can deploy more easily and reliably. </p>



<p>Recently, I’ve been working on something called the k8s plug in, which vastly simplifies the Kubernetes experience for developers and means they can operate without a huge understanding of the platform under the hood. Earlier this year, we open-sourced this plug in, which felt like a really great moment – it’s amazing to think it’s now available to everyone in the tech community worldwide and can benefit so many people outside our organization. That to me is the magical thing about open-source. </p>



<blockquote><p>1:00pm</p></blockquote>



<p>I try to grab a bite to eat around 1pm, although I’m really bad at taking breaks – I get so sucked into what I’m doing that I forget the time, especially if I’m coding. It used to happen when I worked in the office too, even though there was an awesome canteen and a table tennis table to tempt me away from my desk!</p>



<blockquote><p>2:00pm</p></blockquote>



<p>This is when New York starts to wake up, so my work becomes more team-based – I often pair remotely with one of the developers out there, which means jumping on a hangout and sharing our screens, so we can collaborate on a piece of code. Alternatively, we might work together to write documents like RFCs, debug production issues or help other developers with their infrastructure issues. It’s a bit of a mix, really, </p>



<p>Weirdly, I’ve only met one of team-mates in real life, when I first joined up and spent two weeks in the New York office as part of an embed. But we still all work together really well – it’s friendly and we joke around a lot. I think there are some rules you need to learn for remote working and being in different zones – you need to be a bit flexible and not always expect to get your answers straight away. But once you’ve got used to that, things are surprisingly easy – it’s really not a big deal at all. </p>



<blockquote><p>6:00pm </p></blockquote>



<p>I usually finish up at 6ish, although I’m terrible for checking my emails in the evening. To try and switch off, I watch TV, listen to podcasts or play video games – I also started up an engineering book club at the start of the first lockdown. And I’m really into cooking at the moment – there’s something about following a recipe and going through a series of orchestrated steps that reminds me of coding. Although, as the old joke goes, at least your potato peeler never turns out to be ten versions out of date… </p>







<figure><img loading="lazy" width="700" height="111" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="557" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-700x557.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-700x557.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-250x199.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-768x611.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-120x96.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph.png 1520w" sizes="(max-width: 700px) 100vw, 700px"/></figure>

         
         

         <p>
             Published by Spotify Engineering         </p>
     </div>

     
     


 </article>
                

            
        
    </section></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Tue, 09 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            2020 Unwrapped: The people behind the numbers&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/01/2020-unwrapped-the-people-behind-the-numbers/</link>
      <description>2020 Wrapped is a story of gratitude and resilience. And we’re grateful for the people and teams behind the curtain who built this product experience (👏🏽Give them a hand!). The effort behind Wrapped spans the entire company and is founded on communication and collaboration. With the shift to wor</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 1, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/01/2020-unwrapped-the-people-behind-the-numbers/" title="2020 Unwrapped: The people behind the numbers">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg 1510w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-250x122.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-700x341.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-768x374.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-120x58.jpg 120w" sizes="(max-width: 1510px) 100vw, 1510px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/2020-Wrapped.jpg"/>                    </a>
                        
        </p>

        

        
<p>2020 Wrapped is a story of gratitude and resilience. And we’re grateful for the people and teams behind the curtain who built this product experience (👏🏽Give them a hand!). </p>



<p>The effort behind Wrapped spans the entire company and is founded on communication and collaboration. With the shift to working from home, we needed to create a structure that enabled us to collaborate and communicate remotely, and prioritized asynchronous communication over synchronous communication when possible. We were able to adapt to unprecedented challenges and join forces to deliver a personalized product experience for our listeners. Now, let’s take a look at the numbers behind the 2020 Wrapped experience.</p>



<h2><strong>Distributed execution</strong></h2>



<p>2020 Wrapped, like the Wrapped campaigns before it, was a company-wide project that involved collaboration among hundreds of Spotifiers. Over a span of 4 months, two key groups dedicated themselves to building the product experience — <strong>Personalization</strong> <strong>(PZN)</strong>, the minds behind the <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" target="_blank" rel="noreferrer noopener">data</a> magic, and <strong>Edison</strong>, drivers of the rich end-user experience. Around twenty engineers from 4 different squads within Edison and 6 engineers from other squads joined as embeds, all working as a virtual team covering 4 different time zones. </p>



<p>We adapted our ways of working to allow team members to contribute, communicate and collaborate regardless of their location. The team asynchronously worked on 10 technical specification documents, 8 RFC documents, 4 project planners, and more than 15 slide decks to facilitate information sharing.</p>



<figure><ul><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-700x1245.png" alt="" data-id="4235" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer.png" data-link="https://engineering.atspotify.com/?attachment_id=4235" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li><li><figure><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3-700x1245.png" alt="" data-id="4225" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.png" data-link="https://engineering.atspotify.com/?attachment_id=4225"/></figure></li></ul></figure>



<p>With a distributed-first mindset, Slack became our place to build traceability and transparency, allowing us to easily retrace our steps and find documentation that led to business decisions and actions. By the end of the project,  we had created 11 Slack channels each dedicated to brief messages that required quick answers or a brief team discussion around specific topics. One particular channel existed solely for the purpose of sharing memes — 8,116 and counting. 😬</p>



<figure><ul><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-700x1245.png" alt="" data-id="4237" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack.png" data-link="https://engineering.atspotify.com/?attachment_id=4237" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-700x1245.png" alt="" data-id="4238" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison.png" data-link="https://engineering.atspotify.com/?attachment_id=4238" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li></ul></figure>



<p>Ultimately, we kept the team connected and created team spirit by holding brainstorm meetings and building on ideas we generated as a group. It was especially helpful when complex problems arose and we needed to get people on the same page.</p>



<h2><strong>Collaboration</strong></h2>



<p>The core <a href="https://spotify.design/article/how-we-brought-2020-wrapped-to-life-in-the-mobile-app" target="_blank" rel="noreferrer noopener">mobile experience</a> was built by a few teams, but to deliver the product experience, it took the effort of 8 additional squads working simultaneously. We defaulted to open communication and, in most cases, preferred to overshare than undershare. Given the number of teams, oversharing ensured a successful flow of information. </p>



<p>We focused a great deal on expanding the mobile experience for our listeners in 2019, and we were able to reuse some of the components and apply some of the <a href="https://engineering.atspotify.com/2020/09/21/spotify-unwrapped-2019-how-we-built-an-in-app-experience-just-for-you/" target="_blank" rel="noreferrer noopener">lessons from 2019 Wrapped</a> to the 2020 experience. Compared to 2019, we were able to reduce our engineering effort by 50% for Android development, reduce our timeline for iOS development by 30%, and repeat with 60 engineering weeks for Backend. For Web, we increased our scope with an ambitious vision and made a significant investment in our engineering effort compared to 2019.</p>



<p>On the day of launch, individuals from multiple functions — Marketing, Brand + Creative, R&amp;D, Customer Support, Local Marketing, Public Relations, and Localization — gathered in a war room (aka a virtual meeting) testing, troubleshooting, and tackling live issues with tight coordination across 8 hours.</p>



<h2><strong>Wrapped around the world</strong></h2>



<p>With 12 new markets and 4 new languages, 2020 Wrapped was the most global iteration of the project to date 🌎🌍🌏. We served 26 total languages, 8 different fonts, and 87,343 words total — approximately 4,000 words per language in both left-to-right and right-to-left text orientations. Though a challenging task, it was absolutely critical that we embed localization into our process to improve the listening experience for our users across the globe. </p>



<figure><ul><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-700x1245.png" alt="" data-id="4240" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily.png" data-link="https://engineering.atspotify.com/?attachment_id=4240" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li><li><figure><img loading="lazy" width="700" height="1245" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-700x1245.png" alt="" data-id="4241" data-full-url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts.png" data-link="https://engineering.atspotify.com/?attachment_id=4241" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts.png 822w" sizes="(max-width: 700px) 100vw, 700px"/></figure></li></ul></figure>



<p>Shareable cards, like the ones above, hold a number of different corner cases to test, and quality remains top of mind. Because we weren’t physically in the office to pair and review pieces, it became imperative that we find creative solutions to allow for effective and efficient collaboration. For Localization testing and the Design Review process, we created a dedicated tool to reduce the feedback loop and enable the team to use it asynchronously. Using this tool, we were able to increase productivity in our distributed work environment, while respecting flexible work schedules and work/life boundaries.</p>



<figure><img loading="lazy" width="700" height="358" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-700x358.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-700x358.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-250x128.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-768x393.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-1536x786.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-120x61.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8.png 1749w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<figure><img loading="lazy" width="700" height="359" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-700x359.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-700x359.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-250x128.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-768x393.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-1536x787.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-120x61.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton.png 1743w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h2><strong>Closing words from the Wrapped fellowship</strong></h2>



<p>In the shift to working from home on a project that spanned the company, we needed to (quickly) set up a structure to keep productivity high and collaboration and communication flowing, while ensuring we were taking care of ourselves, our family and friends, and our team members. </p>



<p>We hope you enjoyed reading about the people behind the numbers, and hope that Wrapped made the end of your 2020 just a little bit better.</p>



<p>Thank you to everyone who made another year of Wrapped.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Designing a Better Kubernetes Experience for Developers&#xA;</title>
      <link>https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/</link>
      <description>TLDR; If you’re deploying a service with Kubernetes, you shouldn’t have to use all of your cluster management skills just to perform everyday developer tasks (like seeing which pods are experiencing errors or checking autoscaler limits). Backstage Kubernetes simplifies your deployment workflow by co</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>March 1, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/" title="Designing a Better Kubernetes Experience for Developers">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/03/image3.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TLDR;</strong> If you’re deploying a service with Kubernetes, you shouldn’t have to use all of your cluster management skills just to perform everyday developer tasks (like seeing which pods are experiencing errors or checking autoscaler limits). Backstage Kubernetes simplifies your deployment workflow by connecting to your existing Kubernetes implementation and aggregating the status of all your deployments into a single view — even across multiple clusters in multiple regions. </p>



<h2>Navigating the complexity of Kubernetes</h2>



<p>If you’re building a service today, you’re likely deploying it as a container, which is inside a pod, which is inside a cluster (alongside a bunch of other services that don’t belong to you), with deployments on different clusters spinning up and down all around the world. It can be hard to keep track of everything.</p>



<p>But despite widespread adoption of Kubernetes, all the tools for navigating this complexity have been focussed on the needs of cluster admins. This can make something as simple as checking the health of your service somewhat complicated. </p>



<p>That’s why we built a Kubernetes monitoring tool focussed on the needs of service owners and made it a core feature of <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a>, our open platform for building developer portals. We wanted to make the experience of managing services deployed on Kubernetes easier for all developers. </p>



<p>But first, how did we get here?</p>



<h2>The rise of Kubernetes and DevOps</h2>



<p>Since its release in 2014, Kubernetes has become one of the most widely adopted and important open source projects. Capabilities like autoscaling and cost optimisation through container scheduling used to be time-consuming and tricky to get right — now they’ve been democratised. </p>



<p>At the same time, the concept of DevOps has become mainstream. Developers now regularly perform tasks that were traditionally the domain of operations experts. </p>



<p>So, while everyday engineers can do more than ever before, their new powers have also come along with a new set of responsibilities.</p>



<h2>New powers, shifting roles</h2>



<p>When I first started using Kubernetes, cluster admins and service owners were one and the same: the people who built a cluster were usually the same people who owned the services that ran in the cluster. That’s not how it is today. As Kubernetes has achieved widespread adoption there has been a shift in Kubernetes usage as well as a shift in how Kubernetes is managed at the organisation level. </p>



<p>Now organisations tend to have a separate infrastructure team (sometimes not-so-ironically called the “DevOps” team) who build and maintain clusters for the feature developers and service owners. As the teams have become more specialized, the setups have become more advanced. For instance, the infrastructure team might set up Kubernetes clusters in multiple geographic regions in order to reduce end-user latency, wherever the user is in the world. </p>



<p>This is a better experience for the user, and it’s an optimization you might not have considered before Kubernetes existed or without a dedicated infrastructure team. But it also comes with productivity costs for the developer.</p>



<h2>Frustration also scales</h2>



<p>When your deployment environment reaches this kind of complexity and scale, the maintenance overhead for service owners increases. It forces them to use multiple kubectl contexts or multiple UIs just to get an overall view of their system. </p>



<p>It’s a small overhead — but adds up over time — and multiplies as service owners build more services and deploy them to more regions. Just checking the status of a service first requires hunting for it across multiple clusters. This can reduce productivity (and patience) company-wide.</p>



<h2>Better tools for the job</h2>



<p>We believed we could solve the problem through developer tooling. But we soon discovered the available tools weren’t suitable, because they:</p>



<ul><li>Don’t cater well for deploying to multiple Kubernetes clusters,</li><li>Usually require that users have clusterwide permissions, or</li><li>Display everything on a cluster and aren’t focused on the service the user cares about.</li></ul>



<p>As we often do when we want to <a href="https://backstage.io/blog/2020/10/22/cost-insights-plugin">solve a problem involving infrastructure complexity</a>, we wondered, why not build a custom plugin for Backstage, our homegrown developer portal?</p>



<h2>Backstage Kubernetes: Manage your services, not clusters</h2>



<p>Backstage provides vital information from Kubernetes — specifically focussed on the developer’s service. At a glance, the developer can see:</p>



<ul><li>The current status of their systems running in Kubernetes<ul><li>Including information aggregated from multiple clusters/regions</li></ul></li><li>Any errors reported by Kubernetes</li><li>How close the system is to its autoscaling limits</li><li>Container restarts</li></ul>



<figure><img loading="lazy" width="700" height="419" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-700x419.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-700x419.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-250x150.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-768x459.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-1536x919.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-120x72.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1.png 1999w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption><em>A default Kubernetes UI provides a cluster-centric view, including info about software you don’t own. </em><br/><em>(Source: </em><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/"><em>kubernetes.io</em></a><em>)</em></figcaption></figure>



<figure><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x394.png" alt=""/><figcaption><em>The Backstage Kubernetes UI provides a service-centric view, showing you the status of your service no matter how many clusters it’s been deployed to. </em><br/><em>Figures above are for illustrative purposes.</em></figcaption></figure>



<figure><img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-700x382.png" alt=""/><figcaption><em>More detail about your deployments is just a click away. You can see autoscaler limits, errors, and the status of individual pods — all at a glance — and without a trip to the CLI.</em><br/><em>Figures above are for illustrative purposes.</em></figcaption></figure>



<p>Instead of spending 20 minutes in a CLI trying to track down which clusters your service has been deployed to, you get all the information you need to know at a glance. You can learn more about these features on the <a href="https://backstage.io/blog/2021/01/12/new-backstage-feature-kubernetes-for-service-owners" target="_blank" rel="noreferrer noopener">Backstage blog</a> — or watch the demo video below to get an overview.</p>



<figure><p>
<iframe loading="lazy" title="How to monitor your services on Kubernetes with Backstage (Demo)" width="900" height="506" src="https://www.youtube.com/embed/VivuOxn3VQ8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</p></figure>



<h2>Everything about your service in one place</h2>



<p>As a standalone monitoring tool, we think Backstage Kubernetes can improve the experience of any developer who deploys to Kubernetes. Combined with the other features of Backstage, developers get a complete solution for building and managing their services.</p>



<p>At the core of Backstage is its <a href="https://backstage.io/blog/2020/05/22/phase-2-service-catalog" target="_blank" rel="noreferrer noopener">service catalog</a>, which aggregates information about software systems together so you have a consistent UI and one tool for developers to use. For years, Backstage has provided one place for Spotify’s developers to see everything they need to know about their services (APIs, documentation, ownership, etc.). Now that includes the current status of their service, regardless of how many Kubernetes clusters they deploy to.</p>



<p>Now that Backstage is open source, we want to improve on what we have built internally and provide Kubernetes as a core component of Backstage for anyone to contribute to and benefit from. </p>



<h2>Future Iteration</h2>



<p>As we continue to grow and develop Kubernetes in Backstage with the community, we hope to offer support for Kubernetes resources beyond Deployments and Custom Resource Definitions. </p>



<p>Although at Spotify we currently use GKE extensively, Kubernetes in Backstage communicates directly with the Kubernetes API and is cloud agnostic, accordingly. It will work with other cloud providers, including AWS and Azure, as well as managed Kubernetes services, like Red Hat OpenShift.</p>



<p>To contribute or get more information on Kubernetes in Backstage, <a href="https://discord.gg/MUpMjP2" target="_blank" rel="noreferrer noopener">join the discussion on Discord</a>!</p>



<p><em>Ask us anything: Matthew and the Backstage team will be hosting a Reddit AMA on March 3 at 4:00pm GMT. Send questions in <a href="https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/" target="_blank" rel="noreferrer noopener">r/kubernetes</a> starting March 2.</em></p>



<p><em>A version of this article first appeared on </em><a href="https://thenewstack.io" target="_blank" rel="noreferrer noopener"><em>The New Stack</em></a><em>.</em></p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Matthew Clarke, Senior Engineer</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020&#xA;</title>
      <link>https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/</link>
      <description>In this post we&#39;ll discuss how Spotify optimized and sped up elements from our largest Dataflow job, Wrapped 2019, for Wrapped 2020 using a technique called Sort Merge Bucket (SMB) join. We&#39;ll present the design and implementation of SMB and how we incorporated it into our data pipelines. Introdu</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>February 11, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" title="How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image1.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/02/image1.gif"/>                    </a>
                        
        </p>

        

        
<p>In this post we’ll discuss how Spotify optimized and sped up elements from our largest Dataflow job, <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/02/18/spotify-unwrapped-how-we-brought-you-a-decade-of-data/" target="_blank">Wrapped 2019</a>, for <a href="https://open.spotify.com/genre/2020-page">Wrapped 2020</a> using a technique called Sort Merge Bucket (SMB) join. We’ll present the design and implementation of SMB and how we incorporated it into our data pipelines.</p>



<h2>Introduction</h2>



<p>Shuffle is the core building block for many big data transforms, such as a join, GroupByKey, or other reduce operations. Unfortunately, it’s also one of the most expensive steps in many pipelines. Sort Merge Bucket is an optimization that reduces shuffle by doing work up front on the producer side. The intuition is that for datasets commonly and frequently joined on a known key, e.g., user events with user metadata on a user ID, we can write them in bucket files with records bucketed and sorted by that key. By knowing which files contain a subset of keys and in what order, shuffle becomes a matter of merge-sorting values from matching bucket files, completely eliminating costly disk and network I/O of moving key–value pairs around. Andrea Nardelli carried out the original investigation on Sort Merge Buckets for his <a href="http://kth.diva-portal.org/smash/get/diva2:1334587/FULLTEXT01.pdf">2018 master’s thesis</a>, and we started looking into generalizing the idea as a <a rel="noreferrer noopener" href="https://spotify.github.io/scio/extras/Sort-Merge-Bucket.html" target="_blank">Scio module</a> afterwards.</p>



<h2>Design and Implementation</h2>



<p>The majority of the data pipelines at Spotify are written in <a rel="noreferrer noopener" href="https://github.com/spotify/scio" target="_blank">Scio</a>, a Scala API for <a href="https://beam.apache.org/">Apache Beam</a>, and run on the <a href="https://cloud.google.com/dataflow">Google Cloud Dataflow</a> service. We implemented SMB in Java to be closer to the native Beam SDK (and even wrote and collaborated on a <a href="https://docs.google.com/document/d/1AQlonN8t4YJrARcWzepyP7mWHTxHAd6WIECwk1s3LQQ/edit?usp=sharing">design document with the Beam community</a>), and provide Scala syntactic sugar in Scio like many other I/Os. The design is modularized into the main components listed below — we’ll start with the two top-level SMB <a href="https://beam.apache.org/documentation/programming-guide/#transforms" target="_blank" rel="noreferrer noopener">PTransforms</a> — the write and read operations SortedBucketSink and SortedBucketSource.</p>



<h3>SortedBucketSink</h3>



<p>This transform writes a <a rel="noreferrer noopener" href="https://beam.apache.org/documentation/programming-guide/#pcollections" target="_blank">PCollection</a>&lt;T&gt; (where T has a corresponding <a href="https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/FileOperations.java" target="_blank" rel="noreferrer noopener">FileOperations&lt;T&gt;</a> instance) in SMB format. It first extracts keys and assigns bucket IDs using logic provided by <a href="https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/BucketMetadata.java" target="_blank" rel="noreferrer noopener">BucketMetadata</a>, groups key–values by the ID, sorts all values, and then writes them into files corresponding to bucket IDs using the FileOperations instance.</p>



<p>In addition to the bucket files, a JSON file is also written to the output directory representing the information from BucketMetadata that’s necessary to read the source: the number of buckets, the hashing scheme, and the instructions to extract the key from each record (for example, for Avro records we can encode this instruction with the name of the GenericRecord field containing the key).</p>



<figure><img loading="lazy" width="700" height="255" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-250x91.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-768x280.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-120x44.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5.png 1180w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h3>SortedBucketSource</h3>



<p>This transform reads from one or more sources written in SMB format with the same key and hashing scheme. It opens file handles for corresponding buckets from each source (using FileOperations&lt;T&gt; for that input type) and merges them while maintaining sorted order. Results are emitted as <a rel="noreferrer noopener" href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java" target="_blank">CoGbkResult</a> objects per key group, the same class Beam uses for regular Cogroup operations, so the user can extract the results per source with the correct parameterized type.</p>



<figure><img loading="lazy" width="700" height="365" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-250x130.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-768x400.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-120x63.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7.png 1067w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h3>FileOperations</h3>



<p>FileOperations abstracts away the reading and writing of individual bucket files. Since we need fine-grained control over the exact elements and their order in every file, we cannot leverage the existing Beam file I/Os, which operate on a PCollection level and abstract away the locality and order of elements. Instead, SMB file operations happen at a lower level of BoundedSource for input and ParDo for output. Currently Avro, BigQuery TableRow JSON, and TensorFlow TFRecord/Example records are supported. We plan to add other formats like Parquet as well.</p>



<h3>BucketMetadata</h3>



<p>This class abstracts the keying and bucketing of elements, and includes information such as key field, class, number of buckets, shards, and hash function. The metadata is serialized as a JSON file alongside data files when writing, and used to check compatibility when reading SMB sources.</p>



<h3>Optimizations and Variants</h3>



<p>Over the last year and a half we’ve been adopting SMB at Spotify for various use cases, and accumulated many improvements to handle the scale and complexity of our data pipelines.</p>



<ul><li><strong>Date partitioning:</strong> At Spotify, event data is written to Google Cloud Services (GCS) in hourly or daily partitions. A common data engineering use case is to read many partitions in a single pipeline — for example, to compute stream count over the last seven days. For a non-SMB read, this can be easily done in a single PTransform using wildcard file patterns to match files across multiple directories. However, unlike most File I/Os in Beam, the SMB Read API requires the input to be specified as a directory, rather than a file pattern (this is because we need to check the directory’s metadata.json file as well as the actual record files). Additionally, it must match up bucket files across partitions as well as across different sources, while ensuring that the CoGbkResult output correctly groups data from all partitions of a source into the same TupleTag key. We evolved the SMB Read API to accept one or more directories <em>per source</em>. </li></ul>



<ul><li><strong>Sharding:</strong> Although the Murmur class of hash functions we use during bucket assignment usually ensures an even distribution of records across buckets, in some instances one or more buckets may be disproportionately large if the key space is skewed, creating possible OOM errors when grouping and sorting records. In this case, we allow users to specify a number of <em>shards</em> to further split each bucket file. During the bucket assignment step, a value between [0, numShards) is generated randomly <a href="https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence"><em>per bundle</em></a>. Since this value is computed completely orthogonally to the bucket ID, it can break up large key groups across files. Since each shard is still written in sorted order, they can simply be merged together at read time.</li></ul>



<ul><li><strong>Parallelism:</strong> Since the number of buckets in an SMB sink is always a power of 2, we can come up with a joining scheme across sources with different numbers of buckets based off of a desired level of parallelism specified by the user. For example, if the user wants to join Source 1 with 4 buckets and Source 2 with 2 buckets, they can specify either:<ul><li><strong>Minimum parallelism,</strong> or “Merge Greatest Buckets” strategy: 2 parallel readers will be created. Each reader will read 2 buckets from source A and 1 from source B, merging them together. Because bucket IDs are assigned by taking the integer hash value of the key modulo the desired number of buckets, mathematically we know that the key spaces of the merged buckets overlap.</li><li><strong>Maximum parallelism,</strong> or “Least Bucket Replication” strategy: 4 parallel readers will be created. Each reader will read 1 bucket from Source A and 1 from Source B. After merging each key group, the reader will have to rehash the key modulo the greatest number of buckets, to avoid emitting duplicate values. Therefore, even though this strategy achieves a higher level of parallelism, there is some overhead of computing duplicate values and rehashing to eliminate them.</li><li><strong>Auto parallelism:</strong> Creates a number of readers between minimal and maximal amounts, based on a desired split size value provided by the Runner at runtime.</li></ul></li></ul>



<figure><img loading="lazy" width="700" height="459" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-250x164.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-768x504.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-120x79.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3.png 1115w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<ul><li><strong>SortedBucketTransform:</strong> A common usage pattern is for pipelines to enrich an existing dataset by joining it with one or more other sources, then writing it to an output location. We decided to specifically support this in SMB with a unique PTransform that reads, transforms, and writes output using the same keying and bucketing scheme. By doing the read/transform/write logic per bucket on the same worker, we can avoid having to reshuffle the data and recompute buckets — since the key is the same, we know that the transformed elements from bucket M of the inputs also correspond to bucket M in the output, in the same sorted order as they were read from.</li></ul>



<figure><img loading="lazy" width="700" height="320" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-250x114.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-768x351.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-120x55.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4.png 902w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<ul><li><strong>External Sort:</strong> We made a number of improvements to Beam’s <a href="https://github.com/apache/beam/tree/master/sdks/java/extensions/sorter">external sorter extension</a>, including replacing the Hadoop sequence file with the native file I/O, removing the 2GB memory limit, and reducing disk usage and coder overhead.</li></ul>



<h2>Adoption — Core Data Producers</h2>



<p>Since SMB requires data to be bucketed and sorted in a specific fashion, the adoption naturally starts from the producer of that data. A majority of the Spotify data processing relies on a few core data sets that act as single sources of truth for various business domains like streaming activities, user metadata and streaming context. We worked with the maintainer of these data sets to convert a year’s worth of data to SMB format.</p>



<p>Implementation was straightforward since SortedBucketSink is mostly a drop-in replacement for the vanilla Avro sink with some extra settings. We were using Avro sink with the sharding option to control the number and size of output files. After migrating to SMB, we did not notice any major bump in terms of vCPU, vRAM, or wall time since sharding requires a full shuffle similar to the additional cost of SMB sinks. A few other settings we have since had to tweak:</p>



<ul><li>Agree on user_id as a hexadecimal string as bucket and sort key, since we need the same key type and semantic across all SMB datasets.</li><li>Set compression to DEFLATE with level 6 to be consistent with the default Avro sink in Scio. As a nice side effect of data being bucketed and sorted by key, we observed ~50% reduction in storage from better compression due to collocation of similar records.</li><li>Make sure output files are backwards compatible. SMB output files have “bucket-X-shard-Y” in their names but otherwise contain the same records with the same schema. So existing pipelines can consume them without any code change; they just do not leverage the speedup in certain join cases.</li></ul>



<h2>Adoption — Wrapped 2020</h2>



<p>Once the core datasets were available in SMB format, we started Wrapped 2020, building off the work left from the Wrapped 2019 campaign. The architecture was meant to be reusable and was a great place to start. However, the source of data was a large, expensive Bigtable cluster that had to be scaled further up to handle the load of Wrapped jobs. We wanted to save cost and time by moving from Bigtable to SMB sources. This year we also needed to handle new complex requirements for filtering and aggregating streams. This required us to join a large dataset containing stream contextual information to the user’s listening history. This would have been nearly impossible or at the very least extremely expensive because of the considerable size of each of these joins. Instead we tried using SMB to eliminate that join completely and avoid using Bigtable as our listening history source.</p>



<p>To compute Wrapped 2020, we had to read from three main data sources for streaming activity, user metadata and streaming context. These three sources had all the data we needed to generate each person’s Wrapped while filtering based on listening context. Previously, the Bigtable had 5 years’ worth of listening history already keyed by user_id. Now, we are able to read data already keyed by user_id from these three sources through SMB. We then aggregated a year’s worth of data per key to calculate each user’s Wrapped.</p>



<p>Because 1 of the 3 main sources are partitioned hourly while the other 2 are partitioned daily, it would be problematic to read a year’s worth of data in one job due to the excessive number of concurrent reads from the hourly partitioned source. Instead, we first ran smaller jobs that would aggregate a week’s or day’s worth of play counts, msPlayed, and other information on each user. From there, we then aggregated all these smaller partitions to a singular partition of data that would hold a year’s worth of data. </p>



<figure><img loading="lazy" width="700" height="218" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-700x218.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-700x218.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-250x78.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-768x240.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-1536x479.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-120x37.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2.png 1692w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>SMB made this relatively easy. We used sortMergeTransform to combine our three sources of data, read each one keyed by user_id, and write our Wrapped output (play counts, ms played, play context, etc.) in SMB format. </p>



<figure><img loading="lazy" width="700" height="353" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-700x353.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-700x353.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-768x387.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-120x60.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM.png 1246w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Finally, we ran our aggregate job that uses sortMergeGroupByKey to read all Wrapped weekly partitions of SMB, combine a year’s worth of data, and write the output so later jobs can calculate the rest of Wrapped. A key point of flexibility here is that the aggregate job can take any mix of weekly and daily partitions, which is incredibly helpful logistically when running these jobs. The end result in practice looks something like this:</p>



<figure><img loading="lazy" width="700" height="218" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-700x218.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-700x218.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-250x78.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-768x240.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-1536x479.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-120x37.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6.png 1692w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>This ended up being a huge cost savings for us in this year’s Wrapped project. By leveraging SMB, we managed to join roughly a total of 1PB data without using conventional shuffle or Bigtable. We estimate around a 50% decrease in Dataflow costs this year compared to previous years’ Bigtable-based approach. Additionally, we avoided scaling the Bigtable cluster up two to three times its normal capacity (up to around 1,500 nodes at peak) to support the heavy Wrapped jobs. This was a huge win in this year’s campaign as we were able to bring a wonderful experience in a more cost effective way than ever before.</p>



<h2>Conclusion</h2>



<p>By adopting SMB, we were able to perform extremely large joins that were previously either unfeasible or cost-prohibitive, or that required custom workarounds like Bigtable. We achieved significant cost savings and opened up more ways of optimizing our workflows. There’s still much work to be done. We look forward to migrating more workflows to SMB, while handling more edge cases like data skew, composite keys, and more file formats.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Neville Li, Claire McGinty, Sahith Nallapareddy, &amp; Joel Östlund</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Introducing XCMetrics: Our All-in-One Tool for Tracking Xcode Build Metrics&#xA;</title>
      <link>https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/</link>
      <description>TL;DR We just open sourced XCMetrics — a tool for Apple’s developer software, Xcode, that lets you collect, display, and track the valuable metrics hiding inside your team’s Xcode build logs. Are your build times improving or regressing? Which version of Xcode is slowest? Which hardware setup is fas</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>January 20, 2021</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/" title="Introducing XCMetrics: Our All-in-One Tool for Tracking Xcode Build Metrics">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR</strong> We just open sourced <a href="https://xcmetrics.io" target="_blank" rel="noreferrer noopener">XCMetrics</a> — a tool for Apple’s developer software, Xcode, that lets you collect, display, and track the valuable metrics hiding inside your team’s Xcode build logs. Are your build times improving or regressing? Which version of Xcode is slowest? Which hardware setup is fastest? XCMetrics makes it easy to find out all this and more. Made for iOS engineers, by iOS engineers, the tool is written completely in Swift, so it’s easy to customize. Use it to track the metrics you want — and get insights that can help improve both developer experience and productivity.</p>



<h2>The problem: Where do you get good Xcode build data?</h2>



<p>You’ve read before on this blog about how our infrastructure teams are always finding new and innovative ways to make <a href="https://engineering.atspotify.com/2020/07/22/leveraging-mobile-infrastructure-with-data-driven-decisions/" target="_blank" rel="noreferrer noopener">data-driven decisions</a>. But what if you don’t have access to good data in the first place? This is especially challenging for iOS engineers given that most of that platform’s tools are closed source, making it especially tricky to customize them to your needs.</p>



<p>For example, when we first introduced Swift to our music app, a requirement that we set for ourselves was not to worsen the developer experience. One metric for that is build time: is adopting Swift slowing our Xcode build times down or speeding them up? And how do we accurately measure that (without using a stopwatch every time we hit run)?</p>



<h2>Our first solution: Parse the data from Xcode’s log files</h2>



<p>Whenever you run a build in Xcode, whether it’s a test build or a continuous integration build in production, xcodebuild produces a log file called xcactivitylog. Many developers don’t know that this file exists or that it’s useful for inspecting warnings, errors, and other data from past builds, like build times. So, over a year ago we developed and released an open source tool called <a href="https://github.com/spotify/XCLogParser" target="_blank" rel="noreferrer noopener">XCLogParser</a> — which parses those xcactivitylog files and makes all that build data more accessible to developers.</p>



<p>XCLogParser was created for a simple purpose: unearth the data buried in Xcode’s build logs and make it more human readable. But one piece of feedback we received from various teams after open sourcing XCLogParser is that it still requires substantial time to build the infrastructure for continuously collecting those build logs and maintaining them over time. </p>



<p>It was time for us to build a more full-featured tool — one that could integrate with a production environment composed of distributed teams, and provide better insights over time. A collector and a tracker, not just a parser. And that’s how XCMetrics was born.</p>



<h2>A complete solution: Collect, parse, store, track, repeat</h2>



<p>We’ve been developing and testing XCMetrics over the last year, building a whole suite of tools in order to create a complete solution for tracking Xcode build metrics. <strong>Since introducing this system at Spotify, the tools have been used to collect over one million builds and billions of compilation steps — producing over 10TB of data. </strong></p>



<p>With this amount of data, we’ve been able to answer complex questions for our developer teams, such as:</p>



<ul><li>Which function takes the longest to typecheck in our project every day? </li><li>Which pull requests introduce a specific warning or compilation failure?</li><li>How should we configure our engineers’ machines in order to maximize their productivity (hardware specs, installed software, etc.)?</li></ul>



<p>We’ve used these insights to improve the everyday experience and productivity of our developers, and we think other organizations will find these kinds of insights valuable, as well. So we are happy to open source XCMetrics with the world — we’re especially excited to see and learn from the insights other teams uncover.</p>



<h2>Architectural overview: Designed for scale and customization</h2>



<p>XCMetrics is an all-in-one tool that tracks Xcode build metrics for teams of all sizes. We built it with a flexible and extensible architecture in order to fit as many requirements as possible into its plugin system, allowing for customization of the information collected in every build. </p>



<p>XCMetrics is made up of the following components:</p>



<ul><li><strong>A Swift CLI tool</strong> that should be invoked in a post-scheme action after every build completes, whose task is to cache and upload build metrics.</li><li><strong>A backend service</strong> written in Swift receives the log and attaches metadata via a multipart request. The data can be parsed and saved synchronously or asynchronously.<ul><li>If the configuration specifies parsing logs asynchronously, they are enqueued for processing in a Redis instance.</li></ul></li><li><strong>A PostgreSQL database</strong> — once the log is parsed, the data for each build is inserted into the database, partitioned by day, for easy retrieval and historical analysis.</li></ul>



<figure><img loading="lazy" width="700" height="280" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-700x280.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-700x280.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-250x100.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-768x307.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-1536x615.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-120x48.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3.png 1837w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h2>Getting started: Which metrics do you want to track?</h2>



<p>We did our best to make XCMetrics as generic and customizable as possible. The only decisions you have to make are where the backend service should be deployed — and, more interestingly, what type of data you would like to collect.</p>



<h3>Standard metrics</h3>



<p>XCMetrics is distributed as an executable from our GitHub releases page. You can follow the <a href="https://xcmetrics.io/docs/getting-started.html" target="_blank" rel="noreferrer noopener">Getting Started guide</a> to learn how to get XCMetrics on your developer’s machine and execute it in a post-action scheme. Once that’s done, the default set of build metrics will be collected and uploaded to your service. You can check out the default set of collected metrics here.</p>



<h3>Custom metrics</h3>



<p>If you would like to collect even more metrics, you can wrap the XCMetrics Swift Package in your own package in order to invoke it manually. By doing so, you’ll be able to provide even more metrics to be attached to every build. Some examples are:</p>



<ul><li>Anonymized version control information to correlate build times with dirty checkout state</li><li>Thermal throttling of the machine that could affect build times</li><li>Project configuration information that could affect build metrics</li></ul>



<p>This is the minimal example of a XCMetrics plugin that collects the thermal throttling state of the machine and attaches it to each build.</p>



<figure><img loading="lazy" width="700" height="593" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-700x593.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-700x593.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-250x212.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-768x651.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-1536x1302.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-120x102.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2.png 1930w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>The main method forwards the arguments parsing to <em>XCMetrics</em>. You proceed to create a <em>XCMetricsConfiguration</em> and add <em>XCMetricsPlugin</em> to it. Each plugin takes a dictionary of the environment variables passed to the post-action scheme environment and returns a dictionary of the metrics to be collected. You would then distribute your own custom version of XCMetrics and execute it with the same arguments to upload the logs with the new metrics attached.</p>



<h3>Service deployment</h3>



<p>We provide a <a href="https://hub.docker.com/r/spotify/xcmetrics" target="_blank" rel="noreferrer noopener">Docker image</a> that has everything needed to deploy the XCMetrics backend in any infrastructure. We also support a one-click deployment to Google Cloud via Google Cloud Run. Our documentation also contains examples on how to deploy to Kubernetes, if you fancy that.</p>



<p>Needless to say, you don’t need a complex DevOps team to deploy and run XCMetrics. It’s made by iOS engineers, for iOS engineers, so simplicity is at its heart.</p>



<h2>Using XCMetrics at Spotify</h2>



<p>XCMetrics has been in use in production at Spotify for over one year, and it has allowed us to make more informed decisions in regards to our project structure and investments. We have data pipelines and dashboards that are used every day to monitor the state of our codebase and tools. </p>



<figure><img loading="lazy" width="700" height="424" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-700x424.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-700x424.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-250x151.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-768x465.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-1536x930.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-120x73.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4.png 1792w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>Figures above are for illustrative purposes only.</figcaption></figure>



<p>We hope XCMetrics will inspire and help other teams keep track of their build metrics and improve their developer experience.</p>



<p>You can learn more and watch a demo at <a href="https://xcmetrics.io">XCMetrics.io</a>. We are happy to receive bug fixes and improvements on <a href="https://github.com/spotify/XCMetrics/">GitHub</a>. And make sure to check out our <a href="https://github.com/spotify/XCMetrics/blob/master/CONTRIBUTING.md">contribution guide</a>, which explains more advanced concepts of the project.</p>











<p><em>Xcode is a trademark of Apple Inc., registered in the U.S. and other countries.</em></p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Patrick Balestra, Sr. Engineer</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            How We Built It: Spotify Lite, One Year Later&#xA;</title>
      <link>https://engineering.atspotify.com/2020/12/03/how-we-built-it-spotify-lite-one-year-later/</link>
      <description>What if, for some users, the very best Spotify is a little less Spotify? Spotify Lite started as an experiment that had to be proven, both from a technical and a product-market fit perspective. In 2017, we found that a significant portion of registrations in some of our fastest-growing markets were</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>December 3, 2020</span>
                <span>
                    Published by Erik Ghonyan (Senior Engineer), Slava Savitskiy (Senior Engineer), and Tommy Tynjä (Engineering Manager)                </span>
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/12/03/how-we-built-it-spotify-lite-one-year-later/" title="How We Built It: Spotify Lite, One Year Later">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/12/Spotify-Lite_B.png"/>                    </a>
                        
        </p>

        

        
<p>What if, for some users, the very best Spotify is a little less Spotify? Spotify Lite started as an experiment that had to be proven, both from a technical and a product-market fit perspective. In 2017, we found that a significant portion of registrations in some of our fastest-growing markets were happening on Android devices much older than what we were used to seeing in North American and European markets. Because of storage constraints, many of our potential users couldn’t install Spotify, and the ones that could weren’t getting the full “Spotify experience”.</p>



<p>Our mission was clear: we needed to make Spotify accessible to users with constrained resources, i.e., unreliable networks or phones with limited storage, memory, and low-resolution screens. Now we needed a team.</p>



<h2>A flexible, Lite team</h2>



<p>The main Spotify Music Android client is divided into multiple features, all owned by separate teams. But for Spotify Lite, we formed a single, autonomous team to fully own the entire process of designing, developing, and releasing the app. This allowed us to roll out an MVP product in record time.</p>



<p>Before building the new app, the Spotify Lite team — a cross-functional mix of insights, design, product, and engineering — travelled to a number of locations where Lite would be available in order to experience the network and device constraints firsthand. It was absolutely critical to design Spotify Lite with our users in mind, and to experiment and iterate on the streaming experience for cases when devices have poor connectivity or are completely offline. Only then were we able to come up with an optimal, performant solution.</p>



<p>It should be noted that we wouldn’t have achieved success had it not been for the existing tooling that we were able to reuse — tools for enabling recommendations, playback, search, browsing, and instrumentation. We were building on all the work, experience, and knowledge that came before us, giving us the ability to focus on finding solutions for our users.</p>



<h2>Spotify Lite: Spotify’s first separate app</h2>



<p>Creating a more performant and smaller version of the Spotify app proved to be more challenging than we liked, as the codebase hadn’t been modularized. With these challenges in mind, we decided to build a new separate app from scratch, giving us the ability to quickly iterate, obtain feedback, and innovate freely.</p>



<p>Spotify Lite was initially built on an entirely different playback stack than the regular Android app. This allowed Lite to be as small as possible, with minimal memory and network data usage. Having a separate app enabled us to test new performance ideas and to gain insights, such as understanding how application size impacts the new user funnel. We no longer use the initial playback stack, and have evolved towards a tailored setup that guarantees stability and playback quality on unreliable networks.</p>



<p>Building Lite was a lot like packing a backpack for your travels. With limited space, you have to be selective in what you bring. Only the most crucial and necessary components were carried forward.</p>



<h2>A balancing act</h2>



<p>Shrinking the original Spotify app to create Spotify Lite brought up two crucial questions: What key elements of the original Spotify should remain intact to ensure listeners still get the “Spotify experience”? And what sacrifices do we need to make to ensure Spotify Lite does, in fact, remain light? </p>



<p>In answering the first question, we knew that keeping the brand look and feel was absolutely critical to giving listeners a Spotify they could recognize. So, we used the same design philosophy as the original Spotify Android app. However, given a range of constraints (smaller screens, quick/performant interactions), we had to adapt some of our design choices. For example, information density has to be reviewed with smaller screen sizes and lower resolutions in mind, as well as whether information is still readable on a broken or scratched screen (these phones have been around for a while!). We’ve recently added our heuristics for how to design for these constraints to the overall design strategy so that this is kept in mind for other apps and surfaces, as well.</p>



<figure><img loading="lazy" width="700" height="444" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-700x444.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-700x444.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-250x159.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-768x487.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-1536x975.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-2048x1300.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-120x76.png 120w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Along with streamlining the design, we also had to shrink things down under the hood. The team put a lot of work into implementing all the known techniques for binary size reduction, as well as making tradeoffs when selecting features. As the app includes a native shared library for playback, we have experimented with many compiler and linker flags to prioritize a small app size. This includes, for instance, switching to the <em>lld</em> linker, employing link time code optimizations, and disabling certain language features like RTTI. </p>



<p>On the Android side, it was the use of App Bundles for application publishing, optimizing our R8 shrinking, carefully choosing dependency libraries, and stripping unused translations. We made an effort to reduce the install size, too. We store the shared library unpacked in the APK without copying it to the install folder, and allow users to store both the app and its cache and downloads separately on the SD card.</p>



<p>After the initial larger gains, it became harder and harder to reduce the app size. It was a constant balance between keeping it small while adding additional requested features. Along with monitoring the app download and install sizes in the Google Play Store, we added checks to our continuous delivery pipelines to prevent size bloat.</p>



<h2>Lite is different</h2>



<p>Because Lite was a brand-new concept, some of our work went beyond the app itself, leading to improvements to Spotify systems that other teams could benefit from, too.</p>



<p>Before Lite, developers could safely assume there was only one Spotify app for any given platform — the Android platform and the Android app were considered one and the same. Backend services — including those providing application views and deciding which features are enabled — were built with that assumption in mind. Some of these assumptions cascaded through many different parts of our internal systems. </p>



<p>When we added Lite to the mix, developers needed to know exactly which app a user was using, not just what platform they were on. We generalized that issue beyond our own app and built ways to identify all the apps in the Spotify ecosystem. That work paid off again each time anyone introduced a new Spotify app to the Android platform, including our sister apps <a rel="noreferrer noopener" href="https://www.spotify.com/us/kids/?utm_source=us-en_brand_contextual_text&amp;utm_medium=paidsearch&amp;utm_campaign=alwayson_ucanz_us_premiumbusiness_kids_brand+contextual-desktop+text+exact+us-en+google&amp;gclid=CjwKCAiA8Jf-BRB-EiwAWDtEGnamKsxw1Yx_w3KgzFDyJ1g4NKVvIUkc9jRA8fBFdlHCkR8pD4iHmBoCMLAQAvD_BwE&amp;gclsrc=aw.ds" target="_blank">Spotify Kids</a>, <a rel="noreferrer noopener" href="https://www.spotify.com/us/stations/" target="_blank">Spotify Stations</a>, and <a href="https://spotify-everywhere.com/collections/car-audio/products/polestar" target="_blank" rel="noreferrer noopener">Android Automotive</a>.</p>



<p>We also had to redesign parts of Spotify’s playback library with Lite constraints in mind — taking into account smaller download and installation sizes, memory usage, and the reduced feature set. Similar considerations have been applied to Spotify’s music and image transcoding services.</p>



<h2>Making Lite a big deal</h2>



<p>Our ambition is to be the best-in-class Lite app. We are constantly modifying and updating the app to adapt to our users and their ever-evolving needs. As we’ve seen positive adoption of Spotify Lite since launch, we’ve invested in performance improvements, quality, and resilience. We recently rolled out an overhaul of our client architecture to cater to our growing user base and to reduce playback latencies.</p>



<p>The birth of Spotify Lite has given us flexible solutions that our other apps have benefited from. One such example is our backend service that scales down images to use less network traffic. Another is the support for App Bundles, which has allowed us to reduce the app size significantly so that users only download the assets needed for their particular device. Creating a separate app was a first for our build system — one that laid the groundwork for building native dependencies, sharing code components, and setting up crash and ANR reporting for tracking app quality.</p>



<p>We are continuing our work to lower the barrier for people to access Spotify. We have our backlog full of ideas and performance improvements we want to keep investing in, not only for Lite but also for our other apps to benefit from.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            It’s All Just Wiggly Air: Building Infrastructure to Support Audio Research&#xA;</title>
      <link>https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/</link>
      <description>TL;DR We just open sourced Klio — our framework for building smarter data pipelines for audio and other media processing. Based on Python and Apache Beam, Klio helps our teams process Spotify’s massive catalog of music and podcasts, faster and more efficiently. We think Klio’s ease of use — and its</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 4, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/" title="It’s All Just Wiggly Air: Building Infrastructure to Support Audio Research">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio-Blog2.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/10/Klio-Blog2.gif"/>                    </a>
                        
        </p>

        

        
<p><strong>TL;DR </strong><a href="https://venturebeat.com/2020/10/13/spotify-open-sources-klio-a-framework-for-ai-audio-research/" target="_blank" rel="noreferrer noopener">We just open sourced Klio</a> — our framework for building smarter data pipelines for audio and other media processing. Based on Python and Apache Beam, <a href="https://klio.io" target="_blank" rel="noreferrer noopener">Klio</a> helps our teams process Spotify’s massive catalog of music and podcasts, faster and more efficiently. We think Klio’s <a href="https://docs.klio.io" target="_blank" rel="noreferrer noopener">ease of use</a> — and its ability to let anyone leverage modern cloud infrastructure and tooling — has the potential to unlock new possibilities in media and ML research everywhere, from big tech companies to universities and libraries. </p>



<p>But now we’re getting ahead of ourselves. What exactly is Klio and what does it do? Let’s start with the problem of audio itself.</p>



<h2>Audio is hard </h2>



<p>Really, sound is just wiggly air. At a basic level, every violin concerto, love song, dog bark, and knock-knock joke is the result of air compressing and vibrating, which we sense as it moves bones and hair in our ears. Sound is an invisible force that reaches us in ways that we can’t see, but can feel. And that’s what also makes audio so difficult for machines to parse: Humans can tell the difference between a swooning vocal, a danceable beat, and a buzzing bee. Can we teach machines to hear those differences, too? </p>



<p>Machine listening, the field of research focused on getting computers to understand audio, combines expertise and methods from signal processing, music information retrieval, and machine learning — so that all those vibrations in the air result in data that makes a bit more sense for an engineer to work with. When encoded, compressed, and stored on a computer, you’re left with ones and zeroes packed into relatively large binary files. At a glance, a guitar solo can look just like a yodel. So, how do we begin to make sense of it all? And at scale?</p>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_01-Which-is-which-01-1.gif" alt="" width="580" height="525"/><figcaption><em>One is a </em><a href="https://open.spotify.com/show/71mvGXupfKcmO6jlmOJQTP?si=6vu_5jg7TUaxQmySGfzliA" target="_blank" rel="noreferrer noopener"><em>popular podcast</em></a><em>, one is </em><a href="https://open.spotify.com/track/3Zb3SXZdtyNA0Cdq0DWLeC" target="_blank" rel="noreferrer noopener"><em>acoustic guitar</em></a><em>. It’s all just wiggly air. Software can help process the audio — identify voices, find beats per minute, analyze frequencies. But all at once? And 60+ million tracks at a time?</em></figcaption></figure></div>



<h2>One problem multiplied 60 million times</h2>



<p>Processing massive amounts of large binary files: It was a problem that was only getting bigger at Spotify. We’re adding about 40,000 songs a day and are processing our music catalog — about 60 million songs — on a regular basis, with multiple teams around the world doing work at the same time. Besides the problem of engineering that kind of scale and parallelization, we also wanted a way to tie the processing jobs more closely with the work our audio and ML research teams were doing.</p>



<p>We were already building sophisticated data pipelines that supported AI and ML jobs using <a href="https://spotify.github.io/scio" target="_blank" rel="noreferrer noopener">Scio</a>, a precursor to Klio. Scio proved to be a flexible, scalable framework that any team could use to <a href="https://engineering.atspotify.com/tag/scio/" target="_blank" rel="noreferrer noopener">build smarter data pipelines at scale</a>. By tying together large database queries, map-filter-reduce operations, natural language processing, and ML models, teams could create better, more personalized playlists, like Discover Weekly, Release Radar, and dozens of others. </p>



<p>So, Scio created a platform for processing massive amounts of data about the audio. But what about processing the audio itself? </p>



<h2>A uniquely Spotify problem, a uniquely Spotify solution</h2>



<p>While processing metadata for the libraries of 299+ million users is impressive, it’s not the same as processing the content itself — those tens of millions of binary audio files that Spotify hosts and serves all over the world. On top of that, Java-based languages weren’t interfacing well with our Python-based research tools for audio and ML.</p>



<p>We knew that if we could build data pipelines that supported large-scale audio processing, there were untold features and personalizations waiting to be unlocked. We just needed a framework that supported it — and that worked as well with our research tools as our engineering tools. </p>



<p>In 2019, an ad hoc team of data engineers, ML researchers, and audio experts outlined the requirements for creating a framework designed especially for processing media. Scio was a model of success, but still just a starting point. This new framework would need to support:</p>



<ul><li><strong>Large-file input/output: </strong>We wanted to transform audio, videos, images — all kinds of heavy-duty binary media files — in dozens of ways, with both streaming and batch processing.</li></ul>







<ul><li><strong>Scalability, reproducibility, efficiency: </strong>When you’re working with a dataset as large as the world’s music, as well as a burgeoning ecosystem of podcasts, you don’t want to have to redo your work over and over again.</li></ul>







<ul><li><strong>Closer collaboration between researchers and engineers:</strong> This translated into support for both Python (the lingua franca of both audio processing and ML) as well as non-Python dependencies (e.g., libsndfile, ffmpeg, etc.).</li></ul>



<p>In short, we needed a framework that could production-ize audio processing. This wasn’t just about creating data pipelines for media. It was about doing it at Spotify scale and with support for the latest audio and ML research. Let’s dig into that last requirement first.</p>



<h2>Researchers, engineers, and Python: The importance of speaking a common language</h2>



<p>Around this time, we noticed that both our researchers and engineers were beginning to get a little tired of the roadblocks preventing their audio work from getting adopted. Audio researchers were making promising breakthroughs, but the cost of getting new approaches integrated into shipping products was becoming increasingly high. </p>



<p>As much as their counterparts in data and ML engineering wanted to help, those engineers were spending much of their time looking after several distinct, bespoke systems for production audio processing, all built and customized for individual teams. In other words, we had smart people all over the company working on audio, but our <a href="https://research.atspotify.com" target="_blank" rel="noreferrer noopener">world-class researchers</a> and engineers couldn’t work together, until most of the research was rewritten by the engineers. And even then, all that work and effort was siloed.</p>



<p>The solution was simple: Python. It’s the native language of research and well-suited for the engineering problems at hand. Most importantly, allowing everyone to speak without a translation layer puts everyone in a position to focus on what they excel at. Audio and ML researchers get to focus on experimentation and building cutting-edge research tools. Engineers get to focus on building clean, reliable code.</p>



<h2>What is Klio?</h2>



<div><figure><img loading="lazy" width="2409" height="1868" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_02-What-is-02-B.gif" alt=""/></figure></div>



<p>Klio is a framework for building smarter data pipelines for audio and other binary files, enabling you to production-ize media processing at scale.</p>



<ul><li><strong>Streamlined Apache Beam</strong> for a more ergonomic, Python-native experience for researchers and engineers</li></ul>



<ul><li><strong>Open graph of job dependencies</strong> with support for top-down and bottom-up executions</li></ul>



<ul><li><strong>Integration with cloud processing engines</strong> for managed resources and autoscaling production pipelines</li></ul>



<ul><li><strong>Containerization of custom dependencies</strong> for simplified development and easily  reproducible deployment</li></ul>



<ul><li><strong>Batch and streaming pipelines</strong> for continuous processing</li></ul>



<h2>Apache Beam under the hood, Klio in the driver’s seat</h2>



<p>It’s no surprise then that Klio is built on top of <a href="https://beam.apache.org/" target="_blank" rel="noreferrer noopener">Apache Beam</a> for Python, while also aiming to be a more Pythonic experience of Beam. Additionally, Klio offers several advantages over traditional Python Beam for media processing — providing a substantial reduction in boilerplate code (an average of 60%), a focus on heavy file I/O, and standards for connecting multiple streaming jobs together in a jobs dependency graph (with top-down and bottom-up execution). This allows teams to immediately focus on writing new pipelines, with the knowledge that they can easily be extended and connected later. </p>



<p>This ease of use and streamlining of Apache Beam means we can get our state-of-the-art audio research into people’s hands and ears, faster. And while Klio offers this more opinionated way to use Apache Beam for common media processing use cases by default, it also allows the use of core Python Beam at any time if Klio’s opinions don’t fit your use case.</p>



<h2>Efficiency, <s>efficiency,</s> <s>efficiency</s> (DRY: Don’t Repeat Yourself)</h2>



<p>When we were developing Klio, we decided to test it by downsampling every track in Spotify’s 60-million song catalog — amounting to well over 100 million audio files in all (including multiple releases of the same song). Downsampling is often the first step of audio analysis, so it’s a great benchmark of what real-world performance might look like. Previously, the fastest we had accomplished this at Spotify was about three or four weeks. With Klio, we did it in six days, and reduced costs by four times. When you think about the number of songs in our catalog, and our quickly growing podcast library, Klio can have a tremendous impact on our teams and our business.</p>



<div><figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-700x409.png" alt="" width="580" height="338" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-700x409.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-250x146.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-768x449.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-1536x898.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-2048x1197.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-120x70.png 120w" sizes="(max-width: 580px) 100vw, 580px"/><figcaption><em>With Klio’s streamlined framework, pipelines are more efficient and reliable. We can do in days what took weeks. And since jobs don’t have to be repeated (missing dependencies can be recursively created), you don’t have to run files through the whole pipeline again just to apply one more transformation at the end. </em></figcaption></figure></div>



<p>You’ll find these kinds of optimizations throughout Klio’s implementation. Klio pipelines improve processing time and costs by avoiding duplicate work on already processed audio. And the framework is opinionated — encouraging engineers and researchers to write a pipeline focused on one thing, like finding the timestamps of all the beats to a song or measuring a song’s loudness. By creating reusable building blocks, Klio allows for researchers to build more easily on top of previous research and create graphs of pipelines, leading to features like infinite playlists optimized for your current mood, internal tools that help automate the review of new content, and powerful data that personalizes the Spotify experience for each user.</p>



<h2>Scale, reproducibility, and clouds. No infra team required.</h2>



<p>Klio can be run locally, but it really shines in the cloud — and is ready-made for it. In order to achieve the large-scale processing and reproducibility that we require at Spotify, Klio leverages the best parts of modern cloud infrastructures (like managed resources to autoscale production pipelines) and tooling (like containerization for easier deployments).</p>



<p>Klio was designed to be cloud agnostic, and the underlying Apache Beam project is designed to run workloads across any data workflow engine. Right now, it’s configured to work with Google Cloud Platform, but we welcome <a href="https://docs.klio.io/en/latest/contributors.html" target="_blank" rel="noreferrer noopener">contributions</a> to help get Klio running on AWS, Azure, or another infrastructure. </p>



<p>One thing to note: Current limitations to Beam Python prevent all of its features from being used on every engine, but we expect increased compatibility with Apache Flink and Apache Spark as Apache Beam extends its underlying compatibility with these engines. Preliminary work has also been done testing Klio on Amazon AWS and S3 using Klio’s Direct Runner.</p>



<p>We think this cloud integration (infrastructure as a service) can unlock production bottlenecks, as well as encourage experimentation. Engineering teams can rely on Klio to standardize media processing — using data processing and monitoring tools they’re already familiar with — rather than creating architectures from the ground up. Klio’s ability to autoscale production pipelines to handle variable workloads lets engineers focus on the next thing, rather than constantly tuning workloads.</p>



<h2>From Sing Along to dolphin songs: Open and the great unknown</h2>



<p>Klio began as a proof of concept a little less than two years ago. It was invented out of necessity — to overcome challenges we were facing internally. But even from the very beginning, it was built with the intention of being free and open source software. </p>



<p>As we’ve seen with <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a>, our open platform for building developer portals, <a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" target="_blank" rel="noreferrer noopener">Spotify is committed to open source and developer experience</a>. We want to make the lives of engineers easier, so they can focus on building amazing things. So we’re excited to see not only how Klio can help others and advance audio/media research, but also what we can learn from others’ contributions and how Klio can evolve as a result. </p>



<p>Before and after Klio, Spotify has been doing this kind of large-scale audio analysis for nearly a decade, extracting and transforming tracks in our catalog on a weekly, daily, and streaming basis. Audio analysis algorithms power our <a href="https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/" target="_blank" rel="noreferrer noopener">Audio Features API</a> for fingerprinting songs by their unique attributes (illustrated in this interactive <a href="https://www.nytimes.com/interactive/2018/08/09/opinion/do-songs-of-the-summer-sound-the-same.html" target="_blank" rel="noreferrer noopener">New York Times</a> article), in-house tools, like our automated content review screener; and market-specific features, like our Sing Along feature in Japan — which <a href="https://research.atspotify.com/making-sense-of-music-by-extracting-and-analyzing-individual-instruments-in-a-song/" target="_blank" rel="noreferrer noopener">separates the vocals from the instruments</a> as songs are uploaded to the catalog to create interactive versions that people can sing along with.</p>



<p>But as we saw when we open sourced Backstage, the open source community will come up with use cases we never dreamed of. And since Klio enables anyone to do this kind of heavy-duty media processing at scale (not just big tech companies), we’re particularly curious to see what academics and research institutions will build with it. (<a href="https://twitter.com/tomncooper/status/1316071741131759617" target="_blank" rel="noreferrer noopener">Dolphin speech, anyone</a>?)</p>



<p>So, thank you to the Klio team and to everyone who’s ever used Klio or contributed to its development over the years (including its sibling framework, Scio). And thank you to all those reading this right now and who will contribute to its development in the future. It’s a product that only Spotify could have built. But we’re even more proud now that it’s out there for the world to share. Now let’s <a href="https://docs.klio.io" target="_blank" rel="noreferrer noopener">get started</a>.</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by David Riordan and Lynn Root</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio-Blog2.gif" length="0" type="image/gif"></enclosure>
      <pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify’s New Experimentation Platform (Part 2)&#xA;</title>
      <link>https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/</link>
      <description>So you’ve read Part I of our two-part series about the new Experimentation Platform we’ve built at Spotify, and now know why we decided to invest in a new platform. In Part II, you’ll get a more detailed look at how we assigned users to experiments, how we analyze results and ensure test integrity.</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>November 2, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/" title="Spotify’s New Experimentation Platform (Part 2)">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/11/Experimentation-Platform_Part-II_A.png"/>                    </a>
                        
        </p>

        

        
<p>So you’ve read <a href="https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/" target="_blank" rel="noreferrer noopener">Part I</a> of our two-part series about the new Experimentation Platform we’ve built at Spotify, and now know why we decided to invest in a new platform. In Part II, you’ll get a more detailed look at how we assigned users to experiments, how we analyze results and ensure test integrity. </p>



<h2>Coordination, holdbacks, and exclusivity</h2>



<p>A lot of the experiments we run change some small aspect of the user experience in one of our prime surfaces and it’s important for teams to be aware of what other experiments are running at the same time, as well as what other experiments are running in their field of interest.</p>



<p>To accommodate for this, we allow experiments to be put into a “domain”. Domains roughly map different surfaces or systems in our service. Each domain has a timeline that shows what experiments have been running and what’s upcoming.</p>



<div><figure><img loading="lazy" width="700" height="486" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-700x486.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-700x486.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-250x174.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-768x533.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-1536x1067.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-120x83.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption>For illustrative purposes only.</figcaption></figure></div>



<p>When a lot of teams experiment in the same proximity, there’s risk of interaction effects. For this reason many experiments need to run in an exclusive manner, where a user can only be in one of a set of experiments that can potentially impact each other. Currently only experiments in a single domain can be exclusive to each other. We’re planning to decouple exclusivity from the domain concept, to allow for experiments across domains to also be exclusive to each other. </p>



<p>We implement holdbacks (the practice of exempting a set of users from experiments and new features, in order to see long-term effects and combined evaluation) in domains. Each domain can have a set of holdbacks. Users in these holdbacks are exempt from the general experimentation that happens in the domain. </p>



<p>At Spotify we have established a pattern where at the start of a quarter, we create a new holdback. Experiments that run throughout the quarter will never be assigned to any of those users subject to the holdback. When the quarter ends, a single test is run on these users where the combined experience of all (successful) experiments is given to the treatment group. This way we can get a read for the compound effect of everything the team decided to ship during the quarter. Once this test is done, the holdback is released and these users will go into new experiments. </p>



<h2>The Salt Machine</h2>



<p>At Spotify, autonomous are teams free to move at schedules that fit them best. This means that they need to be able to start and stop experiments at any time. With requirements of exclusivity and holdbacks, assigning users to experiments gets quite complex if we do not want to compromise on randomization (and we do not want to).</p>



<p>We have developed something we call the “salt machine” that automatically reshuffles users without the need to stop all experiments. This is done by hashing users into buckets using a tree of “salts” (it’s worth noting that if two experiments are disjoint because of targeting, we do not have to use the same salt tree for them).</p>



<p>For this article, imagine that we split users into 8 buckets:</p>



<div><figure><img loading="lazy" width="700" height="126" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-700x126.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-700x126.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-250x45.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-768x138.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-1536x276.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-120x22.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>A user ends up in bucket 1 if HASH(user id, SALT) % 8 = 1 and so forth. We allocate buckets to experiments. In the image below, experiment E1 has been allocated buckets 0 and 1. Note that we also have a per-experiment salt to spread users from the allocated buckets over treatments, but for simplicity we omit that from the images in this article. </p>



<figure><img loading="lazy" width="700" height="146" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-700x146.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-700x146.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-250x52.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-768x160.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-1536x320.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-120x25.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Since we’re experimenting a lot, most buckets are always allocated to an experiment. So what happens when two experiments (E1and E1) end, releasing some space that can be allocated to a new experiment?</p>



<figure><img loading="lazy" width="700" height="289" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-700x289.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-700x289.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-250x103.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-768x317.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-1536x635.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-120x50.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Now 50% of the buckets are free and we want to start E4 that needs 25% of the population.  How can we allocate buckets safely without jeopardizing randomization? If we were to pick only bucket 0 and 1 we would have a 100% overlap with experiment E1 that just ended, which might lead to biased results due to carryover effects. Not good.</p>



<figure><img loading="lazy" width="700" height="122" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-700x122.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-700x122.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-250x43.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-768x133.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-1536x267.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-120x21.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>What if we shuffled the free users into new buckets using a new salt?</p>



<figure><img loading="lazy" width="700" height="279" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-700x279.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-700x279.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-250x100.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-768x306.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-1536x612.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-120x48.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>Now we have eight free buckets we can use for experiments. But because of the dilution of bucket size (those new eight buckets only get 50% of the traffic), we need to allocate four of them to E4 to get 25% of the population. We call the amount of required overallocation of buckets the “compensation factor” — and in this case it’s 1/.50 = 2. The remaining four buckets can be allocated to some other experiment.</p>



<figure><img loading="lazy" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-700x302.png" alt="" width="579" height="250" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-700x302.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-250x108.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-768x332.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-1536x663.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-120x52.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8.png 1600w" sizes="(max-width: 579px) 100vw, 579px"/></figure>



<p>When experiment E3 ends we can completely get rid of salt 1 — but because we diluted the buckets the released space cannot be used until E4 finishes. In effect, we’re wasting 50% of the users. </p>



<p>The compensation factor changes all the time as experiments start and end. Over time we have learned that it’s good practice to not start new experiments if the compensation factor is higher than 5 (the higher the compensation factor, the more space is being wasted).</p>



<p>We’re currently working on the second iteration of our allocation scheme where we believe we waste less space but still maintain the benefits of randomization.</p>



<h2>Analysis</h2>



<p>To conduct a well-designed experiment we need to decide up front what we want to measure and test. The Experiment Planner asks that all necessary information be specified when an experiment is created.  A metric can have one of two roles in an experiment:</p>



<ul><li>Success metrics to find evidence for the hypothesis.</li><li>Guardrail metrics to find evidence that the experiment is not introducing any harmful side effects. </li></ul>



<p>For each success metric it is possible to choose either a one- or two-sided statistical test. </p>



<figure><img loading="lazy" width="700" height="417" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-700x417.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-700x417.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-250x149.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-768x457.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-1536x914.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-120x71.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>For success metrics, we perform superiority tests and require a relative minimum detectable effect (MDE) to be specified. This is used in power calculations in the result analysis and also in the sample size calculator. </p>



<p>For guardrails, we perform a non-inferiority test where a non-inferiority margin has to be specified so we know when a change is considered non-inferior or not. </p>



<figure><img loading="lazy" width="700" height="243" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-700x243.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-700x243.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-250x87.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-768x266.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-1536x533.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-120x42.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>If the experimenter wants to see results as the experiment is running, they need to choose sequential testing. If they decide to do a fixed horizon test, the results will only be available once the experiment is stopped. Regardless, to minimize weekday biases we recommend that tests are always run for the planned period and are only stopped early if harmful side effects are detected. We also have an optional (but highly recommended) gradual ramp-up assignment of the experiment over a time period to further minimize possible weekday effects. </p>



<p>With a potentially large number of metrics, targeting, different statistical tests, and many treatment groups, it’s not always easy to calculate an accurate required sample size. For this reason we’ve built a sample size calculator and put it into the platform (it’s optional to use for fixed horizon tests, but required for sequential testing).  </p>



<figure><img loading="lazy" width="700" height="266" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-700x266.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-700x266.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-250x95.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-768x291.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-1536x583.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-120x46.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>As can be seen above (numbers redacted), the sample size calculator shows how many users are needed to power the metric for the specified target population. The calculator automatically queries historical data for the specified target population to proxy the control group average and variance needed for the calculations.</p>



<h2>Validity checks</h2>



<p>Many things can go wrong when we run an experiment, and even subtle issues can have a big impact on the result. For this reason we’re continuously monitoring all running experiments for potential problems. If a problem is detected, we notify the owning team so that they can decide what to do.</p>



<p>We have the following checks in place:</p>



<ul><li><strong>Sample ratio mismatch:</strong> We make sure that the targeted proportion between the treatment groups align with exposure. If we see a statistically significant difference, we sound the alarm.</li><li><strong>Pre-exposure activity: </strong>We see if there’s any difference in activity between the groups prior to the experiment starting. </li><li><strong>Increases in crashes:</strong> We ensure that we do not see an increase in client crashes.</li><li><strong>Property collisions:</strong> If two experiments use the same Remote Configuration properties (and are not exclusive to each other), we will warn that the experiments might not get the exposure that was expected. </li></ul>



<p>For checks that require a statistical test, we deploy sequential testing and correct for multiple comparisons. </p>



<h2>Rollouts</h2>



<p>A use case supported by the Experimentation Platform, in addition to experimentation, is gradual rollouts. Once we learn that our change improves the user experience, we want to ship it, and with gradual rollouts we can do that while protecting against unexpected regressions. </p>



<p>There are two ways of doing rollouts: with or without statistical testing. If we select the latter, we will be able to select a set of guardrail metrics and deploy sequential testing so we can continuously monitor the progress. Every day we also provide one of three recommendations to the owning team:</p>



<ul><li>We cannot detect any harmful effects, so the recommendation is to continue the rollout.</li><li>We have statistical evidence of harmful effects, so we recommend aborting the rollout.</li><li>We do not know yet, and we recommend continuing with caution or wait until we have more data.</li></ul>



<p>The ability to get metrics for rollouts is fairly new so we’re still iterating on it, but we plan to make it the default option going forward.</p>



<h2>Summary</h2>



<p>We have spent the last two years rebuilding our experimentation capabilities at Spotify. The new platform is a step change in ease of use and capabilities, but we still feel it’s early for experimentation at Spotify.</p>



<p>We are constantly evolving our Experimentation Platform and practices. If you would like to know more, or if you’re interested in joining the team and contribute to our journey, do not hesitate to reach out.</p>



<p>Johan Rydberg, <a href="mailto:jrydberg@spotify.com" target="_blank" rel="noreferrer noopener">jrydberg@spotify.com</a> / @datamishap<br/>Experimentation Lead</p>
        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Johan Rydberg</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Spotify’s New Experimentation Platform (Part 1)&#xA;</title>
      <link>https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/</link>
      <description>At Spotify we try to be as scientific as possible about how we build our products. Teams generate hypotheses that we test by running experiments — normally in the form of an A/B test — to learn what works and what doesn’t. The learnings give us insights and fuel new product ideas. Want to know wh</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>October 29, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/" title="Spotify’s New Experimentation Platform (Part 1)">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png 4209w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-120x60.png 120w" sizes="(max-width: 4209px) 100vw, 4209px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/10/Experimentation-Platform_01.png"/>                    </a>
                        
        </p>

        

        
<p>At Spotify we try to be as scientific as possible about how we build our products. Teams generate hypotheses that we test by running experiments — normally in the form of an A/B test — to learn what works and what doesn’t. The learnings give us insights and fuel new product ideas.</p>



<p>Want to know why Spotify decided to build a new Experimentation Platform and how it works? In this two-part series, we’ll share what led us to throw out our old A/B testing platform (called ABBA) and details around the new architecture we’ve chosen to substitute it.</p>



<h2>Early days</h2>



<p>Today almost all product decisions are made with some input from one or more A/B tests. But it hasn’t always been like that. Back when Spotify was a small startup in Sweden, a team, simply called Analytics, played around with various kinds of tests. </p>



<p>Over time, interest in A/B testing grew, and in 2013 we decided to spin up a team to take on building a more robust system. Thus was born <a href="https://open.spotify.com/track/2PzCOP5Aj9SABiBgNEZ52G" target="_blank" rel="noreferrer noopener">ABBA</a>, an A/B testing system that allowed us to (more) easily run experiments. Now we had a place to see what A/B tests were actually running, and a pipeline that computed results. The introduction of the system was a step change in productivity, and over time it was integrated into pretty much every aspect of Spotify — in our desktop clients and mobile clients, backend services and data pipelines, in-app messaging, and email campaigns. </p>



<p>ABBA as a system was quite simple. Each experiment (or rollout) mapped one to one to a feature flag, named after the experiment. When a client fetched the value of the feature flag it got back the name of the treatment group — e.g., “Control” or “Enabled” or “Sort according to color” — anything the user decided to name the group. (Fun trivia: some users of ABBA started encoding more elaborate configurations as JSON in the group names. Life finds a way). Every time a feature flag value was resolved, an event was logged, which fed into the exposure and results pipelines. For each experiment, only a small number of metrics were calculated. Many of these metrics were not very sensitive, leading to almost all analysis being performed manually in notebooks.</p>



<h2>Why we decided to build a new system</h2>



<p>Around 2017, the system began to reveal its limitations. We had a few big projects that required a lot of experimentation, and the sentiment at the company was that the system needed to improve.</p>



<p>At a hack week in late 2017, a few senior engineers gathered to sketch out a new system, which aimed to address the following challenges (as well as some others):</p>



<ul><li><strong>Reduce Time:</strong> The 1-1 mapping between an experiment and the feature flag led to some interesting side effects. If there were a problem with the experiment (and often there were) and it needed to be restarted, we simply couldn’t just … restart it. A new experiment would have to be created, and the software would need to be updated to use the new feature flag. The new system would have to reduce the time it took to complete this cycle.</li></ul>



<ul><li><strong>Produce less events</strong>: The volume of events that were logged by the A/B testing system had over time grown to almost 25% of our total event volume. This drove up the cost of processing, and the volume of events caused incidents in the event delivery system.</li></ul>



<ul><li><strong>Improved analysis:</strong> The metrics that ABBA provided out of the box were no way near enough for our analysis needs, and our data scientists were getting tired of performing analyses in notebooks. It was time consuming, and we also didn’t have any consistency across the company when it came to how experiments were analyzed. The new system would have to allow us to add custom metrics and we needed a solid analysis methodology.</li></ul>



<ul><li><strong>Sophisticated coordination:</strong> Over time our needs for how we allocated users to experiments changed, which was done manually by coordinating bucket ranges between teams. This was of course error prone — if someone ended up using the wrong buckets a whole slew of experiments would be impacted; the new system would have to address this.</li></ul>



<h2>The Experimentation Platform</h2>



<p>The new experimentation system, dubbed “The Experimentation Platform”, is composed of three parts:</p>



<ol><li><strong>Remote Configuration</strong> – replaces our feature-flagging service. Instead of “flags”, its model is based on “properties” — a configurable aspect of one of our clients or backend services. An example of a property could be the color of our buttons, or the number of tracks in the top list. </li></ol>



<ol start="2"><li><strong>Metrics Catalog</strong> – a managed environment for running SQL pipelines to ingest metrics into a data warehouse, from where data can be served with sub-second latency to UIs and notebooks. </li></ol>



<ol start="3"><li><strong>Experiment Planner</strong> – manages and orchestrates experiments. This is the part of the platform users interact with when they want to run an experiment.</li></ol>



<div><figure><img loading="lazy" width="700" height="505" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-700x505.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-700x505.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-250x180.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-768x554.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-1536x1109.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-120x87.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<h2>Remote Configuration</h2>



<p>Remote Configuration is a way to change the experience a user receives. This is done through controlling the values of a set of “properties” of the client. A property is a variable with a type (enum or integer) and a default value, and can represent the appearance or behavior of pretty much anything. </p>



<div><figure><img loading="lazy" width="700" height="525" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-700x525.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-700x525.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-250x188.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-768x576.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-1536x1153.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-120x90.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>The example above shows an imaginary version of Spotify’s home page in our mobile apps. It’s made up of a set of shelves, and each shelf has a set of cards. With Remote Configuration properties we can control elements for any purpose, i.e. the number of shelves or font sizes on the home page for experiments, rollouts, or personalization or localization. </p>



<p>The properties are defined in a yaml file living next to the code that uses it. When the code is built, all properties and their default values are gathered and published via an API to the admin interface together with the ID of the client being built and the version number. </p>



<div><figure><img loading="lazy" width="700" height="351" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-700x351.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-250x125.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-768x385.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-120x60.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3.png 1420w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>The default value is critical. It allows us to have a programmatic understanding of what the end user experience will be if a client fails to fetch or apply property values. Also, we only have to transfer values to the client when they differ from the default, which saves a lot of time and data traffic when the client starts up. We know what defaults a client has since it identifies itself with the version number. </p>



<div><figure><img loading="lazy" width="700" height="234" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-700x234.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-700x234.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-250x84.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-768x257.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-1536x514.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-120x40.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>The way different users get different values for properties is through a set of policies that is evaluated when a client requests the configuration. The policy has a set of filtering criteria and a property-value mapping to apply if the filters match. The actual implementation of the policy is a <a href="https://facebook.github.io/planout/" target="_blank" rel="noreferrer noopener">PlanOut</a> script that the Remote Configuration service executes. </p>



<p>An important side effect of the fetching of property values is that two events are being logged:</p>



<ul><li><strong>Config Assigned,</strong> which lets us know that a user has fetched its values. Besides user information, this log message also identifies which policies were applied. This information is later used to determine which experiments a user was exposed to.</li></ul>



<ul><li><strong>Config Applied,</strong> which lets us know that the device has actually started using the property values. We use this event as the trigger event for exposure. </li></ul>



<p>Property values are re-fetched in the background at regular intervals, but are only applied when the app is relaunched. The main reason for this is that we do not want the user experience to change mid-session. </p>



<h2>Metrics Catalog</h2>



<p>The Metrics Catalog is where we manage, store, and serve metrics to the Experimentation Platform. </p>



<div><figure><img loading="lazy" width="700" height="336" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-700x336.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-700x336.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-250x120.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-768x369.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-1536x737.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-120x58.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure></div>



<p>On a high level, raw metric data is fed into a pipeline where it’s joined with information on which experiment groups a user belongs to. This data is then aggregated into a OLAP cube and put into a data warehouse. In front of the data warehouse sits an API that allows other parties to query for information without knowing too much about the underlying storage.</p>



<p>Exposure is assembled from the Config Assigned and Config Applied messages from Remote Configuration.</p>



<figure><img loading="lazy" width="700" height="292" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-700x292.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-700x292.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-250x104.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-768x320.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-1536x640.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-120x50.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<p>A user is considered exposed to an experiment if the following is true:</p>



<ul><li>We have a Config Assigned event that assigns the user to one of the groups in the experiment,<p><strong>AND</strong></p></li></ul>



<ul><li>We have a Config Applied event that tells us that the user started using the configuration of the experiment.<p><strong>AND OPTIONALLY</strong></p></li><li>The user exists in one specified “custom exposure source”.</li></ul>



<p>The custom exposure sources allow us to define finer-grained exposure events, such as when a user visited a certain page in the mobile app.</p>



<h2>Experiment Planner</h2>



<p>The Experiment Planner sits as an orchestrating layer on top of Metrics Catalog and Remote Configuration. This is where we create, launch, and stop experiments, as well as analyze test results.<br/>The UI lives in <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">Backstage</a>, our developer portal. All of our internal teams have access to our internal instance of Backstage and are free to create as many experiments as they like.</p>



<div><figure><img loading="lazy" width="512" height="404" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7.png 512w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7-250x197.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7-120x95.png 120w" sizes="(max-width: 512px) 100vw, 512px"/><figcaption>For illustrative purposes only.</figcaption></figure></div>



<p>When creating an experiment, we have to define the test treatments, what experience users should get for each treatment (by specifying property values), and all the things that go into testing the hypothesis. Having programmatic understanding of available properties in Remote Configuration and their types helps this process and reduces configuration errors. It’s possible to define values for properties belonging to different systems in a single experiment. For example, if Android and iOS are implemented differently, we still can run a single experiment on both platforms.</p>



<h2>Summary</h2>



<p>We have spent the last two years rebuilding our experimentation capabilities at Spotify. The new platform is a step change in ease of use and capabilities, but we still feel it’s early for experimentation at Spotify.</p>



<p>We are constantly evolving our Experimentation Platform and practices. If you would like to know more, or if you’re interested in joining the team and contribute to our journey, do not hesitate to reach out.</p>



<p>Johan Rydberg, Experimentation Lead</p>



<p><a rel="noreferrer noopener" href="mailto:jrydberg@spotify.com" target="_blank">jrydberg@spotify.com</a> / <a rel="noreferrer noopener" href="https://twitter.com/datamishap" target="_blank">@datamishap</a></p>




        <br/>

        
        

        

            </div></div>]]></content:encoded>
      <author>Published by Johan Rydberg</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 29 Oct 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Managing Clouds from the Ground Up: Cost Engineering at Spotify&#xA;</title>
      <link>https://engineering.atspotify.com/2020/09/29/managing-clouds-from-the-ground-up-cost-engineering-at-spotify/</link>
      <description>Like many of those in tech, we invest heavily in our cloud and data infrastructure. While seemingly routine, the ability to manage and scale our infrastructure to support our 299+ million listeners worldwide, 24/7, without missing a beat (or syllable) is crucial for the business and our brand. O</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-3958">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 29, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/09/29/managing-clouds-from-the-ground-up-cost-engineering-at-spotify/" title="Managing Clouds from the Ground Up: Cost Engineering at Spotify">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png" alt="" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-120x60.png 120w" sizes="(max-width: 2105px) 100vw, 2105px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/09/Cost-Engineering_01C-1.png"/>                    </a>
                        
        </p>

        

        
<p>Like many of those in tech, we invest heavily in our cloud and data infrastructure. While seemingly routine, the ability to manage and scale our infrastructure to support our 299+ million listeners worldwide, 24/7, without missing a beat (or syllable) is crucial for the business and our brand. </p>



<p>On top of that, our infrastructure teams are resolute when it comes to upholding a highly valued cultural goal: enabling our autonomous engineering teams (called squads) to work as freely and quickly as they possibly can. Finish that off with the fact that we’re a growing public company, and we’ve created a challenging problem for our cost engineering team.</p>



<figure><img loading="lazy" width="700" height="493" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-700x493.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-700x493.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-250x176.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-768x541.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-1536x1082.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-120x85.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/></figure>



<h2>Introducing Spotify’s Cost Insights tool</h2>



<p>Managing costs in our unique situation is no easy feat, but that certainly doesn’t stop us from innovating on the process. We’re leaving behind the days of reducing costs via top-down requests and moving on to finding fun and rewarding ways engineers can strengthen technology while improving the company’s bottom line. Our new <a href="https://github.com/spotify/backstage/tree/master/plugins/cost-insights" target="_blank" rel="noreferrer noopener">Cost Insights</a> product in <a href="https://backstage.io/" target="_blank" rel="noreferrer noopener">Backstage.io</a> explains cloud costs in a way our engineers can relate to and identifies optimizations that have resulted in some big wins for Spotify.  </p>



<figure><img loading="lazy" width="700" height="460" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-700x460.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-700x460.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-250x164.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-768x505.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-1536x1010.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-120x79.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights.png 1600w" sizes="(max-width: 700px) 100vw, 700px"/><figcaption><em>Figure above for illustrative purposes only.</em></figcaption></figure>



<p>We’ve found that our engineers see these optimizations as an interesting challenge — they improve cost, performance, and reliability, turning our infrastructure into a lean, green execution machine. The Cost Insights tool brings those optimization opportunities to light, so engineers can move quickly in achieving those wins. </p>



<h3>Spotify’s unique take on cost management</h3>



<p>Many great cloud cost management tools are on the market, but we were looking for a way to better understand the relationship between a business’s cloud cost and its overall growth. Without broader context, it’s difficult to determine whether $10,000, for example, might be an appropriate amount to spend. </p>



<p>At Spotify, we believe showcasing cost and business data in a meaningful way will empower our engineers to understand where they are spending and optimize quickly. To encourage engineers to take action, their cloud cost tool should be located where other frequently used products and services are to increase productivity.</p>



<h3>Growing the business while being cost conscious</h3>



<p>Spotify continues to be heavily focused on growth, allowing teams that may be high spending on cloud costs to continue to do so if it results in growth opportunities for the business. We needed a tool that showcases costs and helps engineers, engineering managers, and product managers to be information driven when deciding between growth initiatives and worthwhile cost optimizations. Cost Insights is a solution that allows for engineering teams to:</p>



<ul><li>Become aware of their cloud spend and how it relates to their business unit’s growth.</li><li>Understand how cost optimizations should be prioritized compared to the goals for business growth.</li><li>Receive clear recommendations on how they can optimize or reduce their spend.</li></ul>



<h3>Cost Insights features</h3>



<p>Our goal was to launch the Cost Insights plugin with a strong foundation and a great potential for growth. The open source version includes three key features:</p>



<ul><li><strong>Cost vs. business graph:</strong> Users track how their team or a specific GCP project is trending compared to their company’s business growth.  </li><li><strong>Detailed product panels:</strong> Cost Insights currently includes detailed information on six cloud products. The product panels currently cater to GCP but can be configured to utilize other cloud providers. These panels help users understand the cost of their products down to the resource level and compare their growth over time.</li><li><strong>Project alerting:</strong> Teams are alerted when project costs exceed a chosen threshold, allowing them to deep dive into cost changes at the resource level.</li></ul>



<h3>Preventing over-optimizations</h3>



<p>Developing a cost tool in a growth-focused, autonomous culture can have severe consequences if executed incorrectly. The Cost Insights product is accessible by any Spotifier, so it became essential to be explicit with teams when there is a cost increase to review. We’ve set several thresholds to capture how much a team is spending and to track their growth trends. Only teams that are growing faster than Spotify’s business will be nudged to investigate their spending. </p>



<h3>Upcoming on the roadmap</h3>



<p>Cost Insights allows teams to determine for themselves if the time invested in an optimization is valuable compared to the costs saved. Now that it’s available in open source with <a href="http://backstage.io/" target="_blank" rel="noreferrer noopener">Backstage</a>, our focus will shift to open sourcing the backend, creating detailed cost breakdowns (SKU level), and delivering alert dismissals that incorporate user feedback.</p>



<p>For more information about Cost Insights, reach out to Janisa Anandamohan at <a href="mailto:janisa@spotify.com" target="_blank" rel="noreferrer noopener">janisa@spotify.com</a>.</p>
        <br/>

        
        

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Janisa Anandamohan</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>&#xA;                                            Cloud Native Computing Foundation Accepts Backstage as a Sandbox Project&#xA;</title>
      <link>https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/</link>
      <description>If you hear faint whooping in the background of your playlists today, it’s just us celebrating a new milestone for Spotify’s open source efforts: The Cloud Native Computing Foundation (CNCF) has accepted Backstage, our open source developer portal, as an early stage project in the CNCF Sandbox. It’s</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><section role="main" id="main">

                        

                <article id="post-3947">
    <div>
        
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""/></p><p><span>September 24, 2020</span>
                
            </p>
        </div>
        
        <p><a href="https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/" title="Cloud Native Computing Foundation Accepts Backstage as a Sandbox Project">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png" alt="Cloud Native Computing Foundation Accepts Backstage" loading="lazy" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png 753w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-250x121.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-700x337.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-120x58.png 120w" sizes="(max-width: 753px) 100vw, 753px" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png"/>                    </a>
                        
        </p>

        

        
<p>If you hear faint whooping in the background of your playlists today, it’s just us celebrating a new milestone for Spotify’s open source efforts: <a href="https://www.cncf.io/" target="_blank" rel="noreferrer noopener">The Cloud Native Computing Foundation (CNCF)</a> has accepted <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage</a>, our open source developer portal, as an early stage project in the <a href="https://www.cncf.io/sandbox-projects/" target="_blank" rel="noreferrer noopener">CNCF Sandbox</a>. It’s just the first step in a longer journey with the CNCF, but it’s an important one for Spotify as it underlines our renewed commitment to open source — and developers everywhere.</p>



<h2>Backstage + CNCF = 🎉</h2>



<p>For those of you unfamiliar with the CNCF, you may recognize them as the home of such hits as Google’s Kubernetes and Lyft’s Envoy. With such a strong foundation watching over our community’s efforts and such an impressive roster of projects leading the way before us, we have high hopes for the future of Backstage — one of our most ambitious open source projects to date.</p>



<p>You’ve heard us talk about Backstage before on this blog — back in March when we <a href="https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/" target="_blank" rel="noreferrer noopener">announced the open source project</a> and later when we shared <a href="https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/" target="_blank" rel="noreferrer noopener">how we use Backstage internally at Spotify</a>. In a nutshell: Backstage is an open platform for building developer portals. Built around a centralized service catalog, it’s designed to streamline your development environment from end to end. We built it to improve the everyday experience and productivity of developers — initially, our own developers, and then when we open sourced it, all developers, everywhere.</p>



<h2>Our commitment to improving developer experience</h2>



<p>At Spotify, Backstage enables us to scale safely and onboard quickly, helping us build and ship the product that hundreds of millions of people around the world use every day. We believe it has the potential to transform how all engineers work together, whether they’re in a 50-person startup or a Fortune 50. </p>



<p>Here’s what Backstage can do for companies and tech organizations, and how it improves developer experience:</p>



<ul><li><strong>Restore order to software ecosystems.</strong> For companies whose infrastructure has become a wilderness of competing technologies and orphaned dependencies hiding in the dark corners of their tech stack, the <a rel="noreferrer noopener" href="https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha" target="_blank">Backstage Service Catalog</a> brings back discoverability, accountability, and control — not to mention sanity. Instead of being overwhelmed by fragmentation and information sprawl, the Backstage Service Catalog creates a centralized system for tracking all your software — making it easy for teams to manage 10 services and making it possible for a company to manage thousands of them.</li></ul>



<ul><li><strong>Jumpstart productivity by standardizing software and tooling.</strong> With software templates, engineers can spin up a new software project in minutes instead of hours. <a rel="noreferrer noopener" href="https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates" target="_blank">Backstage Software Templates</a> are like automated getting started guides. After an engineer chooses a template, Backstage takes care of the rest — automatically setting up the repo, deploying the first build, and providing a Hello World project, all ready to go — with your organization’s best practices built right in, right from the start. By reducing the number of low-variance choices a developer is forced to consider when starting a project, templates remove friction and allow developers to spend more cycles solving problems higher up in the stack. Standards can set engineers free.</li></ul>



<ul><li><strong>Get unstuck with great technical documentation made easy. </strong>No one can ever find documentation when they need it — and if they do, it might not be that helpful because it hasn’t been kept up to date. Backstage solves both ends of the problem. With <a rel="noreferrer noopener" href="https://backstage.io/blog/2020/09/08/announcing-tech-docs" target="_blank">our “docs like code” approach</a>, engineers write their technical documentation in Markdown files right alongside their code. Whenever you create a new project in Backstage, a TechDocs site is automatically set up in the same repo — so you can update your code and your documentation with the same pull request. This integrated workflow and centralization makes great documentation easy. Easy to create and maintain. And easy to find and use.</li></ul>



<ul><li><strong>Customize and scale your infrastructure with a growing ecosystems of plugins.</strong> Every company has their own, homegrown infrastructure — Backstage’s plugin architecture makes it simple to make Backstage a perfect fit for yours. Integrating your custom, proprietary tooling is as simple as building an internal plugin for your installation of Backstage. You can also build open source plugins to share with the community. The open source <a rel="noreferrer noopener" href="https://backstage.io/plugins" target="_blank">plugin marketplace</a> for Backstage continues to grow, expanding Backstage’s functionality with each new plugin. It’s like an app store for your infrastructure.</li></ul>



<p>Backstage has already come a long way — and none of these features would be what they are today without contributions from the open source community.</p>



<h2>Our commitment to the open source community</h2>



<p>We were excited by the reception Backstage received when we first released it. But we’ve been even more gratified by how the community of contributors has grown since then, as they’ve built new <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">plugins</a> and added new functionality to the core product. Over <a href="https://github.com/spotify/backstage/graphs/contributors" target="_blank" rel="noreferrer noopener">130 people</a> have contributed to the project, and roughly 40% of pull requests are now coming from external, non-Spotify contributors.</p>



<p>As Principal Product Manager <a href="https://engineering.atspotify.com/2020/04/01/my-beat-stefan-alund/" target="_blank" rel="noreferrer noopener">Stefan Ålund</a> writes on the <a href="https://backstage.io/blog/2020/09/23/backstage-cncf-sandbox" target="_blank" rel="noreferrer noopener">Backstage blog</a>:</p>



<blockquote><div><p>We released the open source version of Backstage ‘early’. That was intentional. Because even though we’ve been using Backstage internally for years, we wanted the open source version to be developed with input and contributions from the community. And that’s exactly the product that’s going into the CNCF Sandbox today.</p><p>Backstage’s ability to simplify tooling and standardize engineering practices has attracted interest from other major tech companies, as well as airlines, auto manufacturers, investment firms, and global retailers. We know that Backstage solves a problem — infrastructure complexity — that’s common to a lot of large and growing companies today. But different companies work differently, use particular toolsets, and have unique use cases. By making Backstage open source, we can build it with people working inside a variety of engineering organizations all over the world. It makes for a better product that serves a wider group of users (beyond that of Spotify’s) and their needs.</p></div></blockquote>



<p>Thank you to everyone who has already contributed to this project, inside and outside of Spotify. And if you’ve been curious about Backstage, now is the perfect time to dive in. Visit <a href="https://backstage.io" target="_blank" rel="noreferrer noopener">Backstage.io</a> to learn more and <a href="https://mailchi.mp/spotify/backstage-community" target="_blank" rel="noreferrer noopener">subscribe to our newsletter</a> for updates. Check out open issues on <a href="https://github.com/spotify/backstage/" target="_blank" rel="noreferrer noopener">GitHub</a> or get started building a <a href="https://backstage.io/plugins" target="_blank" rel="noreferrer noopener">plugin</a> for your favorite tool or service. We look forward to seeing the community grow, and can’t wait to see where open source takes us all next.</p>



<p>We’ll give Remy DeCausemaker — Head of Spotify’s <a href="https://thenewstack.io/does-your-organization-need-an-open-source-program-office/" target="_blank" rel="noreferrer noopener">Open Source Program Office</a> (OSPO) — the last word: </p>



<blockquote><p>We’re excited to embark on this journey with the CNCF community. Backstage isn’t the first open source project Spotify has released, but it is the first one we felt was ready to dedicate to an upstream foundation, and we can’t wait to bring what we’ve learned to the next project. There’s so much great tech being built here, and it’s about time we share it to build even greater products, together.</p></blockquote>
        <br/>

        
        

        

            </div>
    


</article>
                

            
        
    </section></div>]]></content:encoded>
      <author>Published by Spotify Engineering</author>
      <enclosure url="https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>