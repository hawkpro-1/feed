{
  "version": "https://jsonfeed.org/version/1",
  "title": "GooleDevops",
  "home_page_url": "https://cloudblog.withgoogle.com/products/devops-sre/rss/",
  "items": [
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/java-and-go-vulnerability-scanning-support/",
      "title": "Container analysis support for Maven and Go Automatic Scanning of Containers in Public Preview",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eJava and Go vulnerability scanning support\u003c/h3\u003e\u003cp\u003eGoogle Cloud’s Container Scanning API now automatically scans Maven and Go packages for vulnerabilities.\u003c/p\u003e\u003cp\u003eWith the Container Scanning API enabled, any containers including Java (in Maven repositories) and Go language packages that are uploaded to an Artifact Registry repository will be scanned for vulnerabilities. This capability builds on existing Linux OS based vulnerability detection and provides customers with deeper insight into their applications. This feature is in \u003ca href=\"https://cloud.google.com/products#section-23\"\u003ePublic Preview\u003c/a\u003e which makes it available to all Google Cloud customers.\u003c/p\u003e\u003cp\u003eGet started with Artifact Registry via the instructions for \u003ca href=\"https://cloud.google.com/container-analysis/docs/go-scanning-automatically\"\u003eGo\u003c/a\u003e or the instructions for \u003ca href=\"https://cloud.google.com/container-analysis/docs/java-scanning-automatically\"\u003eJava\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eHow it works\u003c/h3\u003e\u003cp\u003eOnce the \u003ca href=\"https://cloud.google.com/container-analysis/docs/enable-container-scanning\"\u003eAPI is enabled\u003c/a\u003e, upload a container image which contains Go and/or Maven packages.\u003c/p\u003e\u003cp\u003eVulnerability totals for each image digest are displayed in the Vulnerabilities column. Customers can then drill down on the vulnerability to get CVE numbers, and if available, a suggested fix.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1__Container_analysis_sup.1000060920000338.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1  Container analysis support.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1__Container_analysis_sup.1000060920000338.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eVulnerabilities can also be displayed via the gcloud CLI and the API.\u003c/p\u003e\u003cp\u003eTo view a list of vulnerabilities from the gcloud CLI, the following can be used.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'gcloud artifacts docker images list --show-occurrences LOCATION-docker.pkg.dev/PROJECT_ID/REPOSITORY/IMAGE_ID --format=json'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e36f7539950\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eTo view a list of vulnerabilities with the API, run the following command.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'curl -X GET -H \"Content-Type: application/json\" -H \\\\\\r\\n \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\\\\r\\n https://containeranalysis.googleapis.com/v1/projects/PROJECT_ID/occurrences'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e36f4ebf290\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eIntegrate your Workflows via API and Pub/Sub\u003c/h3\u003e\u003cp\u003eThis feature now makes it possible to scan Java (in Maven repositories) and Go language packages both via the existing On-Demand scan capability, and with an automatic scan on push to Artifact Registry. Language scanning is in addition to the Linux OS scanning which is already available.\u003c/p\u003e\u003cp\u003eThis capability can be combined with \u003ca href=\"https://cloud.google.com/container-analysis/docs/pub-sub-notifications\"\u003ePub/Sub notifications\u003c/a\u003e to trigger additional actions for the vulnerabilities and other metadata. An example of this is sending an e-mail notification to those who need the information.\u003c/p\u003e\u003cp\u003eOrganizations are increasingly concerned about the supply chain risks associated with building their applications using open source software. Being able to scan applications for vulnerabilities is an important step for customers to enhance their security posture. Language package vulnerabilities are available in the same formats that customers are already familiar with. They appear alongside OS vulnerabilities within the Artifact Registry UI, and are available through existing CLI and APIs. These steps aid customers in identifying the potential vulnerabilities introduced in software packages and make appropriate decisions with that information. \u003c/p\u003e\u003cp\u003eLearn more about \u003ca href=\"https://cloud.google.com/container-analysis/docs/scanning-types\"\u003etypes of vulnerability scanning\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eBuilding a secure CI/CD pipeline using Google Cloud built-in services\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eBuild a secure CI/CD pipeline using Google Cloud's built-in services using Cloud Build, Cloud Deploy, Artifact Registry, Binary Authoriza...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/artifact_registry.jpg",
      "date_published": "2022-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eGreg Mucci\u003c/name\u003e\u003ctitle\u003eProduct Manager • Developer Experience\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/developers-practitioners/gitsops-service-orchestration/",
      "title": "GitOps your service orchestrations",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c59=\"\"\u003e\u003cdiv _ngcontent-c59=\"\" innerhtml=\"\u0026lt;p\u0026gt;In this approach, you have a staging branch where you make changes to a workflow. This triggers a Cloud Build configuration that deploys a test staging workflow and runs some staging tests against it. If all tests pass, Cloud Build deploys the staging workflow. After more manual testing of the staging workflow, you merge changes from the staging branch to the main branch. This triggers the same Cloud Build configuration to deploy a test production workflow, run more production tests, and if all tests pass, deploy the production workflow.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;This approach enables you to have an automated and staged rollout of workflow changes with tests along the way to minimize risk.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Configuration\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Setting up such an automated workflow deployment pipeline is straightforward.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;First, you need a workflow definition file that can benefit from such automation. You can use one of your workflow definition files or \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;code\u0026gt;workflow.yaml\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt;, which simply returns \u0026lt;code\u0026gt;Hello World\u0026lt;/code\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Next, define a Cloud Build configuration file (see \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/cloudbuild.yaml\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;code\u0026gt;cloudbuild.yaml\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt;) with all the stages. In this configuration, Cloud Build deploys a test workflow with the branch name and commit hash, runs the workflow and captures the output, deletes the test workflow, and tests the workflow with the supplied test script. If all the tests pass, it deploys the final workflow in the branch.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Tests for the branch are defined in appropriate \u0026lt;code\u0026gt;test-{branchname}.sh\u0026lt;/code\u0026gt; files. For example, \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-staging.sh\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;code\u0026gt;test-staging.sh\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt; runs against the workflows deployed in the staging branch and only checks the workflow execution state. On the other hand, \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-master.sh\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;code\u0026gt;test-main.sh\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt; runs against the main branch, checks the workflow execution state, and also checks the output of the execution. You can add more tests as you see fit.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Connect your repository to Cloud Build\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Now that you have the basic configuration in place, you connect your (or \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/workflows-demos\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;code\u0026gt;workflows-demos\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt;) repository to Cloud Build before creating triggers. Follow the instructions \u0026lt;a href=\u0026#34;https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github#connecting_a_github_repository\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Create a Cloud Build trigger\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;You now create a Cloud Build trigger to watch for commits to the main and staging branches. General instructions are \u0026lt;a href=\u0026#34;https://cloud.google.com/build/docs/automating-builds/create-manage-triggers\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Go to the \u0026lt;code\u0026gt;Create Trigger\u0026lt;/code\u0026gt; section of Cloud Build in the console and create a trigger with the following properties:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Name: \u0026lt;code\u0026gt;workflows-trigger\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Event: Push to a branch\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Repository: \u0026lt;code\u0026gt;GoogleCloudPlatform/workflows-demos\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Branch: \u0026lt;code\u0026gt;^main$|^staging$\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Included files filter: \u0026lt;code\u0026gt;gitops/workflow.yaml\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Configuration type: Cloud build configuration file\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cloud build configuration file location: \u0026lt;code\u0026gt;gitops/cloudbuild.yaml\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Add a Substitution variable with key/value: \u0026lt;code\u0026gt;_WORKFLOW_NAME\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;workflows-gitops\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Test the staging workflow\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;You\u0026amp;#8217;re now ready to test the build pipeline with the staging branch.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Switch to the staging branch:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;git checkout staging\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Change \u0026lt;code\u0026gt;Hello World\u0026lt;/code\u0026gt; in \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;code\u0026gt;workflow.yaml\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt; to \u0026lt;code\u0026gt;Bye World\u0026lt;/code\u0026gt;:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eIn this approach, you have a staging branch where you make changes to a workflow. This triggers a Cloud Build configuration that deploys a test staging workflow and runs some staging tests against it. If all tests pass, Cloud Build deploys the staging workflow. After more manual testing of the staging workflow, you merge changes from the staging branch to the main branch. This triggers the same Cloud Build configuration to deploy a test production workflow, run more production tests, and if all tests pass, deploy the production workflow.\u003c/p\u003e\u003cp\u003eThis approach enables you to have an automated and staged rollout of workflow changes with tests along the way to minimize risk. \u003c/p\u003e\u003ch3\u003eConfiguration\u003c/h3\u003e\u003cp\u003eSetting up such an automated workflow deployment pipeline is straightforward. \u003c/p\u003e\u003cp\u003eFirst, you need a workflow definition file that can benefit from such automation. You can use one of your workflow definition files or \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e\u003ccode\u003eworkflow.yaml\u003c/code\u003e\u003c/a\u003e, which simply returns \u003ccode\u003eHello World\u003c/code\u003e. \u003c/p\u003e\u003cp\u003eNext, define a Cloud Build configuration file (see \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/cloudbuild.yaml\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e\u003ccode\u003ecloudbuild.yaml\u003c/code\u003e\u003c/a\u003e) with all the stages. In this configuration, Cloud Build deploys a test workflow with the branch name and commit hash, runs the workflow and captures the output, deletes the test workflow, and tests the workflow with the supplied test script. If all the tests pass, it deploys the final workflow in the branch.\u003c/p\u003e\u003cp\u003eTests for the branch are defined in appropriate \u003ccode\u003etest-{branchname}.sh\u003c/code\u003e files. For example, \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-staging.sh\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e\u003ccode\u003etest-staging.sh\u003c/code\u003e\u003c/a\u003e runs against the workflows deployed in the staging branch and only checks the workflow execution state. On the other hand, \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-master.sh\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e\u003ccode\u003etest-main.sh\u003c/code\u003e\u003c/a\u003e runs against the main branch, checks the workflow execution state, and also checks the output of the execution. You can add more tests as you see fit.\u003c/p\u003e\u003ch3\u003eConnect your repository to Cloud Build\u003c/h3\u003e\u003cp\u003eNow that you have the basic configuration in place, you connect your (or \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e\u003ccode\u003eworkflows-demos\u003c/code\u003e\u003c/a\u003e) repository to Cloud Build before creating triggers. Follow the instructions \u003ca href=\"https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github#connecting_a_github_repository\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github#connecting_a_github_repository\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eCreate a Cloud Build trigger\u003c/h3\u003e\u003cp\u003eYou now create a Cloud Build trigger to watch for commits to the main and staging branches. General instructions are \u003ca href=\"https://cloud.google.com/build/docs/automating-builds/create-manage-triggers\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/build/docs/automating-builds/create-manage-triggers\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eGo to the \u003ccode\u003eCreate Trigger\u003c/code\u003e section of Cloud Build in the console and create a trigger with the following properties:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eName: \u003ccode\u003eworkflows-trigger\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEvent: Push to a branch\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRepository: \u003ccode\u003eGoogleCloudPlatform/workflows-demos\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBranch: \u003ccode\u003e^main$|^staging$\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIncluded files filter: \u003ccode\u003egitops/workflow.yaml\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eConfiguration type: Cloud build configuration file\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud build configuration file location: \u003ccode\u003egitops/cloudbuild.yaml\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAdd a Substitution variable with key/value: \u003ccode\u003e_WORKFLOW_NAME\u003c/code\u003e and \u003ccode\u003eworkflows-gitops\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eTest the staging workflow\u003c/h3\u003e\u003cp\u003eYou’re now ready to test the build pipeline with the staging branch. \u003c/p\u003e\u003cp\u003eSwitch to the staging branch:\u003c/p\u003e\u003cp\u003e\u003ccode\u003egit checkout staging\u003c/code\u003e\u003c/p\u003e\u003cp\u003eChange \u003ccode\u003eHello World\u003c/code\u003e in \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e\u003ccode\u003eworkflow.yaml\u003c/code\u003e\u003c/a\u003e to \u003ccode\u003eBye World\u003c/code\u003e:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ca href=\"https://opengitops.dev/\" target=\"_blank\"\u003eGitOps\u003c/a\u003e takes DevOps best practices used for application development (such as version control and CI/CD) and applies them to infrastructure automation. In GitOps, the Git repository serves as the source of truth and the CD pipeline is responsible for building, testing, and deploying the application code and the underlying infrastructure.\u003c/p\u003e\u003cp\u003eNowadays, an application is not just code running on infrastructure that you own and operate. It is usually a set of first-party and third-party microservices working together in an event-driven architecture or with a central service orchestrator such as \u003ca href=\"https://cloud.google.com/workflows\"\u003eWorkflows\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eService orchestrations, which have their own definition files and deployment cycles, can benefit from a GitOps approach. This blog post describes how to set up a simple Git-driven development, testing, and deployment pipeline for Workflows using \u003ca href=\"https://cloud.google.com/build\"\u003eCloud Build\u003c/a\u003e. \u003c/p\u003e\u003ch3\u003eArchitecture\u003c/h3\u003e\u003cp\u003eLet’s take a look at the overall approach. \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"GitOps Blog 1\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_1.max-1000x1000.png\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eArchitechture\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn this approach, you have a staging branch where you make changes to a workflow. This triggers a Cloud Build configuration that deploys a test staging workflow and runs some staging tests against it. If all tests pass, Cloud Build deploys the staging workflow. After more manual testing of the staging workflow, you merge changes from the staging branch to the main branch. This triggers the same Cloud Build configuration to deploy a test production workflow, run more production tests, and if all tests pass, deploy the production workflow.\u003c/p\u003e\u003cp\u003eThis approach enables you to have an automated and staged rollout of workflow changes with tests along the way to minimize risk. \u003c/p\u003e\u003ch3\u003eConfiguration\u003c/h3\u003e\u003cp\u003eSetting up such an automated workflow deployment pipeline is straightforward. \u003c/p\u003e\u003cp\u003eFirst, you need a workflow definition file that can benefit from such automation. You can use one of your workflow definition files or \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml\" target=\"_blank\"\u003e\u003ccode\u003eworkflow.yaml\u003c/code\u003e\u003c/a\u003e, which simply returns \u003ccode\u003eHello World\u003c/code\u003e. \u003c/p\u003e\u003cp\u003eNext, define a Cloud Build configuration file (see \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/cloudbuild.yaml\" target=\"_blank\"\u003e\u003ccode\u003ecloudbuild.yaml\u003c/code\u003e\u003c/a\u003e) with all the stages. In this configuration, Cloud Build deploys a test workflow with the branch name and commit hash, runs the workflow and captures the output, deletes the test workflow, and tests the workflow with the supplied test script. If all the tests pass, it deploys the final workflow in the branch.\u003c/p\u003e\u003cp\u003eTests for the branch are defined in appropriate \u003ccode\u003etest-{branchname}.sh\u003c/code\u003e files. For example, \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-staging.sh\" target=\"_blank\"\u003e\u003ccode\u003etest-staging.sh\u003c/code\u003e\u003c/a\u003e runs against the workflows deployed in the staging branch and only checks the workflow execution state. On the other hand, \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-master.sh\" target=\"_blank\"\u003e\u003ccode\u003etest-main.sh\u003c/code\u003e\u003c/a\u003e runs against the main branch, checks the workflow execution state, and also checks the output of the execution. You can add more tests as you see fit.\u003c/p\u003e\u003ch3\u003eConnect your repository to Cloud Build\u003c/h3\u003e\u003cp\u003eNow that you have the basic configuration in place, you connect your (or \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos\" target=\"_blank\"\u003e\u003ccode\u003eworkflows-demos\u003c/code\u003e\u003c/a\u003e) repository to Cloud Build before creating triggers. Follow the instructions \u003ca href=\"https://cloud.google.com/build/docs/automating-builds/github/connect-repo-github#connecting_a_github_repository\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eCreate a Cloud Build trigger\u003c/h3\u003e\u003cp\u003eYou now create a Cloud Build trigger to watch for commits to the main and staging branches. General instructions are \u003ca href=\"https://cloud.google.com/build/docs/automating-builds/create-manage-triggers\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eGo to the \u003ccode\u003eCreate Trigger\u003c/code\u003e section of Cloud Build in the console and create a trigger with the following properties:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eName: \u003ccode\u003eworkflows-trigger\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEvent: Push to a branch\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRepository: \u003ccode\u003eGoogleCloudPlatform/workflows-demos\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBranch: \u003ccode\u003e^main$|^staging$\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIncluded files filter: \u003ccode\u003egitops/workflow.yaml\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eConfiguration type: Cloud build configuration file\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud build configuration file location: \u003ccode\u003egitops/cloudbuild.yaml\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAdd a Substitution variable with key/value: \u003ccode\u003e_WORKFLOW_NAME\u003c/code\u003e and \u003ccode\u003eworkflows-gitops\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eTest the staging workflow\u003c/h3\u003e\u003cp\u003eYou’re now ready to test the build pipeline with the staging branch. \u003c/p\u003e\u003cp\u003eSwitch to the staging branch:\u003c/p\u003e\u003cp\u003e\u003ccode\u003egit checkout staging\u003c/code\u003e\u003c/p\u003e\u003cp\u003eChange \u003ccode\u003eHello World\u003c/code\u003e in \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml\" target=\"_blank\"\u003e\u003ccode\u003eworkflow.yaml\u003c/code\u003e\u003c/a\u003e to \u003ccode\u003eBye World\u003c/code\u003e:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'- init:\\r\\n assign:\\r\\n- - message: \"Hello World\"\\r\\n+ - message: \"Bye World\"'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e099b360890\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eCommit and push the change to the staging branch:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'git add workflow.yaml\\r\\ngit commit -m \"Update workflow.yaml in staging\"\\r\\ngit push'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e099ac423d0\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou should see the trigger running:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"GOB 2\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_2.max-1000x1000.png\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eBuild trigger running\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAfter a few seconds, the build (including all its stages) is successful:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"BOB 3\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_3.max-1000x1000.png\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eStaging build details\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAnd a staging workflow has been deployed:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"GOB 4\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_4.max-1000x1000.png\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eWorkflows staging\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eTest the production workflow\u003c/h3\u003e\u003cp\u003eOnce you're ready to deploy the staging workflow to production, simply merge the staging branch to the main branch.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'git checkout main\\r\\ngit merge staging\\r\\ngit push'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e0998a25b50\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn this case, however, the build fails:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"GOB 5\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_5.max-1000x1000.png\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eProduction build details\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis is because the test script for the production workflow in \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/test-master.sh\" target=\"_blank\"\u003e\u003ccode\u003etest-main.sh\u003c/code\u003e\u003c/a\u003e is expecting to see \u003ccode\u003eHello World\u003c/code\u003e as output of the workflow.\u003c/p\u003e\u003cp\u003eYou need to go back to the staging branch, and change Bye World in \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/blob/master/gitops/workflow.yaml\" target=\"_blank\"\u003e\u003ccode\u003eworkflow.yaml\u003c/code\u003e\u003c/a\u003e back to \u003ccode\u003eHello World\u003c/code\u003e:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'- init:\\r\\n assign:\\r\\n- - message: \"Bye World\"\\r\\n+ - message: \"Hello World\"'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e0998a25e50\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eCheck in your changes to staging, see the build succeed, and merge to main. Finally, you should also see the build succeed and see that a production workflow has been deployed alongside staging:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"GOB 6\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/GitOps_Blog_7.max-1000x1000.png\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eWorkflows production\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003e\u003cb\u003eNext steps\u003c/b\u003e\u003c/h3\u003e\u003cp\u003eThis post covered how to set up a Git-driven development, testing, and deployment pipeline for Workflows using Cloud Build. All the details and sample configuration files are in our \u003ca href=\"https://github.com/GoogleCloudPlatform/workflows-demos/tree/master/gitops\" target=\"_blank\"\u003e\u003ccode\u003eworkflows-demos/gitops\u003c/code\u003e\u003c/a\u003e repository. \u003c/p\u003e\u003cp\u003eOf course, Cloud Build is not the only way to set up such a pipeline. \u003ca href=\"https://github.com/features/actions\" target=\"_blank\"\u003eGitHub Actions\u003c/a\u003e is another useful tool that can help to set up similar service orchestration pipelines. Feel free to contribute to our repository with GitHub Actions based pipelines and reach out to me on Twitter \u003ca href=\"https://twitter.com/meteatamel\" target=\"_blank\"\u003e@meteatamel\u003c/a\u003e for any questions or feedback.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-09-16T12:00:00Z",
      "author": {
        "name": "\u003cname\u003eMete Atamel\u003c/name\u003e\u003ctitle\u003e Developer Advocate\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/devsecops-and-cicd-using-google-cloud-built-in-services/",
      "title": "Building a secure CI/CD pipeline using Google Cloud built-in services",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e Nitin Vashishtha \u003c/p\u003e\u003cp\u003e Customer Engineer \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e September 13, 2022 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c58=\"\"\u003e\u003cdiv _ngcontent-c58=\"\"\u003e\u003ch4 _ngcontent-c58=\"\"\u003e\u003cspan _ngcontent-c58=\"\"\u003eGoogle Cloud Next \u0026#39;22\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c58=\"\"\u003e\u003cspan _ngcontent-c58=\"\"\u003eRegister for our flagship event October 11–13.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c58=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"Next22_registration\" track-metadata-eventdetail=\"https://cloud.withgoogle.com/next?utm_source=cgc-blog\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY22-Q4-global-ENTD217-onlineevent-er-next-2022-mc\u0026amp;utm_content=left_hand_rail_blog\u0026amp;utm_term=-\" href=\"https://cloud.withgoogle.com/next?utm_source=cgc-blog\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY22-Q4-global-ENTD217-onlineevent-er-next-2022-mc\u0026amp;utm_content=left_hand_rail_blog\u0026amp;utm_term=-\"\u003e\u003cspan _ngcontent-c58=\"\"\u003eRegister Now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;DevOps is a concept that allows software development teams to release software in an automated and stable manner. DevOps itself is not just one thing; it\u0026#39;s a combination of culture and technology, which together make the implementation of DevOps successful.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In this blog, we will be focusing on the tools and technology side of DevOps. At the core of the technical aspect of DevOps, the concept is Continuous Integration and Continuous Delivery (CI/CD). The idea behind CI/CD concept is to create an automated software delivery pipeline that continuously deploys the new software releases in an automated fashion.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The flow begins with the developers committing the code changes to a source code repository, which automatically triggers the delivery pipeline (henceforth called CI/CD pipeline) by building and deploying the code changes into various environments, from non-prod environments to production environments.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Also, as we build the CI/CD pipelines for faster and more reliable software delivery, the security aspect should not be ignored and must be incorporated into the pipeline right from the beginning. When we build our source code, we typically use various open-source libraries and container images. Having some security safeguards within the CI/CD pipeline is imperative to ensure that the software we are building and deploying is free from any vulnerability. Additionally, it\u0026#39;s equally important to control what type of code/container image should be allowed to be deployed on your target runtime environment.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Security is everyone\u0026#39;s responsibility. \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security\u0026#34;\u0026gt;Shifting left\u0026lt;/a\u0026gt; on security is a DevOps practice that allows you to address security concerns early in the software development lifecycle. Vulnerability scanning of container images, putting security policies in place through Binary Authorization, and allowing approved/trusted images to be deployed on GKE are a couple of ways to implement this policy to make your CI/CD pipelines more secure.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;What are we building?\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;This blog post will show how to build a secure CI/CD pipeline using Google Cloud\u0026#39;s built-in services. We will create a secure software delivery pipeline that builds a sample Node.js application as a container image and deploys it on GKE clusters.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;How are we building the CI/CD pipeline?\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We\u0026#39;re going to use the following Google Cloud built-in services to build the pipeline:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/build\u0026#34;\u0026gt;Cloud Build\u0026lt;/a\u0026gt; - Cloud Build is an entirely serverless CI/CD platform that allows you to automate your build, test, and deploy tasks.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry\u0026#34;\u0026gt;Artifact Registry\u0026lt;/a\u0026gt; - Artifact Registry is a secure service to store and manage your build artifacts.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;Cloud Deploy\u0026lt;/a\u0026gt; - Cloud Deploy is a fully managed Continuous Delivery service for GKE and Anthos.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/binary-authorization\u0026#34;\u0026gt;Binary Authorization\u0026lt;/a\u0026gt; - Binary Authorization provides deployment time security controls for GKE and Cloud Run deployments.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;GKE\u0026lt;/a\u0026gt; - GKE is a fully managed Kubernetes platform.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Google Pub/Sub\u0026lt;/a\u0026gt; - Pub/Sub is a serverless messaging platform.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/functions\u0026#34;\u0026gt;Cloud Functions\u0026lt;/a\u0026gt; - Cloud Functions is a serverless platform to run your code.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;We use GitHub as a source code repository and Sendgrid APIs to send email notifications for approval and error logging.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The CI/CD pipeline is set up so that a Cloud Build trigger is configured to sense any code pushed to a particular repository and branch in a GitHub repository and automatically starts the build process.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Below is the flow of how the CI/CD pipeline is set up without any security policy enforcement:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Developer checks in the code to a GitHub repo.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and starts the \u0026#39;build\u0026#39; process. A successful build results in a docker container image.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The container image is stored in the Artifact Registry.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cloud Deploy is configured to go through an approval step before deploying the image to the Production GKE cluster.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A Cloud Function sends an email to a pre-configured email id, notifying you that a Cloud Deploy rollout requires your approval. The email receiver can approve or reject the deployment to the production GKE cluster. Cloud Function code can be found \u0026lt;a href=\u0026#34;https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/cloud-function/index.js\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;To secure this CI/CD pipeline, we will use a couple of Google Cloud\u0026#39;s built-in features and services. First, we will enable vulnerability scans on Artifact Registry, an out-of-the-box feature. Then finally, we will create a security policy using the Binary Authorization service, which only allows a specific image to be deployed to your GKE cluster.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Below is the flow when we try to build and deploy a container image that has vulnerabilities present:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Developer checks in the code to a GitHub repo.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the \u0026#39;build\u0026#39; process.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The build process fails with the error message that vulnerabilities were found in the image.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;Below is the flow when we try to deploy a container image to GKE, which violates a Binary Authorization policy:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Developer checks in the code to a GitHub repo.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the \u0026#39;build\u0026#39; process. A successful build results in a docker container image.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The container image is stored in Artifact Registry.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cloud Deploy fails as the GKE clusters reject the incoming image as it violates the existing Binary Authorization policy. Please note that an approval email is still triggered before the production deployment via the Cloud Function; the email receiver is expected to reject this release based on the failures in the previous stages.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Once the deployment fails due to the Binary Authorization policy violation, Cloud Function sends an email to a pre-configured email id about the deployment failure. Cloud Function code can be found \u0026lt;a href=\u0026#34;https://github.com/sysdesign-code/dev-sec-ops-demo/tree/main/cloud-function/deployment-notification\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;Note: The deployment fails after the timeout value is exceeded, set for Cloud Deploy, which is 10 minutes by default, but you can change this value according to your requirements, see \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt; for more details.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Note: The Cloud Function code provided for the rollout approval email and deployment failure notification is under the folder cloud-functions in this repo. You will still have to create these cloud functions with this code in your Google Cloud project to receive email notifications.\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Solution Architecture\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;The CI/CD pipeline is constructed by combining the aforementioned Google Cloud services. Cloud Build is at the center of automating the pipeline, which contains all the steps we need to build and deploy our container image. Cloud Build executes the steps defined in a YAML file sequentially. It\u0026#39;s quite flexible in terms of how you want to define your \u0026#39;build\u0026#39; and \u0026#39;deploy\u0026#39; process, and the service ensures to execute those steps reliably every time.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Below are solution diagrams of how the CI/CD pipeline is set up :\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDevOps is a concept that allows software development teams to release software in an automated and stable manner. DevOps itself is not just one thing; it\u0026#39;s a combination of culture and technology, which together make the implementation of DevOps successful.\u003c/p\u003e\u003cp\u003eIn this blog, we will be focusing on the tools and technology side of DevOps. At the core of the technical aspect of DevOps, the concept is Continuous Integration and Continuous Delivery (CI/CD). The idea behind CI/CD concept is to create an automated software delivery pipeline that continuously deploys the new software releases in an automated fashion.\u003c/p\u003e\u003cp\u003eThe flow begins with the developers committing the code changes to a source code repository, which automatically triggers the delivery pipeline (henceforth called CI/CD pipeline) by building and deploying the code changes into various environments, from non-prod environments to production environments.\u003c/p\u003e\u003cp\u003eAlso, as we build the CI/CD pipelines for faster and more reliable software delivery, the security aspect should not be ignored and must be incorporated into the pipeline right from the beginning. When we build our source code, we typically use various open-source libraries and container images. Having some security safeguards within the CI/CD pipeline is imperative to ensure that the software we are building and deploying is free from any vulnerability. Additionally, it\u0026#39;s equally important to control what type of code/container image should be allowed to be deployed on your target runtime environment.\u003c/p\u003e\u003cp\u003eSecurity is everyone\u0026#39;s responsibility. \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security\" track-metadata-module=\"post\"\u003eShifting left\u003c/a\u003e on security is a DevOps practice that allows you to address security concerns early in the software development lifecycle. Vulnerability scanning of container images, putting security policies in place through Binary Authorization, and allowing approved/trusted images to be deployed on GKE are a couple of ways to implement this policy to make your CI/CD pipelines more secure.\u003c/p\u003e\u003cp\u003eWhat are we building?\u003c/p\u003e\u003cp\u003eThis blog post will show how to build a secure CI/CD pipeline using Google Cloud\u0026#39;s built-in services. We will create a secure software delivery pipeline that builds a sample Node.js application as a container image and deploys it on GKE clusters.\u003c/p\u003e\u003cp\u003eHow are we building the CI/CD pipeline?\u003c/p\u003e\u003cp\u003eWe\u0026#39;re going to use the following Google Cloud built-in services to build the pipeline:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/build\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/build\" track-metadata-module=\"post\"\u003eCloud Build\u003c/a\u003e - Cloud Build is an entirely serverless CI/CD platform that allows you to automate your build, test, and deploy tasks.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry\" track-metadata-module=\"post\"\u003eArtifact Registry\u003c/a\u003e - Artifact Registry is a secure service to store and manage your build artifacts.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eCloud Deploy\u003c/a\u003e - Cloud Deploy is a fully managed Continuous Delivery service for GKE and Anthos.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/binary-authorization\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/binary-authorization\" track-metadata-module=\"post\"\u003eBinary Authorization\u003c/a\u003e - Binary Authorization provides deployment time security controls for GKE and Cloud Run deployments.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGKE\u003c/a\u003e - GKE is a fully managed Kubernetes platform.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003eGoogle Pub/Sub\u003c/a\u003e - Pub/Sub is a serverless messaging platform.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/functions\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/functions\" track-metadata-module=\"post\"\u003eCloud Functions\u003c/a\u003e - Cloud Functions is a serverless platform to run your code.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eWe use GitHub as a source code repository and Sendgrid APIs to send email notifications for approval and error logging.\u003c/p\u003e\u003cp\u003eThe CI/CD pipeline is set up so that a Cloud Build trigger is configured to sense any code pushed to a particular repository and branch in a GitHub repository and automatically starts the build process.\u003c/p\u003e\u003cp\u003eBelow is the flow of how the CI/CD pipeline is set up without any security policy enforcement:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eDeveloper checks in the code to a GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and starts the \u0026#39;build\u0026#39; process. A successful build results in a docker container image.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe container image is stored in the Artifact Registry.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Deploy is configured to go through an approval step before deploying the image to the Production GKE cluster.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA Cloud Function sends an email to a pre-configured email id, notifying you that a Cloud Deploy rollout requires your approval. The email receiver can approve or reject the deployment to the production GKE cluster. Cloud Function code can be found \u003ca href=\"https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/cloud-function/index.js\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo secure this CI/CD pipeline, we will use a couple of Google Cloud\u0026#39;s built-in features and services. First, we will enable vulnerability scans on Artifact Registry, an out-of-the-box feature. Then finally, we will create a security policy using the Binary Authorization service, which only allows a specific image to be deployed to your GKE cluster.\u003c/p\u003e\u003cp\u003eBelow is the flow when we try to build and deploy a container image that has vulnerabilities present:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eDeveloper checks in the code to a GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the \u0026#39;build\u0026#39; process.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe build process fails with the error message that vulnerabilities were found in the image.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eBelow is the flow when we try to deploy a container image to GKE, which violates a Binary Authorization policy:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eDeveloper checks in the code to a GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the \u0026#39;build\u0026#39; process. A successful build results in a docker container image.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe container image is stored in Artifact Registry.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Deploy fails as the GKE clusters reject the incoming image as it violates the existing Binary Authorization policy. Please note that an approval email is still triggered before the production deployment via the Cloud Function; the email receiver is expected to reject this release based on the failures in the previous stages.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnce the deployment fails due to the Binary Authorization policy violation, Cloud Function sends an email to a pre-configured email id about the deployment failure. Cloud Function code can be found \u003ca href=\"https://github.com/sysdesign-code/dev-sec-ops-demo/tree/main/cloud-function/deployment-notification\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eNote: The deployment fails after the timeout value is exceeded, set for Cloud Deploy, which is 10 minutes by default, but you can change this value according to your requirements, see \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e for more details.\u003c/p\u003e\u003cp\u003eNote: The Cloud Function code provided for the rollout approval email and deployment failure notification is under the folder cloud-functions in this repo. You will still have to create these cloud functions with this code in your Google Cloud project to receive email notifications.\u003c/p\u003e\u003ch2\u003eSolution Architecture\u003c/h2\u003e\u003cp\u003eThe CI/CD pipeline is constructed by combining the aforementioned Google Cloud services. Cloud Build is at the center of automating the pipeline, which contains all the steps we need to build and deploy our container image. Cloud Build executes the steps defined in a YAML file sequentially. It\u0026#39;s quite flexible in terms of how you want to define your \u0026#39;build\u0026#39; and \u0026#39;deploy\u0026#39; process, and the service ensures to execute those steps reliably every time.\u003c/p\u003e\u003cp\u003eBelow are solution diagrams of how the CI/CD pipeline is set up :\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eAs the last step of our CI process, the Cloud Build YAML triggers the Cloud Deploy service, and the container image is deployed to three different GKE clusters. Cloud Deploy automatically emits multiple notifications to pub/Sub topics throughout the deployment process. We are using Cloud Functions to listen to these Pub/Sub topics to send appropriate email notifications about the deployment status and required approvals.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h2\u0026gt;Step-by-Step instructions for creating the CI/CD pipeline\u0026lt;/h2\u0026gt;\u0026lt;h3\u0026gt;I. Prerequisites\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;These steps are required to set up and prepare your GCP environment. We highly recommend you create a new GCP Project as you will run multiple cloud services within the region \u0026amp;#34;us-central1\u0026amp;#34;.\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Fork the following GitHub Repo: \u0026lt;a href=\u0026#34;https://github.com/sysdesign-code/dev-sec-ops-demo\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;https://github.com/sysdesign-code/dev-sec-ops-demo\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Create a new GCP Project, follow the steps here around how to provision and create one: \u0026lt;a href=\u0026#34;https://cloud.google.com/resource-manager/docs/creating-managing-projects\u0026#34;\u0026gt;https://cloud.google.com/resource-manager/docs/creating-managing-projects\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Once your new project is created, enable Cloud SDK to allow CLI access for \u0026lt;code\u0026gt;gcloud\u0026lt;/code\u0026gt; either in Cloud Shell or your local workstation. Follow the steps here: \u0026lt;a href=\u0026#34;https://cloud.google.com/sdk/docs/install\u0026#34;\u0026gt;https://cloud.google.com/sdk/docs/install\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Once you\u0026#39;ve enabled CLI access, either through your Cloud Shell or local workstation, validate or set your project ID:\u0026lt;br\u0026gt;\u0026lt;code\u0026gt;gcloud config set project YOUR_PROJECT_ID\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Run the following one-time script \u0026lt;code\u0026gt;/scripts/gcp_env_setup.sh\u0026lt;/code\u0026gt;, which creates and provisions the necessary GCP cloud services required to create the DevSecOps CI/CD pipeline for deploying a sample docker application.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003ch2\u003eStep-by-Step instructions for creating the CI/CD pipeline\u003c/h2\u003e\u003ch3\u003eI. Prerequisites\u003c/h3\u003e\u003cp\u003eThese steps are required to set up and prepare your GCP environment. We highly recommend you create a new GCP Project as you will run multiple cloud services within the region \u0026#34;us-central1\u0026#34;.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eFork the following GitHub Repo: \u003ca href=\"https://github.com/sysdesign-code/dev-sec-ops-demo\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ehttps://github.com/sysdesign-code/dev-sec-ops-demo\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCreate a new GCP Project, follow the steps here around how to provision and create one: \u003ca href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects\" track-metadata-module=\"post\"\u003ehttps://cloud.google.com/resource-manager/docs/creating-managing-projects\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnce your new project is created, enable Cloud SDK to allow CLI access for \u003ccode\u003egcloud\u003c/code\u003e either in Cloud Shell or your local workstation. Follow the steps here: \u003ca href=\"https://cloud.google.com/sdk/docs/install\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/sdk/docs/install\" track-metadata-module=\"post\"\u003ehttps://cloud.google.com/sdk/docs/install\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnce you\u0026#39;ve enabled CLI access, either through your Cloud Shell or local workstation, validate or set your project ID:\u003cbr/\u003e\u003ccode\u003egcloud config set project YOUR_PROJECT_ID\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRun the following one-time script \u003ccode\u003e/scripts/gcp_env_setup.sh\u003c/code\u003e, which creates and provisions the necessary GCP cloud services required to create the DevSecOps CI/CD pipeline for deploying a sample docker application.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Here are all the service deployments that will occur once the script finishes:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;a) Enables all the required cloud service APIs such as Cloud Build, Binary Authorization, Kubernetes Service, Artifact Registry, Cloud Deploy, and many more.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;b) Create three (3) GKE clusters for test, staging, and production to show image rollout deployments across these clusters using Cloud Deploy. \u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eHere are all the service deployments that will occur once the script finishes:\u003c/p\u003e\u003cp\u003ea) Enables all the required cloud service APIs such as Cloud Build, Binary Authorization, Kubernetes Service, Artifact Registry, Cloud Deploy, and many more.\u003c/p\u003e\u003cp\u003eb) Create three (3) GKE clusters for test, staging, and production to show image rollout deployments across these clusters using Cloud Deploy. \u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;c) Bind all the necessary IAM roles and permissions for Cloud Build and Cloud Deploy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;d) Create a Binary Authorization attestor, associated container note, cryptographic KMS key, and all the associated IAM roles and permissions to allow container note access for the attestor. \u0026lt;/p\u0026gt;\"\u003e\u003cp\u003ec) Bind all the necessary IAM roles and permissions for Cloud Build and Cloud Deploy.\u003c/p\u003e\u003cp\u003ed) Create a Binary Authorization attestor, associated container note, cryptographic KMS key, and all the associated IAM roles and permissions to allow container note access for the attestor. \u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eBy default, the binary authorization policy allows for all images to be deployed to GCP. Later, we will update this policy only to allow attestor-approved images to be deployed to specific GKE clusters. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003ee) Create the Artifact Registry repository where the docker image will be stored.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"f) Finally, create two Pub/Sub topics and Cloud Functions which will allow for email approvals for any GKE deployment to production and error reporting if a release fails.\u0026lt;br\u0026gt;NOTE\u0026lt;br\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;Before you run the script, please validate if your new GCP project already contains a \u0026amp;#34;default\u0026amp;#34; VPC and subnetwork. If you already have a \u0026amp;#34;default\u0026amp;#34; VPC, please go through the script and COMMENT out lines 53-55 which reference the creation of a default VPC and subnetwork. If you already have one, this step is not needed.\u0026lt;br\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;By default, the creation of GKE clusters uses the \u0026amp;#34;default\u0026amp;#34; VPC subnetwork. If you prefer to use a non-default VPC, update the GKE cluster creation commands, starting at line 157, and update the \u0026lt;code\u0026gt;--subnetwork\u0026lt;/code\u0026gt; value for all 3 GKE clusters.\u0026lt;br\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;6. To execute the script, run the following command:\u0026amp;#160;\u0026lt;code\u0026gt;sh /scripts/gcp_env_setup.sh\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;g) This script will approximately take 20-22 minutes to complete. Once finished, the output should look similar to something like \u0026lt;a href=\u0026#34;https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/scripts/gcp_env_setup_OUTPUT.txt\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;this\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;7. Create a SendGRID API Key. Follow the instructions: \u0026lt;a href=\u0026#34;https://app.sendgrid.com/guide/integrate\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;https://app.sendgrid.com/guide/integrate\u0026lt;/a\u0026gt; to create a free \u0026amp;#34;Web API\u0026amp;#34; email integration for cURL and its associated API key. Take note and save your key value and verify the integration. The key details will be needed when you create the Cloud Deploy approval process later in this blog. Note: Using SendGRID APIs DOES require you to create a user account.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;II. Configure Cloud Build\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;This step requires integrating your git repository (from Pre-Requisites, Step 1) as a managed repository to GCP\u0026#39;s cloud build service and creating the necessary Trigger. The goal of this integration is that any updates you make to your application within your GitHub repository will automatically kick off a Cloud Build deployment which will create, enable and deploy your application to GKE.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Create the GitHub Repository Integration for Cloud Build :\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;To start, from your GCP Console homepage, type \u0026amp;#34;Cloud Build\u0026amp;#34; within the search bar and select this service.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;From the left-hand panel, click on \u0026amp;#34;Triggers\u0026amp;#34;. And click on \u0026amp;#34;Connect Repository.\u0026amp;#34;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Select the source as \u0026amp;#34;GitHub (Cloud Build GitHub App)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Authenticate the connection with your GitHub credentials, select the forked repository, and click \u0026amp;#34;Connect\u0026amp;#34;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Once the integration is done, you will see your newly added repository under \u0026amp;#34;Triggers\u0026amp;#34; -\u0026amp;gt; \u0026amp;#34;Manage Repositories.\u0026amp;#34;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003cp\u003ef) Finally, create two Pub/Sub topics and Cloud Functions which will allow for email approvals for any GKE deployment to production and error reporting if a release fails.\u003cbr/\u003eNOTE\u003c/p\u003e\u003col\u003e\u003cli\u003eBefore you run the script, please validate if your new GCP project already contains a \u0026#34;default\u0026#34; VPC and subnetwork. If you already have a \u0026#34;default\u0026#34; VPC, please go through the script and COMMENT out lines 53-55 which reference the creation of a default VPC and subnetwork. If you already have one, this step is not needed.\u003cbr/\u003e\u003c/li\u003e\u003cli\u003eBy default, the creation of GKE clusters uses the \u0026#34;default\u0026#34; VPC subnetwork. If you prefer to use a non-default VPC, update the GKE cluster creation commands, starting at line 157, and update the \u003ccode\u003e--subnetwork\u003c/code\u003e value for all 3 GKE clusters.\u003cbr/\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e6. To execute the script, run the following command: \u003ccode\u003esh /scripts/gcp_env_setup.sh\u003c/code\u003e\u003c/p\u003e\u003cp\u003eg) This script will approximately take 20-22 minutes to complete. Once finished, the output should look similar to something like \u003ca href=\"https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/scripts/gcp_env_setup_OUTPUT.txt\" target=\"_blank\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ethis\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e7. Create a SendGRID API Key. Follow the instructions: \u003ca href=\"https://app.sendgrid.com/guide/integrate\" target=\"_blank\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://app.sendgrid.com\" track-metadata-module=\"post\"\u003ehttps://app.sendgrid.com/guide/integrate\u003c/a\u003e to create a free \u0026#34;Web API\u0026#34; email integration for cURL and its associated API key. Take note and save your key value and verify the integration. The key details will be needed when you create the Cloud Deploy approval process later in this blog. Note: Using SendGRID APIs DOES require you to create a user account.\u003c/p\u003e\u003ch3\u003eII. Configure Cloud Build\u003c/h3\u003e\u003cp\u003eThis step requires integrating your git repository (from Pre-Requisites, Step 1) as a managed repository to GCP\u0026#39;s cloud build service and creating the necessary Trigger. The goal of this integration is that any updates you make to your application within your GitHub repository will automatically kick off a Cloud Build deployment which will create, enable and deploy your application to GKE.\u003c/p\u003e\u003cp\u003eCreate the GitHub Repository Integration for Cloud Build :\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eTo start, from your GCP Console homepage, type \u0026#34;Cloud Build\u0026#34; within the search bar and select this service.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom the left-hand panel, click on \u0026#34;Triggers\u0026#34;. And click on \u0026#34;Connect Repository.\u0026#34;\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSelect the source as \u0026#34;GitHub (Cloud Build GitHub App)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAuthenticate the connection with your GitHub credentials, select the forked repository, and click \u0026#34;Connect\u0026#34;.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnce the integration is done, you will see your newly added repository under \u0026#34;Triggers\u0026#34; -\u0026gt; \u0026#34;Manage Repositories.\u0026#34;\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Create a Trigger for Cloud Build\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;From the \u0026amp;#34;Triggers\u0026amp;#34; page, click on \u0026amp;#34;+ Create Trigger\u0026amp;#34;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Enter/Select the following values for the Trigger:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Name: \u0026lt;code\u0026gt;CI/CD-blog-trigger\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Region: \u0026lt;code\u0026gt;us-central1\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Description: \u0026lt;code\u0026gt;Deploy Docker Image using GCP CI/CD cloud services.\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Event: \u0026lt;code\u0026gt;Push to a branch.\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Repository: Select your forked repository\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Branch: \u0026lt;code\u0026gt;^main$\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Configuration: \u0026lt;code\u0026gt;Cloud Build Configuration File (YAML or JSON)\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Location: \u0026lt;code\u0026gt;Repository\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cloud Build configuration file location: \u0026lt;code\u0026gt;/ cloudbuild.yaml\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Under \u0026amp;#34;Advanced\u0026amp;#34;, add the following TWO environment variables and their values: \u0026lt;code\u0026gt;_CONTAINER_REPO_NAME: test-repo _SEVERITY: CRITICAL\u0026lt;/code\u0026gt; \u0026lt;br\u0026gt;NOTE: The value of these env variables is case sensitive.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\"\u003e\u003cp\u003eCreate a Trigger for Cloud Build\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eFrom the \u0026#34;Triggers\u0026#34; page, click on \u0026#34;+ Create Trigger\u0026#34;\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEnter/Select the following values for the Trigger:\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eName: \u003ccode\u003eCI/CD-blog-trigger\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRegion: \u003ccode\u003eus-central1\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDescription: \u003ccode\u003eDeploy Docker Image using GCP CI/CD cloud services.\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEvent: \u003ccode\u003ePush to a branch.\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRepository: Select your forked repository\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBranch: \u003ccode\u003e^main$\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eConfiguration: \u003ccode\u003eCloud Build Configuration File (YAML or JSON)\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLocation: \u003ccode\u003eRepository\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Build configuration file location: \u003ccode\u003e/ cloudbuild.yaml\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUnder \u0026#34;Advanced\u0026#34;, add the following TWO environment variables and their values: \u003ccode\u003e_CONTAINER_REPO_NAME: test-repo _SEVERITY: CRITICAL\u003c/code\u003e \u003cbr/\u003eNOTE: The value of these env variables is case sensitive.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;3. After the environment values are entered/selected, click \u0026amp;#34;Create\u0026amp;#34;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Once the Trigger is created, it will look like the following:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e3. After the environment values are entered/selected, click \u0026#34;Create\u0026#34;.\u003c/p\u003e\u003cp\u003eOnce the Trigger is created, it will look like the following:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;III. Create Cloud Deploy Pipeline\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Now that we have created GitHub integration and Cloud Build Trigger, the next step is to create the Cloud Deploy pipeline. This will deploy the container image to the three GKE environments: \u0026amp;#34;test,\u0026amp;#34; \u0026amp;#34;staging,\u0026amp;#34; and \u0026amp;#34;prod\u0026amp;#34; once the image release for all three environments is created through Cloud Build. The requirement for image release requires a Cloud Deploy pipeline.\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Edit the clouddeploy.yaml file with your GCP project ID.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Within the file, update lines 22, 32, and 42 with your respective GCP project ID \u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003ch3\u003eIII. Create Cloud Deploy Pipeline\u003c/h3\u003e\u003cp\u003eNow that we have created GitHub integration and Cloud Build Trigger, the next step is to create the Cloud Deploy pipeline. This will deploy the container image to the three GKE environments: \u0026#34;test,\u0026#34; \u0026#34;staging,\u0026#34; and \u0026#34;prod\u0026#34; once the image release for all three environments is created through Cloud Build. The requirement for image release requires a Cloud Deploy pipeline.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eEdit the clouddeploy.yaml file with your GCP project ID.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWithin the file, update lines 22, 32, and 42 with your respective GCP project ID \u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;3. Once this is updated, save the file.\u0026lt;/p\u0026gt;4. Either through Cloud Shell or your local workstation, run the following GCP command to create the environment variables and the Cloud Deploy pipeline called \u0026lt;code\u0026gt;ci-cd-test\u0026lt;/code\u0026gt;:\"\u003e\u003cp\u003e3. Once this is updated, save the file.\u003c/p\u003e\u003cp\u003e4. Either through Cloud Shell or your local workstation, run the following GCP command to create the environment variables and the Cloud Deploy pipeline called \u003ccode\u003eci-cd-test\u003c/code\u003e:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c61=\"\"\u003e\u003cpre _ngcontent-c61=\"\"\u003e  \u003ccode _ngcontent-c61=\"\"\u003e$ PROJECT_ID=\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e$ LOCATION=us-central1\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e$ gcloud deploy apply --file clouddeploy.yaml --region=$LOCATION --project=$PROJECT_ID\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eNOTE: If you run into issues with a failed Cloud Deploy pipeline creation, delete the pipeline using the following gcloud command:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c61=\"\"\u003e\u003cpre _ngcontent-c61=\"\"\u003e  \u003ccode _ngcontent-c61=\"\"\u003egcloud deploy delivery-pipelines delete ci-cd-test --region=us-central1 --force\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003e5. Once the pipeline is created, here is what the output will look like:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c61=\"\"\u003e\u003cpre _ngcontent-c61=\"\"\u003e  \u003ccode _ngcontent-c61=\"\"\u003e$ gcloud deploy apply --file clouddeploy.yaml --region=$LOCATION --project=$PROJECT_ID\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eWaiting for the operation on resource projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/deliveryPipelines/ci-cd-test...done.   \n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eCreated Cloud Deploy resource: projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/deliveryPipelines/ci-cd-test.\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eWaiting for the operation on resource projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/test...done.   \n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eCreated Cloud Deploy resource: projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/test.\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eWaiting for the operation on resource projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/staging...done.   \n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eCreated Cloud Deploy resource: projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/staging.\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eWaiting for the operation on resource projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/prod...done.   \n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eCreated Cloud Deploy resource: projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/prod.\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003e6. From your GCP Console homepage, type \u0026#34;Cloud Deploy\u0026#34; within the search bar and select this service. From the main page, you will see the newly created pipeline. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;IV. Configure email notifications for GKE production cluster deployment\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;As part of a typical CI/CD process, any deployment of production workloads requires some form of approval process by DevOps engineers. Cloud Deploy allows you to inject an \u0026#39;approval\u0026#39; step before deploying a rollout to the next target. We have created this approval check in our pipeline before the deployment to the \u0026#39;prod\u0026#39; GKE cluster. Once the pipeline reaches the step to deploy the rollout to the \u0026#39;prod\u0026#39; GKE cluster, it emits a message in the \u0026lt;code\u0026gt;clouddeploy-approvals\u0026lt;/code\u0026gt; Pub/Sub topic. We have created a Cloud Function to listen to this topic and implement logic to send email notifications via Sendgrid. You can use any other library to send emails via Cloud Functions.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The one-time script has created a Pub/Sub topic and Cloud Function, allowing your cloud build release to send an approver email.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To validate that the Pub/Sub topics and Cloud Function was created, go to those respective services and ensure they were created.\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;From your GCP Console homepage, type \u0026amp;#34;Pub/Sub\u0026amp;#34; within the search bar and select this service. There will be two Pub/Sub topics, and they\u0026#39;re called \u0026lt;code\u0026gt;clouddeploy-approvals\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;clouddeploy-operations\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;From your GCP Console homepage, type \u0026amp;#34;Cloud Functions\u0026amp;#34; within the search bar and select this service. There will be two Cloud Functions, called \u0026lt;code\u0026gt;cd-approval\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;cd-deploy-notification\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click on \u0026lt;code\u0026gt;cd-approval\u0026lt;/code\u0026gt; and select \u0026amp;#34;Variables\u0026amp;#34;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click the \u0026amp;#34;Edit\u0026amp;#34; button and expand the `Runtime, build, connections and security settings.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Scroll down until you get to the \u0026amp;#34;Runtime environment variables.\u0026amp;#34; Here you will update the following three variables.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003ch3\u003eIV. Configure email notifications for GKE production cluster deployment\u003c/h3\u003e\u003cp\u003eAs part of a typical CI/CD process, any deployment of production workloads requires some form of approval process by DevOps engineers. Cloud Deploy allows you to inject an \u0026#39;approval\u0026#39; step before deploying a rollout to the next target. We have created this approval check in our pipeline before the deployment to the \u0026#39;prod\u0026#39; GKE cluster. Once the pipeline reaches the step to deploy the rollout to the \u0026#39;prod\u0026#39; GKE cluster, it emits a message in the \u003ccode\u003eclouddeploy-approvals\u003c/code\u003e Pub/Sub topic. We have created a Cloud Function to listen to this topic and implement logic to send email notifications via Sendgrid. You can use any other library to send emails via Cloud Functions.\u003c/p\u003e\u003cp\u003eThe one-time script has created a Pub/Sub topic and Cloud Function, allowing your cloud build release to send an approver email.\u003c/p\u003e\u003cp\u003eTo validate that the Pub/Sub topics and Cloud Function was created, go to those respective services and ensure they were created.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eFrom your GCP Console homepage, type \u0026#34;Pub/Sub\u0026#34; within the search bar and select this service. There will be two Pub/Sub topics, and they\u0026#39;re called \u003ccode\u003eclouddeploy-approvals\u003c/code\u003e and \u003ccode\u003eclouddeploy-operations\u003c/code\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom your GCP Console homepage, type \u0026#34;Cloud Functions\u0026#34; within the search bar and select this service. There will be two Cloud Functions, called \u003ccode\u003ecd-approval\u003c/code\u003e and \u003ccode\u003ecd-deploy-notification\u003c/code\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick on \u003ccode\u003ecd-approval\u003c/code\u003e and select \u0026#34;Variables\u0026#34;.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick the \u0026#34;Edit\u0026#34; button and expand the `Runtime, build, connections and security settings.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eScroll down until you get to the \u0026#34;Runtime environment variables.\u0026#34; Here you will update the following three variables.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;For \u0026lt;code\u0026gt;FROM_EMAIL\u0026lt;/code\u0026gt;, enter a secondary email account; it could be @gmail or any other domain of your choice. For \u0026lt;code\u0026gt;TO_EMAIL\u0026lt;/code\u0026gt;, select a primary email. For instance, the email of a DevOps Engineer who will be the approver of all production workload deployments to GKE. For \u0026lt;code\u0026gt;SENDGRID_API_KEY\u0026lt;/code\u0026gt;, you will enter your API Key, starting with \u0026amp;#34;SG.\u0026amp;#34;. If you haven\u0026#39;t already, refer to the Prerequisites section above, step 6, around creating this key.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;6. After you\u0026#39;ve updated the cloud function environment variables, click \u0026amp;#34;Next\u0026amp;#34; and \u0026amp;#34;Deploy\u0026amp;#34; the updated function. It will take about 1-2 minutes. Once completed, the function will have a green check mark to validate its running.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;7. Repeat steps 4-6 from above for the other cloud function of \u0026lt;code\u0026gt;cd-approval\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Step-by-step instructions of testing and validating the GCP CI/CD pipeline\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;Now that all the GCP prerequisites and environment setup is complete for Cloud Build, Cloud Deploy, and Email approvals, we\u0026#39;ll next deploy the image to GKE and initiate the pipeline testing.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;A couple of items to note during this test, we\u0026#39;re going to show a \u0026amp;#34;Happy\u0026amp;#34; and \u0026amp;#34;Vulnerable\u0026amp;#34; Image deployment path to GKE.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The \u0026amp;#34;Happy\u0026amp;#34; path will show a successful deployment of the end-to-end pipeline across nine steps for a clean image deployment to GKE. \u0026amp;#34;Clean\u0026amp;#34; refers to the docker image with non-critical vulnerabilities. This path will also update the Binary Authorization policy that allows only the \u0026amp;#34;Happy\u0026amp;#34; image to be deployed to GKE\u0026#39;s \u0026amp;#34;test\u0026amp;#34;, \u0026amp;#34;staging\u0026amp;#34;, and eventually \u0026amp;#34;production\u0026amp;#34; environments, which a DevOps engineer will approve.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eFor \u003ccode\u003eFROM_EMAIL\u003c/code\u003e, enter a secondary email account; it could be @gmail or any other domain of your choice. For \u003ccode\u003eTO_EMAIL\u003c/code\u003e, select a primary email. For instance, the email of a DevOps Engineer who will be the approver of all production workload deployments to GKE. For \u003ccode\u003eSENDGRID_API_KEY\u003c/code\u003e, you will enter your API Key, starting with \u0026#34;SG.\u0026#34;. If you haven\u0026#39;t already, refer to the Prerequisites section above, step 6, around creating this key.\u003c/p\u003e\u003cp\u003e6. After you\u0026#39;ve updated the cloud function environment variables, click \u0026#34;Next\u0026#34; and \u0026#34;Deploy\u0026#34; the updated function. It will take about 1-2 minutes. Once completed, the function will have a green check mark to validate its running.\u003c/p\u003e\u003cp\u003e7. Repeat steps 4-6 from above for the other cloud function of \u003ccode\u003ecd-approval\u003c/code\u003e.\u003c/p\u003e\u003ch2\u003eStep-by-step instructions of testing and validating the GCP CI/CD pipeline\u003c/h2\u003e\u003cp\u003eNow that all the GCP prerequisites and environment setup is complete for Cloud Build, Cloud Deploy, and Email approvals, we\u0026#39;ll next deploy the image to GKE and initiate the pipeline testing.\u003c/p\u003e\u003cp\u003eA couple of items to note during this test, we\u0026#39;re going to show a \u0026#34;Happy\u0026#34; and \u0026#34;Vulnerable\u0026#34; Image deployment path to GKE.\u003c/p\u003e\u003cp\u003eThe \u0026#34;Happy\u0026#34; path will show a successful deployment of the end-to-end pipeline across nine steps for a clean image deployment to GKE. \u0026#34;Clean\u0026#34; refers to the docker image with non-critical vulnerabilities. This path will also update the Binary Authorization policy that allows only the \u0026#34;Happy\u0026#34; image to be deployed to GKE\u0026#39;s \u0026#34;test\u0026#34;, \u0026#34;staging\u0026#34;, and eventually \u0026#34;production\u0026#34; environments, which a DevOps engineer will approve.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;The \u0026amp;#34;Vulnerable\u0026amp;#34; docker path will show a failed deployment of the end-to-end pipeline across seven steps. The pipeline will fail in 2 of these steps because the image has:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Certain vulnerabilities must be addressed before the image can be stored in the Artifact Registry.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A failed deployment to GKE because this is a non-approved image without attestation, violating the updated Binary Authorization policy from the \u0026amp;#34;Happy\u0026amp;#34; path.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;When Binary Authorization is enabled, its default policy allows all images to be deployed to the GKE target environments without attestation. In the \u0026amp;#34;Happy\u0026amp;#34; path, we will update the default Binary Authorization policy where only a specific docker image is approved for deployment to GKE. GKE will reject any other image not approved by the Binary Authorization policy at the deployment time.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To allow other images to be deployed to GKE through an active binary authorization policy, update the following script \u0026lt;code\u0026gt;/scripts/create_binauthz_policy.sh\u0026lt;/code\u0026gt; where you can sign the image digest to the existing attestor and allow for that image deployment to GKE.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the following sections, we\u0026#39;ll go into further detail describing both paths of image deployment to GKE.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;I. Run Cloud Build configuration file for \u0026amp;#34;Happy\u0026amp;#34; path\u0026lt;/h3\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Ensure your GitHub repo is connected as a repository in Cloud Build. Refer to the \u0026amp;#34;Create the GitHub Repository Integration for Cloud Build\u0026amp;#34; section on how to do this.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Ensure your Cloud Build trigger called \u0026lt;code\u0026gt;CI/CD-blog-trigger\u0026lt;/code\u0026gt; is created. Refer to the section \u0026amp;#34;Create a Trigger for Cloud Build\u0026amp;#34; on how to do this.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Since the Trigger is already enabled, any updates to your repository will trigger this Cloud Build deployment.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Open up the \u0026lt;code\u0026gt;cloudbuild.yaml\u0026lt;/code\u0026gt; from your GitHub repo. This is the cloud build configuration file for the \u0026amp;#34;Happy\u0026amp;#34; Docker path.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;To kick off the build, make any update to your codebase such as update the \u0026lt;code\u0026gt;/src/static/js\u0026lt;/code\u0026gt; file for any cosmetic change.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;After you\u0026#39;ve made the change, push the changes to your GitHub repo.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;From the GCP Console, go to the Cloud Build service and click on \u0026amp;#34;History\u0026amp;#34;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Since the Trigger is enabled and integrated with your GitHub page, the build is automatically kicked off, and you can click the custom build number to see the log details. \u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003cp\u003eThe \u0026#34;Vulnerable\u0026#34; docker path will show a failed deployment of the end-to-end pipeline across seven steps. The pipeline will fail in 2 of these steps because the image has:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eCertain vulnerabilities must be addressed before the image can be stored in the Artifact Registry.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA failed deployment to GKE because this is a non-approved image without attestation, violating the updated Binary Authorization policy from the \u0026#34;Happy\u0026#34; path.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhen Binary Authorization is enabled, its default policy allows all images to be deployed to the GKE target environments without attestation. In the \u0026#34;Happy\u0026#34; path, we will update the default Binary Authorization policy where only a specific docker image is approved for deployment to GKE. GKE will reject any other image not approved by the Binary Authorization policy at the deployment time.\u003c/p\u003e\u003cp\u003eTo allow other images to be deployed to GKE through an active binary authorization policy, update the following script \u003ccode\u003e/scripts/create_binauthz_policy.sh\u003c/code\u003e where you can sign the image digest to the existing attestor and allow for that image deployment to GKE.\u003c/p\u003e\u003cp\u003eIn the following sections, we\u0026#39;ll go into further detail describing both paths of image deployment to GKE.\u003c/p\u003e\u003ch3\u003eI. Run Cloud Build configuration file for \u0026#34;Happy\u0026#34; path\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eEnsure your GitHub repo is connected as a repository in Cloud Build. Refer to the \u0026#34;Create the GitHub Repository Integration for Cloud Build\u0026#34; section on how to do this.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEnsure your Cloud Build trigger called \u003ccode\u003eCI/CD-blog-trigger\u003c/code\u003e is created. Refer to the section \u0026#34;Create a Trigger for Cloud Build\u0026#34; on how to do this.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSince the Trigger is already enabled, any updates to your repository will trigger this Cloud Build deployment.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOpen up the \u003ccode\u003ecloudbuild.yaml\u003c/code\u003e from your GitHub repo. This is the cloud build configuration file for the \u0026#34;Happy\u0026#34; Docker path.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTo kick off the build, make any update to your codebase such as update the \u003ccode\u003e/src/static/js\u003c/code\u003e file for any cosmetic change.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAfter you\u0026#39;ve made the change, push the changes to your GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom the GCP Console, go to the Cloud Build service and click on \u0026#34;History\u0026#34;.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSince the Trigger is enabled and integrated with your GitHub page, the build is automatically kicked off, and you can click the custom build number to see the log details. \u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;II. Validate image deployment for \u0026amp;#34;Happy\u0026amp;#34; path\u0026lt;/h3\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Within that build, steps 7-9 highlight the image deployment to GKE through Cloud Deploy. If you click on step 9, the result of the build states that the deployment to \u0026amp;#34;prod\u0026amp;#34; is awaiting approval.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003ch3\u003eII. Validate image deployment for \u0026#34;Happy\u0026#34; path\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eWithin that build, steps 7-9 highlight the image deployment to GKE through Cloud Deploy. If you click on step 9, the result of the build states that the deployment to \u0026#34;prod\u0026#34; is awaiting approval.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;2. Go to the Cloud Deploy homepage from the GCP Console and click on the \u0026lt;code\u0026gt;ci-cd-test\u0026lt;/code\u0026gt; pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;3. Within the pipeline, click on the release associated with the latest cloud build deployment. Here you see that the \u0026amp;#34;Happy\u0026amp;#34; image is deployed successfully to both \u0026amp;#34;test\u0026amp;#34; and \u0026amp;#34;staging\u0026amp;#34;, but there\u0026#39;s an approval process required for the \u0026amp;#34;prod\u0026amp;#34; cluster.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e2. Go to the Cloud Deploy homepage from the GCP Console and click on the \u003ccode\u003eci-cd-test\u003c/code\u003e pipeline.\u003c/p\u003e\u003cp\u003e3. Within the pipeline, click on the release associated with the latest cloud build deployment. Here you see that the \u0026#34;Happy\u0026#34; image is deployed successfully to both \u0026#34;test\u0026#34; and \u0026#34;staging\u0026#34;, but there\u0026#39;s an approval process required for the \u0026#34;prod\u0026#34; cluster.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003e4. From the GCP Console, search for Kubernetes Engine; from the left-hand navigation, click on \u0026#34;Workloads.\u0026#34; Here you can see that the image deployment is successful in the two \u0026#34;test\u0026#34; and \u0026#34;staging\u0026#34; GKE environments. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003e5. Now that the deployment is queued for production, check your primary email and validate that you received a notification for approval. It will look something like this. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;6. From the email, click the \u0026lt;code\u0026gt;here\u0026lt;/code\u0026gt; hyperlink and it will take you to the Cloud deploy pipeline page.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;7. From the Pipeline page, approve or reject the release so the deployment can be pushed to \u0026amp;#34;prod\u0026amp;#34; in GKE. In this case, we will approve.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e6. From the email, click the \u003ccode\u003ehere\u003c/code\u003e hyperlink and it will take you to the Cloud deploy pipeline page.\u003c/p\u003e\u003cp\u003e7. From the Pipeline page, approve or reject the release so the deployment can be pushed to \u0026#34;prod\u0026#34; in GKE. In this case, we will approve.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003e7. If you go back to the Kubernetes workload page, you\u0026#39;ll see that the image rollout to prod was successful. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eIn parallel, validate your Cloud Deploy, continuous deployment pipeline also confirms a successful rollout. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;III. Run Cloud Build configuration file for \u0026amp;#34;Vulnerable\u0026amp;#34; path (container image has vulnerabilities)\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We will show two failure paths with this deployment: image vulnerabilities and Binary Authorization policy enforcement.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;A. First, failed deployment to push docker image to Artifact Registry because of severity-specific vulnerabilities -\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;1. Ensure your GitHub Repo is connected as a repository in Cloud Build. Refer to the \u0026amp;#34;Create the GitHub Repository Integration for Cloud Build\u0026amp;#34; section on how to do this.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;2. Ensure your Cloud Build Trigger called \u0026lt;code\u0026gt;CI/CD-blog-trigger\u0026lt;/code\u0026gt; is created. Refer to the section \u0026amp;#34;Create a Trigger for Cloud Build\u0026amp;#34; on how to do this.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;3. Since the Trigger is already enabled, any updates to your repository will trigger this cloud build deployment.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;4. View the \u0026lt;code\u0026gt;cloudbuild-vulnerable.yaml\u0026lt;/code\u0026gt; file from your GitHub repo. This is the cloud build configuration file for the \u0026amp;#34;Vulnerable\u0026amp;#34; Docker path.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;5. Edit the existing Trigger with the following:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click on the ellipses next to \u0026amp;#34;RUN\u0026amp;#34; and update the \u0026amp;#34;Cloud Build configuration file location\u0026amp;#34; to be: \u0026lt;code\u0026gt;cloudbuild-vulnerable.yaml\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Update the \u0026amp;#34;_SEVERITY\u0026amp;#34; environment variable value to be \u0026lt;code\u0026gt;HIGH\u0026lt;/code\u0026gt;. We\u0026#39;re changing the severity of the vulnerabilities because the vulnerability check will either PASS or FAIL a cloud build deployment if the image contains ANY \u0026lt;code\u0026gt;HIGH\u0026lt;/code\u0026gt; vulnerabilities.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Save the Trigger and validate its status as \u0026amp;#34;Enabled\u0026amp;#34;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;6. To kick off the build, make any update to your codebase, such as updating the \u0026lt;code\u0026gt;/src/static/js\u0026lt;/code\u0026gt; file for any cosmetic change. After you\u0026#39;ve made the change, push the changes to your GitHub repo.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;7. From the GCP Console, go to the Cloud Build service and click on \u0026amp;#34;History\u0026amp;#34;.\u0026lt;/p\u0026gt;8. The build will fail in \u0026lt;code\u0026gt;Step 2: Check For Vulnerabilities within the Image\u0026lt;/code\u0026gt; because this image contains HIGH vulnerabilities, and cloud build will NOT push this image to be stored in the artifact registry.\"\u003e\u003ch3\u003eIII. Run Cloud Build configuration file for \u0026#34;Vulnerable\u0026#34; path (container image has vulnerabilities)\u003c/h3\u003e\u003cp\u003eWe will show two failure paths with this deployment: image vulnerabilities and Binary Authorization policy enforcement.\u003c/p\u003e\u003cp\u003eA. First, failed deployment to push docker image to Artifact Registry because of severity-specific vulnerabilities -\u003c/p\u003e\u003cp\u003e1. Ensure your GitHub Repo is connected as a repository in Cloud Build. Refer to the \u0026#34;Create the GitHub Repository Integration for Cloud Build\u0026#34; section on how to do this.\u003c/p\u003e\u003cp\u003e2. Ensure your Cloud Build Trigger called \u003ccode\u003eCI/CD-blog-trigger\u003c/code\u003e is created. Refer to the section \u0026#34;Create a Trigger for Cloud Build\u0026#34; on how to do this.\u003c/p\u003e\u003cp\u003e3. Since the Trigger is already enabled, any updates to your repository will trigger this cloud build deployment.\u003c/p\u003e\u003cp\u003e4. View the \u003ccode\u003ecloudbuild-vulnerable.yaml\u003c/code\u003e file from your GitHub repo. This is the cloud build configuration file for the \u0026#34;Vulnerable\u0026#34; Docker path.\u003c/p\u003e\u003cp\u003e5. Edit the existing Trigger with the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eClick on the ellipses next to \u0026#34;RUN\u0026#34; and update the \u0026#34;Cloud Build configuration file location\u0026#34; to be: \u003ccode\u003ecloudbuild-vulnerable.yaml\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUpdate the \u0026#34;_SEVERITY\u0026#34; environment variable value to be \u003ccode\u003eHIGH\u003c/code\u003e. We\u0026#39;re changing the severity of the vulnerabilities because the vulnerability check will either PASS or FAIL a cloud build deployment if the image contains ANY \u003ccode\u003eHIGH\u003c/code\u003e vulnerabilities.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSave the Trigger and validate its status as \u0026#34;Enabled\u0026#34;.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e6. To kick off the build, make any update to your codebase, such as updating the \u003ccode\u003e/src/static/js\u003c/code\u003e file for any cosmetic change. After you\u0026#39;ve made the change, push the changes to your GitHub repo.\u003c/p\u003e\u003cp\u003e7. From the GCP Console, go to the Cloud Build service and click on \u0026#34;History\u0026#34;.\u003c/p\u003e\u003cp\u003e8. The build will fail in \u003ccode\u003eStep 2: Check For Vulnerabilities within the Image\u003c/code\u003e because this image contains HIGH vulnerabilities, and cloud build will NOT push this image to be stored in the artifact registry.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;B. Second, a failed image deployment to GKE because of Binary Authorization policy enforcement -\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Go back to the Trigger configuration for this build and change the \u0026amp;#34;_SEVERITY\u0026amp;#34; environment variable value to \u0026lt;code\u0026gt;CRITICAL\u0026lt;/code\u0026gt; instead of \u0026amp;#34;HIGH\u0026amp;#34;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;To kick off the build, make any update to your codebase such as update the \u0026lt;code\u0026gt;/src/static/js\u0026lt;/code\u0026gt; file for any cosmetic change. After you\u0026#39;ve made the change, push the changes to your GitHub repo.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;From the GCP Console, go to the Cloud Deploy pipeline \u0026lt;code\u0026gt;ci-cd-test\u0026lt;/code\u0026gt; and check the results of this latest release.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;From the Cloud Deploy pipeline page, approximately 10 minutes later, the build for \u0026amp;#34;test\u0026amp;#34; and \u0026amp;#34;staging\u0026amp;#34; will eventually fail because the Kubernetes manifest file for this docker image timed out. \u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003cp\u003eB. Second, a failed image deployment to GKE because of Binary Authorization policy enforcement -\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eGo back to the Trigger configuration for this build and change the \u0026#34;_SEVERITY\u0026#34; environment variable value to \u003ccode\u003eCRITICAL\u003c/code\u003e instead of \u0026#34;HIGH\u0026#34;.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTo kick off the build, make any update to your codebase such as update the \u003ccode\u003e/src/static/js\u003c/code\u003e file for any cosmetic change. After you\u0026#39;ve made the change, push the changes to your GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom the GCP Console, go to the Cloud Deploy pipeline \u003ccode\u003eci-cd-test\u003c/code\u003e and check the results of this latest release.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom the Cloud Deploy pipeline page, approximately 10 minutes later, the build for \u0026#34;test\u0026#34; and \u0026#34;staging\u0026#34; will eventually fail because the Kubernetes manifest file for this docker image timed out. \u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;You can change the timeout period to be shorter; additional details can be found \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;5. From the GCP Console, go to the GKE page and click on \u0026amp;#34;Workloads\u0026amp;#34;. Here you will see the image deployments to both the \u0026amp;#34;test\u0026amp;#34; and \u0026amp;#34;staging\u0026amp;#34; GKE environments failed. The reason being is binary authorization policy enforcement. The \u0026amp;#34;vulnerable\u0026amp;#34; docker image is not approved for deployment.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eYou can change the timeout period to be shorter; additional details can be found \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e\u003c/p\u003e\u003cp\u003e5. From the GCP Console, go to the GKE page and click on \u0026#34;Workloads\u0026#34;. Here you will see the image deployments to both the \u0026#34;test\u0026#34; and \u0026#34;staging\u0026#34; GKE environments failed. The reason being is binary authorization policy enforcement. The \u0026#34;vulnerable\u0026#34; docker image is not approved for deployment.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003e6. In parallel to a failed deployment to any of the GKE staging environments, Cloud Function \u003ccode\u003ecd-deploy-notification\u003c/code\u003e will send the following email to check the logs for the pipeline. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003e7. From the email, click on \u003ccode\u003ehere to see deployment logs\u003c/code\u003e, and it will take you to the log files within cloud build around additional details on the failure of the release rollout to GKE.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h2\u0026gt;Conclusion and further reading\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;In this blog post, we built a secure CI/CD pipeline using Google Cloud\u0026#39;s built-in services.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We learned how we can secure a CI/CD pipeline using Google Cloud\u0026#39;s built-in services, such as Binary Authorization and Vulnerability scanning of the container images. We only saw one way to put some control on specific images that can be deployed to a GKE cluster. Binary Authorization also offers \u0026lt;a href=\u0026#34;https://cloud.google.com/binary-authorization/docs/overview#attestations\u0026#34;\u0026gt;Build Verification\u0026lt;/a\u0026gt;, in which Binary Authorization uses attestations to verify that an image was built by a specific build system or continuous integration (CI) pipeline such as Cloud Build.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Additionally, Binary Authorization also writes all the events where the deployment of a container image is blocked due to the constraints defined by the security policy to the audit logs. You can create alerts on these log entries and notify the appropriate team members about the blocked deployment events.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lastly, all of the services used to build and secure the CI/CD pipelines are serverless, which makes it very easy to spin up the whole infrastructure within a few minutes without worrying about maintaining or managing it, so that your teams can focus on building and releasing software in a faster, reliable and cost efficient manner.\u0026lt;/p\u0026gt;\"\u003e\u003ch2\u003eConclusion and further reading\u003c/h2\u003e\u003cp\u003eIn this blog post, we built a secure CI/CD pipeline using Google Cloud\u0026#39;s built-in services.\u003c/p\u003e\u003cp\u003eWe learned how we can secure a CI/CD pipeline using Google Cloud\u0026#39;s built-in services, such as Binary Authorization and Vulnerability scanning of the container images. We only saw one way to put some control on specific images that can be deployed to a GKE cluster. Binary Authorization also offers \u003ca href=\"https://cloud.google.com/binary-authorization/docs/overview#attestations\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/binary-authorization/docs/overview#attestations\" track-metadata-module=\"post\"\u003eBuild Verification\u003c/a\u003e, in which Binary Authorization uses attestations to verify that an image was built by a specific build system or continuous integration (CI) pipeline such as Cloud Build.\u003c/p\u003e\u003cp\u003eAdditionally, Binary Authorization also writes all the events where the deployment of a container image is blocked due to the constraints defined by the security policy to the audit logs. You can create alerts on these log entries and notify the appropriate team members about the blocked deployment events.\u003c/p\u003e\u003cp\u003eLastly, all of the services used to build and secure the CI/CD pipelines are serverless, which makes it very easy to spin up the whole infrastructure within a few minutes without worrying about maintaining or managing it, so that your teams can focus on building and releasing software in a faster, reliable and cost efficient manner.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c59=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDevOps is a concept that allows software development teams to release software in an automated and stable manner. DevOps itself is not just one thing; it's a combination of culture and technology, which together make the implementation of DevOps successful.\u003c/p\u003e\u003cp\u003eIn this blog, we will be focusing on the tools and technology side of DevOps. At the core of the technical aspect of DevOps, the concept is Continuous Integration and Continuous Delivery (CI/CD). The idea behind CI/CD concept is to create an automated software delivery pipeline that continuously deploys the new software releases in an automated fashion.\u003c/p\u003e\u003cp\u003eThe flow begins with the developers committing the code changes to a source code repository, which automatically triggers the delivery pipeline (henceforth called CI/CD pipeline) by building and deploying the code changes into various environments, from non-prod environments to production environments.\u003c/p\u003e\u003cp\u003eAlso, as we build the CI/CD pipelines for faster and more reliable software delivery, the security aspect should not be ignored and must be incorporated into the pipeline right from the beginning. When we build our source code, we typically use various open-source libraries and container images. Having some security safeguards within the CI/CD pipeline is imperative to ensure that the software we are building and deploying is free from any vulnerability. Additionally, it's equally important to control what type of code/container image should be allowed to be deployed on your target runtime environment.\u003c/p\u003e\u003cp\u003eSecurity is everyone's responsibility. \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-shifting-left-on-security\"\u003eShifting left\u003c/a\u003e on security is a DevOps practice that allows you to address security concerns early in the software development lifecycle. Vulnerability scanning of container images, putting security policies in place through Binary Authorization, and allowing approved/trusted images to be deployed on GKE are a couple of ways to implement this policy to make your CI/CD pipelines more secure.\u003c/p\u003e\u003cp\u003eWhat are we building?\u003c/p\u003e\u003cp\u003eThis blog post will show how to build a secure CI/CD pipeline using Google Cloud's built-in services. We will create a secure software delivery pipeline that builds a sample Node.js application as a container image and deploys it on GKE clusters.\u003c/p\u003e\u003cp\u003eHow are we building the CI/CD pipeline?\u003c/p\u003e\u003cp\u003eWe're going to use the following Google Cloud built-in services to build the pipeline:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/build\"\u003eCloud Build\u003c/a\u003e - Cloud Build is an entirely serverless CI/CD platform that allows you to automate your build, test, and deploy tasks.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry\"\u003eArtifact Registry\u003c/a\u003e - Artifact Registry is a secure service to store and manage your build artifacts.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy\"\u003eCloud Deploy\u003c/a\u003e - Cloud Deploy is a fully managed Continuous Delivery service for GKE and Anthos.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/binary-authorization\"\u003eBinary Authorization\u003c/a\u003e - Binary Authorization provides deployment time security controls for GKE and Cloud Run deployments.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGKE\u003c/a\u003e - GKE is a fully managed Kubernetes platform.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/pubsub\"\u003eGoogle Pub/Sub\u003c/a\u003e - Pub/Sub is a serverless messaging platform.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/functions\"\u003eCloud Functions\u003c/a\u003e - Cloud Functions is a serverless platform to run your code.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eWe use GitHub as a source code repository and Sendgrid APIs to send email notifications for approval and error logging.\u003c/p\u003e\u003cp\u003eThe CI/CD pipeline is set up so that a Cloud Build trigger is configured to sense any code pushed to a particular repository and branch in a GitHub repository and automatically starts the build process.\u003c/p\u003e\u003cp\u003eBelow is the flow of how the CI/CD pipeline is set up without any security policy enforcement:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eDeveloper checks in the code to a GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and starts the 'build' process. A successful build results in a docker container image.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe container image is stored in the Artifact Registry.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Deploy is configured to go through an approval step before deploying the image to the Production GKE cluster.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA Cloud Function sends an email to a pre-configured email id, notifying you that a Cloud Deploy rollout requires your approval. The email receiver can approve or reject the deployment to the production GKE cluster. Cloud Function code can be found \u003ca href=\"https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/cloud-function/index.js\" target=\"_blank\"\u003ehere\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eTo secure this CI/CD pipeline, we will use a couple of Google Cloud's built-in features and services. First, we will enable vulnerability scans on Artifact Registry, an out-of-the-box feature. Then finally, we will create a security policy using the Binary Authorization service, which only allows a specific image to be deployed to your GKE cluster.\u003c/p\u003e\u003cp\u003eBelow is the flow when we try to build and deploy a container image that has vulnerabilities present:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eDeveloper checks in the code to a GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the 'build' process.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe build process fails with the error message that vulnerabilities were found in the image.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eBelow is the flow when we try to deploy a container image to GKE, which violates a Binary Authorization policy:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eDeveloper checks in the code to a GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA Cloud Build trigger is configured to sense any new code pushed to this GitHub repo and start the 'build' process. A successful build results in a docker container image.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe container image is stored in Artifact Registry.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe Build process kicks off a Cloud Deploy deployment process that deploys the container image to three different GKE clusters, pre-configured as the deployment pipeline mimicking the test, staging, and production environments.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Deploy fails as the GKE clusters reject the incoming image as it violates the existing Binary Authorization policy. Please note that an approval email is still triggered before the production deployment via the Cloud Function; the email receiver is expected to reject this release based on the failures in the previous stages.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnce the deployment fails due to the Binary Authorization policy violation, Cloud Function sends an email to a pre-configured email id about the deployment failure. Cloud Function code can be found \u003ca href=\"https://github.com/sysdesign-code/dev-sec-ops-demo/tree/main/cloud-function/deployment-notification\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eNote: The deployment fails after the timeout value is exceeded, set for Cloud Deploy, which is 10 minutes by default, but you can change this value according to your requirements, see \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout\"\u003ehere\u003c/a\u003e for more details.\u003c/p\u003e\u003cp\u003eNote: The Cloud Function code provided for the rollout approval email and deployment failure notification is under the folder cloud-functions in this repo. You will still have to create these cloud functions with this code in your Google Cloud project to receive email notifications.\u003c/p\u003e\u003ch2\u003eSolution Architecture\u003c/h2\u003e\u003cp\u003eThe CI/CD pipeline is constructed by combining the aforementioned Google Cloud services. Cloud Build is at the center of automating the pipeline, which contains all the steps we need to build and deploy our container image. Cloud Build executes the steps defined in a YAML file sequentially. It's quite flexible in terms of how you want to define your 'build' and 'deploy' process, and the service ensures to execute those steps reliably every time.\u003c/p\u003e\u003cp\u003eBelow are solution diagrams of how the CI/CD pipeline is set up :\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/A_secure_CICD_pipeline.max-2800x2800.jpeg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"A secure CI:CD pipeline.jpeg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/A_secure_CICD_pipeline.max-1000x1000.jpeg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAs the last step of our CI process, the Cloud Build YAML triggers the Cloud Deploy service, and the container image is deployed to three different GKE clusters. Cloud Deploy automatically emits multiple notifications to pub/Sub topics throughout the deployment process. We are using Cloud Functions to listen to these Pub/Sub topics to send appropriate email notifications about the deployment status and required approvals.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/B_secure_CICD_pipeline.max-2800x2800.jpeg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"B secure CI:CD pipeline.jpeg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/B_secure_CICD_pipeline.max-1000x1000.jpeg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch2\u003eStep-by-Step instructions for creating the CI/CD pipeline\u003c/h2\u003e\u003ch3\u003eI. Prerequisites\u003c/h3\u003e\u003cp\u003eThese steps are required to set up and prepare your GCP environment. We highly recommend you create a new GCP Project as you will run multiple cloud services within the region \"us-central1\".\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eFork the following GitHub Repo: \u003ca href=\"https://github.com/sysdesign-code/dev-sec-ops-demo\" target=\"_blank\"\u003ehttps://github.com/sysdesign-code/dev-sec-ops-demo\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCreate a new GCP Project, follow the steps here around how to provision and create one: \u003ca href=\"https://cloud.google.com/resource-manager/docs/creating-managing-projects\"\u003ehttps://cloud.google.com/resource-manager/docs/creating-managing-projects\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnce your new project is created, enable Cloud SDK to allow CLI access for \u003ccode\u003egcloud\u003c/code\u003e either in Cloud Shell or your local workstation. Follow the steps here: \u003ca href=\"https://cloud.google.com/sdk/docs/install\"\u003ehttps://cloud.google.com/sdk/docs/install\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnce you've enabled CLI access, either through your Cloud Shell or local workstation, validate or set your project ID:\u003cbr/\u003e\u003ccode\u003egcloud config set project YOUR_PROJECT_ID\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRun the following one-time script \u003ccode\u003e/scripts/gcp_env_setup.sh\u003c/code\u003e, which creates and provisions the necessary GCP cloud services required to create the DevSecOps CI/CD pipeline for deploying a sample docker application.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/C_secure_CICD_pipeline.max-2800x2800.jpeg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"C secure CI:CD pipeline.jpeg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/C_secure_CICD_pipeline.max-1000x1000.jpeg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eHere are all the service deployments that will occur once the script finishes:\u003c/p\u003e\u003cp\u003ea) Enables all the required cloud service APIs such as Cloud Build, Binary Authorization, Kubernetes Service, Artifact Registry, Cloud Deploy, and many more.\u003c/p\u003e\u003cp\u003eb) Create three (3) GKE clusters for test, staging, and production to show image rollout deployments across these clusters using Cloud Deploy.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/D_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"D secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/D_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003ec) Bind all the necessary IAM roles and permissions for Cloud Build and Cloud Deploy.\u003c/p\u003e\u003cp\u003ed) Create a Binary Authorization attestor, associated container note, cryptographic KMS key, and all the associated IAM roles and permissions to allow container note access for the attestor.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/E_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"E secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/E_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eBy default, the binary authorization policy allows for all images to be deployed to GCP. Later, we will update this policy only to allow attestor-approved images to be deployed to specific GKE clusters.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/F_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"F secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/F_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003ee) Create the Artifact Registry repository where the docker image will be stored.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/G_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"G secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/G_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003ef) Finally, create two Pub/Sub topics and Cloud Functions which will allow for email approvals for any GKE deployment to production and error reporting if a release fails.\u003cbr/\u003eNOTE\u003cbr/\u003e\u003col\u003e\u003cli\u003eBefore you run the script, please validate if your new GCP project already contains a \"default\" VPC and subnetwork. If you already have a \"default\" VPC, please go through the script and COMMENT out lines 53-55 which reference the creation of a default VPC and subnetwork. If you already have one, this step is not needed.\u003cbr/\u003e\u003c/li\u003e\u003cli\u003eBy default, the creation of GKE clusters uses the \"default\" VPC subnetwork. If you prefer to use a non-default VPC, update the GKE cluster creation commands, starting at line 157, and update the \u003ccode\u003e--subnetwork\u003c/code\u003e value for all 3 GKE clusters.\u003cbr/\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e6. To execute the script, run the following command: \u003ccode\u003esh /scripts/gcp_env_setup.sh\u003c/code\u003e\u003c/p\u003e\u003cp\u003eg) This script will approximately take 20-22 minutes to complete. Once finished, the output should look similar to something like \u003ca href=\"https://github.com/sysdesign-code/dev-sec-ops-demo/blob/main/scripts/gcp_env_setup_OUTPUT.txt\" target=\"_blank\"\u003ethis\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e7. Create a SendGRID API Key. Follow the instructions: \u003ca href=\"https://app.sendgrid.com/guide/integrate\" target=\"_blank\"\u003ehttps://app.sendgrid.com/guide/integrate\u003c/a\u003e to create a free \"Web API\" email integration for cURL and its associated API key. Take note and save your key value and verify the integration. The key details will be needed when you create the Cloud Deploy approval process later in this blog. Note: Using SendGRID APIs DOES require you to create a user account.\u003c/p\u003e\u003ch3\u003eII. Configure Cloud Build\u003c/h3\u003e\u003cp\u003eThis step requires integrating your git repository (from Pre-Requisites, Step 1) as a managed repository to GCP's cloud build service and creating the necessary Trigger. The goal of this integration is that any updates you make to your application within your GitHub repository will automatically kick off a Cloud Build deployment which will create, enable and deploy your application to GKE.\u003c/p\u003e\u003cp\u003eCreate the GitHub Repository Integration for Cloud Build :\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eTo start, from your GCP Console homepage, type \"Cloud Build\" within the search bar and select this service.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom the left-hand panel, click on \"Triggers\". And click on \"Connect Repository.\"\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSelect the source as \"GitHub (Cloud Build GitHub App)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAuthenticate the connection with your GitHub credentials, select the forked repository, and click \"Connect\".\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOnce the integration is done, you will see your newly added repository under \"Triggers\" -\u0026gt; \"Manage Repositories.\"\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/H_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"H secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/H_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eCreate a Trigger for Cloud Build\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eFrom the \"Triggers\" page, click on \"+ Create Trigger\"\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEnter/Select the following values for the Trigger:\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eName: \u003ccode\u003eCI/CD-blog-trigger\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRegion: \u003ccode\u003eus-central1\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDescription: \u003ccode\u003eDeploy Docker Image using GCP CI/CD cloud services.\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEvent: \u003ccode\u003ePush to a branch.\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRepository: Select your forked repository\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBranch: \u003ccode\u003e^main$\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eConfiguration: \u003ccode\u003eCloud Build Configuration File (YAML or JSON)\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLocation: \u003ccode\u003eRepository\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Build configuration file location: \u003ccode\u003e/ cloudbuild.yaml\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUnder \"Advanced\", add the following TWO environment variables and their values: \u003ccode\u003e_CONTAINER_REPO_NAME: test-repo _SEVERITY: CRITICAL\u003c/code\u003e \u003cbr/\u003eNOTE: The value of these env variables is case sensitive.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/I_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"I secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/I_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e3. After the environment values are entered/selected, click \"Create\".\u003c/p\u003e\u003cp\u003eOnce the Trigger is created, it will look like the following:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/J_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"J secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/J_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eIII. Create Cloud Deploy Pipeline\u003c/h3\u003e\u003cp\u003eNow that we have created GitHub integration and Cloud Build Trigger, the next step is to create the Cloud Deploy pipeline. This will deploy the container image to the three GKE environments: \"test,\" \"staging,\" and \"prod\" once the image release for all three environments is created through Cloud Build. The requirement for image release requires a Cloud Deploy pipeline.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eEdit the clouddeploy.yaml file with your GCP project ID.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWithin the file, update lines 22, 32, and 42 with your respective GCP project ID\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/K_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"K secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/K_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e3. Once this is updated, save the file.\u003c/p\u003e4. Either through Cloud Shell or your local workstation, run the following GCP command to create the environment variables and the Cloud Deploy pipeline called \u003ccode\u003eci-cd-test\u003c/code\u003e:\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'$ PROJECT_ID=\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;\\r\\n$ LOCATION=us-central1\\r\\n$ gcloud deploy apply --file clouddeploy.yaml --region=$LOCATION --project=$PROJECT_ID'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3ee4e8ede710\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eNOTE: If you run into issues with a failed Cloud Deploy pipeline creation, delete the pipeline using the following gcloud command:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'gcloud deploy delivery-pipelines delete ci-cd-test --region=us-central1 --force'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3ee4e8ede350\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e5. Once the pipeline is created, here is what the output will look like:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'$ gcloud deploy apply --file clouddeploy.yaml --region=$LOCATION --project=$PROJECT_ID\\r\\nWaiting for the operation on resource projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/deliveryPipelines/ci-cd-test...done. \\r\\nCreated Cloud Deploy resource: projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/deliveryPipelines/ci-cd-test.\\r\\nWaiting for the operation on resource projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/test...done. \\r\\nCreated Cloud Deploy resource: projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/test.\\r\\nWaiting for the operation on resource projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/staging...done. \\r\\nCreated Cloud Deploy resource: projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/staging.\\r\\nWaiting for the operation on resource projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/prod...done. \\r\\nCreated Cloud Deploy resource: projects/\u0026lt;\u0026lt;YOUR_PROJECT_ID\u0026gt;\u0026gt;/locations/us-central1/targets/prod.'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3ee4ea052850\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e6. From your GCP Console homepage, type \"Cloud Deploy\" within the search bar and select this service. From the main page, you will see the newly created pipeline.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/L_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"L secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/L_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eIV. Configure email notifications for GKE production cluster deployment\u003c/h3\u003e\u003cp\u003eAs part of a typical CI/CD process, any deployment of production workloads requires some form of approval process by DevOps engineers. Cloud Deploy allows you to inject an 'approval' step before deploying a rollout to the next target. We have created this approval check in our pipeline before the deployment to the 'prod' GKE cluster. Once the pipeline reaches the step to deploy the rollout to the 'prod' GKE cluster, it emits a message in the \u003ccode\u003eclouddeploy-approvals\u003c/code\u003e Pub/Sub topic. We have created a Cloud Function to listen to this topic and implement logic to send email notifications via Sendgrid. You can use any other library to send emails via Cloud Functions.\u003c/p\u003e\u003cp\u003eThe one-time script has created a Pub/Sub topic and Cloud Function, allowing your cloud build release to send an approver email.\u003c/p\u003e\u003cp\u003eTo validate that the Pub/Sub topics and Cloud Function was created, go to those respective services and ensure they were created.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eFrom your GCP Console homepage, type \"Pub/Sub\" within the search bar and select this service. There will be two Pub/Sub topics, and they're called \u003ccode\u003eclouddeploy-approvals\u003c/code\u003e and \u003ccode\u003eclouddeploy-operations\u003c/code\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom your GCP Console homepage, type \"Cloud Functions\" within the search bar and select this service. There will be two Cloud Functions, called \u003ccode\u003ecd-approval\u003c/code\u003e and \u003ccode\u003ecd-deploy-notification\u003c/code\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick on \u003ccode\u003ecd-approval\u003c/code\u003e and select \"Variables\".\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick the \"Edit\" button and expand the `Runtime, build, connections and security settings.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eScroll down until you get to the \"Runtime environment variables.\" Here you will update the following three variables.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/M_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"M secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/M_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eFor \u003ccode\u003eFROM_EMAIL\u003c/code\u003e, enter a secondary email account; it could be @gmail or any other domain of your choice. For \u003ccode\u003eTO_EMAIL\u003c/code\u003e, select a primary email. For instance, the email of a DevOps Engineer who will be the approver of all production workload deployments to GKE. For \u003ccode\u003eSENDGRID_API_KEY\u003c/code\u003e, you will enter your API Key, starting with \"SG.\". If you haven't already, refer to the Prerequisites section above, step 6, around creating this key.\u003c/p\u003e\u003cp\u003e6. After you've updated the cloud function environment variables, click \"Next\" and \"Deploy\" the updated function. It will take about 1-2 minutes. Once completed, the function will have a green check mark to validate its running.\u003c/p\u003e\u003cp\u003e7. Repeat steps 4-6 from above for the other cloud function of \u003ccode\u003ecd-approval\u003c/code\u003e.\u003c/p\u003e\u003ch2\u003eStep-by-step instructions of testing and validating the GCP CI/CD pipeline\u003c/h2\u003e\u003cp\u003eNow that all the GCP prerequisites and environment setup is complete for Cloud Build, Cloud Deploy, and Email approvals, we'll next deploy the image to GKE and initiate the pipeline testing.\u003c/p\u003e\u003cp\u003eA couple of items to note during this test, we're going to show a \"Happy\" and \"Vulnerable\" Image deployment path to GKE.\u003c/p\u003e\u003cp\u003eThe \"Happy\" path will show a successful deployment of the end-to-end pipeline across nine steps for a clean image deployment to GKE. \"Clean\" refers to the docker image with non-critical vulnerabilities. This path will also update the Binary Authorization policy that allows only the \"Happy\" image to be deployed to GKE's \"test\", \"staging\", and eventually \"production\" environments, which a DevOps engineer will approve.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/N_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"N secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/N_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe \"Vulnerable\" docker path will show a failed deployment of the end-to-end pipeline across seven steps. The pipeline will fail in 2 of these steps because the image has:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eCertain vulnerabilities must be addressed before the image can be stored in the Artifact Registry.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA failed deployment to GKE because this is a non-approved image without attestation, violating the updated Binary Authorization policy from the \"Happy\" path.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhen Binary Authorization is enabled, its default policy allows all images to be deployed to the GKE target environments without attestation. In the \"Happy\" path, we will update the default Binary Authorization policy where only a specific docker image is approved for deployment to GKE. GKE will reject any other image not approved by the Binary Authorization policy at the deployment time.\u003c/p\u003e\u003cp\u003eTo allow other images to be deployed to GKE through an active binary authorization policy, update the following script \u003ccode\u003e/scripts/create_binauthz_policy.sh\u003c/code\u003e where you can sign the image digest to the existing attestor and allow for that image deployment to GKE.\u003c/p\u003e\u003cp\u003eIn the following sections, we'll go into further detail describing both paths of image deployment to GKE.\u003c/p\u003e\u003ch3\u003eI. Run Cloud Build configuration file for \"Happy\" path\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eEnsure your GitHub repo is connected as a repository in Cloud Build. Refer to the \"Create the GitHub Repository Integration for Cloud Build\" section on how to do this.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEnsure your Cloud Build trigger called \u003ccode\u003eCI/CD-blog-trigger\u003c/code\u003e is created. Refer to the section \"Create a Trigger for Cloud Build\" on how to do this.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSince the Trigger is already enabled, any updates to your repository will trigger this Cloud Build deployment.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOpen up the \u003ccode\u003ecloudbuild.yaml\u003c/code\u003e from your GitHub repo. This is the cloud build configuration file for the \"Happy\" Docker path.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTo kick off the build, make any update to your codebase such as update the \u003ccode\u003e/src/static/js\u003c/code\u003e file for any cosmetic change.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAfter you've made the change, push the changes to your GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom the GCP Console, go to the Cloud Build service and click on \"History\".\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSince the Trigger is enabled and integrated with your GitHub page, the build is automatically kicked off, and you can click the custom build number to see the log details.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/O_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"O secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/O_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eII. Validate image deployment for \"Happy\" path\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eWithin that build, steps 7-9 highlight the image deployment to GKE through Cloud Deploy. If you click on step 9, the result of the build states that the deployment to \"prod\" is awaiting approval.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/P_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"P secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/P_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e2. Go to the Cloud Deploy homepage from the GCP Console and click on the \u003ccode\u003eci-cd-test\u003c/code\u003e pipeline.\u003c/p\u003e\u003cp\u003e3. Within the pipeline, click on the release associated with the latest cloud build deployment. Here you see that the \"Happy\" image is deployed successfully to both \"test\" and \"staging\", but there's an approval process required for the \"prod\" cluster.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Q_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Q secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Q_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e4. From the GCP Console, search for Kubernetes Engine; from the left-hand navigation, click on \"Workloads.\" Here you can see that the image deployment is successful in the two \"test\" and \"staging\" GKE environments.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/R_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"R secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/R_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e5. Now that the deployment is queued for production, check your primary email and validate that you received a notification for approval. It will look something like this.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/S_secure_CICD_pipeline_1.1000062520000400.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"S secure CI:CD pipeline.png\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/S_secure_CICD_pipeline_1.1000062520000400.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e6. From the email, click the \u003ccode\u003ehere\u003c/code\u003e hyperlink and it will take you to the Cloud deploy pipeline page.\u003c/p\u003e\u003cp\u003e7. From the Pipeline page, approve or reject the release so the deployment can be pushed to \"prod\" in GKE. In this case, we will approve.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/T_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"T secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/T_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e7. If you go back to the Kubernetes workload page, you'll see that the image rollout to prod was successful.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/U_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"U secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/U_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn parallel, validate your Cloud Deploy, continuous deployment pipeline also confirms a successful rollout.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/V_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"V secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/V_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eIII. Run Cloud Build configuration file for \"Vulnerable\" path (container image has vulnerabilities)\u003c/h3\u003e\u003cp\u003eWe will show two failure paths with this deployment: image vulnerabilities and Binary Authorization policy enforcement.\u003c/p\u003e\u003cp\u003eA. First, failed deployment to push docker image to Artifact Registry because of severity-specific vulnerabilities -\u003c/p\u003e\u003cp\u003e1. Ensure your GitHub Repo is connected as a repository in Cloud Build. Refer to the \"Create the GitHub Repository Integration for Cloud Build\" section on how to do this.\u003c/p\u003e\u003cp\u003e2. Ensure your Cloud Build Trigger called \u003ccode\u003eCI/CD-blog-trigger\u003c/code\u003e is created. Refer to the section \"Create a Trigger for Cloud Build\" on how to do this.\u003c/p\u003e\u003cp\u003e3. Since the Trigger is already enabled, any updates to your repository will trigger this cloud build deployment.\u003c/p\u003e\u003cp\u003e4. View the \u003ccode\u003ecloudbuild-vulnerable.yaml\u003c/code\u003e file from your GitHub repo. This is the cloud build configuration file for the \"Vulnerable\" Docker path.\u003c/p\u003e\u003cp\u003e5. Edit the existing Trigger with the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eClick on the ellipses next to \"RUN\" and update the \"Cloud Build configuration file location\" to be: \u003ccode\u003ecloudbuild-vulnerable.yaml\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUpdate the \"_SEVERITY\" environment variable value to be \u003ccode\u003eHIGH\u003c/code\u003e. We're changing the severity of the vulnerabilities because the vulnerability check will either PASS or FAIL a cloud build deployment if the image contains ANY \u003ccode\u003eHIGH\u003c/code\u003e vulnerabilities.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSave the Trigger and validate its status as \"Enabled\".\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e6. To kick off the build, make any update to your codebase, such as updating the \u003ccode\u003e/src/static/js\u003c/code\u003e file for any cosmetic change. After you've made the change, push the changes to your GitHub repo.\u003c/p\u003e\u003cp\u003e7. From the GCP Console, go to the Cloud Build service and click on \"History\".\u003c/p\u003e8. The build will fail in \u003ccode\u003eStep 2: Check For Vulnerabilities within the Image\u003c/code\u003e because this image contains HIGH vulnerabilities, and cloud build will NOT push this image to be stored in the artifact registry.\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/W_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"W secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/W_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eB. Second, a failed image deployment to GKE because of Binary Authorization policy enforcement -\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eGo back to the Trigger configuration for this build and change the \"_SEVERITY\" environment variable value to \u003ccode\u003eCRITICAL\u003c/code\u003e instead of \"HIGH\".\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTo kick off the build, make any update to your codebase such as update the \u003ccode\u003e/src/static/js\u003c/code\u003e file for any cosmetic change. After you've made the change, push the changes to your GitHub repo.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom the GCP Console, go to the Cloud Deploy pipeline \u003ccode\u003eci-cd-test\u003c/code\u003e and check the results of this latest release.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFrom the Cloud Deploy pipeline page, approximately 10 minutes later, the build for \"test\" and \"staging\" will eventually fail because the Kubernetes manifest file for this docker image timed out.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/X_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"X secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/X_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou can change the timeout period to be shorter; additional details can be found \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application#change_the_deployment_timeout\"\u003ehere\u003c/a\u003e\u003c/p\u003e\u003cp\u003e5. From the GCP Console, go to the GKE page and click on \"Workloads\". Here you will see the image deployments to both the \"test\" and \"staging\" GKE environments failed. The reason being is binary authorization policy enforcement. The \"vulnerable\" docker image is not approved for deployment.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Y_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Y secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Y_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e6. In parallel to a failed deployment to any of the GKE staging environments, Cloud Function \u003ccode\u003ecd-deploy-notification\u003c/code\u003e will send the following email to check the logs for the pipeline.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/S_secure_CICD_pipeline_1.1000062520000400.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"S secure CI:CD pipeline.png\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/S_secure_CICD_pipeline_1.1000062520000400.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e7. From the email, click on \u003ccode\u003ehere to see deployment logs\u003c/code\u003e, and it will take you to the log files within cloud build around additional details on the failure of the release rollout to GKE.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Z_secure_CICD_pipeline.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Z secure CI:CD pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Z_secure_CICD_pipeline.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch2\u003eConclusion and further reading\u003c/h2\u003e\u003cp\u003eIn this blog post, we built a secure CI/CD pipeline using Google Cloud's built-in services.\u003c/p\u003e\u003cp\u003eWe learned how we can secure a CI/CD pipeline using Google Cloud's built-in services, such as Binary Authorization and Vulnerability scanning of the container images. We only saw one way to put some control on specific images that can be deployed to a GKE cluster. Binary Authorization also offers \u003ca href=\"https://cloud.google.com/binary-authorization/docs/overview#attestations\"\u003eBuild Verification\u003c/a\u003e, in which Binary Authorization uses attestations to verify that an image was built by a specific build system or continuous integration (CI) pipeline such as Cloud Build.\u003c/p\u003e\u003cp\u003eAdditionally, Binary Authorization also writes all the events where the deployment of a container image is blocked due to the constraints defined by the security policy to the audit logs. You can create alerts on these log entries and notify the appropriate team members about the blocked deployment events.\u003c/p\u003e\u003cp\u003eLastly, all of the services used to build and secure the CI/CD pipelines are serverless, which makes it very easy to spin up the whole infrastructure within a few minutes without worrying about maintaining or managing it, so that your teams can focus on building and releasing software in a faster, reliable and cost efficient manner.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout_external\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003e\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-09-13T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eNitin Vashishtha\u003c/name\u003e\u003ctitle\u003eCustomer Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-control-plane-metrics-are-generally-available/",
      "title": "Introducing Kubernetes control plane metrics in GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c65=\"\"\u003e\u003cdiv _ngcontent-c65=\"\" innerhtml=\"\u0026lt;p\u0026gt;An essential aspect of operating any application is the ability to observe the health and performance of that application and of the underlying infrastructure to quickly resolve issues as they arise. \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE) already provides \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging\u0026#34;\u0026gt;audit logs\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs\u0026#34;\u0026gt;operational logs\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics\u0026#34;\u0026gt;metrics\u0026lt;/a\u0026gt; along with \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/gke/observing\u0026#34;\u0026gt;out-of-the-box dashboards\u0026lt;/a\u0026gt; and automatic \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/setup/kubernetes-engine\u0026#34;\u0026gt;error reporting\u0026lt;/a\u0026gt; to facilitate running reliable applications at scale. Using these logs and metrics, \u0026lt;a href=\u0026#34;https://cloud.google.com/products/operations\u0026#34;\u0026gt;Cloud Operations\u0026lt;/a\u0026gt; provides the alerts, monitoring dashboards and a \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface\u0026#34;\u0026gt;Logs Explorer\u0026lt;/a\u0026gt; to quickly detect, troubleshoot and resolve issues.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Introducing Kubernetes control plane metrics and why they matter\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;In addition to these existing sources of telemetry data, we are excited to announce that we are now exposing \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics\u0026#34;\u0026gt;Kubernetes control plane metrics\u0026lt;/a\u0026gt;, which are now Generally Available. With GKE, Google fully manages the Kubernetes control plane; however, when troubleshooting issues it can be helpful to have access to certain metrics emitted by the Kubernetes control plane.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As part of our vision to make Kubernetes easier to use and easier to operate, these control plane metrics are directly integrated with Cloud Monitoring, so you don\u0026#39;t need to manage any metric collection or scrape config.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For example, to understand the health of the API server, you can use metrics like \u0026lt;code\u0026gt;apiserver_request_total\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;apiserver_request_duration_seconds\u0026lt;/code\u0026gt; to track the load that the API server is experiencing, the fraction of API server requests that return errors, and the response latency for requests received by the API server. Also, \u0026lt;code\u0026gt;apiserver_storage_objects\u0026lt;/code\u0026gt; can be very useful to monitor the saturation of the API server, especially if you\u0026amp;#8217;re using custom controllers. Breakdown this metric by the resource label to find out which Kubernetes custom resource or controller is problematic.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When a pod is created it is initially placed in a \u0026amp;#34;pending\u0026amp;#34; state, indicating it hasn\u0026#39;t yet been scheduled on a node. In a healthy cluster, pending pods are relatively quickly scheduled on a node, providing the workload the resources it needs to run. However, a sustained increase in the number of pending pods may indicate a problem scheduling those pods, which may be caused by insufficient resources or inappropriate configuration. Metrics like \u0026lt;code\u0026gt;scheduler_pending_pods, scheduler_schedule_attempts_total\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;scheduler_preemption_attempts_total, scheduler_preemption_victims\u0026lt;/code\u0026gt; , and \u0026lt;code\u0026gt;scheduler_scheduling_attempt_duration_seconds\u0026lt;/code\u0026gt; can alert you to potential scheduling issues, so you can act quickly to ensure sufficient resources are available for your pods. Using these metrics in combination will help you better understand the health of your cluster. For instance, if \u0026lt;code\u0026gt;scheduler_preemption_attempts_total\u0026lt;/code\u0026gt; goes up, it means that there are higher priority pods available to be scheduled and the Scheduler is preempting some running pods. However, if the value of \u0026lt;code\u0026gt;scheduler_pending_pods\u0026lt;/code\u0026gt; is also increasing, this may indicate that you don\u0026amp;#8217;t have enough resources to allocate the higher priority pods.\u0026lt;/p\u0026gt;\u0026lt;br\u0026gt;\u0026lt;p\u0026gt;If the Kubernetes scheduler is still unable to find a suitable node for a pod, then the pod will eventually be marked as unschedulable. Kubernetes control plane metrics provide you visibility into pod scheduling errors and unschedulable pods. A spike in either means that the Kubernetes scheduler isn\u0026#39;t able to find an appropriate node on which to run many of your pods, which may ultimately impair the performance of your application. In many cases, a high rate of unschedulable pods will not resolve itself until you take some action to address the underlying cause. A good first place to start troubleshooting the issue is to look for recent \u0026lt;code\u0026gt;FailedScheduling\u0026lt;/code\u0026gt; events. (If you have \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/gke/installing#available-logs\u0026#34;\u0026gt;GKE system logs\u0026lt;/a\u0026gt; enabled, then all Kubernetes events are available in Cloud Logging.) These \u0026lt;code\u0026gt;FailedScheduling\u0026lt;/code\u0026gt; events include a message (for instance, \u0026amp;#34;0/6 nodes are available: 6 Insufficient cpu.\u0026amp;#34;) that very helpfully describes exactly why the pod wasn\u0026#39;t able to be scheduled on any nodes, giving you guidance on how to address the problem.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;A final example: If you see scheduling jobs is very slow, then one possible cause is that a third-party webhooks might be introducing significant latency, causing the API server to take a long time to schedule a job. Kubernetes control plane metrics such as \u0026lt;code\u0026gt;apiserver_admission_webhook_admission_duration_seconds\u0026lt;/code\u0026gt; can expose the admission webhook latency, helping you identify the root cause of slow job scheduling and mitigate the issue.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Displayed in context\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Not only are we making these additional Kubernetes control plane metrics available, we\u0026#39;re also excited to announce that all of these metrics are \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine/docs/how-to/view-observability-metrics\u0026#34;\u0026gt;displayed in the Kubernetes Engine section of the Cloud Console\u0026lt;/a\u0026gt;, making it easy to identify and investigate issues in-context as you\u0026#39;re managing your GKE clusters.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To view these control plane metrics, go to the Kubernetes clusters section of the Cloud Console, select the \u0026amp;#34;Observability\u0026amp;#34; tab, and select \u0026amp;#34;Control plane\u0026amp;#34;:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eAn essential aspect of operating any application is the ability to observe the health and performance of that application and of the underlying infrastructure to quickly resolve issues as they arise. \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE) already provides \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging\" track-metadata-module=\"post\"\u003eaudit logs\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs\" track-metadata-module=\"post\"\u003eoperational logs\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics\" track-metadata-module=\"post\"\u003emetrics\u003c/a\u003e along with \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/observing\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/gke/observing\" track-metadata-module=\"post\"\u003eout-of-the-box dashboards\u003c/a\u003e and automatic \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/kubernetes-engine\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/setup/kubernetes-engine\" track-metadata-module=\"post\"\u003eerror reporting\u003c/a\u003e to facilitate running reliable applications at scale. Using these logs and metrics, \u003ca href=\"https://cloud.google.com/products/operations\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/products/operations\" track-metadata-module=\"post\"\u003eCloud Operations\u003c/a\u003e provides the alerts, monitoring dashboards and a \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-metadata-module=\"post\"\u003eLogs Explorer\u003c/a\u003e to quickly detect, troubleshoot and resolve issues.\u003c/p\u003e\u003ch3\u003eIntroducing Kubernetes control plane metrics and why they matter\u003c/h3\u003e\u003cp\u003eIn addition to these existing sources of telemetry data, we are excited to announce that we are now exposing \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics\" track-metadata-module=\"post\"\u003eKubernetes control plane metrics\u003c/a\u003e, which are now Generally Available. With GKE, Google fully manages the Kubernetes control plane; however, when troubleshooting issues it can be helpful to have access to certain metrics emitted by the Kubernetes control plane.\u003c/p\u003e\u003cp\u003eAs part of our vision to make Kubernetes easier to use and easier to operate, these control plane metrics are directly integrated with Cloud Monitoring, so you don\u0026#39;t need to manage any metric collection or scrape config.\u003c/p\u003e\u003cp\u003eFor example, to understand the health of the API server, you can use metrics like \u003ccode\u003eapiserver_request_total\u003c/code\u003e and \u003ccode\u003eapiserver_request_duration_seconds\u003c/code\u003e to track the load that the API server is experiencing, the fraction of API server requests that return errors, and the response latency for requests received by the API server. Also, \u003ccode\u003eapiserver_storage_objects\u003c/code\u003e can be very useful to monitor the saturation of the API server, especially if you’re using custom controllers. Breakdown this metric by the resource label to find out which Kubernetes custom resource or controller is problematic.\u003c/p\u003e\u003cp\u003eWhen a pod is created it is initially placed in a \u0026#34;pending\u0026#34; state, indicating it hasn\u0026#39;t yet been scheduled on a node. In a healthy cluster, pending pods are relatively quickly scheduled on a node, providing the workload the resources it needs to run. However, a sustained increase in the number of pending pods may indicate a problem scheduling those pods, which may be caused by insufficient resources or inappropriate configuration. Metrics like \u003ccode\u003escheduler_pending_pods, scheduler_schedule_attempts_total\u003c/code\u003e, \u003ccode\u003escheduler_preemption_attempts_total, scheduler_preemption_victims\u003c/code\u003e , and \u003ccode\u003escheduler_scheduling_attempt_duration_seconds\u003c/code\u003e can alert you to potential scheduling issues, so you can act quickly to ensure sufficient resources are available for your pods. Using these metrics in combination will help you better understand the health of your cluster. For instance, if \u003ccode\u003escheduler_preemption_attempts_total\u003c/code\u003e goes up, it means that there are higher priority pods available to be scheduled and the Scheduler is preempting some running pods. However, if the value of \u003ccode\u003escheduler_pending_pods\u003c/code\u003e is also increasing, this may indicate that you don’t have enough resources to allocate the higher priority pods.\u003c/p\u003e\u003cp\u003eIf the Kubernetes scheduler is still unable to find a suitable node for a pod, then the pod will eventually be marked as unschedulable. Kubernetes control plane metrics provide you visibility into pod scheduling errors and unschedulable pods. A spike in either means that the Kubernetes scheduler isn\u0026#39;t able to find an appropriate node on which to run many of your pods, which may ultimately impair the performance of your application. In many cases, a high rate of unschedulable pods will not resolve itself until you take some action to address the underlying cause. A good first place to start troubleshooting the issue is to look for recent \u003ccode\u003eFailedScheduling\u003c/code\u003e events. (If you have \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/installing#available-logs\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/gke/installing#available-logs\" track-metadata-module=\"post\"\u003eGKE system logs\u003c/a\u003e enabled, then all Kubernetes events are available in Cloud Logging.) These \u003ccode\u003eFailedScheduling\u003c/code\u003e events include a message (for instance, \u0026#34;0/6 nodes are available: 6 Insufficient cpu.\u0026#34;) that very helpfully describes exactly why the pod wasn\u0026#39;t able to be scheduled on any nodes, giving you guidance on how to address the problem.\u003c/p\u003e\u003cp\u003eA final example: If you see scheduling jobs is very slow, then one possible cause is that a third-party webhooks might be introducing significant latency, causing the API server to take a long time to schedule a job. Kubernetes control plane metrics such as \u003ccode\u003eapiserver_admission_webhook_admission_duration_seconds\u003c/code\u003e can expose the admission webhook latency, helping you identify the root cause of slow job scheduling and mitigate the issue.\u003c/p\u003e\u003ch3\u003eDisplayed in context\u003c/h3\u003e\u003cp\u003eNot only are we making these additional Kubernetes control plane metrics available, we\u0026#39;re also excited to announce that all of these metrics are \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/view-observability-metrics\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine/docs/how-to/view-observability-metrics\" track-metadata-module=\"post\"\u003edisplayed in the Kubernetes Engine section of the Cloud Console\u003c/a\u003e, making it easy to identify and investigate issues in-context as you\u0026#39;re managing your GKE clusters.\u003c/p\u003e\u003cp\u003eTo view these control plane metrics, go to the Kubernetes clusters section of the Cloud Console, select the \u0026#34;Observability\u0026#34; tab, and select \u0026#34;Control plane\u0026#34;:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAn essential aspect of operating any application is the ability to observe the health and performance of that application and of the underlying infrastructure to quickly resolve issues as they arise. \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE) already provides \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/audit-logging\"\u003eaudit logs\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs\"\u003eoperational logs\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics\"\u003emetrics\u003c/a\u003e along with \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/observing\"\u003eout-of-the-box dashboards\u003c/a\u003e and automatic \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/kubernetes-engine\"\u003eerror reporting\u003c/a\u003e to facilitate running reliable applications at scale. Using these logs and metrics, \u003ca href=\"https://cloud.google.com/products/operations\"\u003eCloud Operations\u003c/a\u003e provides the alerts, monitoring dashboards and a \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\"\u003eLogs Explorer\u003c/a\u003e to quickly detect, troubleshoot and resolve issues.\u003c/p\u003e\u003ch3\u003eIntroducing Kubernetes control plane metrics and why they matter\u003c/h3\u003e\u003cp\u003eIn addition to these existing sources of telemetry data, we are excited to announce that we are now exposing \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#control-plane-metrics\"\u003eKubernetes control plane metrics\u003c/a\u003e, which are now Generally Available. With GKE, Google fully manages the Kubernetes control plane; however, when troubleshooting issues it can be helpful to have access to certain metrics emitted by the Kubernetes control plane.\u003c/p\u003e\u003cp\u003eAs part of our vision to make Kubernetes easier to use and easier to operate, these control plane metrics are directly integrated with Cloud Monitoring, so you don't need to manage any metric collection or scrape config.\u003c/p\u003e\u003cp\u003eFor example, to understand the health of the API server, you can use metrics like \u003ccode\u003eapiserver_request_total\u003c/code\u003e and \u003ccode\u003eapiserver_request_duration_seconds\u003c/code\u003e to track the load that the API server is experiencing, the fraction of API server requests that return errors, and the response latency for requests received by the API server. Also, \u003ccode\u003eapiserver_storage_objects\u003c/code\u003e can be very useful to monitor the saturation of the API server, especially if you’re using custom controllers. Breakdown this metric by the resource label to find out which Kubernetes custom resource or controller is problematic.\u003c/p\u003e\u003cp\u003eWhen a pod is created it is initially placed in a \"pending\" state, indicating it hasn't yet been scheduled on a node. In a healthy cluster, pending pods are relatively quickly scheduled on a node, providing the workload the resources it needs to run. However, a sustained increase in the number of pending pods may indicate a problem scheduling those pods, which may be caused by insufficient resources or inappropriate configuration. Metrics like \u003ccode\u003escheduler_pending_pods, scheduler_schedule_attempts_total\u003c/code\u003e, \u003ccode\u003escheduler_preemption_attempts_total, scheduler_preemption_victims\u003c/code\u003e , and \u003ccode\u003escheduler_scheduling_attempt_duration_seconds\u003c/code\u003e can alert you to potential scheduling issues, so you can act quickly to ensure sufficient resources are available for your pods. Using these metrics in combination will help you better understand the health of your cluster. For instance, if \u003ccode\u003escheduler_preemption_attempts_total\u003c/code\u003e goes up, it means that there are higher priority pods available to be scheduled and the Scheduler is preempting some running pods. However, if the value of \u003ccode\u003escheduler_pending_pods\u003c/code\u003e is also increasing, this may indicate that you don’t have enough resources to allocate the higher priority pods.\u003c/p\u003e\u003cbr/\u003e\u003cp\u003eIf the Kubernetes scheduler is still unable to find a suitable node for a pod, then the pod will eventually be marked as unschedulable. Kubernetes control plane metrics provide you visibility into pod scheduling errors and unschedulable pods. A spike in either means that the Kubernetes scheduler isn't able to find an appropriate node on which to run many of your pods, which may ultimately impair the performance of your application. In many cases, a high rate of unschedulable pods will not resolve itself until you take some action to address the underlying cause. A good first place to start troubleshooting the issue is to look for recent \u003ccode\u003eFailedScheduling\u003c/code\u003e events. (If you have \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/installing#available-logs\"\u003eGKE system logs\u003c/a\u003e enabled, then all Kubernetes events are available in Cloud Logging.) These \u003ccode\u003eFailedScheduling\u003c/code\u003e events include a message (for instance, \"0/6 nodes are available: 6 Insufficient cpu.\") that very helpfully describes exactly why the pod wasn't able to be scheduled on any nodes, giving you guidance on how to address the problem.\u003c/p\u003e\u003cp\u003eA final example: If you see scheduling jobs is very slow, then one possible cause is that a third-party webhooks might be introducing significant latency, causing the API server to take a long time to schedule a job. Kubernetes control plane metrics such as \u003ccode\u003eapiserver_admission_webhook_admission_duration_seconds\u003c/code\u003e can expose the admission webhook latency, helping you identify the root cause of slow job scheduling and mitigate the issue.\u003c/p\u003e\u003ch3\u003eDisplayed in context\u003c/h3\u003e\u003cp\u003eNot only are we making these additional Kubernetes control plane metrics available, we're also excited to announce that all of these metrics are \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/view-observability-metrics\"\u003edisplayed in the Kubernetes Engine section of the Cloud Console\u003c/a\u003e, making it easy to identify and investigate issues in-context as you're managing your GKE clusters.\u003c/p\u003e\u003cp\u003eTo view these control plane metrics, go to the Kubernetes clusters section of the Cloud Console, select the \"Observability\" tab, and select \"Control plane\":\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"control_plane_metrics_screenshot.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/control_plane_metrics_screenshot.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eSince all Kubernetes control plane metrics are ingested into Cloud Monitoring, you can \u003ca href=\"https://cloud.google.com/monitoring/alerts/using-alerting-ui\"\u003ecreate alerting policies in Cloud Alerting\u003c/a\u003e so you're notified as soon as something needs your attention.\u003c/p\u003e\u003ch3\u003ePromQL compatible\u003c/h3\u003e\u003cp\u003eWhen you enable Kubernetes control plane metrics for your GKE clusters, all metrics are collected using \u003ca href=\"https://g.co/cloud/managedprometheus\" target=\"_blank\"\u003eGoogle Cloud Managed Service for Prometheus\u003c/a\u003e. This means the metrics are sent to Cloud Monitoring in the same GCP project as your Kubernetes cluster and can be \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/query\"\u003equeried using PromQL\u003c/a\u003e via the Cloud Monitoring API and Metrics explorer.\u003c/p\u003e\u003cp\u003eFor example, you can monitor any spikes in the 99th percentile API server response latency using this PromQL query:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'sum by (instance, verb) (histogram_quantile(0.99, rate(apiserver_request_duration_seconds_bucket{cluster=\"cluster-name\"}[5m])))'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e5ea65472d0\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eThird-party support\u003c/h3\u003e\u003cp\u003eIf you monitor your GKE cluster using popular third party observability tools, any third party observability tool can ingest these Kubernetes control plane metrics using the Cloud Monitoring API.\u003c/p\u003e\u003cp\u003eFor example, if you're a Datadog customer and you've enabled Kubernetes control plane metrics for your GKE cluster, then Datadog provides enhanced visualizations that include Kubernetes control plane metrics from the API server, scheduler, and controller manager.\u003c/p\u003e\u003ch3\u003ePricing\u003c/h3\u003e\u003cp\u003eAll Kubernetes control plane metrics are charged at the \u003ca href=\"https://cloud.devsite.corp.google.com/stackdriver/pricing#monitoring-pricing-summary\" target=\"_blank\"\u003estandard price for metrics\u003c/a\u003e ingested from Google Cloud Managed Service for Prometheus.\u003c/p\u003e\u003ch3\u003eGet started\u003c/h3\u003e\u003cp\u003eGKE clusters running control plane version 1.23.6 or later can now access metrics from the Kubernetes API server, Scheduler, and Controller Manager. Kubernetes control plane metrics are not available for GKE Autopilot clusters.\u003c/p\u003e\u003cp\u003eThe following gcloud command will update a cluster to enable the collection of metrics from the API server, scheduler, and controller manager:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'gcloud container clusters update [CLUSTER_ID] \\\\\\r\\n --zone=[ZONE] \\\\\\r\\n --project=[PROJECT_ID] \\\\\\r\\n --monitoring=SYSTEM,API_SERVER,SCHEDULER,CONTROLLER_MANAGER'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e5ea53bf950\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eKubernetes control plane metrics can also be \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#enable-control-plane-metrics\"\u003econfigured using Terraform\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eLearn more about \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics#configuring_collection_of_control_plane_metrics\"\u003econfiguring the collection of control plane metrics\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGoogle Cloud Managed Service for Prometheus is now generally available\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnnouncing the GA of Google Cloud Managed Service for Prometheus for the collection, storage, and querying of Kubernetes metrics.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-09-08T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eNathan Beach\u003c/name\u003e\u003ctitle\u003eGroup Product Manager, Google Kubernetes Engine\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/identity-security/introducing-general-availability-of-google-cloud-certificate-manager/",
      "title": "Announcing public availability of Google Cloud Certificate Manager",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;Today we are pleased to announce that \u0026lt;a href=\u0026#34;https://cloud.google.com/certificate-manager/docs/overview\u0026#34;\u0026gt;Cloud Certificate Manager\u0026lt;/a\u0026gt; is now in general availability. Cloud Certificate Manager enables our users to acquire, manage, and deploy public Transport Layer Security (TLS) certificates at scale for use with your Google Cloud workloads. TLS certificates are required to secure browser connections and transactions.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Cloud Certificate Manager supports both self-managed and Google-managed certificates, as well as wildcard certificates, and has monitoring capabilities to alert for expiring certificates.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Scale to support as many domains as you need\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Since our public preview announcement supporting the \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management\u0026#34;\u0026gt;SaaS use cases\u0026lt;/a\u0026gt;, we have scaled the solution to serve millions of managed domains. \u0026lt;a href=\u0026#34;https://il.linkedin.com/in/alonkochba\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Alon Kochba\u0026lt;/a\u0026gt;, head of web performance at \u0026lt;a href=\u0026#34;https://www.wix.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Wix\u0026lt;/a\u0026gt;, shared how Certificate Manager\u0026amp;#8217;s scale and performance helped them lighten their workload.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. Google Cloud\u0026#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to deploy our own custom solution for terminating SSL,\u0026amp;#8221; Kochba said.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Streamline your migrations\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;You can now deploy a new certificate globally in minutes and greatly simplify and accelerate the deployment of TLS for SaaS offerings. Coupled with support for \u0026lt;a href=\u0026#34;https://cloud.google.com/certificate-manager/docs/dns-authorizations#:~:text=When%20you%20create%20a%20DNS,the%20steps%20in%20this%20section.\u0026#34;\u0026gt;DNS Authorizations\u0026lt;/a\u0026gt;, you can now streamline your workload migrations without major disruptions. \u0026lt;a href=\u0026#34;https://www.linkedin.com/in/jameshartig\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;James Hartig\u0026lt;/a\u0026gt;, co-founder of \u0026lt;a href=\u0026#34;https://www.getadmiral.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;GetAdmiral.com\u0026lt;/a\u0026gt;, shared this with Google after the migration experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;I just wanted to say thank you so much for the release of Certificate Manager and its support for SaaS use cases. We just completed our migration to using Google to terminate TLS and everything went really smoothly and we couldn\u0026#39;t be happier.\u0026amp;#8221;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Automate with Kubernetes \u0026amp;amp; self-service ACME certificate enrollment\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We have further introduced a number of automation and observability features including:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-security#certificate-manager\u0026#34;\u0026gt;Kubernetes integration\u0026lt;/a\u0026gt; in public preview with Cloud Certificate Manager\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api\u0026#34;\u0026gt;Self-service ACME certificate enrollment\u0026lt;/a\u0026gt;, now in public preview\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The ability to track Certificate Manager usage in the billing dashboard\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;We also have started work on incorporating \u0026lt;a href=\u0026#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/certificate_manager_certificate\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Terraform automation\u0026lt;/a\u0026gt; with Cloud Certificate Manager, which will simplify your workload automation.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;During the certificate manager private preview of the ACME \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api\u0026#34;\u0026gt;certificate enrollment capability\u0026lt;/a\u0026gt;, our users have acquired millions of certificates for their self-managed TLS deployments. Each of these certificates comes from \u0026lt;a href=\u0026#34;http://pki.goog\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Google Trust Services\u0026lt;/a\u0026gt;, which means our users get the same TLS device compatibility and scalability we demand for our own services. Our Cloud users get this benefit even when they manage the certificate and private key themselves\u0026amp;#8211;all for free.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We look forward to you using \u0026lt;a href=\u0026#34;https://cloud.google.com/certificate-manager/docs/overview\u0026#34;\u0026gt;Certificate Manager\u0026lt;/a\u0026gt; and these new capabilities to improve the reliability of your services and help encourage further adoption of TLS.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eToday we are pleased to announce that \u003ca href=\"https://cloud.google.com/certificate-manager/docs/overview\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/certificate-manager/docs/overview\" track-metadata-module=\"post\"\u003eCloud Certificate Manager\u003c/a\u003e is now in general availability. Cloud Certificate Manager enables our users to acquire, manage, and deploy public Transport Layer Security (TLS) certificates at scale for use with your Google Cloud workloads. TLS certificates are required to secure browser connections and transactions. \u003c/p\u003e\u003cp\u003eCloud Certificate Manager supports both self-managed and Google-managed certificates, as well as wildcard certificates, and has monitoring capabilities to alert for expiring certificates. \u003c/p\u003e\u003ch3\u003eScale to support as many domains as you need\u003c/h3\u003e\u003cp\u003eSince our public preview announcement supporting the \u003ca href=\"https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management\" track-metadata-module=\"post\"\u003eSaaS use cases\u003c/a\u003e, we have scaled the solution to serve millions of managed domains. \u003ca href=\"https://il.linkedin.com/in/alonkochba\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://il.linkedin.com\" track-metadata-module=\"post\"\u003eAlon Kochba\u003c/a\u003e, head of web performance at \u003ca href=\"https://www.wix.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://www.wix.com\" track-metadata-module=\"post\"\u003eWix\u003c/a\u003e, shared how Certificate Manager’s scale and performance helped them lighten their workload.\u003c/p\u003e\u003cp\u003e“As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. Google Cloud\u0026#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to deploy our own custom solution for terminating SSL,” Kochba said. \u003c/p\u003e\u003ch3\u003eStreamline your migrations\u003c/h3\u003e\u003cp\u003eYou can now deploy a new certificate globally in minutes and greatly simplify and accelerate the deployment of TLS for SaaS offerings. Coupled with support for \u003ca href=\"https://cloud.google.com/certificate-manager/docs/dns-authorizations#:~:text=When%20you%20create%20a%20DNS,the%20steps%20in%20this%20section.\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/certificate-manager/docs/dns-authorizations#:~:text=When%20you%20create%20a%20DNS,the%20steps%20in%20this%20section.\" track-metadata-module=\"post\"\u003eDNS Authorizations\u003c/a\u003e, you can now streamline your workload migrations without major disruptions. \u003ca href=\"https://www.linkedin.com/in/jameshartig\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://www.linkedin.com\" track-metadata-module=\"post\"\u003eJames Hartig\u003c/a\u003e, co-founder of \u003ca href=\"https://www.getadmiral.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://www.getadmiral.com\" track-metadata-module=\"post\"\u003eGetAdmiral.com\u003c/a\u003e, shared this with Google after the migration experience.\u003c/p\u003e\u003cp\u003e“I just wanted to say thank you so much for the release of Certificate Manager and its support for SaaS use cases. We just completed our migration to using Google to terminate TLS and everything went really smoothly and we couldn\u0026#39;t be happier.” \u003c/p\u003e\u003ch3\u003eAutomate with Kubernetes \u0026amp; self-service ACME certificate enrollment\u003c/h3\u003e\u003cp\u003eWe have further introduced a number of automation and observability features including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-security#certificate-manager\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-security#certificate-manager\" track-metadata-module=\"post\"\u003eKubernetes integration\u003c/a\u003e in public preview with Cloud Certificate Manager\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api\" track-metadata-module=\"post\"\u003eSelf-service ACME certificate enrollment\u003c/a\u003e, now in public preview\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe ability to track Certificate Manager usage in the billing dashboard\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe also have started work on incorporating \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/certificate_manager_certificate\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://registry.terraform.io\" track-metadata-module=\"post\"\u003eTerraform automation\u003c/a\u003e with Cloud Certificate Manager, which will simplify your workload automation.\u003c/p\u003e\u003cp\u003eDuring the certificate manager private preview of the ACME \u003ca href=\"https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api\" track-metadata-module=\"post\"\u003ecertificate enrollment capability\u003c/a\u003e, our users have acquired millions of certificates for their self-managed TLS deployments. Each of these certificates comes from \u003ca href=\"http://pki.goog\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"http://pki.goog\" track-metadata-module=\"post\"\u003eGoogle Trust Services\u003c/a\u003e, which means our users get the same TLS device compatibility and scalability we demand for our own services. Our Cloud users get this benefit even when they manage the certificate and private key themselves–all for free. \u003c/p\u003e\u003cp\u003eWe look forward to you using \u003ca href=\"https://cloud.google.com/certificate-manager/docs/overview\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/certificate-manager/docs/overview\" track-metadata-module=\"post\"\u003eCertificate Manager\u003c/a\u003e and these new capabilities to improve the reliability of your services and help encourage further adoption of TLS.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eToday we are pleased to announce that \u003ca href=\"https://cloud.google.com/certificate-manager/docs/overview\"\u003eCloud Certificate Manager\u003c/a\u003e is now in general availability. Cloud Certificate Manager enables our users to acquire, manage, and deploy public Transport Layer Security (TLS) certificates at scale for use with your Google Cloud workloads. TLS certificates are required to secure browser connections and transactions. \u003c/p\u003e\u003cp\u003eCloud Certificate Manager supports both self-managed and Google-managed certificates, as well as wildcard certificates, and has monitoring capabilities to alert for expiring certificates. \u003c/p\u003e\u003ch3\u003eScale to support as many domains as you need\u003c/h3\u003e\u003cp\u003eSince our public preview announcement supporting the \u003ca href=\"https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management\"\u003eSaaS use cases\u003c/a\u003e, we have scaled the solution to serve millions of managed domains. \u003ca href=\"https://il.linkedin.com/in/alonkochba\" target=\"_blank\"\u003eAlon Kochba\u003c/a\u003e, head of web performance at \u003ca href=\"https://www.wix.com/\" target=\"_blank\"\u003eWix\u003c/a\u003e, shared how Certificate Manager’s scale and performance helped them lighten their workload.\u003c/p\u003e\u003cp\u003e“As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. Google Cloud's Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to deploy our own custom solution for terminating SSL,” Kochba said. \u003c/p\u003e\u003ch3\u003eStreamline your migrations\u003c/h3\u003e\u003cp\u003eYou can now deploy a new certificate globally in minutes and greatly simplify and accelerate the deployment of TLS for SaaS offerings. Coupled with support for \u003ca href=\"https://cloud.google.com/certificate-manager/docs/dns-authorizations#:~:text=When%20you%20create%20a%20DNS,the%20steps%20in%20this%20section.\"\u003eDNS Authorizations\u003c/a\u003e, you can now streamline your workload migrations without major disruptions. \u003ca href=\"https://www.linkedin.com/in/jameshartig\" target=\"_blank\"\u003eJames Hartig\u003c/a\u003e, co-founder of \u003ca href=\"https://www.getadmiral.com/\" target=\"_blank\"\u003eGetAdmiral.com\u003c/a\u003e, shared this with Google after the migration experience.\u003c/p\u003e\u003cp\u003e“I just wanted to say thank you so much for the release of Certificate Manager and its support for SaaS use cases. We just completed our migration to using Google to terminate TLS and everything went really smoothly and we couldn't be happier.” \u003c/p\u003e\u003ch3\u003eAutomate with Kubernetes \u0026amp; self-service ACME certificate enrollment\u003c/h3\u003e\u003cp\u003eWe have further introduced a number of automation and observability features including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/gateway-security#certificate-manager\"\u003eKubernetes integration\u003c/a\u003e in public preview with Cloud Certificate Manager\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api\"\u003eSelf-service ACME certificate enrollment\u003c/a\u003e, now in public preview\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe ability to track Certificate Manager usage in the billing dashboard\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe also have started work on incorporating \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/certificate_manager_certificate\" target=\"_blank\"\u003eTerraform automation\u003c/a\u003e with Cloud Certificate Manager, which will simplify your workload automation.\u003c/p\u003e\u003cp\u003eDuring the certificate manager private preview of the ACME \u003ca href=\"https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api\"\u003ecertificate enrollment capability\u003c/a\u003e, our users have acquired millions of certificates for their self-managed TLS deployments. Each of these certificates comes from \u003ca href=\"http://pki.goog\" target=\"_blank\"\u003eGoogle Trust Services\u003c/a\u003e, which means our users get the same TLS device compatibility and scalability we demand for our own services. Our Cloud users get this benefit even when they manage the certificate and private key themselves–all for free. \u003c/p\u003e\u003cp\u003eWe look forward to you using \u003ca href=\"https://cloud.google.com/certificate-manager/docs/overview\"\u003eCertificate Manager\u003c/a\u003e and these new capabilities to improve the reliability of your services and help encourage further adoption of TLS.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/identity-security/how-google-cloud-blocked-largest-layer-7-ddos-attack-at-46-million-rps/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_security.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eHow Google Cloud blocked the largest Layer 7 DDoS attack at 46 million rps\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eBy anticipating a DDOS attack, a Google Cloud customer was able to stop it before it took down their site. They just weren’t expecting it...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-08-24T19:00:00Z",
      "author": {
        "name": "\u003cname\u003eBabi Seal\u003c/name\u003e\u003ctitle\u003eProduct Manager, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/snooze-your-alert-policies-cloud-monitoring/",
      "title": "Snooze your alert policies in Cloud Monitoring",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDoes your development team want to snooze alerts during non-business hours? Or proactively prevent the creation of expected alerts for an upcoming expected maintenance window? Cloud Alerting in Google's Cloud operations suite now supports the ability to snooze alert policies for a given period of time. You can create a Snooze by providing specific alert policies and a time period. During this window, if the alert policy is violated, no incidents or notifications are created. When the window ends, the alerting behavior resumes as normal. \u003c/p\u003e\u003cp\u003eYour team can use this feature in a variety of ways. One example is to avoid being paged for non-production environments over the weekend. Another way is to plan for a known maintenance window or cutover period. You can also quiet the noise during a growing outage, among other approaches. \u003c/p\u003e\u003cp\u003eTo create a Snooze, go to Monitoring \u0026gt;  Alerting. See the new table with Snoozes and click on Create Snooze. You provide the name of the Snooze, time period, and select the desired Alert Policies. After you select the criteria, a table lists recent Incidents that match this criteria. Events like those won't cause an alert when the snooze is active.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Snooze_alerts.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Snooze alerts.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Snooze_alerts.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou will  see a timeline visualization of all past, active, and upcoming Snoozes. If you’d like to adjust the duration, you can go back and edit the details. For more information, please see the documentation.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Snooze_alerts.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Snooze alerts.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Snooze_alerts.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn the future, we’ll expand this functionality to allow snoozing by labels. You’ll be able to temporarily silence by the resource, system, metric, and custom labels which will allow you to snooze all alert policies in a specific environment, zone, or team. This functionality will be extended to be supported in the API, allowing you to create Snoozes programmatically for regularly repeating events.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eAdd severity levels to your alert policies in Cloud Monitoring\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAdd static and dynamic severity levels to your alert policies for easier triaging and include these in notifications when sent to 3rd par...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-08-11T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlisa Goldstein\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/increase-developer-productivity-with-query-library/",
      "title": "Accelerate your developer productivity with Query Library",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOur goal in \u003ca href=\"https://cloud.google.com/logging\"\u003eCloud Logging\u003c/a\u003e is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. \u003c/p\u003e\u003cp\u003eThat’s why we recently launched a \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e and other \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\"\u003enew features\u003c/a\u003e to make querying your logs even easier. The \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e in Cloud Logging makes it easier to find logs faster by using common queries.\u003c/p\u003e\u003ch3\u003eBuild queries faster with our templates\u003c/h3\u003e\u003cp\u003eThe new \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#search-text\"\u003etext search\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\"\u003edrop-down\u003c/a\u003e features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the \u003ca href=\"https://cloud.google.com/logging/docs/view/logging-query-language\"\u003eLogging query language\u003c/a\u003e necessary for you. The Query Library extends this simplicity with templates for common GCP queries.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Query Library.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery categories\u003c/b\u003e – Each query is broken down into categories that can be used to easily narrow down to relevant queries. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery occurrences\u003c/b\u003e – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery details\u003c/b\u003e – Each query has a description along with the Logging query \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eRun/Stream\u003c/b\u003e – Run the query or start streaming logs right from the library\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSave\u003c/b\u003e – Save the query in your list of saved queries\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Query Library.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eThe road ahead\u003c/h3\u003e\u003cp\u003eWe’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\"\u003eLogs Explorer\u003c/a\u003e and join the discussion in our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\" target=\"_blank\"\u003eCloud Operations page\u003c/a\u003e on the Google Cloud Community site.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-gets-continuous-delivery-productivity-enhancements/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header_nEzKg5F.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGoogle Cloud Deploy gets continuous delivery productivity enhancements\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eIn this latest release, Google Cloud Deploy got improved onboarding, delivery pipeline management and additional enterprise features.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-08-11T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eCharles Baer\u003c/name\u003e\u003ctitle\u003eProduct Manager, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/snooze-your-alert-policies-cloud-monitoring/",
      "title": "Snooze your alert policies in Cloud Monitoring",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;Does your development team want to snooze alerts during non-business hours? Or proactively prevent the creation of expected alerts for an upcoming expected maintenance window? Cloud Alerting in Google\u0026#39;s Cloud operations suite now supports the ability to snooze alert policies for a given period of time. You can create a Snooze by providing specific alert policies and a time period. During this window, if the alert policy is violated, no incidents or notifications are created. When the window ends, the alerting behavior resumes as normal.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Your team can use this feature in a variety of ways. One example is to avoid being paged for non-production environments over the weekend. Another way is to plan for a known maintenance window or cutover period. You can also quiet the noise during a growing outage, among other approaches.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To create a Snooze, go to Monitoring \u0026amp;gt;\u0026amp;#160; Alerting. See the new table with Snoozes and click on Create Snooze. You provide the name of the Snooze, time period, and select the desired Alert Policies. After you select the criteria, a table lists recent Incidents that match this criteria. Events like those won\u0026#39;t cause an alert when the snooze is active.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDoes your development team want to snooze alerts during non-business hours? Or proactively prevent the creation of expected alerts for an upcoming expected maintenance window? Cloud Alerting in Google\u0026#39;s Cloud operations suite now supports the ability to snooze alert policies for a given period of time. You can create a Snooze by providing specific alert policies and a time period. During this window, if the alert policy is violated, no incidents or notifications are created. When the window ends, the alerting behavior resumes as normal. \u003c/p\u003e\u003cp\u003eYour team can use this feature in a variety of ways. One example is to avoid being paged for non-production environments over the weekend. Another way is to plan for a known maintenance window or cutover period. You can also quiet the noise during a growing outage, among other approaches. \u003c/p\u003e\u003cp\u003eTo create a Snooze, go to Monitoring \u0026gt;  Alerting. See the new table with Snoozes and click on Create Snooze. You provide the name of the Snooze, time period, and select the desired Alert Policies. After you select the criteria, a table lists recent Incidents that match this criteria. Events like those won\u0026#39;t cause an alert when the snooze is active.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDoes your development team want to snooze alerts during non-business hours? Or proactively prevent the creation of expected alerts for an upcoming expected maintenance window? Cloud Alerting in Google's Cloud operations suite now supports the ability to snooze alert policies for a given period of time. You can create a Snooze by providing specific alert policies and a time period. During this window, if the alert policy is violated, no incidents or notifications are created. When the window ends, the alerting behavior resumes as normal. \u003c/p\u003e\u003cp\u003eYour team can use this feature in a variety of ways. One example is to avoid being paged for non-production environments over the weekend. Another way is to plan for a known maintenance window or cutover period. You can also quiet the noise during a growing outage, among other approaches. \u003c/p\u003e\u003cp\u003eTo create a Snooze, go to Monitoring \u0026gt;  Alerting. See the new table with Snoozes and click on Create Snooze. You provide the name of the Snooze, time period, and select the desired Alert Policies. After you select the criteria, a table lists recent Incidents that match this criteria. Events like those won't cause an alert when the snooze is active.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Snooze_alerts.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Snooze alerts.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Snooze_alerts.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou will  see a timeline visualization of all past, active, and upcoming Snoozes. If you’d like to adjust the duration, you can go back and edit the details. For more information, please see the documentation.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Snooze_alerts.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Snooze alerts.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Snooze_alerts.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn the future, we’ll expand this functionality to allow snoozing by labels. You’ll be able to temporarily silence by the resource, system, metric, and custom labels which will allow you to snooze all alert policies in a specific environment, zone, or team. This functionality will be extended to be supported in the API, allowing you to create Snoozes programmatically for regularly repeating events.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eAdd severity levels to your alert policies in Cloud Monitoring\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAdd static and dynamic severity levels to your alert policies for easier triaging and include these in notifications when sent to 3rd par...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-08-11T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlisa Goldstein\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/increase-developer-productivity-with-query-library/",
      "title": "Accelerate your developer productivity with Query Library",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Our goal in \u0026lt;a href=\u0026#34;https://cloud.google.com/logging\u0026#34;\u0026gt;Cloud Logging\u0026lt;/a\u0026gt; is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you\u0026amp;#8217;re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;That\u0026amp;#8217;s why we recently launched a \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries\u0026#34;\u0026gt;Query Library\u0026lt;/a\u0026gt; and other \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\u0026#34;\u0026gt;new features\u0026lt;/a\u0026gt; to make querying your logs even easier. The \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries\u0026#34;\u0026gt;Query Library\u0026lt;/a\u0026gt; in Cloud Logging makes it easier to find logs faster by using common queries.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Build queries faster with our templates\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The new \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#search-text\u0026#34;\u0026gt;text search\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\u0026#34;\u0026gt;drop-down\u0026lt;/a\u0026gt; features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logging-query-language\u0026#34;\u0026gt;Logging query language\u0026lt;/a\u0026gt; necessary for you. The Query Library extends this simplicity with templates for common GCP queries.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eOur goal in \u003ca href=\"https://cloud.google.com/logging\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/logging\" track-metadata-module=\"post\"\u003eCloud Logging\u003c/a\u003e is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. \u003c/p\u003e\u003cp\u003eThat’s why we recently launched a \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-metadata-module=\"post\"\u003eQuery Library\u003c/a\u003e and other \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\" track-metadata-module=\"post\"\u003enew features\u003c/a\u003e to make querying your logs even easier. The \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-metadata-module=\"post\"\u003eQuery Library\u003c/a\u003e in Cloud Logging makes it easier to find logs faster by using common queries.\u003c/p\u003e\u003ch3\u003eBuild queries faster with our templates\u003c/h3\u003e\u003cp\u003eThe new \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#search-text\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#search-text\" track-metadata-module=\"post\"\u003etext search\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\" track-metadata-module=\"post\"\u003edrop-down\u003c/a\u003e features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the \u003ca href=\"https://cloud.google.com/logging/docs/view/logging-query-language\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logging-query-language\" track-metadata-module=\"post\"\u003eLogging query language\u003c/a\u003e necessary for you. The Query Library extends this simplicity with templates for common GCP queries.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries\u0026#34;\u0026gt;Query Library\u0026lt;/a\u0026gt; is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you\u0026amp;#8217;ll notice the following details:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Query categories\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;Each query is broken down into categories that can be used to easily narrow down to relevant queries.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Query occurrences\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Query details\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;Each query has a description along with the Logging query\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Run/Stream\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;Run the query or start streaming logs right from the library\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Save\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;Save the query in your list of saved queries\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\"\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-metadata-module=\"post\"\u003eQuery Library\u003c/a\u003e is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery categories\u003c/b\u003e – Each query is broken down into categories that can be used to easily narrow down to relevant queries. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery occurrences\u003c/b\u003e – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery details\u003c/b\u003e – Each query has a description along with the Logging query \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eRun/Stream\u003c/b\u003e – Run the query or start streaming logs right from the library\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSave\u003c/b\u003e – Save the query in your list of saved queries\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;The road ahead\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We\u0026amp;#8217;re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven\u0026amp;#8217;t already, get started with the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface\u0026#34;\u0026gt;Logs Explorer\u0026lt;/a\u0026gt; and join the discussion in our \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Cloud Operations page\u0026lt;/a\u0026gt; on the Google Cloud Community site.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eThe road ahead\u003c/h3\u003e\u003cp\u003eWe’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-metadata-module=\"post\"\u003eLogs Explorer\u003c/a\u003e and join the discussion in our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003eCloud Operations page\u003c/a\u003e on the Google Cloud Community site.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOur goal in \u003ca href=\"https://cloud.google.com/logging\"\u003eCloud Logging\u003c/a\u003e is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. \u003c/p\u003e\u003cp\u003eThat’s why we recently launched a \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e and other \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\"\u003enew features\u003c/a\u003e to make querying your logs even easier. The \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e in Cloud Logging makes it easier to find logs faster by using common queries.\u003c/p\u003e\u003ch3\u003eBuild queries faster with our templates\u003c/h3\u003e\u003cp\u003eThe new \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#search-text\"\u003etext search\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\"\u003edrop-down\u003c/a\u003e features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the \u003ca href=\"https://cloud.google.com/logging/docs/view/logging-query-language\"\u003eLogging query language\u003c/a\u003e necessary for you. The Query Library extends this simplicity with templates for common GCP queries.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Query Library.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery categories\u003c/b\u003e – Each query is broken down into categories that can be used to easily narrow down to relevant queries. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery occurrences\u003c/b\u003e – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery details\u003c/b\u003e – Each query has a description along with the Logging query \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eRun/Stream\u003c/b\u003e – Run the query or start streaming logs right from the library\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSave\u003c/b\u003e – Save the query in your list of saved queries\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Query Library.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eThe road ahead\u003c/h3\u003e\u003cp\u003eWe’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\"\u003eLogs Explorer\u003c/a\u003e and join the discussion in our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\" target=\"_blank\"\u003eCloud Operations page\u003c/a\u003e on the Google Cloud Community site.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-gets-continuous-delivery-productivity-enhancements/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header_nEzKg5F.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGoogle Cloud Deploy gets continuous delivery productivity enhancements\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eIn this latest release, Google Cloud Deploy got improved onboarding, delivery pipeline management and additional enterprise features.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-08-11T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eCharles Baer\u003c/name\u003e\u003ctitle\u003eProduct Manager, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/increase-developer-productivity-with-query-library/",
      "title": "Accelerate your developer productivity with Query Library",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c59=\"\"\u003e\u003cdiv _ngcontent-c59=\"\" innerhtml=\"\u0026lt;p\u0026gt;Our goal in \u0026lt;a href=\u0026#34;https://cloud.google.com/logging\u0026#34;\u0026gt;Cloud Logging\u0026lt;/a\u0026gt; is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you\u0026amp;#8217;re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;That\u0026amp;#8217;s why we recently launched a \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries\u0026#34;\u0026gt;Query Library\u0026lt;/a\u0026gt; and other \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\u0026#34;\u0026gt;new features\u0026lt;/a\u0026gt; to make querying your logs even easier. The \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries\u0026#34;\u0026gt;Query Library\u0026lt;/a\u0026gt; in Cloud Logging makes it easier to find logs faster by using common queries.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Build queries faster with our templates\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The new \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#search-text\u0026#34;\u0026gt;text search\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\u0026#34;\u0026gt;drop-down\u0026lt;/a\u0026gt; features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logging-query-language\u0026#34;\u0026gt;Logging query language\u0026lt;/a\u0026gt; necessary for you. The Query Library extends this simplicity with templates for common GCP queries.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eOur goal in \u003ca href=\"https://cloud.google.com/logging\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/logging\" track-metadata-module=\"post\"\u003eCloud Logging\u003c/a\u003e is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. \u003c/p\u003e\u003cp\u003eThat’s why we recently launched a \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-metadata-module=\"post\"\u003eQuery Library\u003c/a\u003e and other \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\" track-metadata-module=\"post\"\u003enew features\u003c/a\u003e to make querying your logs even easier. The \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-metadata-module=\"post\"\u003eQuery Library\u003c/a\u003e in Cloud Logging makes it easier to find logs faster by using common queries.\u003c/p\u003e\u003ch3\u003eBuild queries faster with our templates\u003c/h3\u003e\u003cp\u003eThe new \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#search-text\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#search-text\" track-metadata-module=\"post\"\u003etext search\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\" track-metadata-module=\"post\"\u003edrop-down\u003c/a\u003e features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the \u003ca href=\"https://cloud.google.com/logging/docs/view/logging-query-language\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logging-query-language\" track-metadata-module=\"post\"\u003eLogging query language\u003c/a\u003e necessary for you. The Query Library extends this simplicity with templates for common GCP queries.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c59=\"\"\u003e\u003cdiv _ngcontent-c59=\"\" innerhtml=\"\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#library-queries\u0026#34;\u0026gt;Query Library\u0026lt;/a\u0026gt; is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you\u0026amp;#8217;ll notice the following details:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Query categories\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;Each query is broken down into categories that can be used to easily narrow down to relevant queries.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Query occurrences\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Query details\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;Each query has a description along with the Logging query\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Run/Stream\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;Run the query or start streaming logs right from the library\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Save\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;Save the query in your list of saved queries\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\"\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\" track-metadata-module=\"post\"\u003eQuery Library\u003c/a\u003e is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery categories\u003c/b\u003e – Each query is broken down into categories that can be used to easily narrow down to relevant queries. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery occurrences\u003c/b\u003e – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery details\u003c/b\u003e – Each query has a description along with the Logging query \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eRun/Stream\u003c/b\u003e – Run the query or start streaming logs right from the library\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSave\u003c/b\u003e – Save the query in your list of saved queries\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c59=\"\"\u003e\u003cdiv _ngcontent-c59=\"\" innerhtml=\"\u0026lt;h3\u0026gt;The road ahead\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We\u0026amp;#8217;re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven\u0026amp;#8217;t already, get started with the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface\u0026#34;\u0026gt;Logs Explorer\u0026lt;/a\u0026gt; and join the discussion in our \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Cloud Operations page\u0026lt;/a\u0026gt; on the Google Cloud Community site.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eThe road ahead\u003c/h3\u003e\u003cp\u003eWe’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-metadata-module=\"post\"\u003eLogs Explorer\u003c/a\u003e and join the discussion in our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003eCloud Operations page\u003c/a\u003e on the Google Cloud Community site.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOur goal in \u003ca href=\"https://cloud.google.com/logging\"\u003eCloud Logging\u003c/a\u003e is to help increase developer productivity by streamlining the troubleshooting process. The time spent on writing and executing a query, and then analyzing the errors can impact developer productivity. Whether you’re troubleshooting an issue or analyzing your logs, finding the right logs quickly, is critical. \u003c/p\u003e\u003cp\u003eThat’s why we recently launched a \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e and other \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging\"\u003enew features\u003c/a\u003e to make querying your logs even easier. The \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e in Cloud Logging makes it easier to find logs faster by using common queries.\u003c/p\u003e\u003ch3\u003eBuild queries faster with our templates\u003c/h3\u003e\u003cp\u003eThe new \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#search-text\"\u003etext search\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\"\u003edrop-down\u003c/a\u003e features are designed to make querying something that you can achieve with a few mouse clicks. These features automatically generate the \u003ca href=\"https://cloud.google.com/logging/docs/view/logging-query-language\"\u003eLogging query language\u003c/a\u003e necessary for you. The Query Library extends this simplicity with templates for common GCP queries.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Query Library.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Library.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#library-queries\"\u003eQuery Library\u003c/a\u003e is located in the query builder bar next to the Suggested queries. To help find the most relevant queries you’ll notice the following details:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery categories\u003c/b\u003e – Each query is broken down into categories that can be used to easily narrow down to relevant queries. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery occurrences\u003c/b\u003e – To help you pick queries that have the most useful results, sparklines are displayed for queries that have logs in your project. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuery details\u003c/b\u003e – Each query has a description along with the Logging query \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eRun/Stream\u003c/b\u003e – Run the query or start streaming logs right from the library\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSave\u003c/b\u003e – Save the query in your list of saved queries\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Query Library.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Library.0808050715901014.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eThe road ahead\u003c/h3\u003e\u003cp\u003eWe’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\"\u003eLogs Explorer\u003c/a\u003e and join the discussion in our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\" target=\"_blank\"\u003eCloud Operations page\u003c/a\u003e on the Google Cloud Community site.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-gets-continuous-delivery-productivity-enhancements/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header_nEzKg5F.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGoogle Cloud Deploy gets continuous delivery productivity enhancements\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eIn this latest release, Google Cloud Deploy got improved onboarding, delivery pipeline management and additional enterprise features.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-08-11T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eCharles Baer\u003c/name\u003e\u003ctitle\u003eProduct Manager, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-gets-continuous-delivery-productivity-enhancements/",
      "title": "Google Cloud Deploy gets continuous delivery productivity enhancements",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eSince \u003ca href=\"http://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e became \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-now-ga\"\u003egenerally available\u003c/a\u003e in January 2022, we’ve remained focused on our core mission: making it easier to establish and operate software continuous delivery to a \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e environment. \u003c/p\u003e\u003cp\u003eThrough ongoing conversations with developers, DevOps engineers, and business decision makers alike, we’ve received feedback about onboarding speed, delivery pipeline management, and expanding enterprise features.Today, we are pleased to introduce numerous feature additions to Google Cloud Deploy in these areas. \u003c/p\u003e\u003ch3\u003eFaster onboarding\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/skaffold\"\u003eSkaffold\u003c/a\u003e is an open source tool that orchestrates continuous development, continuous integration (CI), and continuous delivery (CD), and it’s \u003ca href=\"https://cloud.google.com/deploy/docs/using-skaffold\"\u003eintegral to Google Cloud Deploy\u003c/a\u003e. Through Skaffold and Google Cloud Deploy, the \u003ca href=\"https://cloud.google.com/deploy/docs/using-skaffold/getting-started-skaffold#using_skaffold_for_local_development\"\u003elocal application development loop\u003c/a\u003e is \u003ca href=\"https://cloud.google.com/deploy/docs/using-skaffold/getting-started-skaffold#using_skaffold_for_cicd\"\u003eseamlessly connected\u003c/a\u003e to a continuous delivery capability, bringing consistency to your end-to-end software delivery lifecycle tooling. \u003c/p\u003e\u003cp\u003eThis may be the first time your team is using Skaffold. To help, Google Cloud Deploy can now \u003ca href=\"https://cloud.google.com/deploy/docs/using-skaffold/getting-started-skaffold#have_generate_your_skaffoldyaml\"\u003egenerate a Skaffold configuration\u003c/a\u003e for single manifest applications when one is not present. \u003c/p\u003e\u003cp\u003eWhen you create a \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#release\"\u003erelease\u003c/a\u003e, the new ‘gcloud deploy releases create … \u003ca href=\"https://cloud.google.com/sdk/gcloud/reference/deploy/releases/create#--from-k8s-manifest\"\u003e--from-k8s-manifest\u003c/a\u003e‘ command provides an application manifest, and generates a Skaffold configuration. This lets your application development teams and continuous delivery operators familiarize themselves with Google Cloud Deploy, reducing early-stage configuration and learning friction as they establish their continuous delivery capabilities. \u003c/p\u003e\u003cp\u003eWhen you use this option, you can review the generated Skaffold configuration, and as your comfort with Skaffold configuration and Google Cloud Deploy increases, you can develop your own Skaffold configurations tailored to your specific delivery pipeline needs.\u003c/p\u003e\u003ch3\u003eDelivery pipeline management\u003c/h3\u003e\u003cp\u003eContinuous \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003edelivery pipelines\u003c/a\u003e are always in use. New releases navigate a progression sequence as they make their way out to the production \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#target\"\u003etarget\u003c/a\u003e. The journey, however, isn’t always smooth. In that case, you may need to manage your delivery pipeline and related resources more discretely. \u003c/p\u003e\u003cp\u003eWith the addition of \u003ca href=\"https://cloud.google.com/deploy/docs/suspend-pipeline\"\u003edelivery pipeline suspension\u003c/a\u003e, you can now temporarily pause problematic delivery pipelines to restrict all release and rollout activity. By pausing the activity, you can undertake an investigation to identify problems and their root cause. \u003c/p\u003e\u003cp\u003eSometimes it isn’t the delivery pipeline that has a problem, but rather a release. Through \u003ca href=\"https://cloud.google.com/deploy/docs/abandon-release\"\u003erelease abandonment\u003c/a\u003e, you can prohibit application releases that have a feature defect, outdated library, or other identified issues from being deployed further. Release abandonment ensures an undesired release won’t be used again, while keeping it available for issue review and troubleshooting.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/suspended_pipeline_blog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"suspended_pipeline_blog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/suspended_pipeline_blog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eA suspended delivery pipeline and abandoned releases\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen reviewing or troubleshooting release application manifest issues, you may want to compare application manifests between releases and target environments to determine when an application configuration changed and why. But comparing applications manifests can be hard, requiring you to use the command line to locate and diff multiple files.\u003c/p\u003e\u003cp\u003eTo help, Google Cloud Deploy now has a \u003ca href=\"https://cloud.google.com/deploy/docs/view-release#viewing_release_artifacts\"\u003eRelease inspector\u003c/a\u003e, which makes it easy to review application manifests and compare against releases and targets within a delivery pipeline.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"release-inspector.gif\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/release-inspector.gif\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eReviewing and comparing application manifests with the Release Inspector\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#rollout\"\u003eRollout\u003c/a\u003e listings within the Google Cloud Deploy console have, to date, been limited to a specific release or target. A complete delivery pipeline rollout listing (and filtering) has been a standing request, and you can now find it on the delivery pipeline details page.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/rollouts_tab_blog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"rollouts_tab_blog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/rollouts_tab_blog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eDelivery pipeline details now with complete Rollouts listing\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eFinally, \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution environments\u003c/a\u003e are an important part of configuring custom render and deploy environments. In addition to the ability to specify custom worker pools, Cloud Storage buckets, and service accounts, we’ve added an \u003ca href=\"https://cloud.google.com/deploy/docs/config-files#executionconfigs\"\u003eexecution timeout\u003c/a\u003e to better support long-running deployments. \u003c/p\u003e\u003ch3\u003eExpanded enterprise features\u003c/h3\u003e\u003cp\u003eEnterprise environments frequently have numerous requirements to be able to operate, such as security controls, logging, Terraform support, and regional availability.\u003c/p\u003e\u003cp\u003eIn a \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-now-ga\"\u003eprevious blog post\u003c/a\u003e, we announced support for VPC Security Controls (VPC-SC) in Preview. We are pleased to announce that \u003ca href=\"https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy\"\u003eGoogle Cloud Deploy VPC-SC\u003c/a\u003e is now generally available. We’ve also \u003ca href=\"https://cloud.google.com/deploy/docs/securing/cmek\"\u003edocumented\u003c/a\u003e how you can configure customer managed encryption keys (CMEK) with services that depend on Google Cloud Deploy.\u003c/p\u003e\u003cp\u003eThere are also times when reviewing manifest-render and application deployment logs may not be sufficient for troubleshooting. For these situations, we’ve added Google Cloud Deploy service \u003ca href=\"https://cloud.google.com/deploy/docs/platform-logs\"\u003eplatform logs\u003c/a\u003e, which may provide additional details towards issue resolution.\u003c/p\u003e\u003cp\u003eTerraform plays an important role in deploying Google Cloud resources. You can now deploy Google Cloud Deploy \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/clouddeploy_delivery_pipeline\" target=\"_blank\"\u003edelivery pipelines\u003c/a\u003e and \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/clouddeploy_target\" target=\"_blank\"\u003etarget\u003c/a\u003e resources using Google Cloud Platform’s \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs\" target=\"_blank\"\u003eTerraform provider\u003c/a\u003e. With this, you can now deploy Google Cloud Deploy resources as part of a broader Google Cloud Platform resource deployment.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/regions\"\u003eRegional availability\u003c/a\u003e is important for businesses that need a regional service presence. Google Cloud Deploy is now available in an additional nine \u003ca href=\"https://cloud.google.com/about/locations\"\u003eregions\u003c/a\u003e, bringing the total number of Google Cloud Deploy worldwide regions to 15.\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software delivery capability, and it’s our hope that Google Cloud Deploy will help you implement complete CI/CD pipelines. And we’re just getting started. Stay tuned as we introduce exciting new capabilities and features to Google Cloud Deploy in the months to come. \u003c/p\u003e\u003cp\u003eIn the meantime, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/deploy-app-gke\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-now-ga/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGoogle Cloud Deploy, now GA, makes it easier to do continuous delivery to GKE\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eGoogle Cloud Deploy managed service, now GA, makes it easier to do continuous delivery to Google Kubernetes Engine\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/blog_post_header_nEzKg5F.jpg",
      "date_published": "2022-08-10T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/using-devops-and-sre-principles-to-manage-looker/",
      "title": "Managing the Looker ecosystem at scale with SRE and DevOps practices",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003crouter-outlet\u003e\u003c/router-outlet\u003e\u003cdynamic-page\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c55=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003cdiv\u003e\u003carticle-cta _nghost-c62=\"\"\u003e\u003cdiv _ngcontent-c62=\"\"\u003e\u003ch4 _ngcontent-c62=\"\"\u003e\u003cspan _ngcontent-c62=\"\"\u003eTry Google Cloud\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c62=\"\"\u003e\u003cspan _ngcontent-c62=\"\"\u003eStart building on Google Cloud with $300 in free credits and 20+ always free products.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c62=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"free trial\" track-metadata-eventdetail=\"https://cloud.google.com/free/\" href=\"https://cloud.google.com/free/\"\u003e\u003cspan _ngcontent-c62=\"\"\u003eFree Trial\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c64=\"\"\u003e\u003cdiv _ngcontent-c64=\"\" innerhtml=\"\u0026lt;p\u0026gt;Many organizations struggle to create data-driven cultures where each employee is empowered to make decisions based on data. This is especially true for enterprises with a variety of systems and tools in use across different teams. If you are a leader, manager, or executive focused on how your team can leverage Google\u0026#39;s SRE practices or wider DevOps practices, definitely you are in the right place!\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;What do today\u0026amp;#8217;s enterprises or mature start-ups look like?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Today large organizations are often segmented into hundreds of small teams which are often working around data in the magnitude of several petabytes and in a wide variety of raw forms. \u0026amp;#8216;Working around data\u0026amp;#8217; could mean any of the following: generating, facilitating, consuming, processing, visualizing or feeding back into the system. Due to a wide variety of responsibilities, the skill sets also vary to a large extent. Numerous people and teams work with data, with jobs that span the entire data ecosystem:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;Centralizing data from raw sources and systems\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Maintaining and transforming data in a warehouse\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Managing access controls and permissions for the data\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Modeling data\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Doing ad-hoc data analysis and exploration\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Building visualizations and reports\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Nevertheless, a common goal across all these teams is keeping services running and downstream customers happy. In other words, the organization might be divided internally, however, they all have the mission to leverage the data to make better business decisions. Hence, despite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive. To support such a diverse set of data sources and the teams supporting them, Looker supports over \u0026lt;a href=\u0026#34;https://docs.looker.com/setup-and-management/database-config\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;60 dialects\u0026lt;/a\u0026gt; (input from a data source) and over \u0026lt;a href=\u0026#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/delivering-looks-explores#delivery_options_for_third-party_integrations\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;35 destinations\u0026lt;/a\u0026gt; (output to a new data source).\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Below is a simplified* picture of how the Looker ecosystem is central to a data-rich organization.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eMany organizations struggle to create data-driven cultures where each employee is empowered to make decisions based on data. This is especially true for enterprises with a variety of systems and tools in use across different teams. If you are a leader, manager, or executive focused on how your team can leverage Google\u0026#39;s SRE practices or wider DevOps practices, definitely you are in the right place!\u003c/p\u003e\u003ch3\u003eWhat do today’s enterprises or mature start-ups look like?\u003c/h3\u003e\u003cp\u003eToday large organizations are often segmented into hundreds of small teams which are often working around data in the magnitude of several petabytes and in a wide variety of raw forms. ‘Working around data’ could mean any of the following: generating, facilitating, consuming, processing, visualizing or feeding back into the system. Due to a wide variety of responsibilities, the skill sets also vary to a large extent. Numerous people and teams work with data, with jobs that span the entire data ecosystem:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCentralizing data from raw sources and systems\u003c/li\u003e\u003cli\u003eMaintaining and transforming data in a warehouse\u003c/li\u003e\u003cli\u003eManaging access controls and permissions for the data\u003c/li\u003e\u003cli\u003eModeling data\u003c/li\u003e\u003cli\u003eDoing ad-hoc data analysis and exploration\u003c/li\u003e\u003cli\u003eBuilding visualizations and reports\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNevertheless, a common goal across all these teams is keeping services running and downstream customers happy. In other words, the organization might be divided internally, however, they all have the mission to leverage the data to make better business decisions. Hence, despite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive. To support such a diverse set of data sources and the teams supporting them, Looker supports over \u003ca href=\"https://docs.looker.com/setup-and-management/database-config\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003e60 dialects\u003c/a\u003e (input from a data source) and over \u003ca href=\"https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/delivering-looks-explores#delivery_options_for_third-party_integrations\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003e35 destinations\u003c/a\u003e (output to a new data source).\u003c/p\u003e\u003cp\u003eBelow is a simplified* picture of how the Looker ecosystem is central to a data-rich organization.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c64=\"\"\u003e\u003cdiv _ngcontent-c64=\"\" innerhtml=\"\u0026lt;p\u0026gt;*The picture hides the complexity of team(s) accountable for each data source. It also hides how a data source may have dependencies on other sources. \u0026lt;a href=\u0026#34;https://marketplace.looker.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Looker Marketplace\u0026lt;/a\u0026gt; can also play an important role in your ecosystem.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;What role can DevOps and SRE practices play?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;In the most ideal state, all these teams will be in harmony as a single-threaded organization with all the internal processes so smooth that everyone is empowered to experiment (i.e. fail, learn, iterate and repeat all the time). With increasing organizational complexities, it is incredibly challenging to achieve such a state because there will be overhead and misaligned priorities. This is where we look up to the guiding principles of DevOps and SRE practices. In case you are not familiar with Google SRE practices, \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt; is a starting point. The core of DevOps and SRE practices are mature communication and collaboration practices.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s focus on the best practices which could help us with our Looker ecosystem.\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Have joint goals\u0026lt;/b\u0026gt;. There should be some goals which are a shared responsibility across two or more teams. This helps establish a culture of psychological safety and transparency across teams.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Visualize how the data flows across the organization\u0026lt;/b\u0026gt;. This enables an understanding how each team plays their role and how to work with them better.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Agree on the \u0026lt;/b\u0026gt;\u0026lt;a href=\u0026#34;https://sre.google/sre-book/monitoring-distributed-systems/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Golden Signals\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;b\u0026gt;(aka core metrics)\u0026lt;/b\u0026gt;. These could mean data freshness, data accuracy, latency on centralized dashboards etc. These signals allow teams to set their \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons\u0026#34;\u0026gt;error budgets\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://sre.google/sre-book/service-level-objectives/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SLIs\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Agree on communication and collaboration methods that work across teams\u0026lt;/b\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Regular bidirectional communication modes - have shared \u0026lt;a href=\u0026#34;https://support.google.com/chat/answer/7659784?hl=en\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Google Chat spaces\u0026lt;/a\u0026gt;/\u0026lt;a href=\u0026#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling-slack\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;slack channels\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Focus on artifacts such as jointly owned documentations pages, shared roadmap items, reusable tooling, etc. For example, \u0026lt;a href=\u0026#34;https://docs.looker.com/admin-options/system-activity/sa-dashboards\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;System Activity Dashboards\u0026lt;/a\u0026gt; could be made available to all the relevant stakeholders and supplemented with notes tailored to your organization.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Set up regular forums where commonly discussed agenda items include major changes, expected downtime and postmortems around the core metrics. Among other agenda items, you could define/refine a common set of standards, for example centrally defined \u0026lt;a href=\u0026#34;https://docs.looker.com/reference/field-params/label-for-field\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;labels\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://docs.looker.com/reference/field-params/group_label\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;group_labels\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://docs.looker.com/reference/field-params/description\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;descriptions\u0026lt;/a\u0026gt;, etc. in the LookML to ensure there is a single terminology across the board.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Promote informal sharing opportunities such as lessons learned, \u0026lt;a href=\u0026#34;https://www.thinkwithgoogle.com/future-of-marketing/management-and-culture/passion-not-perks/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;TGIFs\u0026lt;/a\u0026gt;, Brown bag sessions, and shadowing opportunities. Learning and teaching have an immense impact on how teams evolve. Teams often become closer with side projects that are slightly outside of their usual day-to-day duties.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Have mutually agreed upon change management practices\u0026lt;/b\u0026gt;. Each team has dependencies so making changes may have an impact on other teams. Why not plan those changes systematically? For example, getting common standards across the \u0026lt;a href=\u0026#34;https://docs.looker.com/data-modeling/getting-started/advanced-deploy-mode\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Advance deploy mode\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Promote continuous improvements\u0026lt;/b\u0026gt;. Keep looking for better, faster, cost-optimized versions of something important to the teams.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Revisit your data flow\u0026lt;/b\u0026gt;. After every major reorganization, ensure that organizational change has not broken the established mechanisms.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003cp\u003e*The picture hides the complexity of team(s) accountable for each data source. It also hides how a data source may have dependencies on other sources. \u003ca href=\"https://marketplace.looker.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://marketplace.looker.com\" track-metadata-module=\"post\"\u003eLooker Marketplace\u003c/a\u003e can also play an important role in your ecosystem.\u003c/p\u003e\u003ch3\u003eWhat role can DevOps and SRE practices play?\u003c/h3\u003e\u003cp\u003eIn the most ideal state, all these teams will be in harmony as a single-threaded organization with all the internal processes so smooth that everyone is empowered to experiment (i.e. fail, learn, iterate and repeat all the time). With increasing organizational complexities, it is incredibly challenging to achieve such a state because there will be overhead and misaligned priorities. This is where we look up to the guiding principles of DevOps and SRE practices. In case you are not familiar with Google SRE practices, \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e is a starting point. The core of DevOps and SRE practices are mature communication and collaboration practices. \u003c/p\u003e\u003cp\u003eLet’s focus on the best practices which could help us with our Looker ecosystem.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eHave joint goals\u003c/b\u003e. There should be some goals which are a shared responsibility across two or more teams. This helps establish a culture of psychological safety and transparency across teams.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eVisualize how the data flows across the organization\u003c/b\u003e. This enables an understanding how each team plays their role and how to work with them better.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAgree on the \u003c/b\u003e\u003ca href=\"https://sre.google/sre-book/monitoring-distributed-systems/\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003e\u003cb\u003eGolden Signals\u003c/b\u003e\u003c/a\u003e \u003cb\u003e(aka core metrics)\u003c/b\u003e. These could mean data freshness, data accuracy, latency on centralized dashboards etc. These signals allow teams to set their \u003ca href=\"https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons\" track-metadata-module=\"post\"\u003eerror budgets\u003c/a\u003e and \u003ca href=\"https://sre.google/sre-book/service-level-objectives/\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSLIs\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAgree on communication and collaboration methods that work across teams\u003c/b\u003e. \u003c/p\u003e\u003c/li\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eRegular bidirectional communication modes - have shared \u003ca href=\"https://support.google.com/chat/answer/7659784?hl=en\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://support.google.com\" track-metadata-module=\"post\"\u003eGoogle Chat spaces\u003c/a\u003e/\u003ca href=\"https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling-slack\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003eslack channels\u003c/a\u003e. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFocus on artifacts such as jointly owned documentations pages, shared roadmap items, reusable tooling, etc. For example, \u003ca href=\"https://docs.looker.com/admin-options/system-activity/sa-dashboards\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003eSystem Activity Dashboards\u003c/a\u003e could be made available to all the relevant stakeholders and supplemented with notes tailored to your organization.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSet up regular forums where commonly discussed agenda items include major changes, expected downtime and postmortems around the core metrics. Among other agenda items, you could define/refine a common set of standards, for example centrally defined \u003ca href=\"https://docs.looker.com/reference/field-params/label-for-field\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003elabels\u003c/a\u003e, \u003ca href=\"https://docs.looker.com/reference/field-params/group_label\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003egroup_labels\u003c/a\u003e, \u003ca href=\"https://docs.looker.com/reference/field-params/description\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003edescriptions\u003c/a\u003e, etc. in the LookML to ensure there is a single terminology across the board.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003ePromote informal sharing opportunities such as lessons learned, \u003ca href=\"https://www.thinkwithgoogle.com/future-of-marketing/management-and-culture/passion-not-perks/\" target=\"_blank\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://www.thinkwithgoogle.com\" track-metadata-module=\"post\"\u003eTGIFs\u003c/a\u003e, Brown bag sessions, and shadowing opportunities. Learning and teaching have an immense impact on how teams evolve. Teams often become closer with side projects that are slightly outside of their usual day-to-day duties.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eHave mutually agreed upon change management practices\u003c/b\u003e. Each team has dependencies so making changes may have an impact on other teams. Why not plan those changes systematically? For example, getting common standards across the \u003ca href=\"https://docs.looker.com/data-modeling/getting-started/advanced-deploy-mode\" target=\"_blank\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003eAdvance deploy mode\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003ePromote continuous improvements\u003c/b\u003e. Keep looking for better, faster, cost-optimized versions of something important to the teams.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eRevisit your data flow\u003c/b\u003e. After every major reorganization, ensure that organizational change has not broken the established mechanisms.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-pull-quote-block\u003e\u003cdiv\u003e\u003cp\u003e\u003cq\u003edespite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive.\u003c/q\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-pull-quote-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c64=\"\"\u003e\u003cdiv _ngcontent-c64=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Are you over-engineering?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;There is a possibility that in the process of maturing the ecosystem, we may end up in an overly engineered system - we may unintentionally add \u0026lt;a href=\u0026#34;https://landing.google.com/sre/sre-book/chapters/eliminating-toil/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;toil\u0026lt;/a\u0026gt; to the environment. These are examples of toil that often stem from communication gaps.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Meetings with no outcomes/action plans - This one is among the most common forms of toil, where the original intention of a meeting is no longer valid but the forum has not taken efforts to revisit their decision.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Unnecessary approvals - Being a single threaded team can often create unnecessary dependencies and your teams may lose the ability to make changes.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Unaligned maintenance windows - Changes across multiple teams may not be mutually exclusive hence if there is misalignment then it may create unforeseen impacts on the end user.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Fancy, but unnecessary tooling - Side projects, if not governed, may create unnecessary tooling which is not being used by the business. Collaborations are great when they solve real business problems, hence it is also required to refocus if the priorities are set right.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Gray areas - When you have a shared responsibility model, you also may end up in gray areas which are often gaps with no owner. This can lead to increased complexity in the long run. For example, having the flexibility to \u0026lt;a href=\u0026#34;https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;schedule content delivery\u0026lt;/a\u0026gt; still requires collaboration to reduce \u0026lt;a href=\u0026#34;https://docs.looker.com/admin-options/scheduler/history\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;jobs with failures\u0026lt;/a\u0026gt; because it can impact the performance of your Looker instance.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Contradicting metrics - You may want to pay special attention to how teams are rewarded for internal metrics. For example, if a team focuses on accuracy of data and other one on freshness then at scale they may not align with one another.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;h3\u0026gt;Conclusion\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;To summarize, we learned how data is handled in large organizations with Looker at its heart unifying a universal semantic model. To handle large amounts of diverse data, teams need to start with aligned goals and commit to strong collaboration. We also learned how DevOps and SRE practices can guide us navigate through these complexities. Lastly, we looked at some side effects of excessively structured systems. To go forward from here, it is highly recommended to start with an analysis of how data flows under your scope and how mature the collaboration is across multiple teams.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Further reading and resources\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/data-analytics/business-intelligence-for-cloud-data-with-looker\u0026#34;\u0026gt;Getting to know Looker \u0026amp;#8211; common use cases\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1\u0026#34;\u0026gt;Enterprise DevOps Guidebook\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\u0026#34;\u0026gt;Know thy enemy: how to prioritize and communicate risks\u0026amp;#8212;CRE life lessons\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://www.oreilly.com/content/how-to-get-started-with-site-reliability-engineering-sre/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;How to get started with site reliability engineering (SRE)\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model\u0026#34;\u0026gt;Bring governance and trust to everyone with Looker\u0026amp;#8217;s universal semantic model\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Related articles\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos\u0026#34;\u0026gt;How SREs analyze risks to evaluate SLOs | Google Cloud Blog\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://help.looker.com/hc/en-us/articles/360001766908-Best-Practice-Create-a-Positive-Experience-for-Looker-Users\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Best Practice: Create a Positive Experience for Looker Users\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://help.looker.com/hc/en-us/articles/360001784747-Best-Practice-LookML-Dos-and-Don-ts\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Best Practice: LookML Dos and Don\u0026#39;ts\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003ch3\u003eAre you over-engineering?\u003c/h3\u003e\u003cp\u003eThere is a possibility that in the process of maturing the ecosystem, we may end up in an overly engineered system - we may unintentionally add \u003ca href=\"https://landing.google.com/sre/sre-book/chapters/eliminating-toil/\" target=\"_blank\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://landing.google.com\" track-metadata-module=\"post\"\u003etoil\u003c/a\u003e to the environment. These are examples of toil that often stem from communication gaps. \u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eMeetings with no outcomes/action plans - This one is among the most common forms of toil, where the original intention of a meeting is no longer valid but the forum has not taken efforts to revisit their decision.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUnnecessary approvals - Being a single threaded team can often create unnecessary dependencies and your teams may lose the ability to make changes.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUnaligned maintenance windows - Changes across multiple teams may not be mutually exclusive hence if there is misalignment then it may create unforeseen impacts on the end user.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFancy, but unnecessary tooling - Side projects, if not governed, may create unnecessary tooling which is not being used by the business. Collaborations are great when they solve real business problems, hence it is also required to refocus if the priorities are set right.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGray areas - When you have a shared responsibility model, you also may end up in gray areas which are often gaps with no owner. This can lead to increased complexity in the long run. For example, having the flexibility to \u003ca href=\"https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003eschedule content delivery\u003c/a\u003e still requires collaboration to reduce \u003ca href=\"https://docs.looker.com/admin-options/scheduler/history\" target=\"_blank\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://docs.looker.com\" track-metadata-module=\"post\"\u003ejobs with failures\u003c/a\u003e because it can impact the performance of your Looker instance.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eContradicting metrics - You may want to pay special attention to how teams are rewarded for internal metrics. For example, if a team focuses on accuracy of data and other one on freshness then at scale they may not align with one another.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eConclusion\u003c/h3\u003e\u003cp\u003eTo summarize, we learned how data is handled in large organizations with Looker at its heart unifying a universal semantic model. To handle large amounts of diverse data, teams need to start with aligned goals and commit to strong collaboration. We also learned how DevOps and SRE practices can guide us navigate through these complexities. Lastly, we looked at some side effects of excessively structured systems. To go forward from here, it is highly recommended to start with an analysis of how data flows under your scope and how mature the collaboration is across multiple teams.\u003c/p\u003e\u003cp\u003e\u003cb\u003eFurther reading and resources\u003c/b\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/data-analytics/business-intelligence-for-cloud-data-with-looker\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/data-analytics/business-intelligence-for-cloud-data-with-looker\" track-metadata-module=\"post\"\u003eGetting to know Looker – common use cases\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1\" track-metadata-module=\"post\"\u003eEnterprise DevOps Guidebook\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\" track-metadata-module=\"post\"\u003eKnow thy enemy: how to prioritize and communicate risks—CRE life lessons\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.oreilly.com/content/how-to-get-started-with-site-reliability-engineering-sre/\" target=\"_blank\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://www.oreilly.com\" track-metadata-module=\"post\"\u003eHow to get started with site reliability engineering (SRE)\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model\" track-metadata-module=\"post\"\u003eBring governance and trust to everyone with Looker’s universal semantic model\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cb\u003eRelated articles\u003c/b\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos\" track-metadata-module=\"post\"\u003eHow SREs analyze risks to evaluate SLOs | Google Cloud Blog\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://help.looker.com/hc/en-us/articles/360001766908-Best-Practice-Create-a-Positive-Experience-for-Looker-Users\" target=\"_blank\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://help.looker.com\" track-metadata-module=\"post\"\u003eBest Practice: Create a Positive Experience for Looker Users\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://help.looker.com/hc/en-us/articles/360001784747-Best-Practice-LookML-Dos-and-Don-ts\" target=\"_blank\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://help.looker.com\" track-metadata-module=\"post\"\u003eBest Practice: LookML Dos and Don\u0026#39;ts\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c63=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/dynamic-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eMany organizations struggle to create data-driven cultures where each employee is empowered to make decisions based on data. This is especially true for enterprises with a variety of systems and tools in use across different teams. If you are a leader, manager, or executive focused on how your team can leverage Google's SRE practices or wider DevOps practices, definitely you are in the right place!\u003c/p\u003e\u003ch3\u003eWhat do today’s enterprises or mature start-ups look like?\u003c/h3\u003e\u003cp\u003eToday large organizations are often segmented into hundreds of small teams which are often working around data in the magnitude of several petabytes and in a wide variety of raw forms. ‘Working around data’ could mean any of the following: generating, facilitating, consuming, processing, visualizing or feeding back into the system. Due to a wide variety of responsibilities, the skill sets also vary to a large extent. Numerous people and teams work with data, with jobs that span the entire data ecosystem:\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eCentralizing data from raw sources and systems\u003c/li\u003e\u003cli\u003eMaintaining and transforming data in a warehouse\u003c/li\u003e\u003cli\u003eManaging access controls and permissions for the data\u003c/li\u003e\u003cli\u003eModeling data\u003c/li\u003e\u003cli\u003eDoing ad-hoc data analysis and exploration\u003c/li\u003e\u003cli\u003eBuilding visualizations and reports\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eNevertheless, a common goal across all these teams is keeping services running and downstream customers happy. In other words, the organization might be divided internally, however, they all have the mission to leverage the data to make better business decisions. Hence, despite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive. To support such a diverse set of data sources and the teams supporting them, Looker supports over \u003ca href=\"https://docs.looker.com/setup-and-management/database-config\" target=\"_blank\"\u003e60 dialects\u003c/a\u003e (input from a data source) and over \u003ca href=\"https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/delivering-looks-explores#delivery_options_for_third-party_integrations\" target=\"_blank\"\u003e35 destinations\u003c/a\u003e (output to a new data source).\u003c/p\u003e\u003cp\u003eBelow is a simplified* picture of how the Looker ecosystem is central to a data-rich organization.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Simplified.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Simplified.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Simplified.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eSimplified* Looker ecosystem in a data-rich environment\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e*The picture hides the complexity of team(s) accountable for each data source. It also hides how a data source may have dependencies on other sources. \u003ca href=\"https://marketplace.looker.com/\" target=\"_blank\"\u003eLooker Marketplace\u003c/a\u003e can also play an important role in your ecosystem.\u003c/p\u003e\u003ch3\u003eWhat role can DevOps and SRE practices play?\u003c/h3\u003e\u003cp\u003eIn the most ideal state, all these teams will be in harmony as a single-threaded organization with all the internal processes so smooth that everyone is empowered to experiment (i.e. fail, learn, iterate and repeat all the time). With increasing organizational complexities, it is incredibly challenging to achieve such a state because there will be overhead and misaligned priorities. This is where we look up to the guiding principles of DevOps and SRE practices. In case you are not familiar with Google SRE practices, \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\"\u003ehere\u003c/a\u003e is a starting point. The core of DevOps and SRE practices are mature communication and collaboration practices. \u003c/p\u003e\u003cp\u003eLet’s focus on the best practices which could help us with our Looker ecosystem.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eHave joint goals\u003c/b\u003e. There should be some goals which are a shared responsibility across two or more teams. This helps establish a culture of psychological safety and transparency across teams.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eVisualize how the data flows across the organization\u003c/b\u003e. This enables an understanding how each team plays their role and how to work with them better.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAgree on the\u003c/b\u003e\u003ca href=\"https://sre.google/sre-book/monitoring-distributed-systems/\" target=\"_blank\"\u003e\u003cb\u003eGolden Signals\u003c/b\u003e\u003c/a\u003e \u003cb\u003e(aka core metrics)\u003c/b\u003e. These could mean data freshness, data accuracy, latency on centralized dashboards etc. These signals allow teams to set their \u003ca href=\"https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons\"\u003eerror budgets\u003c/a\u003e and \u003ca href=\"https://sre.google/sre-book/service-level-objectives/\" target=\"_blank\"\u003eSLIs\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAgree on communication and collaboration methods that work across teams\u003c/b\u003e. \u003c/p\u003e\u003c/li\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eRegular bidirectional communication modes - have shared \u003ca href=\"https://support.google.com/chat/answer/7659784?hl=en\" target=\"_blank\"\u003eGoogle Chat spaces\u003c/a\u003e/\u003ca href=\"https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling-slack\" target=\"_blank\"\u003eslack channels\u003c/a\u003e. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFocus on artifacts such as jointly owned documentations pages, shared roadmap items, reusable tooling, etc. For example, \u003ca href=\"https://docs.looker.com/admin-options/system-activity/sa-dashboards\" target=\"_blank\"\u003eSystem Activity Dashboards\u003c/a\u003e could be made available to all the relevant stakeholders and supplemented with notes tailored to your organization.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSet up regular forums where commonly discussed agenda items include major changes, expected downtime and postmortems around the core metrics. Among other agenda items, you could define/refine a common set of standards, for example centrally defined \u003ca href=\"https://docs.looker.com/reference/field-params/label-for-field\" target=\"_blank\"\u003elabels\u003c/a\u003e, \u003ca href=\"https://docs.looker.com/reference/field-params/group_label\" target=\"_blank\"\u003egroup_labels\u003c/a\u003e, \u003ca href=\"https://docs.looker.com/reference/field-params/description\" target=\"_blank\"\u003edescriptions\u003c/a\u003e, etc. in the LookML to ensure there is a single terminology across the board.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003ePromote informal sharing opportunities such as lessons learned, \u003ca href=\"https://www.thinkwithgoogle.com/future-of-marketing/management-and-culture/passion-not-perks/\" target=\"_blank\"\u003eTGIFs\u003c/a\u003e, Brown bag sessions, and shadowing opportunities. Learning and teaching have an immense impact on how teams evolve. Teams often become closer with side projects that are slightly outside of their usual day-to-day duties.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eHave mutually agreed upon change management practices\u003c/b\u003e. Each team has dependencies so making changes may have an impact on other teams. Why not plan those changes systematically? For example, getting common standards across the \u003ca href=\"https://docs.looker.com/data-modeling/getting-started/advanced-deploy-mode\" target=\"_blank\"\u003eAdvance deploy mode\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003ePromote continuous improvements\u003c/b\u003e. Keep looking for better, faster, cost-optimized versions of something important to the teams.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eRevisit your data flow\u003c/b\u003e. After every major reorganization, ensure that organizational change has not broken the established mechanisms.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-pull_quote\"\u003e\u003cdiv class=\"uni-pull-quote h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003cdiv class=\"uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cdiv class=\"uni-pull-quote__inner-wrapper h-c-copy h-c-copy\"\u003e\u003cq class=\"uni-pull-quote__text\"\u003edespite silos and different subgoals, destiny for all these teams is intertwined for the organization to thrive.\u003c/q\u003e\u003c/div\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eAre you over-engineering?\u003c/h3\u003e\u003cp\u003eThere is a possibility that in the process of maturing the ecosystem, we may end up in an overly engineered system - we may unintentionally add \u003ca href=\"https://landing.google.com/sre/sre-book/chapters/eliminating-toil/\" target=\"_blank\"\u003etoil\u003c/a\u003e to the environment. These are examples of toil that often stem from communication gaps. \u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eMeetings with no outcomes/action plans - This one is among the most common forms of toil, where the original intention of a meeting is no longer valid but the forum has not taken efforts to revisit their decision.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUnnecessary approvals - Being a single threaded team can often create unnecessary dependencies and your teams may lose the ability to make changes.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUnaligned maintenance windows - Changes across multiple teams may not be mutually exclusive hence if there is misalignment then it may create unforeseen impacts on the end user.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFancy, but unnecessary tooling - Side projects, if not governed, may create unnecessary tooling which is not being used by the business. Collaborations are great when they solve real business problems, hence it is also required to refocus if the priorities are set right.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGray areas - When you have a shared responsibility model, you also may end up in gray areas which are often gaps with no owner. This can lead to increased complexity in the long run. For example, having the flexibility to \u003ca href=\"https://docs.looker.com/sharing-and-publishing/scheduling-and-sharing/scheduling\" target=\"_blank\"\u003eschedule content delivery\u003c/a\u003e still requires collaboration to reduce \u003ca href=\"https://docs.looker.com/admin-options/scheduler/history\" target=\"_blank\"\u003ejobs with failures\u003c/a\u003e because it can impact the performance of your Looker instance.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eContradicting metrics - You may want to pay special attention to how teams are rewarded for internal metrics. For example, if a team focuses on accuracy of data and other one on freshness then at scale they may not align with one another.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eConclusion\u003c/h3\u003e\u003cp\u003eTo summarize, we learned how data is handled in large organizations with Looker at its heart unifying a universal semantic model. To handle large amounts of diverse data, teams need to start with aligned goals and commit to strong collaboration. We also learned how DevOps and SRE practices can guide us navigate through these complexities. Lastly, we looked at some side effects of excessively structured systems. To go forward from here, it is highly recommended to start with an analysis of how data flows under your scope and how mature the collaboration is across multiple teams.\u003c/p\u003e\u003cp\u003e\u003cb\u003eFurther reading and resources\u003c/b\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/data-analytics/business-intelligence-for-cloud-data-with-looker\"\u003eGetting to know Looker – common use cases\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1\"\u003eEnterprise DevOps Guidebook\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\"\u003eKnow thy enemy: how to prioritize and communicate risks—CRE life lessons\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.oreilly.com/content/how-to-get-started-with-site-reliability-engineering-sre/\" target=\"_blank\"\u003eHow to get started with site reliability engineering (SRE)\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/data-analytics/lookers-universal-semantic-model\"\u003eBring governance and trust to everyone with Looker’s universal semantic model\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cb\u003eRelated articles\u003c/b\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos\"\u003eHow SREs analyze risks to evaluate SLOs | Google Cloud Blog\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://help.looker.com/hc/en-us/articles/360001766908-Best-Practice-Create-a-Positive-Experience-for-Looker-Users\" target=\"_blank\"\u003eBest Practice: Create a Positive Experience for Looker Users\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://help.looker.com/hc/en-us/articles/360001784747-Best-Practice-LookML-Dos-and-Don-ts\" target=\"_blank\"\u003eBest Practice: LookML Dos and Don'ts\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-07-29T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eSaurabh Bangad\u003c/name\u003e\u003ctitle\u003eTechnical Account Manager, Middle East\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/developers-practitioners/how-google-got-to-rolling-linux-releases-for-desktops/",
      "title": "How Google got to rolling Linux releases for Desktops",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv _ngcontent-c63=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026lt;sup\u0026gt;Hero image credit: Markus Teich\u0026lt;/sup\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;At Google we run large production fleets that serve Google products like YouTube and Gmail. To support all our employees, including engineers, we also run a sizable corporate fleet with hundreds of thousands of devices across multiple platforms, models, and locations. To let each Googler work in the environment they are most productive in, we operate many OS-platforms including a Linux system. For a long time, our internal facing Linux distribution, Goobuntu, was based off of Ubuntu LTS releases. In 2018 we completed a move to a rolling release model based on Debian.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Upgrade Toil\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;More than 15 years ago, Ubuntu was chosen as the base for the internal Linux distribution, as it was user-friendly, easy to use, and had lots of fancy extras. The Long Term Support (LTS) releases were picked as it was valued that Canonical provided 2+ years of security updates.\u0026amp;#160;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;However, this two year release cycle for LTS releases also meant that we had to upgrade every machine in our fleet of over 100.000 devices before the end-of-life date of the OS. The complex nature of workloads run on corporate machines meant that reinstalling and fully customizing machines could be a difficult and time consuming operation. The productivity hit of having all engineers configure their workspace from scratch every two years was not a financially responsible option.\u0026amp;#160;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For each OS cycle, we had a rather large version jump in major packages that could require significant changes to software configuration. To automate this process, we wrote an unattended in-place upgrade tool that took care of a lot of the common case problems. This automation focused approach meant that most of the Google employees didn\u0026#39;t have to manually upgrade their machines by re-installing them and recreating all their configuration. To make this possible, however, we needed to do comprehensive testing of the upgrade process and check that all major packages that had changed kept working (in Ubuntu this could be up to several thousands packages to upgrade between major versions). Sometimes it was hard to provide automation in the cases where deprecations happened and engineers had to make decisions on how to move forward.\u0026amp;#160;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;This effort to upgrade our Goobuntu fleet usually took the better part of a year. With a two year support window there was only one year left until we had to go through the same process all over again for the next LTS. This entire process was a huge stress factor for our team, as we got hundreds of bugs with requests for help for corner cases. Once one upgrade was done there was a general sense of being \u0026amp;#8220;close to burnout\u0026amp;#8221; in the team that we barely could recover from until the next round of updates came about. Running off an LTS version also meant that some bugs encountered by users of our distribution might\u0026amp;#8217;ve already been fixed upstream, but those improvements might\u0026amp;#8217;ve never been backported to the LTS version.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;There was also a long tail of special-case upgrades that could sometimes drag on for several years. Handling this process was a huge change management challenge to get engineers to upgrade the machines that didn\u0026amp;#8217;t work in the automatic process. We got creative when motivating our users to upgrade their machines. Measures ranged from nagging messages on their UI, mails, scheduled reboots and even shutting down the machines, to raise awareness that there were still some machines in dire need of an upgrade. Sometimes this caught machines that people had totally forgotten about, like the one machine under a desk that was running a critical pipeline for something important, as it turned out.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Rolling Releases\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;When we designed gLinux Rodete (Rolling Debian Testing), we aimed at removing the two year upgrade cycle and instead spread out the load on the team throughout time. The general move to CI/CD in the industry has shown that smaller incremental changes are easier to control and rollback. Rolling releases with Linux distributions today are getting more common (Arch Linux, NixOS).\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We considered going with other Linux distributions, but ended up choosing Debian because we again wanted to offer a smooth in-place migration. This included considerations towards the availability of packages in Debian, the large Debian community, and also the existing internal packages and tooling that were using the Debian format. While the Debian Stable track follows a roughly two-year jump between releases, the Debian testing track works as a rolling release, as it\u0026#39;s the pool of all packages ingested and built from upstream, waiting for the next stable release to happen.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The time from upstream release to availability in testing is often just a few days (although during freeze periods before a Debian stable release, it can sometimes lag a few months behind). This means we can get much more granular changes in general and provide the newest software to our engineers at Google without having to wait longer periods.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;This frequency of updates required us to redesign a lot of systems and processes. While originally intending more frequent releases, we found that for us, weekly releases were a sweet spot between moving quickly and allowing for proper release qualification, limiting the disruption to developer productivity.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whenever we start a new release,\u0026amp;#160; we take a snapshot of all the packages ingested from Debian at that time. After some acceptance tests, the new hermetic release candidate is then cautiously rolled out to a dedicated testing fleet and a 1% fleet wide canary. The canary is held intentionally over the course of a couple days to detect any problems with Debian packages or Google internal packages before it progresses to the entire fleet.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Introducing Sieve\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;To manage all these complex tasks from building all upstream packages from source, we have built a workflow system called Sieve. Whenever we see any new version of a Debian package, we start a new build. We build packages in package groups, to take into account separate packages that need to be upgraded together. Once the whole group has been built, we run a virtualized test suite to make sure none of our core components and developer workflows are broken. Each group is tested separately with a full system installation, boot and local test suite run on that version of the operating system. While builds for individual packages usually complete within minutes, these tests can take up to an hour given the complexity of the package group.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Once the packages are built and all the tests passed, we merge all the new packages with our latest pool of packages. When we cut a new release, we snapshot that pool with each package version locked in for that release. We then proceed to carefully guide this release to the fleet utilizing SRE principles like incremental canarying and monitoring the fleet health.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;But not all builds succeed on the first attempt. If a package fails to build, we usually check for any known bugs with the Debian bug tracker and potentially report it, should it not be known already. Sometimes our release engineers have to become creative and apply local workarounds/patches to get a package to build within our ecosystem and later on drop those workarounds once upstream has released a fix.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;One issue that we\u0026#39;ve run into a few times, for example, is that in upstream Debian, packages are usually built in Debian unstable. After a few days, these already built packages migrate to Debian testing. In some cases it\u0026#39;s possible, however, that a build-dependency is stuck in unstable and thus building within testing might not (yet) be feasible. We generally try to work upstream first in these cases so we reduce the complexity and maintenance burden to keep these local patches, while also giving back to the community.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If any of the steps fail, Sieve has a toolbox of tricks to retry builds. For example, when it starts the initial build of a group of packages, the system makes an educated guess of which dependencies need to be built together. But sometimes the version information provided in Debian source packages can be incomplete and this guess is wrong. For this reason, Sieve periodically retries building groups that failed. As the latest snapshot of our packages is a moving target, it could happen that after a seemingly independent package group gets added to the snapshot, a previously broken group unexpectedly builds and passes tests correctly. All these workflows are mostly automatic and this highlights the importance of thinking as an SRE in this field. When facing a failure, it usually seems easier to just fix a failing build once, but if we need to apply the same workaround over and over, putting the workaround in code will reduce the overall burden put on our engineers.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;There are also some security benefits to building all of our binaries from source and having additional source code provenance that verifies the origin of the running binary. During a security incident for example, we are able to rebuild quickly and have confidence in the build working with a temporary patch, as we have been building all packages before, that land in our distribution. Additionally, we also reduce the trust envelope that we have to place into upstream Debian and the binary build artifacts produced by their infrastructure. Instead once the source code is ingested and the binary built verifiably, we can cryptographically attest that the running binary originated from exactly that source code.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Upgrading to Rodete\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The last Goobuntu release was based on Ubuntu 14.04 LTS (Codename Trusty). Development on Rodete started in 2015 and it was quickly clear that we couldn\u0026amp;#8217;t just drop support for Trusty and require the entire engineering population to install a fresh new distribution. From the previous experience of updating in-place between LTS versions, we already had some good experience of knowing what awaited us with this migration. Because Ubuntu is a derivative from Debian and uses a lot of the same packaging infrastructure/formats (apt), it wasn\u0026amp;#8217;t a totally crazy idea to upgrade the fleet from Goobuntu 14.04 to Debian in-place. We reused some parts of our previous in-place upgrade tool, and worked to make it more reliable, by adding more automation and a lot more testing.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To make it easier to create such a tool, test it and maintain it for the duration of the migration, we chose to temporarily freeze gLinux Rodete as a snapshot of Debian testing on a specific date which we call baseline. We can advance this baseline at our own choosing, to balance what packages Sieve ingests. To reduce friction, we intentionally set the baseline of Rodete at the current Debian stable release in 2016 which was much closer to the general state of Ubuntu Trusty. That way we could separate in-place upgrading from Trusty to Debian and major package version changes that happened in Debian at a later date.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In 2017, we started to migrate the machines to Rodete and completed the last in place migrations by the end of 2018. We however still had a baseline of packages which at that point dated almost two years in the past. To catch up with Debian Testing, we started a team wide effort to focus on optimizing Sieve behavior and speed up the time needed to build / test packages. Replaying the upgrades in this incremental fashion and having a moving rolling release target that we control eased the workload for Google engineers and our team.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In early 2019 we started to shut down the last remnants of Goobuntu machines. Our baseline has also advanced to only lag behind by ~250 days which at the time meant we were using most of the package versions that were part of buster. By mid-2020 we finally fully caught up at the same time when Debian bullseye was released. We continue to move ahead our baseline and will probably already be using a similar version of the next Debian Stable release, before its release in mid 2023.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Reaching Zen\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Today, the life of a gLinux team member looks very different. We have reduced the amount of engineering time and energy required for releases to one on-duty release engineer that rotates among team members. We no longer have a big push to upgrade our entire fleet. No more need for multi stage alpha, betas and GAs for new LTS releases while simultaneously chasing down older machines that still were running Ubuntu Precise or Lucid.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We also dramatically improved our security stance by operating our fleet closer to upstream releases. While Debian provides a good source of security patches for the stable and oldstable tracks, we realized that not every security hole that gets patches, necessarily has a Debian Security Advisory (DSA) or CVE number. Our rolling release schedule makes sure we patch security holes on the entire fleet quickly without compromising on stability, while previously security engineers had to carefully review each DSA and make sure the fix has made it to our fleet.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Our improved testing suite and integration tests with key partner teams that run critical developer systems also yielded a more stable experience using a Linux distribution that provides the latest versions of the Linux Kernel. Our strong longing for automating everything in the pipeline has significantly reduced toil and stress within the team. It is now also possible for us to report bugs and incompatibilities with other library versions while making sure that Google tools work better within the Linux ecosystem.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you are interested in making rolling releases in your company a success, then consider to balance the needs of the company against upgrade agility. Being in control of our own moving target and baseline has helped to slow down whenever we encountered too many problems and broke any of our team SLOs. Our journey has ultimately reinforced our belief that incremental changes are better manageable than big bang releases.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you are able to control the influx of new work and keep that predictable, we have made the experience that our engineers stay happier and are less stressed out. This ultimately lowered the team churn and made sure that we can build expertise instead of dealing with multiple burning fires at the same time.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the future, we are planning to work even more closely with upstream Debian and contribute more of our internal patches to maintain the Debian package ecosystem.\u0026lt;/p\u0026gt;\" _nghost-c63=\"\"\u003e\u003cp\u003e\u003ci\u003e\u003csup\u003eHero image credit: Markus Teich\u003c/sup\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003eAt Google we run large production fleets that serve Google products like YouTube and Gmail. To support all our employees, including engineers, we also run a sizable corporate fleet with hundreds of thousands of devices across multiple platforms, models, and locations. To let each Googler work in the environment they are most productive in, we operate many OS-platforms including a Linux system. For a long time, our internal facing Linux distribution, Goobuntu, was based off of Ubuntu LTS releases. In 2018 we completed a move to a rolling release model based on Debian.  \u003c/p\u003e\u003ch3\u003eUpgrade Toil\u003c/h3\u003e\u003cp\u003eMore than 15 years ago, Ubuntu was chosen as the base for the internal Linux distribution, as it was user-friendly, easy to use, and had lots of fancy extras. The Long Term Support (LTS) releases were picked as it was valued that Canonical provided 2+ years of security updates. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eHowever, this two year release cycle for LTS releases also meant that we had to upgrade every machine in our fleet of over 100.000 devices before the end-of-life date of the OS. The complex nature of workloads run on corporate machines meant that reinstalling and fully customizing machines could be a difficult and time consuming operation. The productivity hit of having all engineers configure their workspace from scratch every two years was not a financially responsible option. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eFor each OS cycle, we had a rather large version jump in major packages that could require significant changes to software configuration. To automate this process, we wrote an unattended in-place upgrade tool that took care of a lot of the common case problems. This automation focused approach meant that most of the Google employees didn\u0026#39;t have to manually upgrade their machines by re-installing them and recreating all their configuration. To make this possible, however, we needed to do comprehensive testing of the upgrade process and check that all major packages that had changed kept working (in Ubuntu this could be up to several thousands packages to upgrade between major versions). Sometimes it was hard to provide automation in the cases where deprecations happened and engineers had to make decisions on how to move forward. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eThis effort to upgrade our Goobuntu fleet usually took the better part of a year. With a two year support window there was only one year left until we had to go through the same process all over again for the next LTS. This entire process was a huge stress factor for our team, as we got hundreds of bugs with requests for help for corner cases. Once one upgrade was done there was a general sense of being “close to burnout” in the team that we barely could recover from until the next round of updates came about. Running off an LTS version also meant that some bugs encountered by users of our distribution might’ve already been fixed upstream, but those improvements might’ve never been backported to the LTS version.\u003cbr/\u003e\u003c/p\u003e\u003cp\u003eThere was also a long tail of special-case upgrades that could sometimes drag on for several years. Handling this process was a huge change management challenge to get engineers to upgrade the machines that didn’t work in the automatic process. We got creative when motivating our users to upgrade their machines. Measures ranged from nagging messages on their UI, mails, scheduled reboots and even shutting down the machines, to raise awareness that there were still some machines in dire need of an upgrade. Sometimes this caught machines that people had totally forgotten about, like the one machine under a desk that was running a critical pipeline for something important, as it turned out.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eRolling Releases\u003c/h3\u003e\u003cp\u003eWhen we designed gLinux Rodete (Rolling Debian Testing), we aimed at removing the two year upgrade cycle and instead spread out the load on the team throughout time. The general move to CI/CD in the industry has shown that smaller incremental changes are easier to control and rollback. Rolling releases with Linux distributions today are getting more common (Arch Linux, NixOS). \u003c/p\u003e\u003cp\u003eWe considered going with other Linux distributions, but ended up choosing Debian because we again wanted to offer a smooth in-place migration. This included considerations towards the availability of packages in Debian, the large Debian community, and also the existing internal packages and tooling that were using the Debian format. While the Debian Stable track follows a roughly two-year jump between releases, the Debian testing track works as a rolling release, as it\u0026#39;s the pool of all packages ingested and built from upstream, waiting for the next stable release to happen. \u003c/p\u003e\u003cp\u003eThe time from upstream release to availability in testing is often just a few days (although during freeze periods before a Debian stable release, it can sometimes lag a few months behind). This means we can get much more granular changes in general and provide the newest software to our engineers at Google without having to wait longer periods.\u003c/p\u003e\u003cp\u003eThis frequency of updates required us to redesign a lot of systems and processes. While originally intending more frequent releases, we found that for us, weekly releases were a sweet spot between moving quickly and allowing for proper release qualification, limiting the disruption to developer productivity.\u003c/p\u003e\u003cp\u003eWhenever we start a new release,  we take a snapshot of all the packages ingested from Debian at that time. After some acceptance tests, the new hermetic release candidate is then cautiously rolled out to a dedicated testing fleet and a 1% fleet wide canary. The canary is held intentionally over the course of a couple days to detect any problems with Debian packages or Google internal packages before it progresses to the entire fleet. \u003c/p\u003e\u003ch3\u003eIntroducing Sieve\u003c/h3\u003e\u003cp\u003eTo manage all these complex tasks from building all upstream packages from source, we have built a workflow system called Sieve. Whenever we see any new version of a Debian package, we start a new build. We build packages in package groups, to take into account separate packages that need to be upgraded together. Once the whole group has been built, we run a virtualized test suite to make sure none of our core components and developer workflows are broken. Each group is tested separately with a full system installation, boot and local test suite run on that version of the operating system. While builds for individual packages usually complete within minutes, these tests can take up to an hour given the complexity of the package group.\u003c/p\u003e\u003cp\u003eOnce the packages are built and all the tests passed, we merge all the new packages with our latest pool of packages. When we cut a new release, we snapshot that pool with each package version locked in for that release. We then proceed to carefully guide this release to the fleet utilizing SRE principles like incremental canarying and monitoring the fleet health. \u003c/p\u003e\u003cp\u003eBut not all builds succeed on the first attempt. If a package fails to build, we usually check for any known bugs with the Debian bug tracker and potentially report it, should it not be known already. Sometimes our release engineers have to become creative and apply local workarounds/patches to get a package to build within our ecosystem and later on drop those workarounds once upstream has released a fix.\u003c/p\u003e\u003cp\u003eOne issue that we\u0026#39;ve run into a few times, for example, is that in upstream Debian, packages are usually built in Debian unstable. After a few days, these already built packages migrate to Debian testing. In some cases it\u0026#39;s possible, however, that a build-dependency is stuck in unstable and thus building within testing might not (yet) be feasible. We generally try to work upstream first in these cases so we reduce the complexity and maintenance burden to keep these local patches, while also giving back to the community. \u003c/p\u003e\u003cp\u003eIf any of the steps fail, Sieve has a toolbox of tricks to retry builds. For example, when it starts the initial build of a group of packages, the system makes an educated guess of which dependencies need to be built together. But sometimes the version information provided in Debian source packages can be incomplete and this guess is wrong. For this reason, Sieve periodically retries building groups that failed. As the latest snapshot of our packages is a moving target, it could happen that after a seemingly independent package group gets added to the snapshot, a previously broken group unexpectedly builds and passes tests correctly. All these workflows are mostly automatic and this highlights the importance of thinking as an SRE in this field. When facing a failure, it usually seems easier to just fix a failing build once, but if we need to apply the same workaround over and over, putting the workaround in code will reduce the overall burden put on our engineers.\u003c/p\u003e\u003cp\u003eThere are also some security benefits to building all of our binaries from source and having additional source code provenance that verifies the origin of the running binary. During a security incident for example, we are able to rebuild quickly and have confidence in the build working with a temporary patch, as we have been building all packages before, that land in our distribution. Additionally, we also reduce the trust envelope that we have to place into upstream Debian and the binary build artifacts produced by their infrastructure. Instead once the source code is ingested and the binary built verifiably, we can cryptographically attest that the running binary originated from exactly that source code.\u003c/p\u003e\u003ch3\u003eUpgrading to Rodete\u003c/h3\u003e\u003cp\u003eThe last Goobuntu release was based on Ubuntu 14.04 LTS (Codename Trusty). Development on Rodete started in 2015 and it was quickly clear that we couldn’t just drop support for Trusty and require the entire engineering population to install a fresh new distribution. From the previous experience of updating in-place between LTS versions, we already had some good experience of knowing what awaited us with this migration. Because Ubuntu is a derivative from Debian and uses a lot of the same packaging infrastructure/formats (apt), it wasn’t a totally crazy idea to upgrade the fleet from Goobuntu 14.04 to Debian in-place. We reused some parts of our previous in-place upgrade tool, and worked to make it more reliable, by adding more automation and a lot more testing.\u003c/p\u003e\u003cp\u003eTo make it easier to create such a tool, test it and maintain it for the duration of the migration, we chose to temporarily freeze gLinux Rodete as a snapshot of Debian testing on a specific date which we call baseline. We can advance this baseline at our own choosing, to balance what packages Sieve ingests. To reduce friction, we intentionally set the baseline of Rodete at the current Debian stable release in 2016 which was much closer to the general state of Ubuntu Trusty. That way we could separate in-place upgrading from Trusty to Debian and major package version changes that happened in Debian at a later date. \u003c/p\u003e\u003cp\u003eIn 2017, we started to migrate the machines to Rodete and completed the last in place migrations by the end of 2018. We however still had a baseline of packages which at that point dated almost two years in the past. To catch up with Debian Testing, we started a team wide effort to focus on optimizing Sieve behavior and speed up the time needed to build / test packages. Replaying the upgrades in this incremental fashion and having a moving rolling release target that we control eased the workload for Google engineers and our team.\u003c/p\u003e\u003cp\u003eIn early 2019 we started to shut down the last remnants of Goobuntu machines. Our baseline has also advanced to only lag behind by ~250 days which at the time meant we were using most of the package versions that were part of buster. By mid-2020 we finally fully caught up at the same time when Debian bullseye was released. We continue to move ahead our baseline and will probably already be using a similar version of the next Debian Stable release, before its release in mid 2023.\u003c/p\u003e\u003ch3\u003eReaching Zen\u003c/h3\u003e\u003cp\u003eToday, the life of a gLinux team member looks very different. We have reduced the amount of engineering time and energy required for releases to one on-duty release engineer that rotates among team members. We no longer have a big push to upgrade our entire fleet. No more need for multi stage alpha, betas and GAs for new LTS releases while simultaneously chasing down older machines that still were running Ubuntu Precise or Lucid.\u003c/p\u003e\u003cp\u003eWe also dramatically improved our security stance by operating our fleet closer to upstream releases. While Debian provides a good source of security patches for the stable and oldstable tracks, we realized that not every security hole that gets patches, necessarily has a Debian Security Advisory (DSA) or CVE number. Our rolling release schedule makes sure we patch security holes on the entire fleet quickly without compromising on stability, while previously security engineers had to carefully review each DSA and make sure the fix has made it to our fleet.\u003c/p\u003e\u003cp\u003eOur improved testing suite and integration tests with key partner teams that run critical developer systems also yielded a more stable experience using a Linux distribution that provides the latest versions of the Linux Kernel. Our strong longing for automating everything in the pipeline has significantly reduced toil and stress within the team. It is now also possible for us to report bugs and incompatibilities with other library versions while making sure that Google tools work better within the Linux ecosystem.\u003c/p\u003e\u003cp\u003eIf you are interested in making rolling releases in your company a success, then consider to balance the needs of the company against upgrade agility. Being in control of our own moving target and baseline has helped to slow down whenever we encountered too many problems and broke any of our team SLOs. Our journey has ultimately reinforced our belief that incremental changes are better manageable than big bang releases. \u003c/p\u003e\u003cp\u003eIf you are able to control the influx of new work and keep that predictable, we have made the experience that our engineers stay happier and are less stressed out. This ultimately lowered the team churn and made sure that we can build expertise instead of dealing with multiple burning fires at the same time.\u003c/p\u003e\u003cp\u003eIn the future, we are planning to work even more closely with upstream Debian and contribute more of our internal patches to maintain the Debian package ecosystem.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003e\u003csup\u003eHero image credit: Markus Teich\u003c/sup\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003eAt Google we run large production fleets that serve Google products like YouTube and Gmail. To support all our employees, including engineers, we also run a sizable corporate fleet with hundreds of thousands of devices across multiple platforms, models, and locations. To let each Googler work in the environment they are most productive in, we operate many OS-platforms including a Linux system. For a long time, our internal facing Linux distribution, Goobuntu, was based off of Ubuntu LTS releases. In 2018 we completed a move to a rolling release model based on Debian.  \u003c/p\u003e\u003ch3\u003eUpgrade Toil\u003c/h3\u003e\u003cp\u003eMore than 15 years ago, Ubuntu was chosen as the base for the internal Linux distribution, as it was user-friendly, easy to use, and had lots of fancy extras. The Long Term Support (LTS) releases were picked as it was valued that Canonical provided 2+ years of security updates. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eHowever, this two year release cycle for LTS releases also meant that we had to upgrade every machine in our fleet of over 100.000 devices before the end-of-life date of the OS. The complex nature of workloads run on corporate machines meant that reinstalling and fully customizing machines could be a difficult and time consuming operation. The productivity hit of having all engineers configure their workspace from scratch every two years was not a financially responsible option. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eFor each OS cycle, we had a rather large version jump in major packages that could require significant changes to software configuration. To automate this process, we wrote an unattended in-place upgrade tool that took care of a lot of the common case problems. This automation focused approach meant that most of the Google employees didn't have to manually upgrade their machines by re-installing them and recreating all their configuration. To make this possible, however, we needed to do comprehensive testing of the upgrade process and check that all major packages that had changed kept working (in Ubuntu this could be up to several thousands packages to upgrade between major versions). Sometimes it was hard to provide automation in the cases where deprecations happened and engineers had to make decisions on how to move forward. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eThis effort to upgrade our Goobuntu fleet usually took the better part of a year. With a two year support window there was only one year left until we had to go through the same process all over again for the next LTS. This entire process was a huge stress factor for our team, as we got hundreds of bugs with requests for help for corner cases. Once one upgrade was done there was a general sense of being “close to burnout” in the team that we barely could recover from until the next round of updates came about. Running off an LTS version also meant that some bugs encountered by users of our distribution might’ve already been fixed upstream, but those improvements might’ve never been backported to the LTS version.\u003cbr/\u003e\u003c/p\u003e\u003cp\u003eThere was also a long tail of special-case upgrades that could sometimes drag on for several years. Handling this process was a huge change management challenge to get engineers to upgrade the machines that didn’t work in the automatic process. We got creative when motivating our users to upgrade their machines. Measures ranged from nagging messages on their UI, mails, scheduled reboots and even shutting down the machines, to raise awareness that there were still some machines in dire need of an upgrade. Sometimes this caught machines that people had totally forgotten about, like the one machine under a desk that was running a critical pipeline for something important, as it turned out.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eRolling Releases\u003c/h3\u003e\u003cp\u003eWhen we designed gLinux Rodete (Rolling Debian Testing), we aimed at removing the two year upgrade cycle and instead spread out the load on the team throughout time. The general move to CI/CD in the industry has shown that smaller incremental changes are easier to control and rollback. Rolling releases with Linux distributions today are getting more common (Arch Linux, NixOS). \u003c/p\u003e\u003cp\u003eWe considered going with other Linux distributions, but ended up choosing Debian because we again wanted to offer a smooth in-place migration. This included considerations towards the availability of packages in Debian, the large Debian community, and also the existing internal packages and tooling that were using the Debian format. While the Debian Stable track follows a roughly two-year jump between releases, the Debian testing track works as a rolling release, as it's the pool of all packages ingested and built from upstream, waiting for the next stable release to happen. \u003c/p\u003e\u003cp\u003eThe time from upstream release to availability in testing is often just a few days (although during freeze periods before a Debian stable release, it can sometimes lag a few months behind). This means we can get much more granular changes in general and provide the newest software to our engineers at Google without having to wait longer periods.\u003c/p\u003e\u003cp\u003eThis frequency of updates required us to redesign a lot of systems and processes. While originally intending more frequent releases, we found that for us, weekly releases were a sweet spot between moving quickly and allowing for proper release qualification, limiting the disruption to developer productivity.\u003c/p\u003e\u003cp\u003eWhenever we start a new release,  we take a snapshot of all the packages ingested from Debian at that time. After some acceptance tests, the new hermetic release candidate is then cautiously rolled out to a dedicated testing fleet and a 1% fleet wide canary. The canary is held intentionally over the course of a couple days to detect any problems with Debian packages or Google internal packages before it progresses to the entire fleet. \u003c/p\u003e\u003ch3\u003eIntroducing Sieve\u003c/h3\u003e\u003cp\u003eTo manage all these complex tasks from building all upstream packages from source, we have built a workflow system called Sieve. Whenever we see any new version of a Debian package, we start a new build. We build packages in package groups, to take into account separate packages that need to be upgraded together. Once the whole group has been built, we run a virtualized test suite to make sure none of our core components and developer workflows are broken. Each group is tested separately with a full system installation, boot and local test suite run on that version of the operating system. While builds for individual packages usually complete within minutes, these tests can take up to an hour given the complexity of the package group.\u003c/p\u003e\u003cp\u003eOnce the packages are built and all the tests passed, we merge all the new packages with our latest pool of packages. When we cut a new release, we snapshot that pool with each package version locked in for that release. We then proceed to carefully guide this release to the fleet utilizing SRE principles like incremental canarying and monitoring the fleet health. \u003c/p\u003e\u003cp\u003eBut not all builds succeed on the first attempt. If a package fails to build, we usually check for any known bugs with the Debian bug tracker and potentially report it, should it not be known already. Sometimes our release engineers have to become creative and apply local workarounds/patches to get a package to build within our ecosystem and later on drop those workarounds once upstream has released a fix.\u003c/p\u003e\u003cp\u003eOne issue that we've run into a few times, for example, is that in upstream Debian, packages are usually built in Debian unstable. After a few days, these already built packages migrate to Debian testing. In some cases it's possible, however, that a build-dependency is stuck in unstable and thus building within testing might not (yet) be feasible. We generally try to work upstream first in these cases so we reduce the complexity and maintenance burden to keep these local patches, while also giving back to the community. \u003c/p\u003e\u003cp\u003eIf any of the steps fail, Sieve has a toolbox of tricks to retry builds. For example, when it starts the initial build of a group of packages, the system makes an educated guess of which dependencies need to be built together. But sometimes the version information provided in Debian source packages can be incomplete and this guess is wrong. For this reason, Sieve periodically retries building groups that failed. As the latest snapshot of our packages is a moving target, it could happen that after a seemingly independent package group gets added to the snapshot, a previously broken group unexpectedly builds and passes tests correctly. All these workflows are mostly automatic and this highlights the importance of thinking as an SRE in this field. When facing a failure, it usually seems easier to just fix a failing build once, but if we need to apply the same workaround over and over, putting the workaround in code will reduce the overall burden put on our engineers.\u003c/p\u003e\u003cp\u003eThere are also some security benefits to building all of our binaries from source and having additional source code provenance that verifies the origin of the running binary. During a security incident for example, we are able to rebuild quickly and have confidence in the build working with a temporary patch, as we have been building all packages before, that land in our distribution. Additionally, we also reduce the trust envelope that we have to place into upstream Debian and the binary build artifacts produced by their infrastructure. Instead once the source code is ingested and the binary built verifiably, we can cryptographically attest that the running binary originated from exactly that source code.\u003c/p\u003e\u003ch3\u003eUpgrading to Rodete\u003c/h3\u003e\u003cp\u003eThe last Goobuntu release was based on Ubuntu 14.04 LTS (Codename Trusty). Development on Rodete started in 2015 and it was quickly clear that we couldn’t just drop support for Trusty and require the entire engineering population to install a fresh new distribution. From the previous experience of updating in-place between LTS versions, we already had some good experience of knowing what awaited us with this migration. Because Ubuntu is a derivative from Debian and uses a lot of the same packaging infrastructure/formats (apt), it wasn’t a totally crazy idea to upgrade the fleet from Goobuntu 14.04 to Debian in-place. We reused some parts of our previous in-place upgrade tool, and worked to make it more reliable, by adding more automation and a lot more testing.\u003c/p\u003e\u003cp\u003eTo make it easier to create such a tool, test it and maintain it for the duration of the migration, we chose to temporarily freeze gLinux Rodete as a snapshot of Debian testing on a specific date which we call baseline. We can advance this baseline at our own choosing, to balance what packages Sieve ingests. To reduce friction, we intentionally set the baseline of Rodete at the current Debian stable release in 2016 which was much closer to the general state of Ubuntu Trusty. That way we could separate in-place upgrading from Trusty to Debian and major package version changes that happened in Debian at a later date. \u003c/p\u003e\u003cp\u003eIn 2017, we started to migrate the machines to Rodete and completed the last in place migrations by the end of 2018. We however still had a baseline of packages which at that point dated almost two years in the past. To catch up with Debian Testing, we started a team wide effort to focus on optimizing Sieve behavior and speed up the time needed to build / test packages. Replaying the upgrades in this incremental fashion and having a moving rolling release target that we control eased the workload for Google engineers and our team.\u003c/p\u003e\u003cp\u003eIn early 2019 we started to shut down the last remnants of Goobuntu machines. Our baseline has also advanced to only lag behind by ~250 days which at the time meant we were using most of the package versions that were part of buster. By mid-2020 we finally fully caught up at the same time when Debian bullseye was released. We continue to move ahead our baseline and will probably already be using a similar version of the next Debian Stable release, before its release in mid 2023.\u003c/p\u003e\u003ch3\u003eReaching Zen\u003c/h3\u003e\u003cp\u003eToday, the life of a gLinux team member looks very different. We have reduced the amount of engineering time and energy required for releases to one on-duty release engineer that rotates among team members. We no longer have a big push to upgrade our entire fleet. No more need for multi stage alpha, betas and GAs for new LTS releases while simultaneously chasing down older machines that still were running Ubuntu Precise or Lucid.\u003c/p\u003e\u003cp\u003eWe also dramatically improved our security stance by operating our fleet closer to upstream releases. While Debian provides a good source of security patches for the stable and oldstable tracks, we realized that not every security hole that gets patches, necessarily has a Debian Security Advisory (DSA) or CVE number. Our rolling release schedule makes sure we patch security holes on the entire fleet quickly without compromising on stability, while previously security engineers had to carefully review each DSA and make sure the fix has made it to our fleet.\u003c/p\u003e\u003cp\u003eOur improved testing suite and integration tests with key partner teams that run critical developer systems also yielded a more stable experience using a Linux distribution that provides the latest versions of the Linux Kernel. Our strong longing for automating everything in the pipeline has significantly reduced toil and stress within the team. It is now also possible for us to report bugs and incompatibilities with other library versions while making sure that Google tools work better within the Linux ecosystem.\u003c/p\u003e\u003cp\u003eIf you are interested in making rolling releases in your company a success, then consider to balance the needs of the company against upgrade agility. Being in control of our own moving target and baseline has helped to slow down whenever we encountered too many problems and broke any of our team SLOs. Our journey has ultimately reinforced our belief that incremental changes are better manageable than big bang releases. \u003c/p\u003e\u003cp\u003eIf you are able to control the influx of new work and keep that predictable, we have made the experience that our engineers stay happier and are less stressed out. This ultimately lowered the team churn and made sure that we can build expertise instead of dealing with multiple burning fires at the same time.\u003c/p\u003e\u003cp\u003eIn the future, we are planning to work even more closely with upstream Debian and contribute more of our internal patches to maintain the Debian package ecosystem.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/rodete-hero.max-800x800.png",
      "date_published": "2022-07-12T12:00:00Z",
      "author": {
        "name": "\u003cname\u003eSven Mueller\u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/more-support-for-structured-logs-in-new-version-of-go-logging-library/",
      "title": "More support for structured logs in new version of Go logging library",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe new version of the Google logging client library for Go has been released. Version 1.5 adds new features and bug fixes including new structured logging capabilities that complete last year's effort to enrich structured logging support in Google \u003ca href=\"https://cloud.google.com/logging/docs/reference/libraries#client-libraries-install-go\"\u003elogging client libraries\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHere are few of the new features in v1.5:\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cb\u003eFaster and more robust way\u003c/b\u003e to detect and capture Google Cloud resources that the application is running on.\u003c/li\u003e\u003cli\u003e\u003cb\u003eAutomatic source location detection\u003c/b\u003e to support log observability for debugging and troubleshooting.\u003c/li\u003e\u003cli\u003e\u003cb\u003eW3C header\u003c/b\u003e \u003ca href=\"https://www.w3.org/TR/trace-context/\" target=\"_blank\"\u003e\u003ccode\u003etraceparent\u003c/code\u003e\u003c/a\u003e for capturing tracing information within the logged entries.\u003c/li\u003e\u003cli\u003e\u003cb\u003eBetter control over batched ingestion\u003c/b\u003e of the log entries by supporting the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/write#body.request_body.FIELDS.partial_success\"\u003e\u003ccode\u003epartialSuccess\u003c/code\u003e\u003c/a\u003e flag within Logger instances.\u003c/li\u003e\u003cli\u003e\u003cb\u003eSupport for out-of-process ingestion\u003c/b\u003e with redirection of the logs to \u003ccode\u003estdout\u003c/code\u003e and \u003ccode\u003estderr\u003c/code\u003e using a structured logging format.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eLet's look into each closer:\u003c/p\u003e\u003ch3\u003eResource detection\u003c/h3\u003e\u003cp\u003eResource detection is an existing feature of the logging library. It detects a resource on which an application is running. Retrieves the resource's metadata. And implicitly adds this metadata to each log entry the application ingests using the library. It is especially useful for applications that run on Google Cloud since it collects a lot of resource's attributes from the \u003ca href=\"https://cloud.google.com/compute/docs/metadata/overview\"\u003eMetadata server\u003c/a\u003e of the resource. These attributes enrich ingested logs with additional information such as a location of the VM, a name of the container or a service Id of the AppEngine service. The below Json shows a sample of the retrieved information after detecting the resource as a GKE container and retrieving resource metadata according to the \u003ca href=\"https://cloud.google.com/monitoring/api/resources#tag_k8s_container\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'{\\r\\n \"type\": \"k8s_container\",\\r\\n \"labels\": {\\r\\n \"project_id\": \"dev-env-060122\",\\r\\n \"location\": \"us-central1-a\",\\r\\n \"cluster_name\": \"dev-test-cluster-47fg\",\\r\\n \"namespace_name\": \"default\",\\r\\n \"pod_name\" : \"frontend-4fgd4\",\\r\\n \"container_name\": \"frontend-4fgd4-acgf12a5\"\\r\\n }\\r\\n}'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4c9ad10a50\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe implementation is optimized to avoid performance degradation during the data collection process. Previously, the heuristic for identifying the resource was heavily based on environment variables which could result in many false positives. Additionally, the implementation performed too many queries to the metadata server which could sometimes cause delayed responses. In the 1.5 release the heuristic was updated to use additional artifacts beside the environment variables in the resource detection logic and the number of the queries to the metadata server was reduced to a bare minimum. As a result, false detection of GCP resources is decreased by an order of magnitude and the performance penalties to run the heuristic in non-GCP resources is decreased as well. The change does not affect the ingestion process and does not require any changes in the application's code.\u003c/p\u003e\u003ch3\u003eSource location capturing\u003c/h3\u003e\u003cp\u003eIt is useful to capture the location in code where the log was ingested. While the main usage is in troubleshooting and debugging it can be useful in other circumstances. In this version of the library you can configure your logger instance to capture the source location \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#logentrysourcelocation\"\u003emetadata\u003c/a\u003e for each log entry ingested using \u003ca href=\"https://pkg.go.dev/cloud.google.com/go/logging#Logger.Log\"\u003e\u003ccode\u003eLogger.Log()\u003c/code\u003e\u003c/a\u003e or \u003ca href=\"https://pkg.go.dev/cloud.google.com/go/logging#Logger.LogSync\"\u003e\u003ccode\u003eLogger.LogSync()\u003c/code\u003e\u003c/a\u003e functions. Just pass the output of the \u003ca href=\"https://pkg.go.dev/cloud.google.com/go/logging#SourceLocationPopulation\"\u003e\u003ccode\u003eSourceLocationPopulation()\u003c/code\u003e\u003c/a\u003e as a \u003ccode\u003eLoggerOption\u003c/code\u003e argument in the call to \u003ca href=\"https://pkg.go.dev/cloud.google.com/go/logging#Client.Logger\"\u003e\u003ccode\u003eClient.Logger()\u003c/code\u003e\u003c/a\u003e when creating a new instance of the logger. The following snippet creates a logger instance that adds source location metadata into each ingested log with severity set to Debug:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'logger := client.Logger(\"debug-logger\",\\r\\n logging.SourceLocationPopulation(PopulateSourceLocationForDebugEntries))'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee876f50\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe function SourceLocationPopulation() accepts the following constants:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003elogging.DoNotPopulateSourceLocation\u003c/code\u003e ‒ is a default configuration that prevents capturing the source location in the ingested logs\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003elogging.PopulateSourceLocationForDebugEntries\u003c/code\u003e ‒ adds the source location metadata into logs with Debug severity.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003elogging.AlwaysPopulateSourceLocation\u003c/code\u003e ‒ populates the source location in all ingested logs.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis feature has to be enabled explicitly because the operation of capturing the source location in Go may increase the total execution time of the log ingestion by a factor of 2. It is strongly discouraged to enable it for all ingested logs.\u003c/p\u003e\u003ch3\u003eUse W3C context header for tracing\u003c/h3\u003e\u003cp\u003eYou could add tracing information with your logs in the previous versions of the library. The way to do it was directly, by providing trace and span identification and, optionally, the sampling flag. The following code demonstrates the manual setting of the trace and span identifiers:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'logger := client.Logger(\"my-log\")\\r\\n// \\u2026\\r\\nlogger.Log(\\r\\n logging.Entry{\\r\\n Payload: \"keep tracing\",\\r\\n Trace: \"4bf92f3577b34da6a3ce929d0e0e4736\",\\r\\n SpanID: \"00f067aa0ba902b7\",\\r\\n })'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee8fa650\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOr indirectly, by passing an instance of the \u003ccode\u003ehttp.Request\u003c/code\u003e as a part of the Http request metadata:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'logger := client.Logger(\"my-log\")\\r\\n// \\u2026\\r\\nfunc MyHandler(w http.ResponseWriter, r *http.Request) {\\r\\n logger.log(\\r\\n logging.Entry{\\r\\n Payload: \"My handler invoked\",\\r\\n HttpRequest: \u0026amp;logging.HttpRequest{\\r\\n Request: r,\\r\\n },\\r\\n })\\r\\n}'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee8fa110\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn the latter case, the library will try to pull tracing information from the \u003ccode\u003ex-cloud-tracing-context\u003c/code\u003e header. From this release, the library also supports W3C tracing context header. If both headers are present, the tracing information is captured from the W3C traceparent header.\u003c/p\u003e\u003ch3\u003eOut-of-process logs' ingestion\u003c/h3\u003e\u003cp\u003eBy default the library supports synchronous and asynchronous log ingestions by calling the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rpc/google.logging.v2#google.logging.v2.WriteLogEntriesRequest\"\u003eCloud Logging API\u003c/a\u003e directly. In certain cases the log ingestion is better to be done using external \u003ca href=\"https://cloud.google.com/logging/docs/agent\"\u003elogging agents\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/run/docs/logging#a_note_about_logging_agents\"\u003ebuilt-in support\u003c/a\u003e for logs collection. In this release, you can configure a logger instance to write logs to stdout or stderr instead of ingesting it to Cloud Logging directly. The following example creates a logger that redirects logs to stdout using \u003ca href=\"https://cloud.google.com/logging/docs/structured-logging#special-payload-fields\"\u003especially formatted\u003c/a\u003e Json string:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'logger := client.Logger(\"not-ingesting-log\", RedirectAsJSON(os.Stdout)\\r\\nlogger.Log(logging.Entry{Severity: logging.Debug, Payload: \"out of process log\"})'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee3a1c10\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe above code will print something like the following line to the standard output:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'{\"message\":\"out of process log\", \"severity\":\"DEBUG\", \"timestamp\":\"seconds:1656381253\"}'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee3a1310\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn some circumstances, when the standard output cannot be used for printing logs, the logger can be configured to redirect output to the standard error (\u003ccode\u003eos.Stderr\u003c/code\u003e) with the same effect.\u003c/p\u003e\u003cp\u003eThere are a couple of things to be aware of when you use the out-of-process logging:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eMethods Logger.Log()\u003c/code\u003e and \u003ccode\u003eLogger.LogSync()\u003c/code\u003e behave the same way when the logger is configured with the out-of-process logging option. They write the Jsonified logs to the provided io.Write writer. And an external logging agent determines the logs' collection and ingestion.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eYou do not have control over the Log ID. All logs that are ingested by the logging agent or the built-in support of the managed service (e.g. Cloud Run) will use the Log ID that is determined out-of-process.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eMore control over batch ingestion\u003c/h3\u003e\u003cp\u003eWhen you ingest logs using Logger.Log() function, the asynchronous ingestion batches multiple log entries together and ingest them using the \u003ccode\u003eentries.write\u003c/code\u003e \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/write\"\u003eLogging API\u003c/a\u003e. If the ingestion of any of the aggregate logs fails, no logs get ingested. Starting with this release you can control this logic by opting in the partial success \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/write#body.request_body.FIELDS.partial_success\"\u003eflag\u003c/a\u003e. When the flag is set, the Logging API tries to ingest all logs, even if some other log entry fails due to a permanent error such as INVALID_ARGUMENT or PERMISSION_DENIED. This option can be opted-in when creating a new logger using the \u003ccode\u003ePartialSuccess\u003c/code\u003e logger option:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'logger := client.Logger(\"my-log\", PartialSuccess())'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e4cee3a1d10\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWrapping up\u003c/h3\u003e\u003cp\u003eWhen you upgrade to version 1.5 you get a more robust and deterministic resource detection algorithm while keeping the behavior of the library unchanged. Additional functionality such as out-of-process ingestion, source location or batch ingestion control can be opted-in using the logger options. With these new features and fixes the behavior of the library becomes more deterministic and robust. \u003c/p\u003e\u003cp\u003eLearn more about the release at \u003ca href=\"https://pkg.go.dev/cloud.google.com/go/logging\"\u003ego.pkg.dev\u003c/a\u003e. Please also visit the library's project on \u003ca href=\"https://github.com/googleapis/google-cloud-go/tree/main/logging\" target=\"_blank\"\u003eGithub\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-logging-python-client-library-v3-0-0-release/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/logging.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGetting Started with Google Cloud Logging Python v3.0.0\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eLearn how to manage your app's Python logs and related metadata using Google Cloud client libraries.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/logging_nZNFoFp.jpg",
      "date_published": "2022-07-01T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eLeonid Yankulin\u003c/name\u003e\u003ctitle\u003eDeveloper Relations Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/promql-for-cloud-monitoring-metrics-now-available/",
      "title": "Cloud Monitoring metrics, now in Managed Service for Prometheus",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Because we built Managed Service for Prometheus on top of the \u0026lt;a href=\u0026#34;https://research.google/pubs/pub50652/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;same planet-scale time series database\u0026lt;/a\u0026gt; as Cloud Monitoring, all your metrics are stored together and are queryable together. Metrics in Cloud Monitoring are automatically generated when you use Google Cloud services at no additional cost to you. View all your metrics in one place with the query language that developers already know and prefer, opening up possibilities such as:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Correlating spikes in traffic with Redis cache misses using \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing\u0026#34;\u0026gt;Cloud Load Balancing metrics\u0026lt;/a\u0026gt; and Prometheus\u0026amp;#8217; \u0026lt;a href=\u0026#34;https://github.com/oliver006/redis_exporter\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Redis exporter\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Graphing Cloud Logging\u0026amp;#8217;s \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/logs-based-metrics\u0026#34;\u0026gt;logs-based metrics\u0026lt;/a\u0026gt; alongside Prometheus metrics\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Alerting on your \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute\u0026#34;\u0026gt;Compute Engine utilization\u0026lt;/a\u0026gt; or your \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub\u0026#34;\u0026gt;Pub/Sub backlog size\u0026lt;/a\u0026gt; using PromQL and \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/rules-managed\u0026#34;\u0026gt;Managed Service for Prometheus\u0026amp;#8217; rule evaluation\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Substituting paid Istio metrics for their \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/api/metrics_istio\u0026#34;\u0026gt;free Google Cloud Istio or Anthos Service Mesh equivalent\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Exposing these metrics using PromQL means that developers who are familiar with Prometheus can start using all time series telemetry data without first having to learn a new query language. New members of your operations team can ramp up faster, as many industry hires will already be familiar with PromQL from previous experience.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Why Managed Service for Prometheus\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;In addition to PromQL for all metrics, Managed Service for Prometheus offers open-source monitoring combined with the scale and reliability of Google services. Additional benefits include:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gke\u0026#34;\u0026gt;Hybrid- and multi-cloud support\u0026lt;/a\u0026gt;, so you can centralize all your metrics across clouds and on-prem deployments\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Two-year retention of all Prometheus metrics, included in the price\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cost-effective monitoring on a per-sample basis\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Easy \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources\u0026#34;\u0026gt;cost identification and attribution\u0026lt;/a\u0026gt; using Cloud Monitoring\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Your choice of collection, with \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\u0026#34;\u0026gt;managed collection\u0026lt;/a\u0026gt; for those who want a completely hands-off Prometheus experience and \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged\u0026#34;\u0026gt;self-deployed collection\u0026lt;/a\u0026gt; for those who want to keep using existing Prometheus configs\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;How to get started\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;You can query Cloud Monitoring metrics with PromQL by using the \u0026lt;a href=\u0026#34;https://console.cloud.google.com/monitoring/prometheus\u0026#34;\u0026gt;interactive query page in Cloud Console\u0026lt;/a\u0026gt; or Grafana. To learn how to write PromQL for Google Cloud metrics, see \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/promql\u0026#34;\u0026gt;Mapping Cloud Monitoring metric names to PromQL\u0026lt;/a\u0026gt;. To configure a Grafana data source that can read all your metrics in Cloud Monitoring, see \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/query\u0026#34;\u0026gt;Configure a query user interface\u0026lt;/a\u0026gt; in the Managed Service for Prometheus documentation.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To query Prometheus data alongside Cloud Monitoring, you have to first get Prometheus data into the system. For instructions on configuring Managed Service for Prometheus ingestion, see \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\u0026#34;\u0026gt;Get started with managed collection\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eBecause we built Managed Service for Prometheus on top of the \u003ca href=\"https://research.google/pubs/pub50652/\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://research.google\" track-metadata-module=\"post\"\u003esame planet-scale time series database\u003c/a\u003e as Cloud Monitoring, all your metrics are stored together and are queryable together. Metrics in Cloud Monitoring are automatically generated when you use Google Cloud services at no additional cost to you. View all your metrics in one place with the query language that developers already know and prefer, opening up possibilities such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eCorrelating spikes in traffic with Redis cache misses using \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing\" track-metadata-module=\"post\"\u003eCloud Load Balancing metrics\u003c/a\u003e and Prometheus’ \u003ca href=\"https://github.com/oliver006/redis_exporter\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eRedis exporter\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGraphing Cloud Logging’s \u003ca href=\"https://cloud.google.com/logging/docs/logs-based-metrics\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/logs-based-metrics\" track-metadata-module=\"post\"\u003elogs-based metrics\u003c/a\u003e alongside Prometheus metrics\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAlerting on your \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute\" track-metadata-module=\"post\"\u003eCompute Engine utilization\u003c/a\u003e or your \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub\" track-metadata-module=\"post\"\u003ePub/Sub backlog size\u003c/a\u003e using PromQL and \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/rules-managed\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/rules-managed\" track-metadata-module=\"post\"\u003eManaged Service for Prometheus’ rule evaluation\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSubstituting paid Istio metrics for their \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_istio\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/api/metrics_istio\" track-metadata-module=\"post\"\u003efree Google Cloud Istio or Anthos Service Mesh equivalent\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eExposing these metrics using PromQL means that developers who are familiar with Prometheus can start using all time series telemetry data without first having to learn a new query language. New members of your operations team can ramp up faster, as many industry hires will already be familiar with PromQL from previous experience.\u003c/p\u003e\u003ch3\u003eWhy Managed Service for Prometheus\u003c/h3\u003e\u003cp\u003eIn addition to PromQL for all metrics, Managed Service for Prometheus offers open-source monitoring combined with the scale and reliability of Google services. Additional benefits include: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gke\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gke\" track-metadata-module=\"post\"\u003eHybrid- and multi-cloud support\u003c/a\u003e, so you can centralize all your metrics across clouds and on-prem deployments\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTwo-year retention of all Prometheus metrics, included in the price\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCost-effective monitoring on a per-sample basis\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEasy \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources\" track-metadata-module=\"post\"\u003ecost identification and attribution\u003c/a\u003e using Cloud Monitoring\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eYour choice of collection, with \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\" track-metadata-module=\"post\"\u003emanaged collection\u003c/a\u003e for those who want a completely hands-off Prometheus experience and \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged\" track-metadata-module=\"post\"\u003eself-deployed collection\u003c/a\u003e for those who want to keep using existing Prometheus configs\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eHow to get started\u003c/h3\u003e\u003cp\u003eYou can query Cloud Monitoring metrics with PromQL by using the \u003ca href=\"https://console.cloud.google.com/monitoring/prometheus\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://console.cloud.google.com/monitoring/prometheus\" track-metadata-module=\"post\"\u003einteractive query page in Cloud Console\u003c/a\u003e or Grafana. To learn how to write PromQL for Google Cloud metrics, see \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/promql\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/promql\" track-metadata-module=\"post\"\u003eMapping Cloud Monitoring metric names to PromQL\u003c/a\u003e. To configure a Grafana data source that can read all your metrics in Cloud Monitoring, see \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/query\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/query\" track-metadata-module=\"post\"\u003eConfigure a query user interface\u003c/a\u003e in the Managed Service for Prometheus documentation.\u003c/p\u003e\u003cp\u003eTo query Prometheus data alongside Cloud Monitoring, you have to first get Prometheus data into the system. For instructions on configuring Managed Service for Prometheus ingestion, see \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\" track-metadata-module=\"post\"\u003eGet started with managed collection\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAccording to a recent CNCF survey, \u003ca href=\"https://www.cncf.io/blog/2022/03/08/cloud-native-observability-microsurvey-prometheus-leads-the-way-but-hurdles-remain-to-understanding-the-health-of-systems/#:~:text=According%20to%20the%20survey%20report,%2C%20and%20Fluentd%20with%2046%25.\" target=\"_blank\"\u003e86% of the cloud native community reports that they use Prometheus for observability\u003c/a\u003e. As Prometheus becomes more of a standard, an increasing number of developers are becoming fluent in \u003ca href=\"https://prometheus.io/docs/prometheus/latest/querying/basics/\" target=\"_blank\"\u003ePromQL\u003c/a\u003e, Prometheus’ built-in query language. While it is a powerful, flexible, and expressive query language, PromQL is typically only able to query Prometheus time series data. Other sources of telemetry, such as metrics offered by your Cloud provider or metrics generated from logs, remain isolated in separate products and might require developers to learn new query tools in order to access them.\u003c/p\u003e\u003ch3\u003eIntroducing PromQL for Google Cloud Monitoring metrics\u003c/h3\u003e\u003cp\u003ePrometheus metrics alone aren’t enough to get a single pane of glass view of your Cloud footprint. Cloud Monitoring provides \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp\"\u003eover 1,000 free metrics\u003c/a\u003e that let you monitor and alert on your usage of Google Cloud services, including metrics for Compute Engine, Kubernetes Engine, Load Balancing, BigQuery, Cloud Storage, Pub/Sub, and more. We’re excited to announce that you can now \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/promql\"\u003equery all Cloud Monitoring metrics using PromQL and Managed Service for Prometheus\u003c/a\u003e, including \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp\"\u003eGoogle Cloud system metrics\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_kubernetes\"\u003eKubernetes metrics\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/logging/docs/logs-based-metrics\"\u003elog-based metrics\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/monitoring/custom-metrics?hl=en\"\u003ecustom metrics\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"grafana system metrics.gif\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/grafana_system_metrics.gif\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eGoogle Cloud metrics appear within Grafana and can be queried using PromQL.\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eBecause we built Managed Service for Prometheus on top of the \u003ca href=\"https://research.google/pubs/pub50652/\" target=\"_blank\"\u003esame planet-scale time series database\u003c/a\u003e as Cloud Monitoring, all your metrics are stored together and are queryable together. Metrics in Cloud Monitoring are automatically generated when you use Google Cloud services at no additional cost to you. View all your metrics in one place with the query language that developers already know and prefer, opening up possibilities such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eCorrelating spikes in traffic with Redis cache misses using \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing\"\u003eCloud Load Balancing metrics\u003c/a\u003e and Prometheus’ \u003ca href=\"https://github.com/oliver006/redis_exporter\" target=\"_blank\"\u003eRedis exporter\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGraphing Cloud Logging’s \u003ca href=\"https://cloud.google.com/logging/docs/logs-based-metrics\"\u003elogs-based metrics\u003c/a\u003e alongside Prometheus metrics\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAlerting on your \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-compute\"\u003eCompute Engine utilization\u003c/a\u003e or your \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub\"\u003ePub/Sub backlog size\u003c/a\u003e using PromQL and \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/rules-managed\"\u003eManaged Service for Prometheus’ rule evaluation\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSubstituting paid Istio metrics for their \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_istio\"\u003efree Google Cloud Istio or Anthos Service Mesh equivalent\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eExposing these metrics using PromQL means that developers who are familiar with Prometheus can start using all time series telemetry data without first having to learn a new query language. New members of your operations team can ramp up faster, as many industry hires will already be familiar with PromQL from previous experience.\u003c/p\u003e\u003ch3\u003eWhy Managed Service for Prometheus\u003c/h3\u003e\u003cp\u003eIn addition to PromQL for all metrics, Managed Service for Prometheus offers open-source monitoring combined with the scale and reliability of Google services. Additional benefits include: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gke\"\u003eHybrid- and multi-cloud support\u003c/a\u003e, so you can centralize all your metrics across clouds and on-prem deployments\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTwo-year retention of all Prometheus metrics, included in the price\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCost-effective monitoring on a per-sample basis\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEasy \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources\"\u003ecost identification and attribution\u003c/a\u003e using Cloud Monitoring\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eYour choice of collection, with \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\"\u003emanaged collection\u003c/a\u003e for those who want a completely hands-off Prometheus experience and \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged\"\u003eself-deployed collection\u003c/a\u003e for those who want to keep using existing Prometheus configs\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eHow to get started\u003c/h3\u003e\u003cp\u003eYou can query Cloud Monitoring metrics with PromQL by using the \u003ca href=\"https://console.cloud.google.com/monitoring/prometheus\"\u003einteractive query page in Cloud Console\u003c/a\u003e or Grafana. To learn how to write PromQL for Google Cloud metrics, see \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/promql\"\u003eMapping Cloud Monitoring metric names to PromQL\u003c/a\u003e. To configure a Grafana data source that can read all your metrics in Cloud Monitoring, see \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/query\"\u003eConfigure a query user interface\u003c/a\u003e in the Managed Service for Prometheus documentation.\u003c/p\u003e\u003cp\u003eTo query Prometheus data alongside Cloud Monitoring, you have to first get Prometheus data into the system. For instructions on configuring Managed Service for Prometheus ingestion, see \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\"\u003eGet started with managed collection\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGoogle Cloud Managed Service for Prometheus is now generally available\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnnouncing the GA of Google Cloud Managed Service for Prometheus for the collection, storage, and querying of Kubernetes metrics.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-2200x2200.jpg",
      "date_published": "2022-06-30T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eLee Yanco\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/databases/diagnose-query-performance-issues-with-cloud-spanner-query-insights/",
      "title": "Introducing Query Insights for Cloud Spanner: troubleshoot performance issues with pre-built dashboards",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c77=\"\"\u003e\u003cdiv _ngcontent-c77=\"\" innerhtml=\"\u0026lt;p\u0026gt;Today, application development teams are more agile and are shipping features faster than ever before. In addition to these rapid development cycles and the rise of microservices architectures, the end-to-end ownership of feature development (and performance monitoring) has moved to a shared responsibility model between advanced database administrators and full-stack developers. However, most developers don\u0026amp;#8217;t have the years of experience or the time needed to debug complex query performance issues and database administrators are now a scarce resource in most organizations. As a result, there is a dire need for tools for developers and DBAs alike to quickly diagnose performance issues.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Introducing Query Insights for Spanner\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We are delighted to announce the launch of \u0026lt;a href=\u0026#34;https://cloud.google.com/spanner/docs/using-query-insights\u0026#34;\u0026gt;Query Insights\u0026lt;/a\u0026gt; for Spanner,\u0026amp;#160; a set of visualization tools that provide an easy way for developers and database administrators to quickly diagnose query performance issues on Spanner. Using Query Insights, users can now troubleshoot query performance in a self-serve way. We\u0026amp;#8217;ve designed Query Insights using familiar design patterns with world-class visualizations to provide an intuitive experience for anyone who is debugging issues with query performance on Spanner. Query Insights is available at no additional cost.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;By using out-of-the-box visual dashboards and graphs, developers can visualize aberrant behavior like peaks and troughs in various performance metrics over a time-series and quickly identify problematic queries. Time series data provides significant value to organizations because it enables them to analyze important real-time and historical metrics. Data is valuable only if it\u0026amp;#8217;s easy to comprehend;. that\u0026amp;#8217;s where being able to view intuitive dashboards becomes a force multiplier for organizations looking to expose their time series data across teams.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Follow a visual journey with pre-built dashboards\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;With Query Insights, developers can seamlessly move from detection of database performance issues to diagnosis of problematic queries using a single interface. Query Insights will help identify query performance issues easily with pre-built dashboards.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The user could do this by following a simple journey where they can quickly \u0026lt;b\u0026gt;confirm\u0026lt;/b\u0026gt;, \u0026lt;b\u0026gt;identify\u0026lt;/b\u0026gt; and \u0026lt;b\u0026gt;analyze\u0026lt;/b\u0026gt; query performance issues. Let\u0026amp;#8217;s walk through an example scenario.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Understand database performance\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;This journey will start by the user \u0026lt;a href=\u0026#34;https://cloud.google.com/spanner/docs/monitoring-cloud#create-alert\u0026#34;\u0026gt;setting up an alert\u0026lt;/a\u0026gt; on Google Cloud Monitoring for CPU utilization going above a certain threshold. The alert could be configured in a way that if this threshold is crossed, the user will be notified with an email alert, with a link to the \u0026amp;#8220;Monitoring\u0026amp;#8221; dashboard.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Once the user receives this alert, they would click on the link in the email, and navigate to the \u0026amp;#8220;Monitoring\u0026amp;#8221; dashboard. If they observe high CPU Utilization and high read latencies, the possible root cause could be expensive queries. A spike in CPU Utilization could be a strong signal that the system is using more compute than it usually would, due to an inefficient query.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The next step is to identify which query might be the problem, this is where Query Insights comes in. The user can get to this tool by clicking on Query Insights in the left navigation of your Spanner Instance. Here, they can drill down into the CPU usage by query and observe that for a specific database, CPU Utilization (attributed to all queries) is spiking for a particular time window. This confirms that the CPU utilization is due to inefficient queries.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eToday, application development teams are more agile and are shipping features faster than ever before. In addition to these rapid development cycles and the rise of microservices architectures, the end-to-end ownership of feature development (and performance monitoring) has moved to a shared responsibility model between advanced database administrators and full-stack developers. However, most developers don’t have the years of experience or the time needed to debug complex query performance issues and database administrators are now a scarce resource in most organizations. As a result, there is a dire need for tools for developers and DBAs alike to quickly diagnose performance issues. \u003c/p\u003e\u003ch3\u003eIntroducing Query Insights for Spanner\u003c/h3\u003e\u003cp\u003eWe are delighted to announce the launch of \u003ca href=\"https://cloud.google.com/spanner/docs/using-query-insights\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/spanner/docs/using-query-insights\" track-metadata-module=\"post\"\u003eQuery Insights\u003c/a\u003e for Spanner,  a set of visualization tools that provide an easy way for developers and database administrators to quickly diagnose query performance issues on Spanner. Using Query Insights, users can now troubleshoot query performance in a self-serve way. We’ve designed Query Insights using familiar design patterns with world-class visualizations to provide an intuitive experience for anyone who is debugging issues with query performance on Spanner. Query Insights is available at no additional cost.\u003c/p\u003e\u003cp\u003eBy using out-of-the-box visual dashboards and graphs, developers can visualize aberrant behavior like peaks and troughs in various performance metrics over a time-series and quickly identify problematic queries. Time series data provides significant value to organizations because it enables them to analyze important real-time and historical metrics. Data is valuable only if it’s easy to comprehend;. that’s where being able to view intuitive dashboards becomes a force multiplier for organizations looking to expose their time series data across teams.\u003c/p\u003e\u003ch3\u003eFollow a visual journey with pre-built dashboards\u003c/h3\u003e\u003cp\u003eWith Query Insights, developers can seamlessly move from detection of database performance issues to diagnosis of problematic queries using a single interface. Query Insights will help identify query performance issues easily with pre-built dashboards. \u003c/p\u003e\u003cp\u003eThe user could do this by following a simple journey where they can quickly \u003cb\u003econfirm\u003c/b\u003e, \u003cb\u003eidentify\u003c/b\u003e and \u003cb\u003eanalyze\u003c/b\u003e query performance issues. Let’s walk through an example scenario. \u003c/p\u003e\u003ch3\u003eUnderstand database performance\u003c/h3\u003e\u003cp\u003eThis journey will start by the user \u003ca href=\"https://cloud.google.com/spanner/docs/monitoring-cloud#create-alert\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/spanner/docs/monitoring-cloud#create-alert\" track-metadata-module=\"post\"\u003esetting up an alert\u003c/a\u003e on Google Cloud Monitoring for CPU utilization going above a certain threshold. The alert could be configured in a way that if this threshold is crossed, the user will be notified with an email alert, with a link to the “Monitoring” dashboard.\u003c/p\u003e\u003cp\u003eOnce the user receives this alert, they would click on the link in the email, and navigate to the “Monitoring” dashboard. If they observe high CPU Utilization and high read latencies, the possible root cause could be expensive queries. A spike in CPU Utilization could be a strong signal that the system is using more compute than it usually would, due to an inefficient query.\u003c/p\u003e\u003cp\u003eThe next step is to identify which query might be the problem, this is where Query Insights comes in. The user can get to this tool by clicking on Query Insights in the left navigation of your Spanner Instance. Here, they can drill down into the CPU usage by query and observe that for a specific database, CPU Utilization (attributed to all queries) is spiking for a particular time window. This confirms that the CPU utilization is due to inefficient queries.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eToday, application development teams are more agile and are shipping features faster than ever before. In addition to these rapid development cycles and the rise of microservices architectures, the end-to-end ownership of feature development (and performance monitoring) has moved to a shared responsibility model between advanced database administrators and full-stack developers. However, most developers don’t have the years of experience or the time needed to debug complex query performance issues and database administrators are now a scarce resource in most organizations. As a result, there is a dire need for tools for developers and DBAs alike to quickly diagnose performance issues. \u003c/p\u003e\u003ch3\u003eIntroducing Query Insights for Spanner\u003c/h3\u003e\u003cp\u003eWe are delighted to announce the launch of \u003ca href=\"https://cloud.google.com/spanner/docs/using-query-insights\"\u003eQuery Insights\u003c/a\u003e for Spanner,  a set of visualization tools that provide an easy way for developers and database administrators to quickly diagnose query performance issues on Spanner. Using Query Insights, users can now troubleshoot query performance in a self-serve way. We’ve designed Query Insights using familiar design patterns with world-class visualizations to provide an intuitive experience for anyone who is debugging issues with query performance on Spanner. Query Insights is available at no additional cost.\u003c/p\u003e\u003cp\u003eBy using out-of-the-box visual dashboards and graphs, developers can visualize aberrant behavior like peaks and troughs in various performance metrics over a time-series and quickly identify problematic queries. Time series data provides significant value to organizations because it enables them to analyze important real-time and historical metrics. Data is valuable only if it’s easy to comprehend;. that’s where being able to view intuitive dashboards becomes a force multiplier for organizations looking to expose their time series data across teams.\u003c/p\u003e\u003ch3\u003eFollow a visual journey with pre-built dashboards\u003c/h3\u003e\u003cp\u003eWith Query Insights, developers can seamlessly move from detection of database performance issues to diagnosis of problematic queries using a single interface. Query Insights will help identify query performance issues easily with pre-built dashboards. \u003c/p\u003e\u003cp\u003eThe user could do this by following a simple journey where they can quickly \u003cb\u003econfirm\u003c/b\u003e, \u003cb\u003eidentify\u003c/b\u003e and \u003cb\u003eanalyze\u003c/b\u003e query performance issues. Let’s walk through an example scenario. \u003c/p\u003e\u003ch3\u003eUnderstand database performance\u003c/h3\u003e\u003cp\u003eThis journey will start by the user \u003ca href=\"https://cloud.google.com/spanner/docs/monitoring-cloud#create-alert\"\u003esetting up an alert\u003c/a\u003e on Google Cloud Monitoring for CPU utilization going above a certain threshold. The alert could be configured in a way that if this threshold is crossed, the user will be notified with an email alert, with a link to the “Monitoring” dashboard.\u003c/p\u003e\u003cp\u003eOnce the user receives this alert, they would click on the link in the email, and navigate to the “Monitoring” dashboard. If they observe high CPU Utilization and high read latencies, the possible root cause could be expensive queries. A spike in CPU Utilization could be a strong signal that the system is using more compute than it usually would, due to an inefficient query.\u003c/p\u003e\u003cp\u003eThe next step is to identify which query might be the problem, this is where Query Insights comes in. The user can get to this tool by clicking on Query Insights in the left navigation of your Spanner Instance. Here, they can drill down into the CPU usage by query and observe that for a specific database, CPU Utilization (attributed to all queries) is spiking for a particular time window. This confirms that the CPU utilization is due to inefficient queries.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Insights.1000063020000552.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Query Insights.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Query_Insights.1000063020000552.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eIdentifying a problematic query\u003c/h3\u003e\u003cp\u003eThe user now observes the \u003ca href=\"https://cloud.google.com/spanner/docs/introspection/query-statistics#cpu-by-query\"\u003eTopN\u003c/a\u003e (Top queries by CPU Utilization) query graph to see the TopN queries by CPU Utilization. From the graph, it is very easy to visualize and identify the top queries which could be causing the spike in CPU Utilization.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Insights.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Query Insights.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Query_Insights.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn the above screenshot, we can see that the first query in the table is showing a clear spike at 10:33 PM consuming 48.81% of total CPU. This is  a clear indication that this query could be problematic, and the user should investigate further.\u003c/p\u003e\u003ch3\u003eAnalyzing the query performance\u003c/h3\u003e\u003cp\u003eOnce they have identified the problematic query, they can now drill down into this query shape to confirm, identify the root cause of the high CPU utilization. \u003c/p\u003e\u003cp\u003eThey can do this by clicking on the Fingerprint ID for the specific query from the topN table, and navigating to the Query Details page where they will be able to see a list of metrics (Latency, CPU Utilization, Execution count, Rows Scanned / Rows Returned) over a time series for that specific query.  \u003c/p\u003e\u003cp\u003eIn this example, we notice that the average number of rows scanned for this specific query are very high (~ 600k rows scanned to return ~ 12k rows), which could point to a poor query design, resulting in an inefficient query. We can also observe that latency is high (1.4s) for this query.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Query_Insights.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"3 Query Insights.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Query_Insights.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eFixing the issue\u003c/h3\u003e\u003cp\u003eTo fix the problem in this scenario, the user could optimize this query by \u003ca href=\"https://cloud.google.com/spanner/docs/secondary-indexes#index-directive\"\u003especifying a secondary index\u003c/a\u003e in the query using a FORCE_INDEX query hint to provide an index directive. This would provide more consistent performance, make the query more efficient, and lower CPU utilization for this query.\u003c/p\u003e\u003cp\u003eIn the screenshot below, you can see that after specifying the index in the query, the query performance dramatically increases in terms of CPU, rows scanned (54K vs 630k) and also in terms of query latency (536 ns vs 1.4 s).\u003c/p\u003e\u003cp\u003eUnoptimized Query:\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Query_Insights.1000064320000620.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"4 Query Insights.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Query_Insights.1000064320000620.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOptimized Query:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Query_Insights.1000064120000632.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"5 Query Insights.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Query_Insights.1000064120000632.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eBy following this simple visual journey, the user can easily detect, diagnose and debug inefficient queries on Spanner.\u003c/p\u003e\u003ch3\u003eGet started with Query Insights today\u003c/h3\u003e\u003cp\u003eTo learn more about Query Insights, review the documentation \u003ca href=\"https://cloud.google.com/spanner/docs/using-query-insights\"\u003ehere\u003c/a\u003e. Query Insights is enabled by default. In the Spanner console, you can click on Query Insights in the left navigation and start visualizing your query performance metrics! \u003c/p\u003e\u003cp\u003eNew to Spanner? Get started in minutes \u003ca href=\"https://cloud.google.com/spanner\"\u003ewith a new database\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/databases/seeing-into-the-performance-of-cloud-native-database-spanner/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_spanner.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eImproved troubleshooting with Cloud Spanner introspection capabilities\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eCloud-native database Spanner has new introspection capabilities to monitor database performance and optimize application efficiency.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-06-29T15:00:00Z",
      "author": {
        "name": "\u003cname\u003eMohit Gulati\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/ensuring-consistent-api-quota-limits-between-dev-and-prod/",
      "title": "Incorporating quota regression detection into your release pipeline",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOn Google Cloud, one of the ways an organization may want to enforce fairness in how much of a resource can be consumed is through the use of \u003ca href=\"https://cloud.google.com/docs/quota\"\u003equotas\u003c/a\u003e. Limiting resource consumption on services is one way that companies can better manage their cloud costs. Oftentimes, people associate quotas with APIs to access that said resource. Although an endpoint may be able to handle a high number of Queries Per Second (QPS), the quota gives them a means to ensure that no one user or customer has monopoly over the available capacity. This is where fairness comes into play. It allows people to put limits that can be scoped per user or per customer and allows them to increase or lower those limits.\u003c/p\u003e\u003cp\u003eAlthough quota limits address the issue of fairness from a resource providers’ point of view — in this case, Google Cloud — you still need a way as the resource \u003ci\u003econsumer\u003c/i\u003e to ensure that those limits are adhered to and, just as importantly, ensure that you don’t inadvertently violate those limits. This is especially important in a continuous integration and continuous delivery (CI/CD) environment, where there is so much automation going on. CI/CD is heavily based on automating product releases and you want to ensure that the products released are always stable. This brings us to the issue of quota regression.\u003c/p\u003e\u003ch3\u003eWhat is quota regression and how can it occur? \u003c/h3\u003e\u003cp\u003eQuota regression refers to the unplanned change in an allocated quota that oftentimes results in a reduced capacity for resource consumption. \u003c/p\u003e\u003cp\u003eLet's take for example an accountant firm. I have many friends in this sector and they can never hang out with me during their busy season between January and April. At least, that’s the excuse. During the busy season, they have an extraordinarily high caseload, and a low caseload the rest of the year. Let’s assume that these caseloads actually have an immediate impact on your resource costs on Google Cloud. Since this high caseload only occurs at a particular point throughout the year, it may not be necessary to maintain a high quota at all times. It’s not financially prudent since resources are paid on a “per-usage” model. \u003c/p\u003e\u003cp\u003eIf the accountant firm has an in-house engineering team that has built load-tests to ensure the system is functioning as intended, you would expect the load capacity to increase before the busy season. If the load test is being done in an environment separate from the serving one (which it should be due to reasons such as security and avoiding unnecessary access grants to data), this is where you might start to see a quota regression. An example of this is load testing in your non-prod Google Cloud project (e.g.\u003ci\u003eyour-project-name-nonprod\u003c/i\u003e) and promoting images to your serving project (e.g.\u003ci\u003eyour-project-name-prod\u003c/i\u003e).\u003c/p\u003e\u003cp\u003eIn order for the load tests to pass, there must be a sufficient quota allocated to the \u003ci\u003eload testing environment\u003c/i\u003e. However, there exists a possibility that that quota has not been granted in the \u003ci\u003eserving environment\u003c/i\u003e. It could be due to simply an oversight in the process where the admin needed to request the additional quota in the serving environment, or it could be because that quota was reverted after a busy season and thus went unnoticed. Whatever the reason, it still depends on human intervention to assert that the quotas are consistent across environments. If this is missed, the firm can go into a busy season with passing load tests and still have a system outage due to lack of quota in the serving environment.\u003c/p\u003e\u003ch3\u003eWhy not just use traditional monitoring?\u003c/h3\u003e\u003cp\u003eThis brings to mind the argument of “\u003ca href=\"https://www.youtube.com/watch?v=x8FNVsbnwWE\" target=\"_blank\"\u003eSecurity Monitor vs Security Guard\u003c/a\u003e.” Even with monitoring to detect such inconsistencies, alerts can be ignored and alerts can be late. Alerts work if there is no automation tied to the behavior. In the example above, alerts may just suffice. However, in the context of CI/CD, it’s likely for a deployment that introduces a higher QPS on dependencies to be promoted from a lower environment to the serving environment, because the load tests pass if the lower environment has sufficient quota. The problem here is that now that deployment is automatically pushed to production with alerts probably occurring with the outage. \u003c/p\u003e\u003cp\u003eThe best way to handle these scenarios is to incorporate an automated way of not just monitoring and alerting, but a means for preventing promotion of that regressive behavior to the serving environment. The last thing you want is new logic that requires a higher resource quota than what is granted being automatically promoted to prod.\u003c/p\u003e\u003cp\u003eWhy not use existing checks in tests? The software engineering discipline offers several types of tests (unit, integration, performance, load, smoke, etc…), none of which address something as complex as cross-environment consistency. Most of them focus on the user and expected behaviors. The only test that really focuses on infrastructure is the load test, but a quota regression is not necessarily part of the load test. It's not something you're going to detect since a load test occurs in its own environment and is agnostic of where it's actually running. \u003c/p\u003e\u003cp\u003eIn other words, a quota regression test needs to be aware of the environments — it needs an expected baseline environment where the load test occurs and an actual serving environment where the product will be deployed. What I am proposing is an environment aware test to be included in the suite of many other tests.\u003c/p\u003e\u003ch3\u003eQuota regression testing on Google Cloud\u003c/h3\u003e\u003cp\u003eGoogle Cloud already provides services that you can use to easily incorporate this feature. This is more of a systems architecture practice that you can exercise. \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/service-infrastructure/docs/service-consumer-management/reference/rest\"\u003eService Consumer Management API\u003c/a\u003e provides the tools you need to create your own quota regression test. Take for example the \u003ca href=\"https://cloud.google.com/service-infrastructure/docs/service-consumer-management/reference/rest/v1beta1/services.consumerQuotaMetrics.limits#ConsumerQuotaLimit\"\u003eConsumerQuotaLimit\u003c/a\u003e Resource that’s returned via the \u003ca href=\"https://cloud.google.com/service-infrastructure/docs/service-consumer-management/reference/rest/v1beta1/services.consumerQuotaMetrics/list\"\u003elist api\u003c/a\u003e. For the remainder of this discussion, let’s assume an environment setup such as this:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/extremely_simple_deployme.1000065520001058.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"extremely simple deployment pipeline.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/extremely_simple_deployme.1000065520001058.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eDiagram demonstrating an extremely simple deployment pipeline for a resource provider.\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn the diagram above, we have a simplified deployment pipeline:\u003cbr/\u003e\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eDevelopers submit code to some repository\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe Cloud Build build and deployment trigger gets fired\u003c/p\u003e\u003c/li\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eTests are run\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDeployment images are pushed if the prerequisite steps succeed\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cli\u003e\u003cp\u003eImages are pushed to their respective environments (in this case build to dev, and previous dev to prod)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eQuotas\u003c/b\u003e are defined for the endpoints on deployment\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Load Balancer makes the endpoints available to end users\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003cb\u003eQuota limits\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWith this mental model, let’s hone in on the role quotas play in the big picture. Let’s assume we have the following service definition for an endpoint called “\u003ci\u003eFooService\u003c/i\u003e”. The service name, metric label and quota limit value are what we care about for this example.\u003cbr/\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003egRPC Cloud Endpoint Yaml Example\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'type: google.api.Service\\r\\nconfig_version: 3\\r\\nname: fooservice.endpoints.my-project-id.cloud.goog\\r\\ntitle: Foo Service gRPC Cloud Endpoints\\r\\napis:\\r\\n - name: com.foos.demo.proto.v1.FooService\\r\\nusage:\\r\\n rules:\\r\\n # ListFoos methods can be called without an API Key.\\r\\n - selector: com.foos.demo.proto.v1.FooService.ListFoos\\r\\n allow_unregistered_calls: true\\r\\n # GetFoo methods can be called without an API Key.\\r\\n - selector: com.foos.demo.proto.v1.FooService.GetFoo\\r\\n allow_unregistered_calls: true\\r\\n # UpdateFoo methods can be called without an API Key.\\r\\n - selector: com.foos.demo.proto.v1.FooService.UpdateFoo\\r\\n allow_unregistered_calls: true\\r\\nmetrics:\\r\\n - name: library.googleapis.com/read_calls\\r\\n display_name: \"Read Quota\"\\r\\n value_type: INT64\\r\\n metric_kind: DELTA\\r\\n - name: library.googleapis.com/write_calls\\r\\n display_name: \"Write Quota\"\\r\\n value_type: INT64\\r\\n metric_kind: DELTA\\r\\nquota:\\r\\n limits:\\r\\n - name: \"apiReadQpmPerProject\"\\r\\n metric: library.googleapis.com/read_calls\\r\\n unit: \"1/min/{project}\"\\r\\n values:\\r\\n STANDARD: 1\\r\\n - name: \"apiWriteQpmPerProject\"\\r\\n metric: library.googleapis.com/write_calls\\r\\n unit: \"1/min/{project}\"\\r\\n values:\\r\\n STANDARD: 1\\r\\n # By default, all calls are measured with a cost of 1:1 for QPM.\\r\\n # See https://github.com/googleapis/googleapis/blob/master/google/api/quota.proto\\r\\n metric_rules:\\r\\n - selector: \"*\"\\r\\n metric_costs:\\r\\n library.googleapis.com/read_calls: 1\\r\\n - selector: com.foos.demo.proto.v1.FooService.UpdateFoo\\r\\n metric_costs:\\r\\n library.googleapis.com/write_calls: 2'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e6e633e6750\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn our definition we’ve established:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eService Name: \u003ccode\u003efooservice.endpoints.my-project-id.cloud.goog\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eMetric Label: \u003ccode\u003elibrary.googleapis.com/read_calls\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eQuota Limit: \u003ccode\u003e1\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003eWith these elements defined, we’ve now restricted read calls to exactly one per minute for the \u003cb\u003eservice\u003c/b\u003e. Given a project number, (e.g., 123456789) we can now issue a call to the Consumer Quota Metrics Service to \u003ca href=\"https://cloud.google.com/service-usage/docs/manage-quota?hl=en_US#displaying_service_quota\"\u003edisplay the service quota\u003c/a\u003e.\u003cp\u003eExample commands and output.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'$ alias gcurl=\\'curl -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\"\\'\\r\\n$ gcurl https://serviceconsumermanagement.googleapis.com/v1beta1/services/fooservice.endpoints.my-project-id.cloud.goog/projects/my-project-id/consumerQuotaMetrics'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e6e4d888910\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eResponse example (truncated)\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'{\\r\\n \"metrics\": [\\r\\n {\\r\\n \"name\": \"services/fooservice.endpoints.my-project-id.cloud.goog/projects/123456789/consumerQuotaMetrics/library.googleapis.com%2Fread_calls\",\\r\\n \"displayName\": \"Read Quota\",\\r\\n \"consumerQuotaLimits\": [\\r\\n {\\r\\n \"name\": \"services/fooservice.endpoints.my-project-id.cloud.goog/projects/123456789/consumerQuotaMetrics/library.googleapis.com%2Fread_calls/limits/%2Fmin%2Fproject\",\\r\\n \"unit\": \"1/min/{project}\",\\r\\n \"metric\": \"library.googleapis.com/read_calls\",\\r\\n \"quotaBuckets\": [\\r\\n {\\r\\n \"effectiveLimit\": \"1\",\\r\\n \"defaultLimit\": \"1\"\\r\\n }\\r\\n ]\\r\\n }\\r\\n ],\\r\\n \"metric\": \"library.googleapis.com/read_calls\"\\r\\n }\\r\\n \\u2026'), (u'language', u''), (u'caption', \u0026lt;wagtail.wagtailcore.rich_text.RichText object at 0x3e6e621b8150\u0026gt;)])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn the above response, the most important thing to note is the effective limit for a given service’s metric. The \u003ca href=\"https://cloud.google.com/service-infrastructure/docs/service-consumer-management/reference/rest/v1beta1/services.consumerQuotaMetrics.limits#quotabucket\"\u003eeffective limit\u003c/a\u003e is the limit being applied to a resource consumer when enforcing customer fairness as discussed earlier.\u003c/p\u003e\u003cp\u003eNow that we’ve established how to get the \u003ci\u003eeffectiveLimit\u003c/i\u003e for a quota definition on a resource per project, we can define the assertion of quota consistency as: \u003c/p\u003e\u003cp\u003e\u003ci\u003eLoad Test Environment Quota Effective Limit \u0026lt;= Serving Environment Quota Effective Limit \u003c/i\u003e\u003c/p\u003e\u003cp\u003eHaving a test like this, you can then integrate that with something like Cloud Build to block the promotion of your image from the lower environment to your serving environment if that test fails to pass. That saves you from introducing regressive behavior from the new image into the serving environment that would otherwise result in an outage. \u003c/p\u003e\u003ch3\u003eThe importance of early detection\u003c/h3\u003e\u003cp\u003eIt’s not enough to alert on a detected quota regression and block the image promotion to prod. It’s better to raise alarms as soon as possible. If resources are lacking when it’s time to promote to production, you’re now faced with the problem of wrangling enough resources in time. This may not always be possible in the desired timeline; it’s possible that the resource provider needs to scale up its resources to handle the increase in quota. This is not always something that can just be done in a day. For example, is the service hosted on \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE)? Even with autoscale, what if the \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler#limitations\"\u003eip pool is exhausted\u003c/a\u003e? Cloud infrastructure changes, although elastic, are not instant. Part of production planning needs to account for the time needed to scale.\u003c/p\u003e\u003cp\u003eIn summary, quota regression testing is a key component that should be added to the entire concept of handling overload and dealing with load balancing in any cloud service — not just Google Cloud. It is important for product stability with the dips and spikes in demands, which will inevitably show up as a problem in many spaces. If you continue to rely on human intervention to ensure consistency of your quota across your configurations, you will only guarantee that eventually, you will have an outage when that consistency is not met. For more on working with quotas, \u003ca href=\"https://cloud.google.com/docs/quota\"\u003echeck out the documentation\u003c/a\u003e. \u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/application-development/5-principles-for-cloud-native-architecture-what-it-is-and-how-to-master-it/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e5 principles for cloud-native architecture—what it is and how to master it\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eLearn to maximize your use of Google Cloud by adopting a cloud-native architecture.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-06-28T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eNethaneel Edwards\u003c/name\u003e\u003ctitle\u003eSenior Software Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/compute/google-cloud-spot-vm-use-cases-and-best-practices/",
      "title": "Top 5 use cases for Google Cloud Spot VMs explained + best practices",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eCloud was built on the premise of flexible infrastructure that grows and shrinks with your application demands. Applications that can take advantage of this elastic infrastructure and scale horizontally with the demands of your application offer significant advantages over competitors by allowing infrastructure costs to scale up and down along with the demand. \u003c/p\u003e\u003cp\u003eGoogle Cloud’s Spot VMs enable our customers to make the most of our idle capacity where and when it is available. Spot VMs are offered at a \u003ca href=\"https://cloud.google.com/spot-vms\"\u003esignificant discount\u003c/a\u003e from list price to drive maximum savings provided customers have flexible, stateless workloads that can handle preemption. Spot VMs can be reclaimed by Google (with a 30 second notice). When you deploy the right workloads on Spot VMs, you are able to maintain elasticity while also taking advantage of the best discounts Google has to offer.\u003c/p\u003e\u003cp\u003eThis blog discusses a few common use cases and design patterns we have seen customers utilize Spot VMs for and discusses the best practices for these use cases. While this is not an exhaustive list, this blog serves as a template to help customers make the most of the Spot VM savings while still reaching their application and workload objectives. \u003c/p\u003e\u003ch3\u003eMedia rendering\u003c/h3\u003e\u003cp\u003eRendering workloads (such as rendering 2D or 3D elements) can be both compute and time intensive, requiring skilled IT resources to manage render farms. Job management becomes even more difficult when the render farm is at 100% utilization. Spot VMs are ideal resources for fault-tolerant rendering workloads; when combined with a \u003ca href=\"https://cloud.google.com/architecture/building-a-hybrid-render-farm#managing_queues\"\u003equeuing system\u003c/a\u003e customers can integrate the preemption notice to track preempted jobs. This allows you to build a render farm which benefits from reduced TCO. If your renderer supports \u003ca href=\"https://cloud.google.com/architecture/building-a-hybrid-render-farm#choosing_between_standard_and_preemptible_vms\"\u003etaking snapshots\u003c/a\u003e of in-progress renders at specified intervals, writing these snapshots to a persistent data store (\u003ca href=\"https://cloud.google.com/storage\"\u003eCloud Storage\u003c/a\u003e) will limit any loss in work in the event the Spot VM is preempted. As subsequent Spot VMs are created, they can pick up where the old ones left off by using the snapshots on Cloud Storage. You can also leverage the new “\u003ca href=\"https://cloud.google.com/compute/docs/instances/suspend-resume-instance\"\u003esuspend and resume a VM\u003c/a\u003e” feature which allows you to keep the VM instances during the preemption event but not incur any charges for it while the VM is not in use.\u003c/p\u003e\u003cp\u003eAdditionally, we have helped customers combine local render farms in their existing datacenters with cloud-based render farms, allowing a \u003ca href=\"https://cloud.google.com/architecture/building-a-hybrid-render-farm\"\u003ehybrid approach\u003c/a\u003e for large or numerous render workloads without increasing their investment in their physical datacenters. Not only does this reduce their capital expenses, but it adds flexible scalability to the existing farm and provides a better experience for their business partners. \u003c/p\u003e\u003ch3\u003eFinancial modeling\u003c/h3\u003e\u003cp\u003eCapital market firms have significant investments in their infrastructure to create state-of-the-art, world-class compute grids. Since compute grids began, in-house researchers leverage these large grids in physical datacenters to test their trading hypotheses and perform backtesting. But as the business grows, what happens when all the researchers each have a brilliant idea and want to test that out at the same time? Researchers then have to compete with one another for the same limited resources, which leads to queueing their jobs and increased lead times for testing their ideas. And in financial markets, time is always scarce. Enter cloud computing and Spot VMs. Capital market firms can use Google Cloud as an extension of their on-premises grid by spinning up temporary compute resources. Or they can go all in on cloud and build their grid in Google Cloud entirely. In either scenario, Spot VMs are ideal candidates for bursting research workloads given the transient nature of the workload and heavily discounted prices of VMs. This enables researchers to test more hypotheses at a lower cost per test, in turn producing better models for firms. Google Cloud Spot VM discounts not only apply to the VMs themselves, but also to any \u003ca href=\"http://cloud.google.com/gpu\"\u003eGPU accelerator\u003c/a\u003e attached to them, providing even more processing power to a firm looking to process larger more complex models. Once these jobs have completed, Spot VMs can be quickly spun down, maintaining strict control on costs. \u003c/p\u003e\u003ch3\u003eCI/CD pipelines\u003c/h3\u003e\u003cp\u003eContinuous integration (CI) and Continuous delivery (CD) tools are very common for the modern application developer. These tools allow developers to create a testing pipeline that enables developers and quality engineers to ensure the newly created code works with their environment and that the deployment process does not break anything during deployment. CI/CD tools and test environments are great workloads to run on Spot VMs since CI/CD pipelines are not mission-critical for most companies — a delay in deployment or testing by 15 minutes, or even a few hours, is not material to their business. This means that companies can lower the cost of operating their CI/CD pipeline significantly through the use of Spot VMs. \u003c/p\u003e\u003cp\u003eA simple example of this would be to install the Jenkins Master Server in a \u003ca href=\"https://cloud.google.com/compute/docs/instance-groups\"\u003eManaged Instance Group\u003c/a\u003e (MIG) with the \u003ca href=\"https://cloud.google.com/compute/docs/instances/spot#spot-with-instance-groups\"\u003eVM type set to Spot\u003c/a\u003e. If the VM gets preempted, the CI/CD pipelines will stall until the MIG can find resources again to spin up a new VM. The first reaction may be concern that Jenkins persists data locally, which is problematic for Spot VMs. However, customers can move the Jenkins directory (/var/lib/Jenkins) to \u003ca href=\"https://cloud.google.com/filestore\"\u003eGoogle Cloud Filestore\u003c/a\u003e and preserve this data. Then when the new Spot VM spins up, it will reconnect to the directory. In the case of a large-scale Jenkins deployment, build VMs can utilize Spot VMs as part of a MIG to scale as necessary while ensuring that the builds can be maintained with on-demand VMs. This blended approach removes any risk to the builds, while still allowing customers to save up to 91% in costs of the additional VMs versus traditional on-demand VMs.\u003c/p\u003e\u003ch3\u003eWeb services and apps\u003c/h3\u003e\u003cp\u003eLarge online retailers have found ways to drive massive increases in order volume. Typically companies like this target a specific time each month, such as the last day of the month, through a unique promotion process. This means that they are in many cases creating a Black Friday/Cyber Monday-style event, each and every month! In order to support this, companies traditionally used a “Build it like a stadium for Super Bowl Sunday” model. The issue with that, and a reason most professional sports teams have practice facilities, is that it’s very expensive to keep all the lights, climate control, and ancillary equipment running for the sole purpose of practice. 29-30 days of a month most infrastructure sits idle, wasting HVAC, electricity, etc. However, using the elasticity of cloud, we could manage this capacity and turn it up only when necessary. But to drive even more optimization and savings, we turn to Spot VMs. \u003c/p\u003e\u003cp\u003eSpot VMs really shine during these kinds of scale-out events. Imagine the above scenario: what if behind a load balancer we could have:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eOne MIG to help scale the web frontends. This MIG will be sized with on-demand VMs to handle day-to-day traffic.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA second MIG for Spot VMs that scales up starting at 11:45pm the night prior to the end of month. The first and second MIG can now handle ~80-90% of the workload. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA third MIG of on-demand VMs that spins up as a workload bursts to handle any remaining traffic, should the Spot MIG not be able to find enough capacity, thus ensuring we’re meeting our SLAs as well as keeping costs as tight as possible. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eKubernetes\u003c/h3\u003e\u003cp\u003eNow you may say “Well that’s all well and good, but we’re a fully modernized container shop, using Google Kubernetes Engine (GKE).” You are in luck — Spot VMs are integrated with \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGKE\u003c/a\u003e, enabling you to quickly and easily save on your GKE workloads by using \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms\"\u003eSpot VMs\u003c/a\u003e with standard GKE clusters or \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/autopilot-spot-pods\"\u003eSpot Pods\u003c/a\u003e with your \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview\"\u003eAutopilot\u003c/a\u003eclusters. GKE supports \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms#termination-graceful-shutdown\"\u003egracefully shutting down\u003c/a\u003eSpot VMs, notifying your workloads that they will be shut down and giving them time to cleanly exit. GKE then automatically reschedules your deployments. With Spot Pods, you can use Kubernetes nodeSelectors and/or Node affinity to control the placement of spot workloads, striking the right balance between cost and availability across spot and on-demand compute.\u003c/p\u003e\u003ch3\u003eGeneral best practices\u003c/h3\u003e\u003cp\u003eTo \u003ca href=\"https://cloud.google.com/blog/products/compute/google-cloud-spot-vm\"\u003etake advantage of Spot VMs\u003c/a\u003e, your use case doesn’t have to be an exact match to any of those described above. If the workload is stateless, scalable, can be stopped and checkpointed in less than 30 seconds, or is location- and hardware-flexible, then they may be a good fit for Spot VMs.\u003c/p\u003e\u003cp\u003eThere are many several actions you can take to help ensure your Spot workloads run as smoothly as possible. Below we outline a few best practices you should consider:\u003c/p\u003e\u003cp\u003e1. Deploy Spot behind\u003ca href=\"https://cloud.google.com/compute/docs/instance-groups/regional-migs\"\u003eRegional Managed Instance Groups (RMIGs)\u003c/a\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eRMIGs are a great fit for Spot workloads given the RMIG’s ability to recreate instances which are preempted.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUsing your workload’s profile, determine the RMIG’s \u003ca href=\"https://cloud.google.com/compute/docs/instance-groups/regional-migs#target_distribution_shape\"\u003etarget distribution shape\u003c/a\u003e. For example, with a batch research workload, you might select an ANY target distribution shape. This will allow for Spot instances to be distributed in any manner across the various zones, thereby taking advantage of any underutilized resources. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eYou can use a mix of on-demand RMIGs and Spot RMIGs to maintain stateful applications while increasing availability in a cost effective manner.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e2. Ensure you have a \u003ca href=\"https://cloud.google.com/compute/docs/shutdownscript\"\u003eshutdown script\u003c/a\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eIn the event of Spot VM preemptions, use a shutdown script to enable checkpointing to Cloud Storage for your workloads as well as perform any graceful shutdown processes.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhen drafting your shutdown script, test it out on an instance by either \u003ca href=\"https://cloud.google.com/compute/docs/instances/spot#preemption-process\"\u003emanually stopping or deleting\u003c/a\u003e the instance with the shutdown script attached and validate the intended behavior.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e3. Write check-point files to Cloud Storage.\u003c/p\u003e\u003cp\u003e4. Consider using multiple MIGs behind your load balancer.\u003c/p\u003e\u003cp\u003eWhether your workload is graphics rendering, financial modeling, scaled-out ecommerce, or any other stateless use case, Spot VMs are the best and easiest way to reduce your cost of operating it by more than 60%. By following the examples and best practices above, you can ensure that Spot VMs will create the right outcome. Get started today with a \u003ca href=\"https://console.cloud.google.com/freetrial\"\u003efree trial\u003c/a\u003e of Google Cloud. \u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003e\u003csup\u003eAcknowledgement\u003cbr/\u003eSpecial thanks to Dan Sheppard, Product Manager for Cloud Compute, for contributing to this post.\u003c/sup\u003e\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-06-22T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eStefan Salandy\u003c/name\u003e\u003ctitle\u003eCustomer Engineer - Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1/",
      "title": "Enterprise DevOps Guidebook - Chapter 1",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c53=\"\"\u003e\u003cdiv _ngcontent-c53=\"\" innerhtml=\"\u0026lt;p\u0026gt;To give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/dora-enterprise-guidebook\u0026#34;\u0026gt;DevOps Enterprise Guidebook\u0026lt;/a\u0026gt;. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud\u0026amp;#8217;s DORA research to initiate performance improvements.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use \u0026lt;a href=\u0026#34;https://cloud.google.com/devops\u0026#34;\u0026gt;DORA\u0026amp;#8217;s resources\u0026lt;/a\u0026gt; to measure your performance and to begin your first DevOps team experiment. Some resources include the \u0026lt;a href=\u0026#34;https://www.devops-research.com/quickcheck.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DevOps Quick check\u0026lt;/a\u0026gt;, where you can measure your teams\u0026amp;#8217; software delivery performance in less than a minute with just five multiple choice questions, or a more indepth\u0026lt;a href=\u0026#34;https://cloud.google.com/camp\u0026#34;\u0026gt; capabilities assessment\u0026lt;/a\u0026gt;, an assessment we deploy in your organization that gives us a robust measurement of your organization\u0026amp;#8217;s capabilities as they pertain to software delivery.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Future chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/dora-enterprise-guidebook\u0026#34;\u0026gt;guidebook\u0026lt;/a\u0026gt; we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;2022 State of DevOps Survey\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;2022 State of DevOps survey\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;survey\u0026lt;/a\u0026gt; will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eTo give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the \u003ca href=\"https://cloud.google.com/resources/dora-enterprise-guidebook\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/resources/dora-enterprise-guidebook\" track-metadata-module=\"post\"\u003eDevOps Enterprise Guidebook\u003c/a\u003e. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud’s DORA research to initiate performance improvements.\u003c/p\u003e\u003cp\u003eWe will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use \u003ca href=\"https://cloud.google.com/devops\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/devops\" track-metadata-module=\"post\"\u003eDORA’s resources\u003c/a\u003e to measure your performance and to begin your first DevOps team experiment. Some resources include the \u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDevOps Quick check\u003c/a\u003e, where you can measure your teams’ software delivery performance in less than a minute with just five multiple choice questions, or a more indepth\u003ca href=\"https://cloud.google.com/camp\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/camp\" track-metadata-module=\"post\"\u003e capabilities assessment\u003c/a\u003e, an assessment we deploy in your organization that gives us a robust measurement of your organization’s capabilities as they pertain to software delivery.\u003c/p\u003e\u003cp\u003eFuture chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the \u003ca href=\"https://cloud.google.com/resources/dora-enterprise-guidebook\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/resources/dora-enterprise-guidebook\" track-metadata-module=\"post\"\u003eguidebook\u003c/a\u003e we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.\u003c/p\u003e\u003ch3\u003e2022 State of DevOps Survey\u003c/h3\u003e\u003cp\u003eFor the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. \u003c/p\u003e\u003cp\u003eWe invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003e2022 State of DevOps survey\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003esurvey\u003c/a\u003e will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe Google Cloud DORA team has been hard at work releasing our yearly \u003ca href=\"https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\"\u003eAccelerate State of DevOps report\u003c/a\u003e. This research provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance. Year over year, the \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003eState of DevOps\u003c/a\u003ereport helps organizations benchmark themselves against others in the industry as elite, high, medium, or low performers and provides recommendations for how organizations can continually improve. \u003c/p\u003e\u003cp\u003eThe table below highlights elite, high, medium, and low performers at a glance from the \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003elast report.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Enterprise_DevOps_Guidebook.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Enterprise DevOps Guidebook.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Enterprise_DevOps_Guidebook.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eTo give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the \u003ca href=\"https://cloud.google.com/resources/dora-enterprise-guidebook\"\u003eDevOps Enterprise Guidebook\u003c/a\u003e. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud’s DORA research to initiate performance improvements.\u003c/p\u003e\u003cp\u003eWe will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use \u003ca href=\"https://cloud.google.com/devops\"\u003eDORA’s resources\u003c/a\u003e to measure your performance and to begin your first DevOps team experiment. Some resources include the \u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\"\u003eDevOps Quick check\u003c/a\u003e, where you can measure your teams’ software delivery performance in less than a minute with just five multiple choice questions, or a more indepth\u003ca href=\"https://cloud.google.com/camp\"\u003ecapabilities assessment\u003c/a\u003e, an assessment we deploy in your organization that gives us a robust measurement of your organization’s capabilities as they pertain to software delivery.\u003c/p\u003e\u003cp\u003eFuture chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the \u003ca href=\"https://cloud.google.com/resources/dora-enterprise-guidebook\"\u003eguidebook\u003c/a\u003e we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.\u003c/p\u003e\u003ch3\u003e2022 State of DevOps Survey\u003c/h3\u003e\u003cp\u003eFor the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. \u003c/p\u003e\u003cp\u003eWe invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\"\u003e2022 State of DevOps survey\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\"\u003esurvey\u003c/a\u003e will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-05-26T23:00:00Z",
      "author": {
        "name": "\u003cname\u003eRob Edwards\u003c/name\u003e\u003ctitle\u003eTechnology Practice Lead, DevOps\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/devops-enterprise-guidebook-chapter-1/",
      "title": "Enterprise DevOps Guidebook - Chapter 1",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;To give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/dora-enterprise-guidebook\u0026#34;\u0026gt;DevOps Enterprise Guidebook\u0026lt;/a\u0026gt;. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud\u0026amp;#8217;s DORA research to initiate performance improvements.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use \u0026lt;a href=\u0026#34;https://cloud.google.com/devops\u0026#34;\u0026gt;DORA\u0026amp;#8217;s resources\u0026lt;/a\u0026gt; to measure your performance and to begin your first DevOps team experiment. Some resources include the \u0026lt;a href=\u0026#34;https://www.devops-research.com/quickcheck.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DevOps Quick check\u0026lt;/a\u0026gt;, where you can measure your teams\u0026amp;#8217; software delivery performance in less than a minute with just five multiple choice questions, or a more indepth\u0026lt;a href=\u0026#34;https://cloud.google.com/camp\u0026#34;\u0026gt; capabilities assessment\u0026lt;/a\u0026gt;, an assessment we deploy in your organization that gives us a robust measurement of your organization\u0026amp;#8217;s capabilities as they pertain to software delivery.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Future chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/dora-enterprise-guidebook\u0026#34;\u0026gt;guidebook\u0026lt;/a\u0026gt; we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;2022 State of DevOps Survey\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;2022 State of DevOps survey\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;survey\u0026lt;/a\u0026gt; will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eTo give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the \u003ca href=\"https://cloud.google.com/resources/dora-enterprise-guidebook\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/resources/dora-enterprise-guidebook\" track-metadata-module=\"post\"\u003eDevOps Enterprise Guidebook\u003c/a\u003e. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud’s DORA research to initiate performance improvements.\u003c/p\u003e\u003cp\u003eWe will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use \u003ca href=\"https://cloud.google.com/devops\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/devops\" track-metadata-module=\"post\"\u003eDORA’s resources\u003c/a\u003e to measure your performance and to begin your first DevOps team experiment. Some resources include the \u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDevOps Quick check\u003c/a\u003e, where you can measure your teams’ software delivery performance in less than a minute with just five multiple choice questions, or a more indepth\u003ca href=\"https://cloud.google.com/camp\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/camp\" track-metadata-module=\"post\"\u003e capabilities assessment\u003c/a\u003e, an assessment we deploy in your organization that gives us a robust measurement of your organization’s capabilities as they pertain to software delivery.\u003c/p\u003e\u003cp\u003eFuture chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the \u003ca href=\"https://cloud.google.com/resources/dora-enterprise-guidebook\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/resources/dora-enterprise-guidebook\" track-metadata-module=\"post\"\u003eguidebook\u003c/a\u003e we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.\u003c/p\u003e\u003ch3\u003e2022 State of DevOps Survey\u003c/h3\u003e\u003cp\u003eFor the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. \u003c/p\u003e\u003cp\u003eWe invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003e2022 State of DevOps survey\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003esurvey\u003c/a\u003e will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe Google Cloud DORA team has been hard at work releasing our yearly \u003ca href=\"https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\"\u003eAccelerate State of DevOps report\u003c/a\u003e. This research provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance. Year over year, the \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003eState of DevOps\u003c/a\u003ereport helps organizations benchmark themselves against others in the industry as elite, high, medium, or low performers and provides recommendations for how organizations can continually improve. \u003c/p\u003e\u003cp\u003eThe table below highlights elite, high, medium, and low performers at a glance from the \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003elast report.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Enterprise_DevOps_Guidebook.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Enterprise DevOps Guidebook.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Enterprise_DevOps_Guidebook.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eTo give more prescriptive advice on how to successfully implement DORA best practices with Google Cloud, we are excited to announce the \u003ca href=\"https://cloud.google.com/resources/dora-enterprise-guidebook\"\u003eDevOps Enterprise Guidebook\u003c/a\u003e. The guidebook will be your resource providing a concrete action plan for implementing recommendations using Google Cloud’s DORA research to initiate performance improvements.\u003c/p\u003e\u003cp\u003eWe will release the guidebook in chapter increments. The goal of this first chapter is to give your organization a better understanding of how to use \u003ca href=\"https://cloud.google.com/devops\"\u003eDORA’s resources\u003c/a\u003e to measure your performance and to begin your first DevOps team experiment. Some resources include the \u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\"\u003eDevOps Quick check\u003c/a\u003e, where you can measure your teams’ software delivery performance in less than a minute with just five multiple choice questions, or a more indepth\u003ca href=\"https://cloud.google.com/camp\"\u003ecapabilities assessment\u003c/a\u003e, an assessment we deploy in your organization that gives us a robust measurement of your organization’s capabilities as they pertain to software delivery.\u003c/p\u003e\u003cp\u003eFuture chapters will touch on other main topics we have identified in the State of DevOps reports such as shifting left on security, cloud adoption, and easy to use DevOps tools. We want to make it easy for your organization to get the most out of investing in DevOps and with the launch of the \u003ca href=\"https://cloud.google.com/resources/dora-enterprise-guidebook\"\u003eguidebook\u003c/a\u003e we believe the focused recommendations will help more organizations successfully implement DevOps practices that will lead to business and organizational success.\u003c/p\u003e\u003ch3\u003e2022 State of DevOps Survey\u003c/h3\u003e\u003cp\u003eFor the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. This year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. \u003c/p\u003e\u003cp\u003eWe invite you to join the over 32,000 professionals worldwide who have participated in the DORA reports by completing our \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\"\u003e2022 State of DevOps survey\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\"\u003esurvey\u003c/a\u003e will remain open until midnight PDT on July 22, 2022. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. We look forward to hearing from you and your teams!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-05-26T23:00:00Z",
      "author": {
        "name": "\u003cname\u003eRob Edwards\u003c/name\u003e\u003ctitle\u003eTechnology Practice Lead, DevOps\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/take-the-2022-state-of-devops-survey/",
      "title": "Take the 2022 Accelerate State of DevOps Survey",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c53=\"\"\u003e\u003cdiv _ngcontent-c53=\"\" innerhtml=\"\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\u0026#34;\u0026gt;State of DevOps report\u0026lt;/a\u0026gt; by Google Cloud and the DORA research team is the largest and longest running research of its kind with inputs from over 32,000 professionals worldwide. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Today, Google Cloud and the \u0026lt;a href=\u0026#34;https://www.devops-research.com/research.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DORA\u0026lt;/a\u0026gt; research team are excited to announce the launch of the \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;2022 State of DevOps survey\u0026lt;/a\u0026gt;. For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. As technology teams continue to accelerate and evolve, so do the quantity and sophistication of security threats. Security can no longer be an afterthought or the final step before delivery, it must be integrated throughout the software development process.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Shift Left\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The industry must shift from reactive practices to proactive and diagnostic measures, where software teams should assume that their systems are already compromised and build security into their supply chain. In the \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report\u0026#34;\u0026gt;2021 State of DevOps report\u0026lt;/a\u0026gt; we found that elite performers who met or exceeded their reliability targets were twice as likely to have shifted their security practices left, i.e., implemented security practices earlier on in the software development lifecycle, and deliver reliable software quickly, and safely. Not only that, but teams who integrate security best practices throughout their development process are 1.6 times more likely to meet or exceed their organizational goals.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;But how do companies know where to start when it comes to getting security right? In last year\u0026amp;#8217;s report we found that companies can integrate security, improve software delivery and operational performance, and improve organizational performance by leveraging the following practices:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\" track-metadata-module=\"post\"\u003eState of DevOps report\u003c/a\u003e by Google Cloud and the DORA research team is the largest and longest running research of its kind with inputs from over 32,000 professionals worldwide. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance.  \u003c/p\u003e\u003cp\u003eToday, Google Cloud and the \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDORA\u003c/a\u003e research team are excited to announce the launch of the \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003e2022 State of DevOps survey\u003c/a\u003e. For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. As technology teams continue to accelerate and evolve, so do the quantity and sophistication of security threats. Security can no longer be an afterthought or the final step before delivery, it must be integrated throughout the software development process. \u003c/p\u003e\u003ch3\u003eShift Left\u003c/h3\u003e\u003cp\u003eThe industry must shift from reactive practices to proactive and diagnostic measures, where software teams should assume that their systems are already compromised and build security into their supply chain. In the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report\" track-metadata-module=\"post\"\u003e2021 State of DevOps report\u003c/a\u003e we found that elite performers who met or exceeded their reliability targets were twice as likely to have shifted their security practices left, i.e., implemented security practices earlier on in the software development lifecycle, and deliver reliable software quickly, and safely. Not only that, but teams who integrate security best practices throughout their development process are 1.6 times more likely to meet or exceed their organizational goals.\u003c/p\u003e\u003cp\u003eBut how do companies know where to start when it comes to getting security right? In last year’s report we found that companies can integrate security, improve software delivery and operational performance, and improve organizational performance by leveraging the following practices:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\"\u003eState of DevOps report\u003c/a\u003e by Google Cloud and the DORA research team is the largest and longest running research of its kind with inputs from over 32,000 professionals worldwide. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region, can employ to drive better performance.  \u003c/p\u003e\u003cp\u003eToday, Google Cloud and the \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\"\u003eDORA\u003c/a\u003e research team are excited to announce the launch of the \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\"\u003e2022 State of DevOps survey\u003c/a\u003e. For the 2022 State of DevOps report we will be focusing on a topic that has been top of mind recently: security. As technology teams continue to accelerate and evolve, so do the quantity and sophistication of security threats. Security can no longer be an afterthought or the final step before delivery, it must be integrated throughout the software development process. \u003c/p\u003e\u003ch3\u003eShift Left\u003c/h3\u003e\u003cp\u003eThe industry must shift from reactive practices to proactive and diagnostic measures, where software teams should assume that their systems are already compromised and build security into their supply chain. In the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report\"\u003e2021 State of DevOps report\u003c/a\u003e we found that elite performers who met or exceeded their reliability targets were twice as likely to have shifted their security practices left, i.e., implemented security practices earlier on in the software development lifecycle, and deliver reliable software quickly, and safely. Not only that, but teams who integrate security best practices throughout their development process are 1.6 times more likely to meet or exceed their organizational goals.\u003c/p\u003e\u003cp\u003eBut how do companies know where to start when it comes to getting security right? In last year’s report we found that companies can integrate security, improve software delivery and operational performance, and improve organizational performance by leveraging the following practices:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_DevOps_Survey.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 DevOps Survey.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_DevOps_Survey.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003e2022 State of DevOps Survey\u003c/h3\u003e\u003cp\u003eLike the past six research reports, our goal this year is to perform detailed analysis to help teams benchmark their performance against the industry and provide strategies that teams can employ to improve their performance. For the first time in \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003elast year’s report\u003c/a\u003e, high and elite performers make up two-thirds of respondents. We can confidently say that as the industry continues to accelerate its adoption of DevOps principles teams see meaningful benefits as a result.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_DevOps_Survey.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 DevOps Survey.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_DevOps_Survey.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis year we are doing a deeper investigation into how security practices and capabilities predict overall software delivery and operations performance. \u003c/p\u003e\u003cp\u003eAchieving elite performance is a team endeavor and diverse, inclusive teams drive the best performance. The research program benefits from the participation of a diverse group of people. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. \u003c/p\u003e\u003cp\u003eThis \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\"\u003esurvey\u003c/a\u003e is for everyone. No matter where you are on your DevOps journey, the size of your organization, your organization's industry, or how you identify. There are no right or wrong answers, in fact we often hear feedback that questions in the survey prompt ideas for improvement.  \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_2aXfK0Zw75lvCl0?source=blog\" target=\"_blank\"\u003esurvey\u003c/a\u003e will remain open until midnight PDT on July 22, 2022. We look forward to hearing from you and your teams!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-05-26T23:00:00Z",
      "author": {
        "name": "\u003cname\u003eClaire Peters\u003c/name\u003e\u003ctitle\u003eUser Experience Researcher\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/get-more-insights-with-the-new-version-of-the-nodejs-library/",
      "title": "Get more insights with the new version of the Node.js library",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c31=\"\"\u003e\u003cdiv _ngcontent-c31=\"\" innerhtml=\"\u0026lt;p\u0026gt;Two critical features of the latest \u0026lt;a href=\u0026#34;https://www.npmjs.com/package/@google-cloud/logging\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Cloud Logging Library for Node.js\u0026lt;/a\u0026gt; release are writing structured log entries to standard output and error handling with a default callback. Let\u0026#39;s dig in deeper.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Writing structured log entries to standard output\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;LogSync\u0026lt;/a\u0026gt; class helps users write context-rich structured logs to \u0026lt;code\u0026gt;stdout\u0026lt;/code\u0026gt; or any other \u0026lt;code\u0026gt;Writable\u0026lt;/code\u0026gt; interface. This class extracts additional log properties like trace context from HTTP headers, and can be used to toggle between writing to the Cloud Logging endpoint or to \u0026lt;code\u0026gt;stdout\u0026lt;/code\u0026gt; during local development.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In addition, writing structured logging to \u0026lt;code\u0026gt;stdout\u0026lt;/code\u0026gt; can be integrated with a \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/agent/logging\u0026#34;\u0026gt;Logging agent\u0026lt;/a\u0026gt;. Once a log is written to \u0026lt;code\u0026gt;stdout\u0026lt;/code\u0026gt;, a Logging agent then picks up those logs and delivers those to Cloud Logging out-of-process. \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/agent/logging\u0026#34;\u0026gt;Logging agents\u0026lt;/a\u0026gt; can add more properties to each entry before streaming it to the Logging API.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We recommend serverless applications (i.e. applications running in Cloud Functions and Cloud Run) to use the \u0026lt;a href=\u0026#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;LogSync\u0026lt;/a\u0026gt; class as async logs delivery may be dropped due to lack of CPU or other environmental factors\u0026amp;#160; preventing the logs from being sent immediately to the Logging API. Cloud Functions and Cloud Run applications by their nature are ephemeral and can have a short lifespan which will cause logging data drops when an instance is shut down before the logs have been sent to Cloud Logging servers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Today, Google Cloud managed services automatically install Logging agents for all Google serverless environments in the resources that they provision - this means that you can use \u0026lt;a href=\u0026#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;LogSync\u0026lt;/a\u0026gt; in your application to seamlessly deliver logs to Cloud Logging through standard output.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Below is a sample how to use \u0026lt;a href=\u0026#34;https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;LogSync\u0026lt;/a\u0026gt; class:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eTwo critical features of the latest \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://www.npmjs.com\" track-metadata-module=\"post\"\u003eCloud Logging Library for Node.js\u003c/a\u003e release are writing structured log entries to standard output and error handling with a default callback. Let\u0026#39;s dig in deeper. \u003c/p\u003e\u003ch3\u003eWriting structured log entries to standard output\u003c/h3\u003e\u003cp\u003eThe \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\" target=\"_blank\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eLogSync\u003c/a\u003e class helps users write context-rich structured logs to \u003ccode\u003estdout\u003c/code\u003e or any other \u003ccode\u003eWritable\u003c/code\u003e interface. This class extracts additional log properties like trace context from HTTP headers, and can be used to toggle between writing to the Cloud Logging endpoint or to \u003ccode\u003estdout\u003c/code\u003e during local development.\u003c/p\u003e\u003cp\u003eIn addition, writing structured logging to \u003ccode\u003estdout\u003c/code\u003e can be integrated with a \u003ca href=\"https://cloud.google.com/logging/docs/agent/logging\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/agent/logging\" track-metadata-module=\"post\"\u003eLogging agent\u003c/a\u003e. Once a log is written to \u003ccode\u003estdout\u003c/code\u003e, a Logging agent then picks up those logs and delivers those to Cloud Logging out-of-process. \u003ca href=\"https://cloud.google.com/logging/docs/agent/logging\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/agent/logging\" track-metadata-module=\"post\"\u003eLogging agents\u003c/a\u003e can add more properties to each entry before streaming it to the Logging API.\u003c/p\u003e\u003cp\u003eWe recommend serverless applications (i.e. applications running in Cloud Functions and Cloud Run) to use the \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eLogSync\u003c/a\u003e class as async logs delivery may be dropped due to lack of CPU or other environmental factors  preventing the logs from being sent immediately to the Logging API. Cloud Functions and Cloud Run applications by their nature are ephemeral and can have a short lifespan which will cause logging data drops when an instance is shut down before the logs have been sent to Cloud Logging servers. \u003c/p\u003e\u003cp\u003eToday, Google Cloud managed services automatically install Logging agents for all Google serverless environments in the resources that they provision - this means that you can use \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\" target=\"_blank\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eLogSync\u003c/a\u003e in your application to seamlessly deliver logs to Cloud Logging through standard output.\u003c/p\u003e\u003cp\u003eBelow is a sample how to use \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eLogSync\u003c/a\u003e class:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe’re thrilled to announce the release of a new update to the \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging\" target=\"_blank\"\u003eCloud Logging Library for Node.js\u003c/a\u003e with the key new features of improved error handling and writing structured logging to standard output which becomes handy if you run applications in serverless environments like Google Functions!\u003c/p\u003e\u003cp\u003eThe latest v9.9.0 of \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging\" target=\"_blank\"\u003eCloud Logging Library for Node.js\u003c/a\u003e makes it even easier for Node.js developers to send and read logs from Google Cloud providing real-time insight into what is happening in your application through comprehensive tools like \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\"\u003eLog Explorer\u003c/a\u003e. If you are a Node.js developer working with Google Cloud, now is a great time to try out \u003ca href=\"https://cloud.google.com/logging/docs\"\u003eCloud Logging\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe latest features of the Node.js library are also integrated and available in other packages which are based on \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging\" target=\"_blank\"\u003eCloud Logging Library for Node.js\u003c/a\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging-winston\" target=\"_blank\"\u003e@google-cloud/logging-winston\u003c/a\u003e - this package integrates \u003ca href=\"https://cloud.google.com/logging/docs\"\u003eCloud Logging\u003c/a\u003e with the \u003ca href=\"https://www.npmjs.com/package/winston\" target=\"_blank\"\u003eWinston\u003c/a\u003e logging library. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging-bunyan\" target=\"_blank\"\u003e@google-cloud/logging-bunyan\u003c/a\u003e - this package integrates \u003ca href=\"https://cloud.google.com/logging/docs\"\u003eCloud Logging\u003c/a\u003e with the \u003ca href=\"https://www.npmjs.com/package/bunyan\" target=\"_blank\"\u003eBunyan\u003c/a\u003e logging library. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you are unfamiliar with the \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging\" target=\"_blank\"\u003eCloud Logging Library for Node.js\u003c/a\u003e, start by running following command to add the library to your project:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'npm install @google-cloud/logging'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOnce the library is installed, you can use it in your project. Below, I demonstrate how to initialize the logging library, create a client assigned configured with a project ID,  and log a single entry 'Your log message':\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"// Imports the Google Cloud client library \\r\\n const { Logging } = require('@google-cloud/logging');\\r\\n // Creates a client with predefined project Id and a path to\\r\\n // credentials JSON file to be used for auth with Cloud Logging\\r\\n const logging = new Logging(\\r\\n {\\r\\n projectId: 'your-project-id',\\r\\n keyFilename: '/path/to/key.json',\\r\\n }\\r\\n );\\r\\n // Create a log with desired log name\\r\\n const log = logging.log('your-log-name');\\r\\n // Create a simple log entry without any metadata\\r\\n const entry = log.entry({}, 'Your log message');\\r\\n // Log your record!!!\\r\\n log.info(entry);\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eHere's the log message generated by this code in Log Explorer:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"1 nodejs library.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_nodejs_library.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eTwo critical features of the latest \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging\" target=\"_blank\"\u003eCloud Logging Library for Node.js\u003c/a\u003e release are writing structured log entries to standard output and error handling with a default callback. Let's dig in deeper. \u003c/p\u003e\u003ch3\u003eWriting structured log entries to standard output\u003c/h3\u003e\u003cp\u003eThe \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\" target=\"_blank\"\u003eLogSync\u003c/a\u003e class helps users write context-rich structured logs to \u003ccode\u003estdout\u003c/code\u003e or any other \u003ccode\u003eWritable\u003c/code\u003e interface. This class extracts additional log properties like trace context from HTTP headers, and can be used to toggle between writing to the Cloud Logging endpoint or to \u003ccode\u003estdout\u003c/code\u003e during local development.\u003c/p\u003e\u003cp\u003eIn addition, writing structured logging to \u003ccode\u003estdout\u003c/code\u003e can be integrated with a \u003ca href=\"https://cloud.google.com/logging/docs/agent/logging\"\u003eLogging agent\u003c/a\u003e. Once a log is written to \u003ccode\u003estdout\u003c/code\u003e, a Logging agent then picks up those logs and delivers those to Cloud Logging out-of-process. \u003ca href=\"https://cloud.google.com/logging/docs/agent/logging\"\u003eLogging agents\u003c/a\u003e can add more properties to each entry before streaming it to the Logging API.\u003c/p\u003e\u003cp\u003eWe recommend serverless applications (i.e. applications running in Cloud Functions and Cloud Run) to use the \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\" target=\"_blank\"\u003eLogSync\u003c/a\u003e class as async logs delivery may be dropped due to lack of CPU or other environmental factors  preventing the logs from being sent immediately to the Logging API. Cloud Functions and Cloud Run applications by their nature are ephemeral and can have a short lifespan which will cause logging data drops when an instance is shut down before the logs have been sent to Cloud Logging servers. \u003c/p\u003e\u003cp\u003eToday, Google Cloud managed services automatically install Logging agents for all Google serverless environments in the resources that they provision - this means that you can use \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\" target=\"_blank\"\u003eLogSync\u003c/a\u003e in your application to seamlessly deliver logs to Cloud Logging through standard output.\u003c/p\u003e\u003cp\u003eBelow is a sample how to use \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log-sync.ts#L59\" target=\"_blank\"\u003eLogSync\u003c/a\u003e class:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"const { Logging } = require('@google-cloud/logging');\\r\\n const logging = new Logging(\\r\\n {\\r\\n projectId: 'your-project-id',\\r\\n keyFilename: '/path/to/key.json',\\r\\n }\\r\\n );\\r\\n// Create a LogSync transport, defaulting to `process.stdout`\\r\\nconst log = logging.logSync('Your-log-name');\\r\\nconst entry = log.entry({}, 'Your log message');\\r\\nlog.write(entry);\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you use \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging-winston\" target=\"_blank\"\u003e@google-cloud/logging-winston\u003c/a\u003e  or \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging-bunyan\" target=\"_blank\"\u003e@google-cloud/logging-bunyan\u003c/a\u003e library, you can set the \u003ccode\u003eredirectToStdout\u003c/code\u003e parameter in \u003ca href=\"https://github.com/googleapis/nodejs-logging-winston/blob/2470dbc2b225e77b78f8066477a2f63126e3b1cd/src/index.ts#L181\" target=\"_blank\"\u003eLoggingWinston\u003c/a\u003e or \u003ca href=\"https://github.com/googleapis/nodejs-logging-bunyan/blob/75d7811c277e4de87a2232db3ce856669964cb45/src/index.ts#L165\" target=\"_blank\"\u003eLoggingBunyan\u003c/a\u003e constructor options respectively. Below is a sample code how to redirect structured logging output to \u003ccode\u003estdout\u003c/code\u003e for \u003ca href=\"https://github.com/googleapis/nodejs-logging-winston/blob/2470dbc2b225e77b78f8066477a2f63126e3b1cd/src/index.ts#L181\" target=\"_blank\"\u003eLoggingWinston\u003c/a\u003e class:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"// Imports the Google Cloud client library for Winston\\r\\nconst {LoggingWinston} = require('@google-cloud/logging-winston');\\r\\n\\r\\n// Creates a client that writes logs to stdout\\r\\nconst loggingWinston = new LoggingWinston({\\r\\n projectId: 'your-project-id',\\r\\n keyFilename: '/path/to/key.json',\\r\\n redirectToStdout: true,\\r\\n});\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eError Handling with a default callback\u003c/h3\u003e\u003cp\u003eThe \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log.ts#L117\" target=\"_blank\"\u003eLog\u003c/a\u003e class provides users the ability to write and delete logs asynchronously. However, there are cases when log entries cannot be written or deleted and an error is thrown - if the error is not handled properly, it can crash the application. \u003c/p\u003eOne possible way to handle the error is to await the log write/delete calls and wrap it with \u003ccode\u003etry/catch\u003c/code\u003e. However, waiting for every write or delete call may introduce delays which could be avoided by simply adding a callback as shown below:\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"// Asynchronously write the log entry and handle response or \\r\\n // any errors in provided callback\\r\\n log.write(entry, err =\u0026gt; {\\r\\n if (err) {\\r\\n // The log entry was not written.\\r\\n console.log(err.message);\\r\\n } else {\\r\\n console.log('No error in write callback!');\\r\\n }\\r\\n });\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAdding a callback to each write or delete call is duplicate code and remembering to include it for each call may be toilsome, especially if  the code handling the error is always the same. To eliminate this burden, we introduced the ability to provide a default callback for the \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log.ts#L117\" target=\"_blank\"\u003eLog\u003c/a\u003e class which can be set through the \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log.ts#L59\" target=\"_blank\"\u003eLogOptions\u003c/a\u003e passed to the \u003ca href=\"https://github.com/googleapis/nodejs-logging/blob/6066776743bf3e44b724e19b65aba5834ed15712/src/log.ts#L117\" target=\"_blank\"\u003eLog\u003c/a\u003e constructor as in example below:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"const {Logging} = require('@google-cloud/logging');\\r\\n const logging = new Logging();\\r\\n \\r\\n // Create options with default callback to be called on \\r\\n // every write/delete response or error\\r\\n const options = {\\r\\n defaultWriteDeleteCallback: function (err) {\\r\\n if (err) {\\r\\n console.log('Error is: ' + err);\\r\\n } else {\\r\\n console.log('No error, all is good!');\\r\\n }\\r\\n },\\r\\n };\\r\\n\\r\\n const log = logging.log('my-log', options);\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you use \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging-winston\" target=\"_blank\"\u003e@google-cloud/logging-winston\u003c/a\u003e  or \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging-bunyan\" target=\"_blank\"\u003e@google-cloud/logging-bunyan\u003c/a\u003e library, you can set the callback through \u003ccode\u003edefaultCallback\u003c/code\u003e parameter in \u003ca href=\"https://github.com/googleapis/nodejs-logging-winston/blob/2470dbc2b225e77b78f8066477a2f63126e3b1cd/src/index.ts#L181\" target=\"_blank\"\u003eLoggingWinston\u003c/a\u003e or \u003ca href=\"https://github.com/googleapis/nodejs-logging-bunyan/blob/75d7811c277e4de87a2232db3ce856669964cb45/src/index.ts#L165\" target=\"_blank\"\u003eLoggingBunyan\u003c/a\u003e constructor options respectively. Here is an example of  how to set a default callback for \u003ca href=\"https://github.com/googleapis/nodejs-logging-winston/blob/2470dbc2b225e77b78f8066477a2f63126e3b1cd/src/index.ts#L181\" target=\"_blank\"\u003eLoggingWinston\u003c/a\u003e class:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"// Imports the Google Cloud client library for Winston\\r\\nconst {LoggingWinston} = require('@google-cloud/logging-winston');\\r\\n\\r\\n// Creates a client\\r\\nconst loggingWinston = new LoggingWinston({\\r\\n projectId: 'your-project-id',\\r\\n keyFilename: '/path/to/key.json',\\r\\n defaultCallback: err =\u0026gt; {\\r\\n if (err) {\\r\\n console.log('Error occurred: ' + err);\\r\\n }\\r\\n },\\r\\n});\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eNext Steps\u003c/h3\u003e\u003cul\u003e\u003cli\u003eNow, when you integrate the \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging\" target=\"_blank\"\u003eCloud Logging Library for Node.js\u003c/a\u003e in your project, you can start using the latest features. \u003c/li\u003e\u003cli\u003eTo try the latest Node.js library in Google Cloud you can follow this quickstart walkthrough guide:\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--small h-c-grid__col h-c-grid__col--2 h-c-grid__col--offset-5 \"\u003e\u003ca href=\"https://console.cloud.google.com/?walkthrough_id=logging__logging-nodejs\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"guide me button.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/guide_me_button.0998009406980188.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor more information on the latest check out for \u003ca href=\"https://www.npmjs.com/package/@google-cloud/logging\" target=\"_blank\"\u003eCloud Logging Library for Node.js\u003c/a\u003e user guide.\u003cbr/\u003e\u003c/li\u003e\u003cli\u003eFor any feedback or contributions, feel free to open issues in our \u003ca href=\"https://github.com/googleapis/nodejs-logging/issues\" target=\"_blank\"\u003eCloud Logging Library for Node.js GitHub repo\u003c/a\u003e. Issues can be also opened for bugs, questions about library usage and new feature requests.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/managed-service-for-prometheus-offers-new-pricing-tier/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eIntroducing a high-usage tier for Managed Service for Prometheus\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eNew pricing tier for our managed Prometheus service users with over 500 billion metric samples per month. Pricing for existing tiers redu...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/nodejs.max-2200x2200.jpg",
      "date_published": "2022-05-20T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlexander Losovsky\u003c/name\u003e\u003ctitle\u003eDeveloper Relations Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/maisons-du-monde-improved-their-kubernetes-observability/",
      "title": "Maisons du Monde’s journey to a managed service for Prometheus",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026lt;b\u0026gt;Editor\u0026amp;#8217;s note\u0026lt;/b\u0026gt;: Today we hear from Maisons du Monde, a furniture and home decor company that was founded in France over 25 years ago. They have 357 stores across France, Italy, Spain, Belgium, Luxembourg, Germany, Austria, Switzerland, Netherlands, and Portugal, and are a Google Cloud customer. They worked with their Customer Engineer, Adrien Aflalo, to prepare this story.\u0026amp;#160;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;hr\u0026gt;\u0026lt;p\u0026gt;Without telemetry data like logs and metrics, we are blind in production. Our team of Operations Engineers and Site Reliability Engineers (SRE) rely on metrics data in particular to run the \u0026lt;a href=\u0026#34;https://www.maisonsdumonde.com/UK/en\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Maisons du Monde website\u0026lt;/a\u0026gt;, our APIs, and our omnichannel services in a secure and reliable manner. This means that choosing a metrics platform and provider is not just a technical decision, but one that\u0026amp;#8217;s critical to our business as well.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As we moved more of our operations to Google Kubernetes Engine, we evaluated new metrics platforms and about eight months ago decided on Prometheus. Prometheus is a good fit for our environment, which contains cloud native applications that are built on Kubernetes and run on ephemeral compute infrastructure. Although first we built and ran our own Prometheus environment, we decided that Google Cloud Managed Service for Prometheus is a better solution for us. It allows us to focus on using our metrics instead of managing metrics infrastructure, providing the following features:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;Long-term retention of metrics (2 years)\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Seamless support for high availability of Prometheus instances\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Scraping and evaluating rules using lightweight Kubernetes Custom Resources\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;A global query view\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fully managed collection and querying, out of the box\u0026amp;#160;\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;For other organizations that are running their own Prometheus or deciding whether to run their own Prometheus, we wanted to provide you a look into our journey moving from self-hosted Prometheus to using a managed Prometheus service:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;We first switched from traditional managed metrics services to Prometheus to support our growing Kubernetes environment.\u0026amp;#160;\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;We then built out and maintained our own Prometheus environment, but realized we needed additional features to support production workloads (such as high availability and faster MTTR) and scale.\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Next we investigated solutions to help support our use of Prometheus in production, which led us to adopting Thanos.\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Then we discovered we were still investing lots of time and personnel to run these technologies and manage infrastructure.\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Finally we adopted Google Cloud Managed Service for Prometheus, which so far has met our needs quite well.\u0026amp;#160;\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;h3\u0026gt;Open source Prometheus worked well when we had a smaller deployment\u0026amp;#160;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Commonly, organizations approach metrics using either a full service monitoring and metrics storage tool or an open source database to store their metrics, displaying them using a visualization tool like Grafana.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For the past five years, we used managed monitoring and storage services from two providers. Using different vendors created complexity for our operations teams and it made it hard for metrics to be shared with the teams who would eventually be responsible for maintaining reliability: the application teams! \u0026lt;b\u0026gt;It is our belief that ownership of the metrics should reside with the application teams. It allows developers and product owners to maintain the metrics they deem essential for alerting and dashboarding.\u0026amp;#160;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;After we made the switch to Prometheus, our experience was great\u0026amp;#8212;at first. Prometheus\u0026amp;#8217; design allows each application to expose granular metrics which are gathered with a Prometheus collector and stored in a Prometheus database.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;However, as we deployed it on an increasing number of GKE clusters used to run our production applications, we ran into some constraints. These included:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;\u0026lt;i\u0026gt;Support for scaled management\u0026amp;#160;\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We\u0026amp;#8217;re moving our business towards an \u0026amp;#8220;infrastructure-as-code\u0026amp;#8221; model to deploy and manage resources because it is more efficient and results in fewer errors. We need a simple way to deploy Prometheus in each\u0026lt;a href=\u0026#34;https://kubernetes.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; Kubernetes\u0026lt;/a\u0026gt; cluster by policy.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;\u0026lt;i\u0026gt;Retention\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Prometheus\u0026amp;#8217; default time series database retention is set to \u0026lt;a href=\u0026#34;https://prometheus.io/docs/prometheus/latest/storage/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;15 days\u0026lt;/a\u0026gt;. The database retention is configurable, but it will increase your costs and resource consumption to keep your metrics on disk for longer periods. We need a better way to manage metrics retention for longer periods of time for all our Kubernetes clusters and applications.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;\u0026lt;i\u0026gt;Backup/Disaster recovery and restoration\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Business continuity considerations are important for any service used in production. In Prometheus we found disk failures and backups to be a pain point. We need ways to scalably backup and restore data on Prometheus instances when failures occur to avoid data loss.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;\u0026lt;i\u0026gt;Operational scalability\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Furthermore, Prometheus doesn\u0026amp;#8217;t offer a native sharding feature, which may be a strength from an administration or deployment point of view, but ends up being a weakness if you have multiple clusters to monitor.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Updates\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Prometheus stores rules within a static file, which means you have to reboot your Prometheus instances in order to apply rules file updates.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Using Prometheus and Thanos to address some (but not all) needs\u0026amp;#160;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;After we ran into the issues raised above, we searched for ways to address them and found \u0026lt;a href=\u0026#34;http://thanos.io\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Thanos\u0026lt;/a\u0026gt;. Thanos is an open-source project released in 2018 by \u0026lt;a href=\u0026#34;https://www.improbable.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Improbable.\u0026lt;/a\u0026gt; It helped us with multi-cluster management and data storage. Prometheus metrics can be sent to object storage services such as Google Cloud Storage, Azure Blob Storage, or AWS\u0026amp;#8217; S3.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Like Prometheus, Thanos\u0026amp;#8217; architecture is extensible by design:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s note\u003c/b\u003e: Today we hear from Maisons du Monde, a furniture and home decor company that was founded in France over 25 years ago. They have 357 stores across France, Italy, Spain, Belgium, Luxembourg, Germany, Austria, Switzerland, Netherlands, and Portugal, and are a Google Cloud customer. They worked with their Customer Engineer, Adrien Aflalo, to prepare this story. \u003c/i\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003eWithout telemetry data like logs and metrics, we are blind in production. Our team of Operations Engineers and Site Reliability Engineers (SRE) rely on metrics data in particular to run the \u003ca href=\"https://www.maisonsdumonde.com/UK/en\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://www.maisonsdumonde.com\" track-metadata-module=\"post\"\u003eMaisons du Monde website\u003c/a\u003e, our APIs, and our omnichannel services in a secure and reliable manner. This means that choosing a metrics platform and provider is not just a technical decision, but one that’s critical to our business as well.\u003c/p\u003e\u003cp\u003eAs we moved more of our operations to Google Kubernetes Engine, we evaluated new metrics platforms and about eight months ago decided on Prometheus. Prometheus is a good fit for our environment, which contains cloud native applications that are built on Kubernetes and run on ephemeral compute infrastructure. Although first we built and ran our own Prometheus environment, we decided that Google Cloud Managed Service for Prometheus is a better solution for us. It allows us to focus on using our metrics instead of managing metrics infrastructure, providing the following features: \u003c/p\u003e\u003cul\u003e\u003cli\u003eLong-term retention of metrics (2 years)\u003c/li\u003e\u003cli\u003eSeamless support for high availability of Prometheus instances \u003c/li\u003e\u003cli\u003eScraping and evaluating rules using lightweight Kubernetes Custom Resources \u003c/li\u003e\u003cli\u003eA global query view \u003c/li\u003e\u003cli\u003eFully managed collection and querying, out of the box  \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor other organizations that are running their own Prometheus or deciding whether to run their own Prometheus, we wanted to provide you a look into our journey moving from self-hosted Prometheus to using a managed Prometheus service:\u003c/p\u003e\u003col\u003e\u003cli\u003eWe first switched from traditional managed metrics services to Prometheus to support our growing Kubernetes environment.  \u003c/li\u003e\u003cli\u003eWe then built out and maintained our own Prometheus environment, but realized we needed additional features to support production workloads (such as high availability and faster MTTR) and scale.\u003c/li\u003e\u003cli\u003eNext we investigated solutions to help support our use of Prometheus in production, which led us to adopting Thanos.\u003c/li\u003e\u003cli\u003eThen we discovered we were still investing lots of time and personnel to run these technologies and manage infrastructure. \u003c/li\u003e\u003cli\u003eFinally we adopted Google Cloud Managed Service for Prometheus, which so far has met our needs quite well.  \u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eOpen source Prometheus worked well when we had a smaller deployment \u003c/h3\u003e\u003cp\u003eCommonly, organizations approach metrics using either a full service monitoring and metrics storage tool or an open source database to store their metrics, displaying them using a visualization tool like Grafana.\u003c/p\u003e\u003cp\u003eFor the past five years, we used managed monitoring and storage services from two providers. Using different vendors created complexity for our operations teams and it made it hard for metrics to be shared with the teams who would eventually be responsible for maintaining reliability: the application teams! \u003cb\u003eIt is our belief that ownership of the metrics should reside with the application teams. It allows developers and product owners to maintain the metrics they deem essential for alerting and dashboarding. \u003c/b\u003e\u003c/p\u003e\u003cp\u003eAfter we made the switch to Prometheus, our experience was great—at first. Prometheus’ design allows each application to expose granular metrics which are gathered with a Prometheus collector and stored in a Prometheus database. \u003c/p\u003e\u003cp\u003eHowever, as we deployed it on an increasing number of GKE clusters used to run our production applications, we ran into some constraints. These included: \u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eSupport for scaled management \u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWe’re moving our business towards an “infrastructure-as-code” model to deploy and manage resources because it is more efficient and results in fewer errors. We need a simple way to deploy Prometheus in each\u003ca href=\"https://kubernetes.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://kubernetes.io\" track-metadata-module=\"post\"\u003e Kubernetes\u003c/a\u003e cluster by policy. \u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eRetention\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003ePrometheus’ default time series database retention is set to \u003ca href=\"https://prometheus.io/docs/prometheus/latest/storage/\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://prometheus.io\" track-metadata-module=\"post\"\u003e15 days\u003c/a\u003e. The database retention is configurable, but it will increase your costs and resource consumption to keep your metrics on disk for longer periods. We need a better way to manage metrics retention for longer periods of time for all our Kubernetes clusters and applications.  \u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eBackup/Disaster recovery and restoration\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003eBusiness continuity considerations are important for any service used in production. In Prometheus we found disk failures and backups to be a pain point. We need ways to scalably backup and restore data on Prometheus instances when failures occur to avoid data loss.\u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eOperational scalability\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003eFurthermore, Prometheus doesn’t offer a native sharding feature, which may be a strength from an administration or deployment point of view, but ends up being a weakness if you have multiple clusters to monitor. \u003c/p\u003e\u003cp\u003eUpdates\u003c/p\u003e\u003cp\u003ePrometheus stores rules within a static file, which means you have to reboot your Prometheus instances in order to apply rules file updates.  \u003c/p\u003e\u003ch3\u003eUsing Prometheus and Thanos to address some (but not all) needs \u003c/h3\u003e\u003cp\u003eAfter we ran into the issues raised above, we searched for ways to address them and found \u003ca href=\"http://thanos.io\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"http://thanos.io\" track-metadata-module=\"post\"\u003eThanos\u003c/a\u003e. Thanos is an open-source project released in 2018 by \u003ca href=\"https://www.improbable.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://www.improbable.io\" track-metadata-module=\"post\"\u003eImprobable.\u003c/a\u003e It helped us with multi-cluster management and data storage. Prometheus metrics can be sent to object storage services such as Google Cloud Storage, Azure Blob Storage, or AWS’ S3. \u003c/p\u003e\u003cp\u003eLike Prometheus, Thanos’ architecture is extensible by design:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s note\u003c/b\u003e: Today we hear from Maisons du Monde, a furniture and home decor company that was founded in France over 25 years ago. They have 357 stores across France, Italy, Spain, Belgium, Luxembourg, Germany, Austria, Switzerland, Netherlands, and Portugal, and are a Google Cloud customer. They worked with their Customer Engineer, Adrien Aflalo, to prepare this story. \u003c/i\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003eWithout telemetry data like logs and metrics, we are blind in production. Our team of Operations Engineers and Site Reliability Engineers (SRE) rely on metrics data in particular to run the \u003ca href=\"https://www.maisonsdumonde.com/UK/en\" target=\"_blank\"\u003eMaisons du Monde website\u003c/a\u003e, our APIs, and our omnichannel services in a secure and reliable manner. This means that choosing a metrics platform and provider is not just a technical decision, but one that’s critical to our business as well.\u003c/p\u003e\u003cp\u003eAs we moved more of our operations to Google Kubernetes Engine, we evaluated new metrics platforms and about eight months ago decided on Prometheus. Prometheus is a good fit for our environment, which contains cloud native applications that are built on Kubernetes and run on ephemeral compute infrastructure. Although first we built and ran our own Prometheus environment, we decided that Google Cloud Managed Service for Prometheus is a better solution for us. It allows us to focus on using our metrics instead of managing metrics infrastructure, providing the following features: \u003c/p\u003e\u003cul\u003e\u003cli\u003eLong-term retention of metrics (2 years)\u003c/li\u003e\u003cli\u003eSeamless support for high availability of Prometheus instances \u003c/li\u003e\u003cli\u003eScraping and evaluating rules using lightweight Kubernetes Custom Resources \u003c/li\u003e\u003cli\u003eA global query view \u003c/li\u003e\u003cli\u003eFully managed collection and querying, out of the box  \u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor other organizations that are running their own Prometheus or deciding whether to run their own Prometheus, we wanted to provide you a look into our journey moving from self-hosted Prometheus to using a managed Prometheus service:\u003c/p\u003e\u003col\u003e\u003cli\u003eWe first switched from traditional managed metrics services to Prometheus to support our growing Kubernetes environment.  \u003c/li\u003e\u003cli\u003eWe then built out and maintained our own Prometheus environment, but realized we needed additional features to support production workloads (such as high availability and faster MTTR) and scale.\u003c/li\u003e\u003cli\u003eNext we investigated solutions to help support our use of Prometheus in production, which led us to adopting Thanos.\u003c/li\u003e\u003cli\u003eThen we discovered we were still investing lots of time and personnel to run these technologies and manage infrastructure. \u003c/li\u003e\u003cli\u003eFinally we adopted Google Cloud Managed Service for Prometheus, which so far has met our needs quite well.  \u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eOpen source Prometheus worked well when we had a smaller deployment \u003c/h3\u003e\u003cp\u003eCommonly, organizations approach metrics using either a full service monitoring and metrics storage tool or an open source database to store their metrics, displaying them using a visualization tool like Grafana.\u003c/p\u003e\u003cp\u003eFor the past five years, we used managed monitoring and storage services from two providers. Using different vendors created complexity for our operations teams and it made it hard for metrics to be shared with the teams who would eventually be responsible for maintaining reliability: the application teams! \u003cb\u003eIt is our belief that ownership of the metrics should reside with the application teams. It allows developers and product owners to maintain the metrics they deem essential for alerting and dashboarding. \u003c/b\u003e\u003c/p\u003e\u003cp\u003eAfter we made the switch to Prometheus, our experience was great—at first. Prometheus’ design allows each application to expose granular metrics which are gathered with a Prometheus collector and stored in a Prometheus database. \u003c/p\u003e\u003cp\u003eHowever, as we deployed it on an increasing number of GKE clusters used to run our production applications, we ran into some constraints. These included: \u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eSupport for scaled management \u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWe’re moving our business towards an “infrastructure-as-code” model to deploy and manage resources because it is more efficient and results in fewer errors. We need a simple way to deploy Prometheus in each\u003ca href=\"https://kubernetes.io/\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e cluster by policy. \u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eRetention\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003ePrometheus’ default time series database retention is set to \u003ca href=\"https://prometheus.io/docs/prometheus/latest/storage/\" target=\"_blank\"\u003e15 days\u003c/a\u003e. The database retention is configurable, but it will increase your costs and resource consumption to keep your metrics on disk for longer periods. We need a better way to manage metrics retention for longer periods of time for all our Kubernetes clusters and applications.  \u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eBackup/Disaster recovery and restoration\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003eBusiness continuity considerations are important for any service used in production. In Prometheus we found disk failures and backups to be a pain point. We need ways to scalably backup and restore data on Prometheus instances when failures occur to avoid data loss.\u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eOperational scalability\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003eFurthermore, Prometheus doesn’t offer a native sharding feature, which may be a strength from an administration or deployment point of view, but ends up being a weakness if you have multiple clusters to monitor. \u003c/p\u003e\u003cp\u003eUpdates\u003c/p\u003e\u003cp\u003ePrometheus stores rules within a static file, which means you have to reboot your Prometheus instances in order to apply rules file updates.  \u003c/p\u003e\u003ch3\u003eUsing Prometheus and Thanos to address some (but not all) needs \u003c/h3\u003e\u003cp\u003eAfter we ran into the issues raised above, we searched for ways to address them and found \u003ca href=\"http://thanos.io\" target=\"_blank\"\u003eThanos\u003c/a\u003e. Thanos is an open-source project released in 2018 by \u003ca href=\"https://www.improbable.io/\" target=\"_blank\"\u003eImprobable.\u003c/a\u003e It helped us with multi-cluster management and data storage. Prometheus metrics can be sent to object storage services such as Google Cloud Storage, Azure Blob Storage, or AWS’ S3. \u003c/p\u003e\u003cp\u003eLike Prometheus, Thanos’ architecture is extensible by design:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_mdm.0484037508910657.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 mdm.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_mdm.0484037508910657.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003eArchitecture diagram from \u003ca href=\"https://github.com/thanos-io/thanos\"\u003eThanos’ Github page\u003c/a\u003e\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eUsing Prometheus and Thanos together helped us solve the following issues, which we noted above:\u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eRetention\u003c/i\u003e\u003c/b\u003e \u003c/p\u003e\u003cp\u003eUnlike Prometheus, Thanos is query-based instead of collection-based. Thanos sidecars are deployed alongside Prometheus instances and gather only metrics they are asked to expose. Thanos’ \u003ca href=\"https://thanos.io/tip/thanos/getting-started.md/\" target=\"_blank\"\u003edocumentation\u003c/a\u003e describes each role. If Prometheus retention has been configured, and metrics aren’t available on the local disk, it will ask its Store Gateway component to retrieve the metrics from the remote storage location. With this feature, we can address the metric retention issue raised with standalone Prometheus. Additionally, Thanos addresses some other common Prometheus needs:\u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eFederation\u003c/i\u003e \u003c/b\u003e\u003c/p\u003e\u003cp\u003eThanos allows us to set up a global view of our multi-cluster environments, whereas Prometheus could not. This requires us to set up one \u003ca href=\"https://thanos.io/tip/components/query.md/\" target=\"_blank\"\u003eQuerier\u003c/a\u003e per Kubernetes cluster and one Querier “federator,” which you can see in the diagram below.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_mdm.0648038612970773.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 mdm.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_mdm.0648038612970773.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003eOur Architecture Diagram\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe Querier components can be added to our multi-cluster environments via the addition of a simple configuration (see example code below) to get a global view of our metrics.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'apiVersion: apps/v1\\r\\nkind: Deployment\\r\\nmetadata:\\r\\n name: thanos-querier\\r\\n namespace: monitoring\\r\\n labels:\\r\\n app: thanos-querier\\r\\nspec:\\r\\n replicas: 3\\r\\n selector:\\r\\n matchLabels:\\r\\n app: thanos-querier\\r\\n template:\\r\\n metadata:\\r\\n labels:\\r\\n app: thanos-querier\\r\\n spec:\\r\\n containers:\\r\\n - name: thanos\\r\\n image: quay.io/thanos/thanos:v0.23.1\\r\\n args:\\r\\n - query\\r\\n - --log.level=debug\\r\\n - --query.replica-label=replica\\r\\n - --store=dnssrv+thanos-store-gateway:10901\\r\\n ports:\\r\\n - name: http\\r\\n containerPort: 10902\\r\\n - name: grpc\\r\\n containerPort: 10901\\r\\n livenessProbe:\\r\\n httpGet:\\r\\n port: http\\r\\n path: /-/healthy\\r\\n readinessProbe:\\r\\n httpGet:\\r\\n port: http\\r\\n path: /-/ready'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003e\u003csup\u003eThanos querier deployment configuration example \u003c/sup\u003e\u003c/i\u003e\u003c/p\u003e\u003ch3\u003ePrometheus and Thanos works, but increased complexity\u003c/h3\u003e\u003cp\u003eThanos helped us a lot by dealing with issues raised by standalone Prometheus. However, it came with a lot of components which increased our complexity. This led to the following downsides for us: \u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eDevelopment time\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003eThe multi-cluster, global environment that we described above required engineering resources and time to set up and maintain. Our engineers’ time is very valuable, and we would rather spend it developing new features instead of maintaining a state-of-the-art metrics system. \u003c/p\u003e\u003cp\u003e\u003cb\u003e\u003ci\u003eIncreased infrastructure load\u003c/i\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003eConfiguring each Kubernetes cluster with the Thanos Queriers to enable remote storage led to increased network bandwidth consumption. In addition, we had now more components added to Prometheus which meant more system consumption (CPU, RAM).   \u003c/p\u003e\u003cp\u003eAfter going through the initial deployment of Prometheus and then trying to solve problems on our own with yet another solution (Thanos), we decided that it was time to look into a managed alternative.\u003c/p\u003e\u003ch3\u003eSwitching to a managed service: Google Cloud Managed Service for Prometheus \u003c/h3\u003e\u003cp\u003eIn October of 2021, Google Cloud released the public preview of Managed Service for Prometheus, which we understood to be a drop-in replacement for an existing Prometheus stack.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_mdm.0426021708240395.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"3 mdm.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_mdm.0426021708240395.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eArchitecture diagram from \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eGoogle Cloud Managed Service for Prometheus\u003c/a\u003e documentation\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eMetrics for the service are retrieved by collectors, which are a fork of the open source Prometheus technology. The collectors send metrics to Google’s global time-series database named \u003ca href=\"https://research.google/pubs/pub50652/\" target=\"_blank\"\u003eMonarch\u003c/a\u003e, which removed the need for \u003ca href=\"https://thanos.io/\" target=\"_blank\"\u003eThanos\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eGoogle Cloud gave us two modes for using \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eManaged Service for Prometheus.\u003c/a\u003e In our case we are using managed collection, which allows us to reduce the complexity of deploying and managing Prometheus instances. \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eManaged Service for Prometheus\u003c/a\u003e provides an operator to configure Custom Resources (CRs) for scraping metrics, evaluating rules, and more. All our \u003ca href=\"https://prometheus.io/\" target=\"_blank\"\u003ePrometheus\u003c/a\u003e operations are handled by the Kubernetes operator. \u003c/p\u003e\u003cp\u003eIn addition, this solution supports more current Prometheus use cases (e.g. migrating from ServiceMonitor to PodMonitoring scrape configs).  \u003c/p\u003e\u003cp\u003eBecause we expect our metrics data to steadily grow alongside our company’s growth, we know that managing metrics at scale ourselves will likely become very painful. Google Cloud Managed Service for Prometheus helped us achieve scaled metrics infrastructure in a straightforward way, as a managed service, without devoting hundreds of servers to this effort. We want to focus our attention on building a functional and strategic metrics-based operations practice, instead of building a competency in managing long-term storage and Prometheus infrastructure.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_mdm.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"4 mdm.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_mdm.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eGlobal solution architecture\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eManaged Service for Prometheus is not the perfect solution for us yet as it can be deployed using the Google Cloud Console, gcloud cli, or the kubectl tool. We hear that Terraform support is in the works. We needed to add additional engineering resources to deploy it using Helm charts. We invested in building Helm charts to help automate some of our processes and are happy to share this code with you to make your journey easier. Please visit the \u003ca href=\"https://medium.com/maisonsdumonde/metrics-management-with-google-cloud-managed-service-for-prometheus-15e226f73257\" target=\"_blank\"\u003eMedium blog\u003c/a\u003e we wrote on this topic to see our code snippets.\u003c/p\u003e\u003ch3\u003eGet started on your journey \u003c/h3\u003e\u003cp\u003eTo sum it all up, while the structure, capabilities, and ecosystem of Prometheus are a good fit for our business, running the infrastructure and software was not worth it for us at scale. That’s why we chose Managed Service for Prometheus. We use Google Cloud’s fully managed service to monitor and manage alert notifications for our workloads. It scales with our needs and does not require management or maintenance.\u003c/p\u003e\u003cp\u003eIf you’re getting started soon with your own Managed Service for Prometheus deployment, we recommend you check out \u003ca href=\"https://youtu.be/X4qAEa8_JxQ\" target=\"_blank\"\u003ethis video\u003c/a\u003e that walks you through the first steps or visit the \u003ca href=\"https://cloud.google.com/managed-prometheus\"\u003eManaged Service for Prometheus page\u003c/a\u003e for more information. You can also take the fast track and join our teams to work on our stack. Check out \u003ca href=\"https://recrutement.maisonsdumonde.com/postulez/offres-demploi/#page-fr---search---set-vacsearchfront_function-004---fonction-informatique-web-16\" target=\"_blank\"\u003eour website\u003c/a\u003e — we have a lot of \u003ca href=\"https://www.welcometothejungle.com/fr/companies/maisons-du-monde/jobs\" target=\"_blank\"\u003eopen positions\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGoogle Cloud Managed Service for Prometheus is now generally available\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnnouncing the GA of Google Cloud Managed Service for Prometheus for the collection, storage, and querying of Kubernetes metrics.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus.max-2200x2200.jpg",
      "date_published": "2022-05-16T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eVictor Ladouceur\u003c/name\u003e\u003ctitle\u003eSRE, Maisons du Monde\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/databases/cloud-sql-alerting/",
      "title": "Alerting on error log messages in Cloud SQL for SQL Server",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003crouter-outlet\u003e\u003c/router-outlet\u003e\u003cdynamic-page\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c56=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e Bryan Hamilton \u003c/p\u003e\u003cp\u003e Database Engineer, SQL Server, Google Cloud \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e May 16, 2022 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c58=\"\"\u003e\u003cdiv _ngcontent-c58=\"\"\u003e\u003ch4 _ngcontent-c58=\"\"\u003e\u003cspan _ngcontent-c58=\"\"\u003eTry Google Cloud\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c58=\"\"\u003e\u003cspan _ngcontent-c58=\"\"\u003eStart building on Google Cloud with $300 in free credits and 20+ always free products.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c58=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"free trial\" track-metadata-eventdetail=\"https://cloud.google.com/free/\" href=\"https://cloud.google.com/free/\"\u003e\u003cspan _ngcontent-c58=\"\"\u003eFree Trial\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;With Cloud SQL for SQL Server, you can bring your existing SQL Server on-premises workloads to Google Cloud. Cloud SQL takes care of infrastructure, maintenance, and patching so you can focus on your application and users. A great way to take better care of your application is by monitoring the SQL Server error log for issues that may be affecting your users such as deadlocks, job failures, and changes in database health.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Cloud SQL for SQL Server and Cloud Operations Suite\u0026amp;#160;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;You can monitor and alert on messages in the Cloud SQL for SQL Server error log using the Google Cloud Operations Suite. Operations Suite is Google\u0026amp;#8217;s Cloud Observability solution allowing customers to have visibility into their infrastructure and applications. Using Cloud Operations Suite, you can monitor and alert for multiple instances at scale, and can set up alerting through your preferred method such as PagerDuty, Slack, email or a custom webhook.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The tools we will be using from the Operations Suite are Cloud Monitoring and Cloud Logging. Cloud Logging allows you to view logs from applications and services and allows you to create custom metrics from those logs. Cloud Monitoring allows you to create alerting policies to notify you when metrics, health check, and uptime check results meet specified criteria. To demonstrate how this works, we will enable deadlock detection on our Cloud SQL Instance, create a log based metric to monitor when deadlocks are detected, and create an alerting policy on the newly created log based metric. The architecture for monitoring SQL Server error log messages is shown below:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWith Cloud SQL for SQL Server, you can bring your existing SQL Server on-premises workloads to Google Cloud. Cloud SQL takes care of infrastructure, maintenance, and patching so you can focus on your application and users. A great way to take better care of your application is by monitoring the SQL Server error log for issues that may be affecting your users such as deadlocks, job failures, and changes in database health.\u003c/p\u003e\u003ch3\u003eCloud SQL for SQL Server and Cloud Operations Suite \u003c/h3\u003e\u003cp\u003eYou can monitor and alert on messages in the Cloud SQL for SQL Server error log using the Google Cloud Operations Suite. Operations Suite is Google’s Cloud Observability solution allowing customers to have visibility into their infrastructure and applications. Using Cloud Operations Suite, you can monitor and alert for multiple instances at scale, and can set up alerting through your preferred method such as PagerDuty, Slack, email or a custom webhook. \u003c/p\u003e\u003cp\u003eThe tools we will be using from the Operations Suite are Cloud Monitoring and Cloud Logging. Cloud Logging allows you to view logs from applications and services and allows you to create custom metrics from those logs. Cloud Monitoring allows you to create alerting policies to notify you when metrics, health check, and uptime check results meet specified criteria. To demonstrate how this works, we will enable deadlock detection on our Cloud SQL Instance, create a log based metric to monitor when deadlocks are detected, and create an alerting policy on the newly created log based metric. The architecture for monitoring SQL Server error log messages is shown below:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Configuration of Cloud SQL for SQL Server\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;What you will need:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/sql/docs/sqlserver/quickstart\u0026#34;\u0026gt;Cloud SQL for SQL Server\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Azure Data Studio\u0026lt;/a\u0026gt;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/sql/docs/sqlserver/sql-proxy\u0026#34;\u0026gt;Cloud SQL Proxy\u0026amp;#160;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Email Address for alert message\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Let\u0026#39;s break down how you can set this up. First you need a Cloud SQL for SQL Server instance here are the steps to set up one quickly:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;1 .In the Google Cloud Console, go to the \u0026lt;a href=\u0026#34;https://console.cloud.google.com/sql\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Cloud SQL Instances\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt; page.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;2. Click \u0026lt;b\u0026gt;Create Instance\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;3. Click \u0026lt;b\u0026gt;Choose SQL Server\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;4. Enter a name for \u0026lt;b\u0026gt;Instance ID\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;5. Enter a password for the sqlserver user.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;6. Expand \u0026lt;b\u0026gt;Show Configuration Options\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;7. Under \u0026lt;b\u0026gt;Flags and Parameters\u0026lt;/b\u0026gt; add the following trace flags:\u0026lt;br\u0026gt;a. 1222\u0026lt;br\u0026gt;b. 1204\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;8. Click \u0026lt;b\u0026gt;Create Instance\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you already have a Cloud SQL for SQL Server instance you would need to edit your Cloud SQL for SQL Server Instance.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eConfiguration of Cloud SQL for SQL Server\u003c/h3\u003e\u003cp\u003eWhat you will need: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/quickstart\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/sql/docs/sqlserver/quickstart\" track-metadata-module=\"post\"\u003eCloud SQL for SQL Server\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://docs.microsoft.com\" track-metadata-module=\"post\"\u003eAzure Data Studio\u003c/a\u003e \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/sql-proxy\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/sql/docs/sqlserver/sql-proxy\" track-metadata-module=\"post\"\u003eCloud SQL Proxy \u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEmail Address for alert message\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLet\u0026#39;s break down how you can set this up. First you need a Cloud SQL for SQL Server instance here are the steps to set up one quickly:\u003c/p\u003e\u003cp\u003e1 .In the Google Cloud Console, go to the \u003ca href=\"https://console.cloud.google.com/sql\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://console.cloud.google.com/sql\" track-metadata-module=\"post\"\u003e\u003cb\u003eCloud SQL Instances\u003c/b\u003e\u003c/a\u003e page.\u003c/p\u003e\u003cp\u003e2. Click \u003cb\u003eCreate Instance\u003c/b\u003e.\u003c/p\u003e\u003cp\u003e3. Click \u003cb\u003eChoose SQL Server\u003c/b\u003e.\u003c/p\u003e\u003cp\u003e4. Enter a name for \u003cb\u003eInstance ID\u003c/b\u003e.\u003c/p\u003e\u003cp\u003e5. Enter a password for the sqlserver user.\u003c/p\u003e\u003cp\u003e6. Expand \u003cb\u003eShow Configuration Options\u003c/b\u003e\u003c/p\u003e\u003cp\u003e7. Under \u003cb\u003eFlags and Parameters\u003c/b\u003e add the following trace flags:\u003cbr/\u003ea. 1222\u003cbr/\u003eb. 1204\u003c/p\u003e\u003cp\u003e8. Click \u003cb\u003eCreate Instance\u003c/b\u003e.\u003c/p\u003e\u003cp\u003eIf you already have a Cloud SQL for SQL Server instance you would need to edit your Cloud SQL for SQL Server Instance.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eIn the edit screen you will need to go to “Flags and parameters” to add and enable SQL Server trace flags 1204 and 1222. These flags enable deadlock detection messages into the SQL Server error log. Your instance will need to be restarted after this change. More details on editing your Cloud SQL for SQL Server instance can be found \u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/edit-instance\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/sql/docs/sqlserver/edit-instance\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Connecting to your Cloud SQL for SQL Server instance\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Perform the following steps to connect to your Cloud SQL for SQL Server Instance from your local machine.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;1. Install the \u0026lt;a href=\u0026#34;https://cloud.google.com/sdk/docs\u0026#34;\u0026gt;Google Cloud CLI\u0026lt;/a\u0026gt;. The Google Cloud CLI provides the gcloud CLI to interact with Cloud SQL and other Google Cloud services. The gcloud CLI uses the Admin API to access Cloud SQL, so you must \u0026lt;a href=\u0026#34;https://cloud.google.com/sql/docs/sqlserver/admin-api#enabling_the_api\u0026#34;\u0026gt;Enable the Admin API\u0026lt;/a\u0026gt; before using the gcloud CLI to access Cloud SQL.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;2. In a bash shell command prompt or in Windows PowerShell, run the following command to initialize the gcloud CLI:\u0026amp;#160;gcloud auth login\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;3. Run the following command to authenticate the gcloud CLI:\u0026amp;#160;gcloud auth login\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;4. Download and install the Cloud SQL Auth proxy (see \u0026lt;a href=\u0026#34;https://cloud.google.com/sql/docs/sqlserver/connect-admin-proxy#install\u0026#34;\u0026gt;Installing the Cloud SQL Auth proxy\u0026lt;/a\u0026gt;). Note the location of the Cloud SQL Auth proxy because you will run the Cloud SQL Auth proxy in the next step.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;5. Run the Cloud SQL Auth proxy by using a bash shell command prompt (or by using Windows PowerShell). Specifically, run the following command, replacing Instance-connection-name with the corresponding value from the Google Cloud Console\u0026#39;s Overview tab (for your instance):\u0026amp;#160;./cloud_sql_proxy -instances=INSTANCE_CONNECTION_NAME=tcp:1433\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;6. In Azure Data Studio Create a New Connection\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eConnecting to your Cloud SQL for SQL Server instance\u003c/h3\u003e\u003cp\u003ePerform the following steps to connect to your Cloud SQL for SQL Server Instance from your local machine.\u003c/p\u003e\u003cp\u003e1. Install the \u003ca href=\"https://cloud.google.com/sdk/docs\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/sdk/docs\" track-metadata-module=\"post\"\u003eGoogle Cloud CLI\u003c/a\u003e. The Google Cloud CLI provides the gcloud CLI to interact with Cloud SQL and other Google Cloud services. The gcloud CLI uses the Admin API to access Cloud SQL, so you must \u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/admin-api#enabling_the_api\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/sql/docs/sqlserver/admin-api#enabling_the_api\" track-metadata-module=\"post\"\u003eEnable the Admin API\u003c/a\u003e before using the gcloud CLI to access Cloud SQL.\u003c/p\u003e\u003cp\u003e2. In a bash shell command prompt or in Windows PowerShell, run the following command to initialize the gcloud CLI: gcloud auth login \u003c/p\u003e\u003cp\u003e3. Run the following command to authenticate the gcloud CLI: gcloud auth login\u003c/p\u003e\u003cp\u003e4. Download and install the Cloud SQL Auth proxy (see \u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/connect-admin-proxy#install\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/sql/docs/sqlserver/connect-admin-proxy#install\" track-metadata-module=\"post\"\u003eInstalling the Cloud SQL Auth proxy\u003c/a\u003e). Note the location of the Cloud SQL Auth proxy because you will run the Cloud SQL Auth proxy in the next step.\u003c/p\u003e\u003cp\u003e5. Run the Cloud SQL Auth proxy by using a bash shell command prompt (or by using Windows PowerShell). Specifically, run the following command, replacing Instance-connection-name with the corresponding value from the Google Cloud Console\u0026#39;s Overview tab (for your instance): ./cloud_sql_proxy -instances=INSTANCE_CONNECTION_NAME=tcp:1433\u003c/p\u003e\u003cp\u003e6. In Azure Data Studio Create a New Connection\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;7. Enter the following values in the \u0026lt;b\u0026gt;Connection\u0026lt;/b\u0026gt; dialog:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;a. For Server Type, enter \u0026lt;b\u0026gt;Microsoft SQL Server\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;b. For Server, enter 127.0.0.1 as the IP address of your SQL Server instance.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;For TCP connections, the Cloud SQL Auth proxy listens on localhost(127.0.0.1) by default and since we are using Cloud SQL Auth Proxy to connect Azure Data Studio to our Cloud SQL instance that is the IP address we must use.\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;c. For Authentication, enter \u0026lt;b\u0026gt;SQL Login\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;d. For Login, enter \u0026lt;b\u0026gt;sqlserver\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;e. For Password, enter the password used when the instance was created.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e7. Enter the following values in the \u003cb\u003eConnection\u003c/b\u003e dialog:\u003c/p\u003e\u003cp\u003ea. For Server Type, enter \u003cb\u003eMicrosoft SQL Server\u003c/b\u003e\u003c/p\u003e\u003cp\u003eb. For Server, enter 127.0.0.1 as the IP address of your SQL Server instance.\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor TCP connections, the Cloud SQL Auth proxy listens on localhost(127.0.0.1) by default and since we are using Cloud SQL Auth Proxy to connect Azure Data Studio to our Cloud SQL instance that is the IP address we must use.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003ec. For Authentication, enter \u003cb\u003eSQL Login\u003c/b\u003e.\u003c/p\u003e\u003cp\u003ed. For Login, enter \u003cb\u003esqlserver\u003c/b\u003e.\u003c/p\u003e\u003cp\u003ee. For Password, enter the password used when the instance was created.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;8. Click Connect\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Creating a deadlock\u0026amp;#160;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Now that you are connected to Azure Data Studio you can run the follow T-SQL code to create temporary tables on the SQL Server instance.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e8. Click Connect \u003c/p\u003e\u003ch3\u003eCreating a deadlock \u003c/h3\u003e\u003cp\u003eNow that you are connected to Azure Data Studio you can run the follow T-SQL code to create temporary tables on the SQL Server instance.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c61=\"\"\u003e\u003cpre _ngcontent-c61=\"\"\u003e  \u003ccode _ngcontent-c61=\"\"\u003eCREATE TABLE ##Product (\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e   ProductId INT IDENTITY,\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e   ProductName VARCHAR(10),\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e   Description VARCHAR(12)\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e)\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eGO\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eINSERT INTO ##Product (ProductName, Description)\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eVALUES (\u0026#39;Boat\u0026#39;, \u0026#39;Water\u0026#39;), (\u0026#39;Plane\u0026#39;, \u0026#39;Air\u0026#39;), (\u0026#39;Car\u0026#39;, \u0026#39;Ground\u0026#39;)\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eGO\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eCREATE TABLE ##Vendor(\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e   VendorId INT IDENTITY,\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e   VendorName VARCHAR(10),\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e   State VARCHAR(2)\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e)\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eGO\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eINSERT INTO ##Vendor (VendorName, State)\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eVALUES (\u0026#39;XYZ\u0026#39;, \u0026#39;NY\u0026#39;), (\u0026#39;ABC\u0026#39;, \u0026#39;OH\u0026#39;)\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eGO\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eNext to create a deadlock you will need to open two query sessions in Azure Data Studio and you must run each command one step at a time in the order specified here:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eYou should receive an error saying one of your sessions was deadlocked. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Viewing the error log in Log Explorer\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Now we can view the SQL Server Error Log by going to Cloud Logging in the Google Cloud Console. Logging can be found in the Operations section of the navigation bar or you can type \u0026amp;#8220;logging\u0026amp;#8221; into the search bar in Google Cloud Console.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eViewing the error log in Log Explorer\u003c/h3\u003e\u003cp\u003eNow we can view the SQL Server Error Log by going to Cloud Logging in the Google Cloud Console. Logging can be found in the Operations section of the navigation bar or you can type “logging” into the search bar in Google Cloud Console.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Now in Cloud Logging Log Explorer section you will want to create a query to filter the proper results:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Resource should be \u0026amp;#8594; Cloud SQL Database \u0026amp;#8594; Cloud SQL For SQL Server Instance Name\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Log should be \u0026amp;#8594; Cloud SQL Log \u0026amp;#8594; sqlserver.err\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Now you should be able to see the deadlock messages in the log.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eNow in Cloud Logging Log Explorer section you will want to create a query to filter the proper results:\u003c/p\u003e\u003cp\u003eResource should be → Cloud SQL Database → Cloud SQL For SQL Server Instance Name\u003c/p\u003e\u003cp\u003eLog should be → Cloud SQL Log → sqlserver.err \u003c/p\u003e\u003cp\u003eNow you should be able to see the deadlock messages in the log.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Creating a custom log-based metric and alerting policy\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;To identify a deadlock message to use for your custom metric, we should create a custom query filter in log explorer. You can enter the query below into log explorer.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eCreating a custom log-based metric and alerting policy\u003c/h3\u003e\u003cp\u003eTo identify a deadlock message to use for your custom metric, we should create a custom query filter in log explorer. You can enter the query below into log explorer.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c61=\"\"\u003e\u003cpre _ngcontent-c61=\"\"\u003e  \u003ccode _ngcontent-c61=\"\"\u003eresource.type=\u0026#34;cloudsql_database\u0026#34; resource.labels.database_id=\u0026#34;\u0026lt;YourGoogleCloudProject\u0026gt;:\u0026lt;YourCloudSQLInstance\u0026gt;\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003elogName=\u0026#34;projects/\u0026lt;YourGoogleCloudProject\u0026gt;/logs/cloudsql.googleapis.com%2Fsqlserver.err\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003etextPayload=~\u0026#34;Deadlock encountered .... Printing deadlock information\u0026#34;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026amp;lt;YourGoogleCloudProject\u0026amp;gt; is the name of the project your Cloud SQL instance is in and \u0026amp;lt;YourCloudSQLInstance\u0026amp;gt; is the name of your Cloud SQL for SQL Server instance.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Now you will see the single deadlock log entry. In the query results section there is an \u0026lt;b\u0026gt;action\u0026lt;/b\u0026gt; button on the right hand side. Click action and select \u0026amp;#8220;Create metric\u0026amp;#8221;.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u0026lt;YourGoogleCloudProject\u0026gt; is the name of the project your Cloud SQL instance is in and \u0026lt;YourCloudSQLInstance\u0026gt; is the name of your Cloud SQL for SQL Server instance. \u003c/p\u003e\u003cp\u003eNow you will see the single deadlock log entry. In the query results section there is an \u003cb\u003eaction\u003c/b\u003e button on the right hand side. Click action and select “Create metric”.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eThis will open a new window called \u003cb\u003eCreate logs metric\u003c/b\u003e. Here you can give your custom metric a name and description. Keep it as a counter metric and leave the unit as 1. Add any \u003ca href=\"https://cloud.google.com/resource-manager/docs/creating-managing-labels\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/resource-manager/docs/creating-managing-labels\" track-metadata-module=\"post\"\u003elabels\u003c/a\u003e you like and click \u003cb\u003eCreate Metric\u003c/b\u003e. A label is a key-value pair that helps you organize your Google Cloud resources.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;This will give you a new user-defined metric to monitor and track deadlocks. In the User-defined metrics section, click on the three dots on the right side of your custom metric name. You will see options to \u0026lt;b\u0026gt;View in Metrics Explorer\u0026lt;/b\u0026gt; and \u0026lt;b\u0026gt;Create alert from metric\u0026lt;/b\u0026gt;. If you want to view the metric in Metric Explorer you will need to trigger a new deadlock to see data.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Now, let\u0026amp;#8217;s create an alert policy. Click on \u0026lt;b\u0026gt;Create alert from metric\u0026lt;/b\u0026gt; to define an alerting policy for your new deadlock metric.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThis will give you a new user-defined metric to monitor and track deadlocks. In the User-defined metrics section, click on the three dots on the right side of your custom metric name. You will see options to \u003cb\u003eView in Metrics Explorer\u003c/b\u003e and \u003cb\u003eCreate alert from metric\u003c/b\u003e. If you want to view the metric in Metric Explorer you will need to trigger a new deadlock to see data. \u003c/p\u003e\u003cp\u003eNow, let’s create an alert policy. Click on \u003cb\u003eCreate alert from metric\u003c/b\u003e to define an alerting policy for your new deadlock metric.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eClicking the \u003cb\u003eCreate alert from metric\u003c/b\u003e link should have taken you straight to the alerting policy UI, where you can create an alert and identify specific conditions in which that alert should fire. In the condition section, your custom metric should have already been selected for you. You can leave everything as default and then select \u003cb\u003eNotifications and name\u003c/b\u003e.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Next decide who should be notified when this alert is triggered. Before you do that you need to set a \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/support/notification-options#creating_channels\u0026#34;\u0026gt;notification channel\u0026lt;/a\u0026gt;. Notification channels can be an email address or it can be various integration tools such as Slack and PagerDuty.\u0026amp;#160; After you select who should be notified, name your alert and add instructions on how to resolve the alert. Now save the alert and you are done! I would recommend you test out the new alert by forcing another deadlock. Congratulations - now you know how to create alerts based on SQL Server Error Log messages.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;You can create these types of alerts for more than just deadlocks: you can set alerts to monitor for other messages that show up in the error log such as crash dumps, connections issues, and corruption.\u0026amp;#160; You can also create alerts based on SQL Server Agent Log messages.\u0026amp;#160; Here are a few more examples listed below with the string from the SQL error logs that you can use as the text Payload in your custom metric.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Agent XPs disabled:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;SQL Server Error Logs will include the string \u0026lt;code\u0026gt;\u0026amp;#34;Configuration option \u0026#39;Agent XPs\u0026#39; changed from 1 to 0.\u0026amp;#34;\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;SQL Server Agent Status:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;SQL Server Agent Logs will include the string \u0026lt;code\u0026gt;\u0026amp;#8220;SQLServerAgent terminated\u0026amp;#34;\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Job Failures:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;SQL Server Agent Logs will include a string similar to\u0026amp;#160; \u0026amp;#8220;\u0026lt;code\u0026gt;SQL Server Scheduled Job \u0026#39;demo\u0026#39; (0xB83611A22D4FD74B8900ADDFDC9CDD9C) - Status: Failed - Invoked on:\u0026lt;/code\u0026gt;\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Option below needs to be checked or set using tsql\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/ul\u0026gt;\"\u003e\u003cp\u003eNext decide who should be notified when this alert is triggered. Before you do that you need to set a \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options#creating_channels\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/support/notification-options#creating_channels\" track-metadata-module=\"post\"\u003enotification channel\u003c/a\u003e. Notification channels can be an email address or it can be various integration tools such as Slack and PagerDuty.  After you select who should be notified, name your alert and add instructions on how to resolve the alert. Now save the alert and you are done! I would recommend you test out the new alert by forcing another deadlock. Congratulations - now you know how to create alerts based on SQL Server Error Log messages. \u003c/p\u003e\u003cp\u003eYou can create these types of alerts for more than just deadlocks: you can set alerts to monitor for other messages that show up in the error log such as crash dumps, connections issues, and corruption.  You can also create alerts based on SQL Server Agent Log messages.  Here are a few more examples listed below with the string from the SQL error logs that you can use as the text Payload in your custom metric. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eAgent XPs disabled:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSQL Server Error Logs will include the string \u003ccode\u003e\u0026#34;Configuration option \u0026#39;Agent XPs\u0026#39; changed from 1 to 0.\u0026#34;\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003eSQL Server Agent Status:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSQL Server Agent Logs will include the string \u003ccode\u003e“SQLServerAgent terminated\u0026#34;\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003eJob Failures:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSQL Server Agent Logs will include a string similar to  “\u003ccode\u003eSQL Server Scheduled Job \u0026#39;demo\u0026#39; (0xB83611A22D4FD74B8900ADDFDC9CDD9C) - Status: Failed - Invoked on:\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOption below needs to be checked or set using tsql\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Database Status:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;SQL Server Error logs will include one of the following strings:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;\u0026lt;code\u0026gt;Database % cannot be opened. It has been marked SUSPECT\u0026lt;/code\u0026gt;\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;\u0026lt;code\u0026gt;Database % database is in emergency or suspect mode\u0026lt;/code\u0026gt;\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;\u0026lt;code\u0026gt;database % is marked EMERGENCY_MODE\u0026lt;/code\u0026gt;\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;\u0026lt;code\u0026gt;Database % cannot be opened because it is offline.\u0026lt;/code\u0026gt;\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;\u0026lt;code\u0026gt;Setting database option OFFLINE to ON for database\u0026lt;/code\u0026gt;\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Custom error messages\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Using SQL Agent jobs, the \u0026lt;a href=\u0026#34;https://docs.microsoft.com/en-us/sql/t-sql/language-elements/raiserror-transact-sql?view=sql-server-ver15\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;RAISERROR\u0026lt;/a\u0026gt; with log command can be used to write custom messages in the SQL error logs. This could be triggered when any application or database condition is met. One way to do this is to create a SQL Agent job and define a job step with a simple query like the one below. \u0026lt;br\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\"\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDatabase Status:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSQL Server Error logs will include one of the following strings:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003eDatabase % cannot be opened. It has been marked SUSPECT\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003eDatabase % database is in emergency or suspect mode\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003edatabase % is marked EMERGENCY_MODE\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003eDatabase % cannot be opened because it is offline.\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003eSetting database option OFFLINE to ON for database\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003eCustom error messages\u003c/p\u003e\u003c/li\u003e\u003cli\u003eUsing SQL Agent jobs, the \u003ca href=\"https://docs.microsoft.com/en-us/sql/t-sql/language-elements/raiserror-transact-sql?view=sql-server-ver15\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://docs.microsoft.com\" track-metadata-module=\"post\"\u003eRAISERROR\u003c/a\u003e with log command can be used to write custom messages in the SQL error logs. This could be triggered when any application or database condition is met. One way to do this is to create a SQL Agent job and define a job step with a simple query like the one below. \u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c61=\"\"\u003e\u003cpre _ngcontent-c61=\"\"\u003e  \u003ccode _ngcontent-c61=\"\"\u003edeclare\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  @LongRunningJobThreshold int=300,\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  @runningtime int=0\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e \n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eselect @runningtime=max(er.total_elapsed_time)/1000\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003efrom sys.dm_exec_requests er\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003einner join sys.sysprocesses p on er.session_id = p.spid\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003ewhere p.program_name like \u0026#39;%SQLAgent%\u0026#39;\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e \n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eif @runningtime \u0026gt; @LongRunningJobThreshold\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003eRAISERROR (\u0026#39;long running sql agent job\u0026#39;,16,1) with LOG\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cp\u003eThen schedule this to run every couple of minutes. This will produce an error log  message and textPayload as in the image below. The same steps can be used for alerting and monitoring as described above for notifications\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c59=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/dynamic-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWith Cloud SQL for SQL Server, you can bring your existing SQL Server on-premises workloads to Google Cloud. Cloud SQL takes care of infrastructure, maintenance, and patching so you can focus on your application and users. A great way to take better care of your application is by monitoring the SQL Server error log for issues that may be affecting your users such as deadlocks, job failures, and changes in database health.\u003c/p\u003e\u003ch3\u003eCloud SQL for SQL Server and Cloud Operations Suite \u003c/h3\u003e\u003cp\u003eYou can monitor and alert on messages in the Cloud SQL for SQL Server error log using the Google Cloud Operations Suite. Operations Suite is Google’s Cloud Observability solution allowing customers to have visibility into their infrastructure and applications. Using Cloud Operations Suite, you can monitor and alert for multiple instances at scale, and can set up alerting through your preferred method such as PagerDuty, Slack, email or a custom webhook. \u003c/p\u003e\u003cp\u003eThe tools we will be using from the Operations Suite are Cloud Monitoring and Cloud Logging. Cloud Logging allows you to view logs from applications and services and allows you to create custom metrics from those logs. Cloud Monitoring allows you to create alerting policies to notify you when metrics, health check, and uptime check results meet specified criteria. To demonstrate how this works, we will enable deadlock detection on our Cloud SQL Instance, create a log based metric to monitor when deadlocks are detected, and create an alerting policy on the newly created log based metric. The architecture for monitoring SQL Server error log messages is shown below:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_CloudSQLErrorLog.0444023208200400.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_CloudSQLErrorLog.0444023208200400.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eConfiguration of Cloud SQL for SQL Server\u003c/h3\u003e\u003cp\u003eWhat you will need: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/quickstart\"\u003eCloud SQL for SQL Server\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15\" target=\"_blank\"\u003eAzure Data Studio\u003c/a\u003e \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/sql-proxy\"\u003eCloud SQL Proxy \u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEmail Address for alert message\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eLet's break down how you can set this up. First you need a Cloud SQL for SQL Server instance here are the steps to set up one quickly:\u003c/p\u003e\u003cp\u003e1 .In the Google Cloud Console, go to the \u003ca href=\"https://console.cloud.google.com/sql\"\u003e\u003cb\u003eCloud SQL Instances\u003c/b\u003e\u003c/a\u003e page.\u003c/p\u003e\u003cp\u003e2. Click \u003cb\u003eCreate Instance\u003c/b\u003e.\u003c/p\u003e\u003cp\u003e3. Click \u003cb\u003eChoose SQL Server\u003c/b\u003e.\u003c/p\u003e\u003cp\u003e4. Enter a name for \u003cb\u003eInstance ID\u003c/b\u003e.\u003c/p\u003e\u003cp\u003e5. Enter a password for the sqlserver user.\u003c/p\u003e\u003cp\u003e6. Expand \u003cb\u003eShow Configuration Options\u003c/b\u003e\u003c/p\u003e\u003cp\u003e7. Under \u003cb\u003eFlags and Parameters\u003c/b\u003e add the following trace flags:\u003cbr/\u003ea. 1222\u003cbr/\u003eb. 1204\u003c/p\u003e\u003cp\u003e8. Click \u003cb\u003eCreate Instance\u003c/b\u003e.\u003c/p\u003e\u003cp\u003eIf you already have a Cloud SQL for SQL Server instance you would need to edit your Cloud SQL for SQL Server Instance.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_CloudSQLErrorLog.1000064320000386.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_CloudSQLErrorLog.1000064320000386.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn the edit screen you will need to go to “Flags and parameters” to add and enable SQL Server trace flags 1204 and 1222. These flags enable deadlock detection messages into the SQL Server error log. Your instance will need to be restarted after this change. More details on editing your Cloud SQL for SQL Server instance can be found \u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/edit-instance\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_CloudSQLErrorLog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"3 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_CloudSQLErrorLog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eConnecting to your Cloud SQL for SQL Server instance\u003c/h3\u003e\u003cp\u003ePerform the following steps to connect to your Cloud SQL for SQL Server Instance from your local machine.\u003c/p\u003e\u003cp\u003e1. Install the \u003ca href=\"https://cloud.google.com/sdk/docs\"\u003eGoogle Cloud CLI\u003c/a\u003e. The Google Cloud CLI provides the gcloud CLI to interact with Cloud SQL and other Google Cloud services. The gcloud CLI uses the Admin API to access Cloud SQL, so you must \u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/admin-api#enabling_the_api\"\u003eEnable the Admin API\u003c/a\u003e before using the gcloud CLI to access Cloud SQL.\u003c/p\u003e\u003cp\u003e2. In a bash shell command prompt or in Windows PowerShell, run the following command to initialize the gcloud CLI: gcloud auth login \u003c/p\u003e\u003cp\u003e3. Run the following command to authenticate the gcloud CLI: gcloud auth login\u003c/p\u003e\u003cp\u003e4. Download and install the Cloud SQL Auth proxy (see \u003ca href=\"https://cloud.google.com/sql/docs/sqlserver/connect-admin-proxy#install\"\u003eInstalling the Cloud SQL Auth proxy\u003c/a\u003e). Note the location of the Cloud SQL Auth proxy because you will run the Cloud SQL Auth proxy in the next step.\u003c/p\u003e\u003cp\u003e5. Run the Cloud SQL Auth proxy by using a bash shell command prompt (or by using Windows PowerShell). Specifically, run the following command, replacing Instance-connection-name with the corresponding value from the Google Cloud Console's Overview tab (for your instance): ./cloud_sql_proxy -instances=INSTANCE_CONNECTION_NAME=tcp:1433\u003c/p\u003e\u003cp\u003e6. In Azure Data Studio Create a New Connection\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_CloudSQLErrorLog.1000063420000487.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"4 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_CloudSQLErrorLog.1000063420000487.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e7. Enter the following values in the \u003cb\u003eConnection\u003c/b\u003e dialog:\u003c/p\u003e\u003cp\u003ea. For Server Type, enter \u003cb\u003eMicrosoft SQL Server\u003c/b\u003e\u003c/p\u003e\u003cp\u003eb. For Server, enter 127.0.0.1 as the IP address of your SQL Server instance.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eFor TCP connections, the Cloud SQL Auth proxy listens on localhost(127.0.0.1) by default and since we are using Cloud SQL Auth Proxy to connect Azure Data Studio to our Cloud SQL instance that is the IP address we must use.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003ec. For Authentication, enter \u003cb\u003eSQL Login\u003c/b\u003e.\u003c/p\u003e\u003cp\u003ed. For Login, enter \u003cb\u003esqlserver\u003c/b\u003e.\u003c/p\u003e\u003cp\u003ee. For Password, enter the password used when the instance was created.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/5_CloudSQLErrorLog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"5 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/5_CloudSQLErrorLog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e8. Click Connect \u003c/p\u003e\u003ch3\u003eCreating a deadlock \u003c/h3\u003e\u003cp\u003eNow that you are connected to Azure Data Studio you can run the follow T-SQL code to create temporary tables on the SQL Server instance.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"CREATE TABLE ##Product (\\r\\n ProductId INT IDENTITY,\\r\\n ProductName VARCHAR(10),\\r\\n Description VARCHAR(12)\\r\\n)\\r\\nGO\\r\\nINSERT INTO ##Product (ProductName, Description)\\r\\nVALUES ('Boat', 'Water'), ('Plane', 'Air'), ('Car', 'Ground')\\r\\nGO\\r\\nCREATE TABLE ##Vendor(\\r\\n VendorId INT IDENTITY,\\r\\n VendorName VARCHAR(10),\\r\\n State VARCHAR(2)\\r\\n)\\r\\nGO\\r\\nINSERT INTO ##Vendor (VendorName, State)\\r\\nVALUES ('XYZ', 'NY'), ('ABC', 'OH')\\r\\nGO\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eNext to create a deadlock you will need to open two query sessions in Azure Data Studio and you must run each command one step at a time in the order specified here:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/6_CloudSQLErrorLog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"6 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/6_CloudSQLErrorLog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou should receive an error saying one of your sessions was deadlocked.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/7_CloudSQLErrorLog.1000067920000475.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"7 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/7_CloudSQLErrorLog.1000067920000475.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eViewing the error log in Log Explorer\u003c/h3\u003e\u003cp\u003eNow we can view the SQL Server Error Log by going to Cloud Logging in the Google Cloud Console. Logging can be found in the Operations section of the navigation bar or you can type “logging” into the search bar in Google Cloud Console.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/8_CloudSQLErrorLog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"8 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/8_CloudSQLErrorLog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eNow in Cloud Logging Log Explorer section you will want to create a query to filter the proper results:\u003c/p\u003e\u003cp\u003eResource should be → Cloud SQL Database → Cloud SQL For SQL Server Instance Name\u003c/p\u003e\u003cp\u003eLog should be → Cloud SQL Log → sqlserver.err \u003c/p\u003e\u003cp\u003eNow you should be able to see the deadlock messages in the log.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/9_CloudSQLErrorLog.1000064720000445.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"9 CloudSQLErrorLog.png\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/9_CloudSQLErrorLog.1000064720000445.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eCreating a custom log-based metric and alerting policy\u003c/h3\u003e\u003cp\u003eTo identify a deadlock message to use for your custom metric, we should create a custom query filter in log explorer. You can enter the query below into log explorer.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'resource.type=\"cloudsql_database\" resource.labels.database_id=\"\u0026lt;YourGoogleCloudProject\u0026gt;:\u0026lt;YourCloudSQLInstance\u0026gt;\"\\r\\nlogName=\"projects/\u0026lt;YourGoogleCloudProject\u0026gt;/logs/cloudsql.googleapis.com%2Fsqlserver.err\"\\r\\ntextPayload=~\"Deadlock encountered .... Printing deadlock information\"'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u0026lt;YourGoogleCloudProject\u0026gt; is the name of the project your Cloud SQL instance is in and \u0026lt;YourCloudSQLInstance\u0026gt; is the name of your Cloud SQL for SQL Server instance. \u003c/p\u003e\u003cp\u003eNow you will see the single deadlock log entry. In the query results section there is an \u003cb\u003eaction\u003c/b\u003e button on the right hand side. Click action and select “Create metric”.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/10_CloudSQLErrorLog.max-2800x2800.png\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"10 CloudSQLErrorLog.png\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/10_CloudSQLErrorLog.max-1000x1000.png\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis will open a new window called \u003cb\u003eCreate logs metric\u003c/b\u003e. Here you can give your custom metric a name and description. Keep it as a counter metric and leave the unit as 1. Add any \u003ca href=\"https://cloud.google.com/resource-manager/docs/creating-managing-labels\"\u003elabels\u003c/a\u003e you like and click \u003cb\u003eCreate Metric\u003c/b\u003e. A label is a key-value pair that helps you organize your Google Cloud resources.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"11 CloudSQLErrorLog.png\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/11_CloudSQLErrorLog.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis will give you a new user-defined metric to monitor and track deadlocks. In the User-defined metrics section, click on the three dots on the right side of your custom metric name. You will see options to \u003cb\u003eView in Metrics Explorer\u003c/b\u003e and \u003cb\u003eCreate alert from metric\u003c/b\u003e. If you want to view the metric in Metric Explorer you will need to trigger a new deadlock to see data. \u003c/p\u003e\u003cp\u003eNow, let’s create an alert policy. Click on \u003cb\u003eCreate alert from metric\u003c/b\u003e to define an alerting policy for your new deadlock metric.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/12_CloudSQLErrorLog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"12 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/12_CloudSQLErrorLog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eClicking the \u003cb\u003eCreate alert from metric\u003c/b\u003e link should have taken you straight to the alerting policy UI, where you can create an alert and identify specific conditions in which that alert should fire. In the condition section, your custom metric should have already been selected for you. You can leave everything as default and then select \u003cb\u003eNotifications and name\u003c/b\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/13_CloudSQLErrorLog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"13 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/13_CloudSQLErrorLog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eNext decide who should be notified when this alert is triggered. Before you do that you need to set a \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options#creating_channels\"\u003enotification channel\u003c/a\u003e. Notification channels can be an email address or it can be various integration tools such as Slack and PagerDuty.  After you select who should be notified, name your alert and add instructions on how to resolve the alert. Now save the alert and you are done! I would recommend you test out the new alert by forcing another deadlock. Congratulations - now you know how to create alerts based on SQL Server Error Log messages. \u003c/p\u003e\u003cp\u003eYou can create these types of alerts for more than just deadlocks: you can set alerts to monitor for other messages that show up in the error log such as crash dumps, connections issues, and corruption.  You can also create alerts based on SQL Server Agent Log messages.  Here are a few more examples listed below with the string from the SQL error logs that you can use as the text Payload in your custom metric. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eAgent XPs disabled:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSQL Server Error Logs will include the string \u003ccode\u003e\"Configuration option 'Agent XPs' changed from 1 to 0.\"\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003eSQL Server Agent Status:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSQL Server Agent Logs will include the string \u003ccode\u003e“SQLServerAgent terminated\"\u003c/code\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003eJob Failures:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSQL Server Agent Logs will include a string similar to  “\u003ccode\u003eSQL Server Scheduled Job 'demo' (0xB83611A22D4FD74B8900ADDFDC9CDD9C) - Status: Failed - Invoked on:\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOption below needs to be checked or set using tsql\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/14_CloudSQLErrorLog.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"14 CloudSQLErrorLog.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/14_CloudSQLErrorLog.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDatabase Status:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSQL Server Error logs will include one of the following strings:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003eDatabase % cannot be opened. It has been marked SUSPECT\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003eDatabase % database is in emergency or suspect mode\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003edatabase % is marked EMERGENCY_MODE\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003eDatabase % cannot be opened because it is offline.\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e“\u003ccode\u003eSetting database option OFFLINE to ON for database\u003c/code\u003e”\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003eCustom error messages\u003c/p\u003e\u003c/li\u003e\u003cli\u003eUsing SQL Agent jobs, the \u003ca href=\"https://docs.microsoft.com/en-us/sql/t-sql/language-elements/raiserror-transact-sql?view=sql-server-ver15\" target=\"_blank\"\u003eRAISERROR\u003c/a\u003e with log command can be used to write custom messages in the SQL error logs. This could be triggered when any application or database condition is met. One way to do this is to create a SQL Agent job and define a job step with a simple query like the one below. \u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"declare\\r\\n @LongRunningJobThreshold int=300,\\r\\n @runningtime int=0\\r\\n \\r\\nselect @runningtime=max(er.total_elapsed_time)/1000\\r\\nfrom sys.dm_exec_requests er\\r\\ninner join sys.sysprocesses p on er.session_id = p.spid\\r\\nwhere p.program_name like '%SQLAgent%'\\r\\n \\r\\nif @runningtime \u0026gt; @LongRunningJobThreshold\\r\\nRAISERROR ('long running sql agent job',16,1) with LOG\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThen schedule this to run every couple of minutes. This will produce an error log  message and textPayload as in the image below. The same steps can be used for alerting and monitoring as described above for notifications\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/15_CloudSQLErrorLog.max-2800x2800.png\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"15 CloudSQLErrorLog.png\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/15_CloudSQLErrorLog.max-1000x1000.png\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThanks for reading.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-05-16T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eBryan Hamilton\u003c/name\u003e\u003ctitle\u003eDatabase Engineer, SQL Server, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/managed-service-for-prometheus-offers-new-pricing-tier/",
      "title": "Introducing a high-usage tier for Managed Service for Prometheus",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Prometheus is considered the de facto standard for Kubernetes application metrics, but running it yourself can strain engineering time and infrastructure resources when your usage grows. In March, we announced the general availability of Google Cloud \u0026lt;a href=\u0026#34;https://cloud.google.com/managed-prometheus\u0026#34;\u0026gt;Managed Service for Prometheus\u0026lt;/a\u0026gt; to help you offload that burden, and today, we\u0026amp;#8217;re excited to announce a new low-cost, high-usage pricing tier designed for customers who are moving large volumes of Kubernetes metrics over to the service. Furthermore, we\u0026amp;#8217;re lowering the price of the current usage tiers.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whether you are a mature enterprise, a rapidly scaling digital native startup, or somewhere in between, this new pricing structure makes it an easier decision to scale your production Kubernetes metrics with Managed Service for Prometheus. Free up your engineering resources to concentrate on building your next big application, not your metrics infrastructure.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Introducing a high-usage tier and lower prices\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Since it launched \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\u0026#34;\u0026gt;just two months ago\u0026lt;/a\u0026gt;, companies of all shapes and sizes have begun to adopt Managed Service for Prometheus, but we heard that cost at high data volumes was an issue for customers that move their entire Kubernetes metrics operation to our service. That\u0026amp;#8217;s why we created a new tier, which is priced 50% lower than the previous highest-usage tier. You get scale and ease of use at a price that works.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We didn\u0026amp;#8217;t forget about customers with lower metric volumes, however. Whether you\u0026amp;#8217;re just getting started with Managed Service for Prometheus or you haven\u0026amp;#8217;t yet migrated all your Kubernetes metrics to our service, cost is always top of mind. As part of the new pricing, list prices for the lower-usage tiers are now 25% lower than they were originally.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;See the table below for comparisons:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003ePrometheus is considered the de facto standard for Kubernetes application metrics, but running it yourself can strain engineering time and infrastructure resources when your usage grows. In March, we announced the general availability of Google Cloud \u003ca href=\"https://cloud.google.com/managed-prometheus\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/managed-prometheus\" track-metadata-module=\"post\"\u003eManaged Service for Prometheus\u003c/a\u003e to help you offload that burden, and today, we’re excited to announce a new low-cost, high-usage pricing tier designed for customers who are moving large volumes of Kubernetes metrics over to the service. Furthermore, we’re lowering the price of the current usage tiers.\u003c/p\u003e\u003cp\u003eWhether you are a mature enterprise, a rapidly scaling digital native startup, or somewhere in between, this new pricing structure makes it an easier decision to scale your production Kubernetes metrics with Managed Service for Prometheus. Free up your engineering resources to concentrate on building your next big application, not your metrics infrastructure.\u003c/p\u003e\u003ch3\u003eIntroducing a high-usage tier and lower prices\u003c/h3\u003e\u003cp\u003eSince it launched \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\" track-metadata-module=\"post\"\u003ejust two months ago\u003c/a\u003e, companies of all shapes and sizes have begun to adopt Managed Service for Prometheus, but we heard that cost at high data volumes was an issue for customers that move their entire Kubernetes metrics operation to our service. That’s why we created a new tier, which is priced 50% lower than the previous highest-usage tier. You get scale and ease of use at a price that works. \u003c/p\u003e\u003cp\u003eWe didn’t forget about customers with lower metric volumes, however. Whether you’re just getting started with Managed Service for Prometheus or you haven’t yet migrated all your Kubernetes metrics to our service, cost is always top of mind. As part of the new pricing, list prices for the lower-usage tiers are now 25% lower than they were originally.\u003c/p\u003e\u003cp\u003eSee the table below for comparisons:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003ePrometheus is considered the de facto standard for Kubernetes application metrics, but running it yourself can strain engineering time and infrastructure resources when your usage grows. In March, we announced the general availability of Google Cloud \u003ca href=\"https://cloud.google.com/managed-prometheus\"\u003eManaged Service for Prometheus\u003c/a\u003e to help you offload that burden, and today, we’re excited to announce a new low-cost, high-usage pricing tier designed for customers who are moving large volumes of Kubernetes metrics over to the service. Furthermore, we’re lowering the price of the current usage tiers.\u003c/p\u003e\u003cp\u003eWhether you are a mature enterprise, a rapidly scaling digital native startup, or somewhere in between, this new pricing structure makes it an easier decision to scale your production Kubernetes metrics with Managed Service for Prometheus. Free up your engineering resources to concentrate on building your next big application, not your metrics infrastructure.\u003c/p\u003e\u003ch3\u003eIntroducing a high-usage tier and lower prices\u003c/h3\u003e\u003cp\u003eSince it launched \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\"\u003ejust two months ago\u003c/a\u003e, companies of all shapes and sizes have begun to adopt Managed Service for Prometheus, but we heard that cost at high data volumes was an issue for customers that move their entire Kubernetes metrics operation to our service. That’s why we created a new tier, which is priced 50% lower than the previous highest-usage tier. You get scale and ease of use at a price that works. \u003c/p\u003e\u003cp\u003eWe didn’t forget about customers with lower metric volumes, however. Whether you’re just getting started with Managed Service for Prometheus or you haven’t yet migrated all your Kubernetes metrics to our service, cost is always top of mind. As part of the new pricing, list prices for the lower-usage tiers are now 25% lower than they were originally.\u003c/p\u003e\u003cp\u003eSee the table below for comparisons:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_H8hBh7w.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Prometheus.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_H8hBh7w.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eNote that samples are counted per billing account. See full pricing details on the Google Cloud's operations suite \u003ca href=\"https://cloud.google.com/stackdriver/pricing\"\u003epricing page\u003c/a\u003e.\u003c/i\u003e\u003cbr/\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eEarly success at enterprise scale\u003c/h3\u003e\u003cp\u003eMaisons du Monde, a French furniture and home decor giant, \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/maisons-du-monde-improved-their-kubernetes-observability\" target=\"_blank\"\u003erecently published a story\u003c/a\u003e about their experience with Google Kubernetes Engine (GKE) application metrics. They started with open source Prometheus, then added Thanos to deal with Prometheus scaling issues, before finally choosing Managed Service for Prometheus.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-pull_quote\"\u003e\u003cdiv class=\"uni-pull-quote h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003cdiv class=\"uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cdiv class=\"uni-pull-quote__inner-wrapper h-c-copy h-c-copy\"\u003e\u003cq class=\"uni-pull-quote__text\"\u003eOur engineers’ time is very valuable and we would rather spend it developing new features instead of maintaining a state-of-the-art metrics system.\u003c/q\u003e \u003ccite class=\"uni-pull-quote__author\"\u003e\u003cspan class=\"uni-pull-quote__author-meta\"\u003e\u003cstrong class=\"h-u-font-weight-medium\"\u003eVictor Ladouceur\u003c/strong\u003e\u003cbr/\u003e SRE, Maisons du Monde\u003c/span\u003e\u003c/cite\u003e\u003c/div\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWhy Managed Service for Prometheus\u003c/h3\u003e\u003cp\u003eThe service is designed to be a drop-in replacement for running your own Prometheus stack, so you can gather, store, and alert on your metrics. Some of the benefits include: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eTwo-year retention of all metrics, included in the price\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCost-effective monitoring on a per-sample basis\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEasy \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources\"\u003ecost identification and attribution\u003c/a\u003e using Cloud Monitoring\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNo changes needed to existing Prometheus querying or alerting workflows, with \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\"\u003emanaged\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged\"\u003eself-deployed\u003c/a\u003e collection options\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAbility to view Prometheus and \u003ca href=\"https://cloud.google.com/blog/products/operations/in-depth-explanation-of-operational-metrics-at-google-cloud\"\u003eGoogle Cloud system metrics\u003c/a\u003e together\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eGet ready to scale your Kubernetes metrics\u003c/h3\u003e\u003cp\u003eIf you’re ready to learn more about how managed metrics can help you improve your Kubernetes monitoring, or if you’re ready to get hands on, check out the following resources:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eTry out our \u003ca href=\"https://go.qwiklabs.com/Google-Cloud-Workshops-Prometheus\" target=\"_blank\"\u003enew Managed Service for Prometheus Qwiklab\u003c/a\u003e at \u003cb\u003eno charge\u003c/b\u003e now through June 15. Walk through 4.5 hours of content covering migration, metrics collection, analysis, and cost saving strategies.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eJoin us tomorrow (May 17th) for our session, \u003ca href=\"https://cloudonair.withgoogle.com/events/kubecon-eu-2022\" target=\"_blank\"\u003eEasy, scalable metrics for Kubernetes with Managed Service for Prometheus\u003c/a\u003e, during our Day Zero event for \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/google-cloud-at-kubecon-eu-2022\"\u003eKubeCon EMEA 2022\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloudonair.withgoogle.com/events/managed-service-prometheus\" target=\"_blank\"\u003eRegister to attend a webinar\u003c/a\u003e on June 1, where we will talk about the service, the price cut, and how easy it is to migrate your metrics collection. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"http://g.co/cloud/managedprometheus\" target=\"_blank\"\u003eStart using the service today\u003c/a\u003e by visiting the onboarding documentation.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGoogle Cloud Managed Service for Prometheus is now generally available\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnnouncing the GA of Google Cloud Managed Service for Prometheus for the collection, storage, and querying of Kubernetes metrics.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus_HCKF6h9.max-2200x2200.jpg",
      "date_published": "2022-05-16T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eLee Yanco\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/data-analytics/simplify-your-splunk-dataflow-ops-with-improved-pipeline-observability/",
      "title": "New observability features for your Splunk Dataflow streaming pipelines",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003crouter-outlet\u003e\u003c/router-outlet\u003e\u003cdynamic-page\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c49=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003esplunk.jpg\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003carticle-cta _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\"\u003e\u003ch4 _ngcontent-c60=\"\"\u003e\u003cspan _ngcontent-c60=\"\"\u003eTry Google Cloud\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c60=\"\"\u003e\u003cspan _ngcontent-c60=\"\"\u003eStart building on Google Cloud with $300 in free credits and 20+ always free products.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c60=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"free trial\" track-metadata-eventdetail=\"https://cloud.google.com/free/\" href=\"https://cloud.google.com/free/\"\u003e\u003cspan _ngcontent-c60=\"\"\u003eFree Trial\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c62=\"\"\u003e\u003cdiv _ngcontent-c62=\"\" innerhtml=\"\u0026lt;p\u0026gt;We\u0026amp;#8217;re thrilled to announce several new observability features for the \u0026lt;a href=\u0026#34;https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk\u0026#34;\u0026gt;Pub/Sub to Splunk Dataflow template\u0026lt;/a\u0026gt; to help operators keep a tab on their streaming pipeline performance. \u0026lt;a href=\u0026#34;https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-enterprise-gcp\u0026#34;\u0026gt;Splunk Enterprise\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-cloud\u0026#34;\u0026gt;Splunk Cloud\u0026lt;/a\u0026gt; customers use the Splunk Dataflow template to \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow\u0026#34;\u0026gt;reliably export Google Cloud logs\u0026lt;/a\u0026gt; for in-depth analytics for security, IT or business use cases. With newly added metrics and improved logging for Splunk IO sink, it\u0026amp;#8217;s now easier to answer operational questions such as:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Is the Dataflow pipeline keeping up with the volume of logs generated?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;What is the latency and throughput (Event Per Second or EPS) when writing to Splunk?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;What is the response status breakdown of downstream Splunk HTTP Event Collector (HEC) and potential error messages?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;This critical visibility helps you derive your log export \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli\u0026#34;\u0026gt;service-level indicators (SLIs)\u0026lt;/a\u0026gt; and monitor for any pipeline performance regressions. You can also more easily root cause potential downstream failures between Dataflow \u0026amp;amp; Splunk such as Splunk HEC network connections or server issues, and fix the problem before it cascades. \u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To help you quickly chart these new metrics, we\u0026amp;#8217;ve included them in the custom dashboard as part of the updated \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Terraform module for Splunk Dataflow\u0026lt;/a\u0026gt;. You can use those Terraform templates to deploy the entire infrastructure for log export to Splunk, or just the \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Monitoring dashboard\u0026lt;/a\u0026gt; alone.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWe’re thrilled to announce several new observability features for the \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk\" track-metadata-module=\"post\"\u003ePub/Sub to Splunk Dataflow template\u003c/a\u003e to help operators keep a tab on their streaming pipeline performance. \u003ca href=\"https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-enterprise-gcp\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-enterprise-gcp\" track-metadata-module=\"post\"\u003eSplunk Enterprise\u003c/a\u003e and \u003ca href=\"https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-cloud\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-cloud\" track-metadata-module=\"post\"\u003eSplunk Cloud\u003c/a\u003e customers use the Splunk Dataflow template to \u003ca href=\"https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow\" track-metadata-module=\"post\"\u003ereliably export Google Cloud logs\u003c/a\u003e for in-depth analytics for security, IT or business use cases. With newly added metrics and improved logging for Splunk IO sink, it’s now easier to answer operational questions such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eIs the Dataflow pipeline keeping up with the volume of logs generated?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhat is the latency and throughput (Event Per Second or EPS) when writing to Splunk?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhat is the response status breakdown of downstream Splunk HTTP Event Collector (HEC) and potential error messages?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis critical visibility helps you derive your log export \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli\" track-metadata-module=\"post\"\u003eservice-level indicators (SLIs)\u003c/a\u003e and monitor for any pipeline performance regressions. You can also more easily root cause potential downstream failures between Dataflow \u0026amp; Splunk such as Splunk HEC network connections or server issues, and fix the problem before it cascades. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eTo help you quickly chart these new metrics, we’ve included them in the custom dashboard as part of the updated \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-splunk-log-export\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eTerraform module for Splunk Dataflow\u003c/a\u003e. You can use those Terraform templates to deploy the entire infrastructure for log export to Splunk, or just the \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eMonitoring dashboard\u003c/a\u003e alone.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c62=\"\"\u003e\u003cdiv _ngcontent-c62=\"\" innerhtml=\"\u0026lt;h2\u0026gt;More metrics\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;In your Dataflow Console, you may have noticed several new custom metrics (highlighted below) for launched jobs as of template version \u0026lt;code\u0026gt;2022-03-21-00_RC01\u0026lt;/code\u0026gt;, that is \u0026lt;code\u0026gt;gs://dataflow-templates/2022-03-21-00_RC01/Cloud_PubSub_to_Splunk\u0026lt;/code\u0026gt; or later:\u0026lt;/p\u0026gt;\"\u003e\u003ch2\u003eMore metrics\u003c/h2\u003e\u003cp\u003eIn your Dataflow Console, you may have noticed several new custom metrics (highlighted below) for launched jobs as of template version \u003ccode\u003e2022-03-21-00_RC01\u003c/code\u003e, that is \u003ccode\u003egs://dataflow-templates/2022-03-21-00_RC01/Cloud_PubSub_to_Splunk\u003c/code\u003e or later:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c62=\"\"\u003e\u003cdiv _ngcontent-c62=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Pipeline instrumentation\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Before we dive into the new metrics, let\u0026amp;#8217;s take a step back and go over the Splunk Dataflow job steps. The following flowchart represents the different stages that comprise a Splunk Dataflow job along with corresponding custom metrics:\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003ePipeline instrumentation\u003c/h3\u003e\u003cp\u003eBefore we dive into the new metrics, let’s take a step back and go over the Splunk Dataflow job steps. The following flowchart represents the different stages that comprise a Splunk Dataflow job along with corresponding custom metrics:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c62=\"\"\u003e\u003cdiv _ngcontent-c62=\"\" innerhtml=\"\u0026lt;p\u0026gt;In this pipeline, we utilize two types of \u0026lt;a href=\u0026#34;https://beam.apache.org/documentation/programming-guide/#types-of-metrics\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Apache Beam custom metrics\u0026lt;/a\u0026gt;:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;Counter metrics, labeled 1 through 10 above, used to count messages and requests (both successful and failed).\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Distribution metrics, labeled A through C above, used to report on distribution of request latency (both successful and failed) and batch size.\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Downstream request visibility\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Splunk Dataflow operators have relied on some of these pre-built custom metrics to monitor log messages progress through the different pipeline stages, particularly in the last stage \u0026lt;code\u0026gt;Write To Splunk\u0026lt;/code\u0026gt;, with metrics \u0026lt;code\u0026gt;outbound-successful-events\u0026lt;/code\u0026gt; (counter #6 above) and \u0026lt;code\u0026gt;outbound-failed-events\u0026lt;/code\u0026gt; (counter #7 above) to track the number of messages that were successfully exported (or not) to Splunk. While operators had visibility of the outbound message success rate, they lacked visibility at the HEC request level. Splunk Dataflow operators can now monitor not only the number of successful and failed HEC requests over time, but also the response status breakdown to determine if request failed due to a client request issue (e.g. invalid Splunk index or HEC token), or a transient network or Splunk issue (e.g. server busy or down) all from Dataflow Console with the addition of counters #7-10 above, that is:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;http-valid-requests\u0026lt;/code\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;http-invalid-requests\u0026lt;/code\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;http-server-error-requests\u0026lt;/code\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Splunk Dataflow operators can also now track average latency of downstream requests to Splunk HEC, as well as average request batch size, by using the new distribution metrics #A-C, that is:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;successful_write_to_splunk_latency_ms\u0026lt;/code\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;unsuccessful_write_to_splunk_latency_ms\u0026lt;/code\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;code\u0026gt;write_to_splunk_batch\u0026lt;/code\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Note that a Distribution metric in Beam is reported by Dataflow as four sub-metrics suffixed with _MAX, _MIN, _MEAN and _COUNT. That is why those 3 new distribution metrics translate to 12 new metrics in Cloud Monitoring, as you can see in the earlier job info screenshot from Dataflow Console. Dataflow \u0026lt;a href=\u0026#34;https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#custom_metrics\u0026#34;\u0026gt;currently does not support creating a histogram\u0026lt;/a\u0026gt; to visualize the breakdown of these metrics\u0026amp;#8217; values. Therefore, _MEAN metric is the only useful sub-metric for our purposes. As an all-time average value, _MEAN cannot be used to track changes over arbitrary time intervals (e.g. hourly), but it is useful to capture baseline, track trend or to compare different pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Dataflow custom metrics, including aforementioned metrics reported by Splunk Dataflow template, are a chargeable feature of Cloud Monitoring. For more information on metrics pricing, see \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/pricing#monitoring-costs\u0026#34;\u0026gt;Pricing for Cloud Monitoring\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Improved logging\u0026lt;/h2\u0026gt;\u0026lt;h3\u0026gt;Logging HEC errors\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;To further root cause downstream issues, HEC request errors are now adequately logged, including both response status code and message:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eIn this pipeline, we utilize two types of \u003ca href=\"https://beam.apache.org/documentation/programming-guide/#types-of-metrics\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://beam.apache.org\" track-metadata-module=\"post\"\u003eApache Beam custom metrics\u003c/a\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCounter metrics, labeled 1 through 10 above, used to count messages and requests (both successful and failed).\u003c/li\u003e\u003cli\u003eDistribution metrics, labeled A through C above, used to report on distribution of request latency (both successful and failed) and batch size. \u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eDownstream request visibility\u003c/h3\u003e\u003cp\u003eSplunk Dataflow operators have relied on some of these pre-built custom metrics to monitor log messages progress through the different pipeline stages, particularly in the last stage \u003ccode\u003eWrite To Splunk\u003c/code\u003e, with metrics \u003ccode\u003eoutbound-successful-events\u003c/code\u003e (counter #6 above) and \u003ccode\u003eoutbound-failed-events\u003c/code\u003e (counter #7 above) to track the number of messages that were successfully exported (or not) to Splunk. While operators had visibility of the outbound message success rate, they lacked visibility at the HEC request level. Splunk Dataflow operators can now monitor not only the number of successful and failed HEC requests over time, but also the response status breakdown to determine if request failed due to a client request issue (e.g. invalid Splunk index or HEC token), or a transient network or Splunk issue (e.g. server busy or down) all from Dataflow Console with the addition of counters #7-10 above, that is:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode\u003ehttp-valid-requests\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003ehttp-invalid-requests\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003ehttp-server-error-requests\u003c/code\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSplunk Dataflow operators can also now track average latency of downstream requests to Splunk HEC, as well as average request batch size, by using the new distribution metrics #A-C, that is:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode\u003esuccessful_write_to_splunk_latency_ms\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003eunsuccessful_write_to_splunk_latency_ms\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003ewrite_to_splunk_batch\u003c/code\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote that a Distribution metric in Beam is reported by Dataflow as four sub-metrics suffixed with _MAX, _MIN, _MEAN and _COUNT. That is why those 3 new distribution metrics translate to 12 new metrics in Cloud Monitoring, as you can see in the earlier job info screenshot from Dataflow Console. Dataflow \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#custom_metrics\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#custom_metrics\" track-metadata-module=\"post\"\u003ecurrently does not support creating a histogram\u003c/a\u003e to visualize the breakdown of these metrics’ values. Therefore, _MEAN metric is the only useful sub-metric for our purposes. As an all-time average value, _MEAN cannot be used to track changes over arbitrary time intervals (e.g. hourly), but it is useful to capture baseline, track trend or to compare different pipelines.\u003c/p\u003e\u003cp\u003eDataflow custom metrics, including aforementioned metrics reported by Splunk Dataflow template, are a chargeable feature of Cloud Monitoring. For more information on metrics pricing, see \u003ca href=\"https://cloud.google.com/stackdriver/pricing#monitoring-costs\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/pricing#monitoring-costs\" track-metadata-module=\"post\"\u003ePricing for Cloud Monitoring\u003c/a\u003e.\u003c/p\u003e\u003ch2\u003eImproved logging\u003c/h2\u003e\u003ch3\u003eLogging HEC errors\u003c/h3\u003e\u003cp\u003eTo further root cause downstream issues, HEC request errors are now adequately logged, including both response status code and message:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c62=\"\"\u003e\u003cdiv _ngcontent-c62=\"\" innerhtml=\"\u0026lt;p\u0026gt;You can retrieve them directly in Worker Logs from Dataflow Console by setting log severity to Error.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Alternatively, for those who prefer using \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface\u0026#34;\u0026gt;Logs Explorer\u0026lt;/a\u0026gt;, you can use the following query.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eYou can retrieve them directly in Worker Logs from Dataflow Console by setting log severity to Error.\u003c/p\u003e\u003cp\u003eAlternatively, for those who prefer using \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-metadata-module=\"post\"\u003eLogs Explorer\u003c/a\u003e, you can use the following query.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c63=\"\"\u003e\u003cpre _ngcontent-c63=\"\"\u003e  \u003ccode _ngcontent-c63=\"\"\u003elog_id(\u0026#34;dataflow.googleapis.com/worker\u0026#34;)\n\u003c/code\u003e\u003ccode _ngcontent-c63=\"\"\u003eresource.type=\u0026#34;dataflow_step\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c63=\"\"\u003eresource.labels.step_id=\u0026#34;WriteToSplunk/Write Splunk events\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c63=\"\"\u003eseverity=ERROR\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c62=\"\"\u003e\u003cdiv _ngcontent-c62=\"\" innerhtml=\"\u0026lt;h2\u0026gt;Disabling batch logs\u0026lt;/h2\u0026gt;By default, Splunk Dataflow workers log every HEC request as follows:\"\u003e\u003ch2\u003eDisabling batch logs\u003c/h2\u003e\u003cp\u003eBy default, Splunk Dataflow workers log every HEC request as follows:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c62=\"\"\u003e\u003cdiv _ngcontent-c62=\"\" innerhtml=\"\u0026lt;p\u0026gt;Even though these requests are often batched events, these \u0026amp;#8216;batch logs\u0026amp;#8217; are chatty as they add 2 log messages for every HEC request. With the addition of request-level counters (\u0026lt;code\u0026gt;http-*-requests\u0026lt;/code\u0026gt;), latency \u0026amp;amp; batch size distributions, and HEC error logging mentioned above, these batch logs are generally redundant. To control worker log volume, you can now disable these batch logs by setting the new optional template parameter \u0026lt;code\u0026gt;enableBatchLogs\u0026lt;/code\u0026gt; to \u0026lt;code\u0026gt;false\u0026lt;/code\u0026gt;, when deploying the Splunk Dataflow job. For more details on latest template parameters, refer to \u0026lt;a href=\u0026#34;https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk\u0026#34;\u0026gt;template user documentation\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Enabling debug level logs\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The default logging level for Google provided templates written using the Apache Beam Java SDK is INFO, which means all messages of INFO and higher i.e. WARN and ERROR will be logged. If you\u0026amp;#8217;d like to enable lower log levels like DEBUG, you can do so by setting the -\u0026lt;i\u0026gt;-defaultWorkerLogLevel\u0026lt;/i\u0026gt; flag to DEBUG while starting the pipeline using gcloud command-line tool.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;You can also override log levels for specific packages or classes with the \u0026lt;i\u0026gt;--workerLogLevelOverrides \u0026lt;/i\u0026gt;flag. For example, the \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/src/main/java/com/google/cloud/teleport/splunk/HttpEventPublisher.java\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;HttpEventPublisher\u0026lt;/a\u0026gt; class logs the final payload sent to Splunk at the DEBUG level. You can set the \u0026lt;i\u0026gt;--workerLogLevelOverrides \u0026lt;/i\u0026gt;flag to {\u0026amp;#34;com.google.cloud.teleport.splunk.HttpEventPublisher\u0026amp;#34;:\u0026amp;#34;DEBUG\u0026amp;#34;} to view the final message in the logs before it is sent to Splunk, and keep the log level at INFO for other classes. Exercise caution while using this as it will log \u0026lt;b\u0026gt;all\u0026lt;/b\u0026gt; messages sent to Splunk under the \u0026lt;i\u0026gt;Worker Logs\u0026lt;/i\u0026gt; tab in the console, which might lead to \u0026lt;a href=\u0026#34;https://cloud.google.com/dataflow/docs/guides/logging#LogLimits\u0026#34;\u0026gt;log throttling\u0026lt;/a\u0026gt; or reveal sensitive information.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Putting it all together\u0026lt;/h2\u0026gt;We put all this together in a single \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Monitoring dashboard\u0026lt;/a\u0026gt; that you can readily use to monitor your log export operations:\u0026lt;br\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eEven though these requests are often batched events, these ‘batch logs’ are chatty as they add 2 log messages for every HEC request. With the addition of request-level counters (\u003ccode\u003ehttp-*-requests\u003c/code\u003e), latency \u0026amp; batch size distributions, and HEC error logging mentioned above, these batch logs are generally redundant. To control worker log volume, you can now disable these batch logs by setting the new optional template parameter \u003ccode\u003eenableBatchLogs\u003c/code\u003e to \u003ccode\u003efalse\u003c/code\u003e, when deploying the Splunk Dataflow job. For more details on latest template parameters, refer to \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk\" track-metadata-module=\"post\"\u003etemplate user documentation\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eEnabling debug level logs\u003c/h3\u003e\u003cp\u003eThe default logging level for Google provided templates written using the Apache Beam Java SDK is INFO, which means all messages of INFO and higher i.e. WARN and ERROR will be logged. If you’d like to enable lower log levels like DEBUG, you can do so by setting the -\u003ci\u003e-defaultWorkerLogLevel\u003c/i\u003e flag to DEBUG while starting the pipeline using gcloud command-line tool. \u003c/p\u003e\u003cp\u003eYou can also override log levels for specific packages or classes with the \u003ci\u003e--workerLogLevelOverrides \u003c/i\u003eflag. For example, the \u003ca href=\"https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/src/main/java/com/google/cloud/teleport/splunk/HttpEventPublisher.java\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eHttpEventPublisher\u003c/a\u003e class logs the final payload sent to Splunk at the DEBUG level. You can set the \u003ci\u003e--workerLogLevelOverrides \u003c/i\u003eflag to {\u0026#34;com.google.cloud.teleport.splunk.HttpEventPublisher\u0026#34;:\u0026#34;DEBUG\u0026#34;} to view the final message in the logs before it is sent to Splunk, and keep the log level at INFO for other classes. Exercise caution while using this as it will log \u003cb\u003eall\u003c/b\u003e messages sent to Splunk under the \u003ci\u003eWorker Logs\u003c/i\u003e tab in the console, which might lead to \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/logging#LogLimits\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/dataflow/docs/guides/logging#LogLimits\" track-metadata-module=\"post\"\u003elog throttling\u003c/a\u003e or reveal sensitive information.\u003c/p\u003e\u003ch2\u003ePutting it all together\u003c/h2\u003e\u003cp\u003eWe put all this together in a single \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf\" target=\"_blank\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eMonitoring dashboard\u003c/a\u003e that you can readily use to monitor your log export operations:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c62=\"\"\u003e\u003cp\u003eThis dashboard is a single pane of glass for monitoring your Pub/Sub to Splunk Dataflow pipeline. Use it to ensure your log export is meeting your dynamic log volume requirements, by scaling to adequate throughput (EPS) rate, while keeping latency and backlog to a minimum. There’s also a panel to track pipeline resource usage and utilization, to help you validate that the pipeline is running cost-efficiently during steady-state.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c61=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/dynamic-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe’re thrilled to announce several new observability features for the \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk\"\u003ePub/Sub to Splunk Dataflow template\u003c/a\u003e to help operators keep a tab on their streaming pipeline performance. \u003ca href=\"https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-enterprise-gcp\"\u003eSplunk Enterprise\u003c/a\u003e and \u003ca href=\"https://console.cloud.google.com/marketplace/product/gcp-marketplace-lve-1/splunk-cloud\"\u003eSplunk Cloud\u003c/a\u003e customers use the Splunk Dataflow template to \u003ca href=\"https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow\"\u003ereliably export Google Cloud logs\u003c/a\u003e for in-depth analytics for security, IT or business use cases. With newly added metrics and improved logging for Splunk IO sink, it’s now easier to answer operational questions such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eIs the Dataflow pipeline keeping up with the volume of logs generated?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhat is the latency and throughput (Event Per Second or EPS) when writing to Splunk?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWhat is the response status breakdown of downstream Splunk HTTP Event Collector (HEC) and potential error messages?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis critical visibility helps you derive your log export \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring#defn-sli\"\u003eservice-level indicators (SLIs)\u003c/a\u003e and monitor for any pipeline performance regressions. You can also more easily root cause potential downstream failures between Dataflow \u0026amp; Splunk such as Splunk HEC network connections or server issues, and fix the problem before it cascades. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eTo help you quickly chart these new metrics, we’ve included them in the custom dashboard as part of the updated \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-splunk-log-export\" target=\"_blank\"\u003eTerraform module for Splunk Dataflow\u003c/a\u003e. You can use those Terraform templates to deploy the entire infrastructure for log export to Splunk, or just the \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf\" target=\"_blank\"\u003eMonitoring dashboard\u003c/a\u003e alone.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Log_Export_Ops_Dashboard_for_Splunk_Data.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Log Export Ops Dashboard for Splunk Dataflow.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Log_Export_Ops_Dashboard_for_Splunk_Data.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eLog Export Ops Dashboard for Splunk Dataflow\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch2\u003eMore metrics\u003c/h2\u003e\u003cp\u003eIn your Dataflow Console, you may have noticed several new custom metrics (highlighted below) for launched jobs as of template version \u003ccode\u003e2022-03-21-00_RC01\u003c/code\u003e, that is \u003ccode\u003egs://dataflow-templates/2022-03-21-00_RC01/Cloud_PubSub_to_Splunk\u003c/code\u003e or later:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_More_metrics.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 More metrics.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_More_metrics.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003ePipeline instrumentation\u003c/h3\u003e\u003cp\u003eBefore we dive into the new metrics, let’s take a step back and go over the Splunk Dataflow job steps. The following flowchart represents the different stages that comprise a Splunk Dataflow job along with corresponding custom metrics:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Pipeline_instrumentation.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"3 Pipeline instrumentation.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Pipeline_instrumentation.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn this pipeline, we utilize two types of \u003ca href=\"https://beam.apache.org/documentation/programming-guide/#types-of-metrics\" target=\"_blank\"\u003eApache Beam custom metrics\u003c/a\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCounter metrics, labeled 1 through 10 above, used to count messages and requests (both successful and failed).\u003c/li\u003e\u003cli\u003eDistribution metrics, labeled A through C above, used to report on distribution of request latency (both successful and failed) and batch size. \u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eDownstream request visibility\u003c/h3\u003e\u003cp\u003eSplunk Dataflow operators have relied on some of these pre-built custom metrics to monitor log messages progress through the different pipeline stages, particularly in the last stage \u003ccode\u003eWrite To Splunk\u003c/code\u003e, with metrics \u003ccode\u003eoutbound-successful-events\u003c/code\u003e (counter #6 above) and \u003ccode\u003eoutbound-failed-events\u003c/code\u003e (counter #7 above) to track the number of messages that were successfully exported (or not) to Splunk. While operators had visibility of the outbound message success rate, they lacked visibility at the HEC request level. Splunk Dataflow operators can now monitor not only the number of successful and failed HEC requests over time, but also the response status breakdown to determine if request failed due to a client request issue (e.g. invalid Splunk index or HEC token), or a transient network or Splunk issue (e.g. server busy or down) all from Dataflow Console with the addition of counters #7-10 above, that is:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode\u003ehttp-valid-requests\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003ehttp-invalid-requests\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003ehttp-server-error-requests\u003c/code\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSplunk Dataflow operators can also now track average latency of downstream requests to Splunk HEC, as well as average request batch size, by using the new distribution metrics #A-C, that is:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode\u003esuccessful_write_to_splunk_latency_ms\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003eunsuccessful_write_to_splunk_latency_ms\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode\u003ewrite_to_splunk_batch\u003c/code\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNote that a Distribution metric in Beam is reported by Dataflow as four sub-metrics suffixed with _MAX, _MIN, _MEAN and _COUNT. That is why those 3 new distribution metrics translate to 12 new metrics in Cloud Monitoring, as you can see in the earlier job info screenshot from Dataflow Console. Dataflow \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring#custom_metrics\"\u003ecurrently does not support creating a histogram\u003c/a\u003e to visualize the breakdown of these metrics’ values. Therefore, _MEAN metric is the only useful sub-metric for our purposes. As an all-time average value, _MEAN cannot be used to track changes over arbitrary time intervals (e.g. hourly), but it is useful to capture baseline, track trend or to compare different pipelines.\u003c/p\u003e\u003cp\u003eDataflow custom metrics, including aforementioned metrics reported by Splunk Dataflow template, are a chargeable feature of Cloud Monitoring. For more information on metrics pricing, see \u003ca href=\"https://cloud.google.com/stackdriver/pricing#monitoring-costs\"\u003ePricing for Cloud Monitoring\u003c/a\u003e.\u003c/p\u003e\u003ch2\u003eImproved logging\u003c/h2\u003e\u003ch3\u003eLogging HEC errors\u003c/h3\u003e\u003cp\u003eTo further root cause downstream issues, HEC request errors are now adequately logged, including both response status code and message:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Logging_HEC_errors.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"4 Logging HEC errors.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Logging_HEC_errors.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou can retrieve them directly in Worker Logs from Dataflow Console by setting log severity to Error.\u003c/p\u003e\u003cp\u003eAlternatively, for those who prefer using \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\"\u003eLogs Explorer\u003c/a\u003e, you can use the following query.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'log_id(\"dataflow.googleapis.com/worker\")\\r\\nresource.type=\"dataflow_step\"\\r\\nresource.labels.step_id=\"WriteToSplunk/Write Splunk events\"\\r\\nseverity=ERROR'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch2\u003eDisabling batch logs\u003c/h2\u003eBy default, Splunk Dataflow workers log every HEC request as follows:\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Disabling_batch_logs.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"5 Disabling batch logs.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/5_Disabling_batch_logs.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eEven though these requests are often batched events, these ‘batch logs’ are chatty as they add 2 log messages for every HEC request. With the addition of request-level counters (\u003ccode\u003ehttp-*-requests\u003c/code\u003e), latency \u0026amp; batch size distributions, and HEC error logging mentioned above, these batch logs are generally redundant. To control worker log volume, you can now disable these batch logs by setting the new optional template parameter \u003ccode\u003eenableBatchLogs\u003c/code\u003e to \u003ccode\u003efalse\u003c/code\u003e, when deploying the Splunk Dataflow job. For more details on latest template parameters, refer to \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/templates/provided-streaming#pubsub-to-splunk\"\u003etemplate user documentation\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eEnabling debug level logs\u003c/h3\u003e\u003cp\u003eThe default logging level for Google provided templates written using the Apache Beam Java SDK is INFO, which means all messages of INFO and higher i.e. WARN and ERROR will be logged. If you’d like to enable lower log levels like DEBUG, you can do so by setting the -\u003ci\u003e-defaultWorkerLogLevel\u003c/i\u003e flag to DEBUG while starting the pipeline using gcloud command-line tool. \u003c/p\u003e\u003cp\u003eYou can also override log levels for specific packages or classes with the \u003ci\u003e--workerLogLevelOverrides\u003c/i\u003eflag. For example, the \u003ca href=\"https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/main/src/main/java/com/google/cloud/teleport/splunk/HttpEventPublisher.java\" target=\"_blank\"\u003eHttpEventPublisher\u003c/a\u003e class logs the final payload sent to Splunk at the DEBUG level. You can set the \u003ci\u003e--workerLogLevelOverrides\u003c/i\u003eflag to {\"com.google.cloud.teleport.splunk.HttpEventPublisher\":\"DEBUG\"} to view the final message in the logs before it is sent to Splunk, and keep the log level at INFO for other classes. Exercise caution while using this as it will log \u003cb\u003eall\u003c/b\u003e messages sent to Splunk under the \u003ci\u003eWorker Logs\u003c/i\u003e tab in the console, which might lead to \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/logging#LogLimits\"\u003elog throttling\u003c/a\u003e or reveal sensitive information.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003ch2\u003ePutting it all together\u003c/h2\u003eWe put all this together in a single \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-splunk-log-export/blob/main/monitoring.tf\" target=\"_blank\"\u003eMonitoring dashboard\u003c/a\u003e that you can readily use to monitor your log export operations:\u003cbr/\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/6_Pipeline_Throughput.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"6 Pipeline Throughput.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/6_Pipeline_Throughput.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003ePipeline Throughput, Latency \u0026amp; Errors\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis dashboard is a single pane of glass for monitoring your Pub/Sub to Splunk Dataflow pipeline. Use it to ensure your log export is meeting your dynamic log volume requirements, by scaling to adequate throughput (EPS) rate, while keeping latency and backlog to a minimum. There’s also a panel to track pipeline resource usage and utilization, to help you validate that the pipeline is running cost-efficiently during steady-state.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/7_Pipeline_Utilization.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"7 Pipeline Utilization.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/7_Pipeline_Utilization.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003ePipeline Utilization and Worker Logs\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eFor specific guidance on handling and replaying failed messages, refer to \u003ca href=\"https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow#troubleshoot_failed_messages\"\u003eTroubleshoot failed messages\u003c/a\u003e as part of the Splunk Dataflow reference guide. For general information on troubleshooting any Dataflow pipeline, check out the \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline\"\u003eTroubleshooting and debugging\u003c/a\u003e documentation, and for a list of common errors and their resolutions look through the \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/common-errors\"\u003eCommon error guidance\u003c/a\u003e documentation. If you encounter any issue, please \u003ca href=\"https://github.com/GoogleCloudPlatform/DataflowTemplates/issues\" target=\"_blank\"\u003eopen an issue\u003c/a\u003e in the \u003ca href=\"https://github.com/GoogleCloudPlatform/DataflowTemplates\" target=\"_blank\"\u003eDataflow templates GitHub repository\u003c/a\u003e, or \u003ca href=\"https://console.cloud.google.com/support/cases\"\u003eopen a support case\u003c/a\u003e directly in your Google Cloud Console.\u003c/p\u003e\u003cp\u003eFor a step-by-step guide on how to export GCP logs to Splunk, check out the \u003ca href=\"https://cloud.google.com/architecture/deploying-production-ready-log-exports-to-splunk-using-dataflow\"\u003eDeploy production-ready log exports to Splunk using Dataflow\u003c/a\u003e tutorial, or use the accompanying \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-splunk-log-export\" target=\"_blank\"\u003eTerraform scripts\u003c/a\u003e to automate the setup of your log export infrastructure along with the associated operational dashboard.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/data-analytics/whats-new-splunk-dataflow-template/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DataAnalytics_B_1_Kp1Qjnf.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eWhat’s new with Splunk Dataflow template: Automatic log parsing, UDF support, and more\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnnouncing new features for Splunk Dataflow template with improved compatibility with Splunk Add-on for GCP, more extensibility using use...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/splunk_ISRQYWX.max-2200x2200.jpg",
      "date_published": "2022-05-13T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eRoy Arsan\u003c/name\u003e\u003ctitle\u003eSolutions Architect\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/how-sres-analyze-risks-to-evaluate-slos/",
      "title": "Are your SLOs realistic? How to analyze your risks like an SRE",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eSetting up Service Level Objectives (SLOs) is one of the foundational tasks of Site Reliability Engineering (SRE) practices, giving the SRE team a target against which to evaluate whether or not a service is running reliably enough. The inverse of your SLO is your \u003ca href=\"https://sre.google/sre-book/embracing-risk/\" target=\"_blank\"\u003eerror budget\u003c/a\u003e — how much unreliability you are willing to tolerate. Once you’ve identified those targets and \u003ca href=\"https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos\"\u003elearned how to set SLOs\u003c/a\u003e, the next question you should ask yourself is whether your SLOs are realistic, given your application architecture and team practices? Are you sure that you can meet them? And what’s most likely to spend the error budget?\u003c/p\u003e\u003cp\u003eAt Google, SREs answer these questions up front when they take on a new service, as part of \u003ca href=\"https://sre.google/sre-book/evolving-sre-engagement-model/\" target=\"_blank\"\u003ea Production Readiness Review (PRR)\u003c/a\u003e. The intention of this risk analysis is not to prompt you to change your SLOs, but rather to \u003ca href=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\"\u003eprioritize and communicate the risks\u003c/a\u003e to a given service, so you can evaluate whether you’ll be able to actually meet your SLOs, with or without any changes to the service. In addition, it can help you identify which risks are the most important to prioritize and mitigate, using the best available data.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-pull_quote\"\u003e\u003cdiv class=\"uni-pull-quote h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003cdiv class=\"uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cdiv class=\"uni-pull-quote__inner-wrapper h-c-copy h-c-copy\"\u003e\u003cq class=\"uni-pull-quote__text\"\u003eYou can make your service more reliable by identifying and mitigating risks.\u003c/q\u003e\u003c/div\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eRisk analysis basics\u003c/h3\u003e\u003cp\u003eBefore you can evaluate and prioritize your risks, though, you need to come up with a comprehensive list of things to watch out for. In this post, we’ll provide some guidelines for teams tasked with brainstorming all the potential risks to an application. Then, with that list in hand, we’ll show you how to actually analyze and prioritize the risks you’ve identified. \u003c/p\u003e\u003ch3\u003eWhat risks do you want to consider?\u003c/h3\u003e\u003cp\u003eWhen brainstorming risks, it’s important to try to map risks in different categories — risks that are related to your dependencies, monitoring, capacity, operations, and release process. And for each of those, imagine what will happen if specific failures happen, for example, if a third party is down, or if you introduce an application or configuration bug. Thus, when thinking about your measurements, ask yourself: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eAre there any observability gaps? \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you have alerts for this specific SLI? \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you even currently collect those metrics? \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAlso be sure to also map any monitoring and alerting dependencies. For example, what happens if a managed system that you use goes down?\u003c/p\u003e\u003cp\u003eIdeally, you want to identify the risks associated with each failure point for each critical component in a critical user journey, or CUJ. And after identifying those risks, you will want to quantify them:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eWhat percentage of users was affected by the failure?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eHow often do you estimate that failure will occur?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eHow long did it take to detect the failure? \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIt’s also helpful to gather information about any incidents that happened in the last year that affected CUJs. Compared with gut feelings, relying on historical data can provide more accurate estimates and a good starting point for actual incidents. For example, you may want to consider incidents such as:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eA configuration mishap that reduces capacity, causing overload and dropped requests\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA new release that breaks a small set of requests; the failure is not detected for a day; quick rollback when detected.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA cloud provider’s single-zone VM/network outage\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA cloud provider’s regional VM/network outage\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe operator accidentally deletes a database, requiring a restore from backup\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAnother aspect to think about is risk factors; these are global factors that affect the overall time to detection (TTD) and time to repair (TTR). These tend to be operational factors that can increase the time needed to detect outages (for example when using log-based metrics) or alert the on-call engineers. Another example could be a lack of playbooks/documentation or lack of automatic procedures. For example, you have:  \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eEstimated time to detection (ETTD) of +30m due to operational overload such as noisy alerting\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA 10% greater frequency of a possible failure, due to lack of postmortems or action item follow-up\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eBrainstorming guidelines: Recommendation for the facilitator\u003c/h3\u003e\u003cp\u003eBeyond the technical aspects of what to look for in a potential risk to your service, there are some best practices to consider when holding a brainstorming session with your team. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eStart the discussion with a high-level block diagram of the service, its users, and its dependencies. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGet a set of diverse opinions in the room — different roles that intersect with the product differently than you do. Also, avoid having only one party speak. Ask participants for the ways in which each element of the diagram could cause an error to be served to the user. Group similar root causes together into a single risk category, such as \"database outage\".\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTry to avoid spending too long discussing things where the estimated time between a given failure is longer than a couple of years, or where the impact is limited to a very small subset of users.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eCreating your risk catalog \u003c/h3\u003e\u003cp\u003eYou don't need to capture an endless list of risks; seven to 12 risks per Service Level Indicator (SLI) are sufficient. The important thing is that the data capture high probability and critical risks. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eStarting with real outages is best. Those can be as simple as unavailability of \u0026lt;depended service or network\u0026gt;.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCapture both infrastructure- and software-related issues. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThink about risks that can affect the SLI, the time-to-detect and time-to-resolve, and frequency — more on those metrics below.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCapture both risks in the risk catalog and risk factors (global factors). For example, the risk of not having a playbook adds to your time-to-repair; not having alerts for the CUJ adds to the time-to-detection; the risk of a log sync delay of x minutes increases your time-to-detection by the same amount. Then, catalog all these risks and their associated impacts to a global impacts tab.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHere are a few examples of risks: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eA new release breaks a small set of requests; not detected for a day; quick rollback when detected.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA new release breaks a sizable subset of requests; and no automatic rollback.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA configuration mishap reduces capacity / Unnoticed growth in usage hits max.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cb\u003eRecommendation\u003c/b\u003e: Examining the data/result of \u003cb\u003eimplementing\u003c/b\u003e the SLI will give you a good indication of where you stand in regard to achieving your targets. I recommend starting with creating one dashboard for each CUJ — ideally a dashboard that includes metrics that will also allow us to troubleshoot and debug problems in achieving the SLOs.\u003c/p\u003e\u003ch3\u003eAnalyzing the risks\u003c/h3\u003e\u003cp\u003eNow that you’ve generated a list of potential risks, it’s time to analyze them, in order to prioritize their likelihood, and potentially find ways to mitigate against them. It’s time, in other words, to do a risk analysis. \u003c/p\u003e\u003cp\u003eRisk analysis provides a data-driven approach to address and prioritize the needed risks, by estimating four key dimensions: the above-mentioned TTD and TTR, as well as time-between failures (TBF), and their impact on users.\u003c/p\u003e\u003cp\u003eIn \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons\"\u003eShrinking the impact of production incidents using SRE principles\u003c/a\u003e, we introduced a diagram of the production incident cycle. Blue represents when users are happy, and red represents when users are unhappy. \u003c/p\u003e\u003cp\u003eThe time that your services are unreliable and your users are unhappy consists of the time-to-detect and the time-to-repair, and is affected by the frequency of incidents (which can be translated to time-between-failures).\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SRE_e5I0CIV.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"SRE.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SRE_e5I0CIV.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cbr/\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eTherefore, \u003cb\u003ewe can improve reliability\u003c/b\u003e by increasing the \u003cb\u003etime between failures\u003c/b\u003e, decreasing the \u003cb\u003etime-to-detect\u003c/b\u003e or \u003cb\u003etime-to-repair\u003c/b\u003e, and of course, \u003cb\u003ereducing the impact of the outages\u003c/b\u003e in the first place.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/compute/docs/tutorials/robustsystems#distribute\"\u003eEngineering your service for resiliency\u003c/a\u003e can reduce the frequency of total failures. You should avoid single points of failure in your architecture, whether it be an individual instance, availability zone, or even an entire region, which can prevent a smaller, localized outage from snowballing into global downtime.\u003c/p\u003e\u003cp\u003eYou can reduce the impact on your users by reducing the percentage of infrastructure or users affected or the requests (e.g., throttling part of the requests vs. all of them). In order to reduce the blast radius of outages, avoid global changes and adopt advanced deployments strategies that allow you to gradually deploy changes. Consider progressive and canary rollouts over the course of hours, days, or weeks, which allow you to reduce the risk and to identify an issue before all your users are affected.\u003c/p\u003e\u003cp\u003eFurther, having robust Continuous Integration and Continuous Delivery (CI/CD) pipelines allows you to deploy and roll back with confidence and reduce customer impact (See: SRE Book: \u003ca href=\"https://sre.google/sre-book/release-engineering/\" target=\"_blank\"\u003eChapter 8 - Release Engineering\u003c/a\u003e). Creating an integrated process of code review and testing will help you find the issues early on before users are affected. \u003c/p\u003e\u003cp\u003eImproving the \u003cb\u003etime to detect\u003c/b\u003e means that you catch outages faster. As a reminder, having an \u003cb\u003eestimated TTD\u003c/b\u003e expresses how long until a human being is informed of the problem. For example, imagine someone receives and acts upon a page. TTD also includes any delays until the 'detection' like data processing. For example, if I'm using a log-based alert, and my log system has an ingestion time of 5 minutes, this increases the TTD for every alert by 5 minutes.\u003c/p\u003e\u003cp\u003e\u003cb\u003eETTR\u003c/b\u003e (estimated time-to-repair) is the time between the time a human sees the alert and the time your users are happy. Improving \u003cb\u003etime-to-repair\u003c/b\u003e means that we fix outages quicker, in principle. That said, our focus should still be \"does this incident still affect our users?\" In most cases \u003ca href=\"https://www.oreilly.com/content/generic-mitigations/\" target=\"_blank\"\u003emitigations\u003c/a\u003e like rolling back new releases or diverting traffic to unaffected regions can reduce or eliminate the impact of an ongoing outage on users much faster than trying to roll forward to a new, patched build. The root cause isn't yet fixed, but the users don't know or care — all they see is that the service is working again. \u003c/p\u003e\u003cp\u003eWhile it takes the human out of the loop, using automation can reduce the TTR and can be crucial to achieving higher reliability targets. However, it doesn't eliminate the TTR altogether, because even if a mitigation such as failing over to a different region is automated, it still takes time for it to have an impact.\u003c/p\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eA note about “estimated” values\u003c/b\u003e: At the beginning of a risk analysis, you might start with rough estimates for these metrics. But as you collect more data from incidents data you can update these estimates based on data from prior outages. \u003c/i\u003e\u003c/p\u003e\u003ch3\u003eRisk analysis process at a high level \u003c/h3\u003e\u003cp\u003eThe risk analysis process starts by brainstorming risks for each of your SLOs, and more correctly for each one of your SLIs, as different SLIs will be exposed to different risks. In the next phase, build a risk catalog and iterate on it.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eCreate a risk analysis sheet for two or three SLIs, using this \u003ca href=\"http://goo.gl/bnsPj7\" target=\"_blank\"\u003etemplate\u003c/a\u003e. Read more at \u003ca href=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\"\u003eHow to prioritize and communicate risks\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBrainstorm risks internally, considering the things that can affect your SLOs, and gathering some initial data. Do this first with the engineering team and then include the product team.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe risk analysis sheets for each of your SLIs should include ETTD, ETTR, impact, and frequency. Include global factors and suggested risks and whether these risks are acceptable or not.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCollect historical data and consult with the product team regarding the SLO-business needs. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIterate and update data based on incidents in production.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eAccepting risks\u003c/h3\u003e\u003cp\u003eAfter building the risk catalog and capturing the risk factors, finalize the SLOs according to business need and risk analysis. This step means you need to evaluate whether your SLO is achievable given the risks, and if it isn’t — what do you need to do to achieve your targets? It is crucial that PMs be part of this review process especially as they might need to prioritize engineering work that mitigates or eliminates any unacceptable risks.\u003c/p\u003e\u003cp\u003eIn \u003ca href=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\"\u003ehow to prioritize and communicate risks\u003c/a\u003e, we introduce how to use the 'Risk Stack Rank' sheet to see how much a given risk may “cost” you, and which risks you can accept (or not) for a given SLO. For example, in the \u003ca href=\"https://docs.google.com/spreadsheets/d/1XTsPG79XCCiaOEMj8K4mgPg39ZWB1l5fzDc1aDjLW2Y/view#gid=1494250520\" target=\"_blank\"\u003etemplate sheet\u003c/a\u003e, you could accept all risks and achieve 99.5% reliability, some of the risks to achieve 99.9% and none of them to achieve 99.99%. If you can't accept a risk because you estimate that it will burn more error budget than your SLO affords you, that is a clear argument for dedicating engineering time to either fixing the root cause or building some sort of mitigation.\u003c/p\u003e\u003cp\u003eOne final note: similar to SLOs, you will want to iterate on your risk refining your ETTD based on actual TTD observed during outages, and similarly for ETTR. After incidents, you need to update the data and see where you stand regarding those estimates. In addition, revisit those estimates periodically to evaluate whether your risks are still relevant, if your estimates are correct, or if there are any additional risks that you need to account for. Like the SRE principle of \u003ca href=\"https://sre.google/sre-book/evolving-sre-engagement-model/\" target=\"_blank\"\u003econtinuous improvement\u003c/a\u003e, it’s work that’s never truly done, but that is well worth the effort!\u003c/p\u003e\u003cp\u003eFor more on this topic, check out my upcoming \u003ca href=\"http://devopsdays.org/\" target=\"_blank\"\u003eDevOpsDays 2022\u003c/a\u003e talk, taking place in \u003ca href=\"https://devopsdays.org/events/2022-birmingham-uk/\" target=\"_blank\"\u003eBirmingham\u003c/a\u003e on May 6 and in \u003ca href=\"https://devopsdays.org/events/2022-prague/welcome/\" target=\"_blank\"\u003ePrague\u003c/a\u003e on May 24.   \u003c/p\u003e\u003ch3\u003eFurther reading and resources\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.coursera.org/learn/site-reliability-engineering-slos\" target=\"_blank\"\u003e\u003cb\u003eSite Reliability Engineering: Measuring and Managing Reliability\u003c/b\u003e\u003c/a\u003e (Coursera course)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/architecture/framework/reliability\"\u003eGoogle Cloud Architecture Framework: Reliability\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://queue.acm.org/detail.cfm?id=3096459\" target=\"_blank\"\u003eThe Calculus of Service Availability\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\"\u003eKnow thy enemy: how to prioritize and communicate risks—CRE life lessons\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://sre.google/resources/practices-and-processes/incident-metrics-in-sre/\" target=\"_blank\"\u003eIncident Metrics in SRE - Google - Site Reliability Engineering\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/sre\"\u003eSRE on Google Cloud\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_aWHZoxD.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eKnow thy enemy: How to prioritize and communicate risks—CRE life lessons\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eHow to effectively communicate and stack-rank risks in your system.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-05-04T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eAyelet Sachto\u003c/name\u003e\u003ctitle\u003eStrategic Cloud Engineer, Infra, AppMod, SRE\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/querying-logs-just-got-easier-in-cloud-logging/",
      "title": "Announcing new simple query options in Cloud Logging",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c60=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;p\u0026gt;When you\u0026amp;#8217;re troubleshooting an issue, finding the root cause often involves finding specific logs generated by infrastructure and application code. The faster you can find logs, the faster you can confirm or refute your hypothesis about the root cause and resolve the issue! Today, we\u0026amp;#8217;re pleased to announce a dramatically simpler way to find logs in Logs Explorer.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Making querying even easier!\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Over the past 2 years, we heard feedback that many users needed simple free text search to find their logs. We also heard that users wanted to build a query using the dropdown selectors. We took all that feedback to heart and made many critical changes to the Logs Explorer to address this feedback and make searching logs even easier.\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Simple text search\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;a new simple text search box for global text searches\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Advanced query\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;a new toggle to show/hide the Logging query language for the query\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Date/time picker \u0026lt;/b\u0026gt;\u0026amp;#8211; the date/time range picker is now a part of the query builder\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Date/time preferences\u0026lt;/b\u0026gt; \u0026amp;#8211; the date/time display now respects date/time preferences set in the Cloud Console settings\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Dropdown selectors\u0026lt;/b\u0026gt; \u0026amp;#8211; prominently display the resource, logName and severity dropdown selectors\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Dropdown selector state\u0026lt;/b\u0026gt; \u0026amp;#8211;\u0026amp;#160;maintain the state in the resource, logName, severity and free text search boxes whether building query via dropdown or by editing the Logging query language\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Default summary fields\u0026lt;/b\u0026gt; \u0026amp;#8211; a new option to disable default summary fields for a more basic log view\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\"\u003e\u003cp\u003eWhen you’re troubleshooting an issue, finding the root cause often involves finding specific logs generated by infrastructure and application code. The faster you can find logs, the faster you can confirm or refute your hypothesis about the root cause and resolve the issue! Today, we’re pleased to announce a dramatically simpler way to find logs in Logs Explorer. \u003c/p\u003e\u003ch3\u003eMaking querying even easier!\u003c/h3\u003e\u003cp\u003eOver the past 2 years, we heard feedback that many users needed simple free text search to find their logs. We also heard that users wanted to build a query using the dropdown selectors. We took all that feedback to heart and made many critical changes to the Logs Explorer to address this feedback and make searching logs even easier.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSimple text search\u003c/b\u003e – a new simple text search box for global text searches\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAdvanced query\u003c/b\u003e – a new toggle to show/hide the Logging query language for the query\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDate/time picker \u003c/b\u003e– the date/time range picker is now a part of the query builder\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDate/time preferences\u003c/b\u003e – the date/time display now respects date/time preferences set in the Cloud Console settings\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDropdown selectors\u003c/b\u003e – prominently display the resource, logName and severity dropdown selectors\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDropdown selector state\u003c/b\u003e – maintain the state in the resource, logName, severity and free text search boxes whether building query via dropdown or by editing the Logging query language\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDefault summary fields\u003c/b\u003e – a new option to disable default summary fields for a more basic log view\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Simple text search\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The new text search box performs global free text searches across your logs for the strings added to the text search box. For example, a simple \u0026amp;#8220;POST OR GET\u0026amp;#8221; will find any logs including the text \u0026amp;#8220;POST\u0026amp;#8221; or \u0026amp;#8220;GET\u0026amp;#8221; in any log field.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eSimple text search\u003c/h3\u003e\u003cp\u003eThe new text search box performs global free text searches across your logs for the strings added to the text search box. For example, a simple “POST OR GET” will find any logs including the text “POST” or “GET” in any log field.  \u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cp\u003eAdditionally, you’ll see your query results highlighted both in the log summary line and the individual log entry itself.  \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Show/hide query toggle\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The new features simplify the query experience for many users, but the Logs Explorer still needs to allow users to to write complex queries for advanced use cases. That\u0026amp;#8217;s why we added the \u0026lt;i\u0026gt;Show/hide\u0026lt;/i\u0026gt; query toggle which expands and closes the Logging query language behind the query.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;You can use the \u0026lt;i\u0026gt;Show/hide query\u0026lt;/i\u0026gt; toggle when the dropdowns just don\u0026amp;#8217;t cut it for your use case and you need to build conditional logic or regexes into your queries. You can update the Logging query language directly by selecting the \u0026lt;i\u0026gt;Show/hide query\u0026lt;/i\u0026gt; toggle.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eShow/hide query toggle\u003c/h3\u003e\u003cp\u003eThe new features simplify the query experience for many users, but the Logs Explorer still needs to allow users to to write complex queries for advanced use cases. That’s why we added the \u003ci\u003eShow/hide\u003c/i\u003e query toggle which expands and closes the Logging query language behind the query. \u003c/p\u003e\u003cp\u003eYou can use the \u003ci\u003eShow/hide query\u003c/i\u003e toggle when the dropdowns just don’t cut it for your use case and you need to build conditional logic or regexes into your queries. You can update the Logging query language directly by selecting the \u003ci\u003eShow/hide query\u003c/i\u003e toggle.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Date/time picker\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We moved the date/time picker location to be featured prominently as the first item in the query builder which makes it easier to find. While this move represents a relocation in the Logs Explorer user interface, we have a series of improvements that the team is actively developing to make it even easier to find the right logs for a date/time range.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eDate/time picker\u003c/h3\u003e\u003cp\u003eWe moved the date/time picker location to be featured prominently as the first item in the query builder which makes it easier to find. While this move represents a relocation in the Logs Explorer user interface, we have a series of improvements that the team is actively developing to make it even easier to find the right logs for a date/time range.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Date/time display based on Cloud Console settings\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Whether you\u0026amp;#8217;re a developer, DevOps engineer, SRE, or anywhere in between, working with dates can be difficult because of the different representations. With this change, the \u0026lt;i\u0026gt;Jump to time and Enter custom range\u0026lt;/i\u0026gt; options in the date/time range selector will now respect your date and time format preferences set in the Cloud Console settings. This means, if you select mm/dd/yyyy or dd/mm/yyyy in your Cloud Console settings, you\u0026amp;#8217;ll see the date/time options in that format. When you select a 24-hour time format or an AM/PM time format, you\u0026amp;#8217;ll see the time options in your selected format.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eDate/time display based on Cloud Console settings\u003c/h3\u003e\u003cp\u003eWhether you’re a developer, DevOps engineer, SRE, or anywhere in between, working with dates can be difficult because of the different representations. With this change, the \u003ci\u003eJump to time and Enter custom range\u003c/i\u003e options in the date/time range selector will now respect your date and time format preferences set in the Cloud Console settings. This means, if you select mm/dd/yyyy or dd/mm/yyyy in your Cloud Console settings, you’ll see the date/time options in that format. When you select a 24-hour time format or an AM/PM time format, you’ll see the time options in your selected format.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Resource, logName and severity dropdown selectors\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The Logs Explorer now prominently displays the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\u0026#34;\u0026gt;dropdown selectors\u0026lt;/a\u0026gt; for resources, logNames and severity. These dropdown selectors have also been improved so that they run the query each time a selection is made. This makes it easier to narrow logs quickly with each dropdown selection. Together, these changes make the dropdown selectors easier to find and more responsive.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eResource, logName and severity dropdown selectors\u003c/h3\u003e\u003cp\u003eThe Logs Explorer now prominently displays the \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\" track-metadata-module=\"post\"\u003edropdown selectors\u003c/a\u003e for resources, logNames and severity. These dropdown selectors have also been improved so that they run the query each time a selection is made. This makes it easier to narrow logs quickly with each dropdown selection. Together, these changes make the dropdown selectors easier to find and more responsive.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Keeping the Logging query language and dropdowns in sync\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;When you use the resource, logName, severity dropdowns or add search text, Logs Explorer now builds a Logging query language query for you. For basic queries you don\u0026amp;#8217;t even need to look at the Logging query language. For more complex queries, you can edit the query language directly.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Here\u0026amp;#8217;s the interesting part: when you edit the query directly, the resource, logName, severity dropdowns and search text will be updated to match the Logging query language terms if they can be parsed and don\u0026amp;#8217;t include complex logical conditions.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For example, if you use the \u0026lt;i\u0026gt;Show logs\u0026lt;/i\u0026gt; toggle to show the Logging query language and add the \u0026lt;code\u0026gt;severity=ERROR\u0026lt;/code\u0026gt;, the severity dropdown is updated to show that ERROR is selected.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eKeeping the Logging query language and dropdowns in sync\u003c/h3\u003e\u003cp\u003eWhen you use the resource, logName, severity dropdowns or add search text, Logs Explorer now builds a Logging query language query for you. For basic queries you don’t even need to look at the Logging query language. For more complex queries, you can edit the query language directly. \u003c/p\u003e\u003cp\u003eHere’s the interesting part: when you edit the query directly, the resource, logName, severity dropdowns and search text will be updated to match the Logging query language terms if they can be parsed and don’t include complex logical conditions. \u003c/p\u003e\u003cp\u003eFor example, if you use the \u003ci\u003eShow logs\u003c/i\u003e toggle to show the Logging query language and add the \u003ccode\u003eseverity=ERROR\u003c/code\u003e, the severity dropdown is updated to show that ERROR is selected.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cp\u003eNext, if you select  “DEBUG” from the severity dropdown, the query is updated to \u003ccode\u003eseverity=(ERROR OR DEBUG)\u003c/code\u003e. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;p\u0026gt;Maintaining the query state regardless of whether you\u0026amp;#8217;re selecting from the dropdowns or typing in queries means there is one less detail to remember when you\u0026amp;#8217;re querying your logs.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Disabling default summary fields for a basic log view\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The Logs Explorer adds default \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields\u0026#34;\u0026gt;summary fields\u0026lt;/a\u0026gt; to the log results to highlight useful information and make it easy to take action directly from the log line. For example, on App Engine logs, the default summary field chips highlight the latency which can help you more easily filter logs.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eMaintaining the query state regardless of whether you’re selecting from the dropdowns or typing in queries means there is one less detail to remember when you’re querying your logs.\u003c/p\u003e\u003ch3\u003eDisabling default summary fields for a basic log view\u003c/h3\u003e\u003cp\u003eThe Logs Explorer adds default \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields\" track-metadata-module=\"post\"\u003esummary fields\u003c/a\u003e to the log results to highlight useful information and make it easy to take action directly from the log line. For example, on App Engine logs, the default summary field chips highlight the latency which can help you more easily filter logs.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;p\u0026gt;Logs Explorer also enables you to add your own \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields\u0026#34;\u0026gt;custom summary fields\u0026lt;/a\u0026gt; to the log lines so you can view what\u0026amp;#8217;s most important to you about the log lines.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We\u0026amp;#8217;ve heard feedback that sometimes all that\u0026amp;#8217;s needed is the raw text of logs. To show raw text logs, we\u0026amp;#8217;ve added a toggle to turn off/on the default summary fields for your log results for the duration of your session. By turning off the default summary fields, you\u0026amp;#8217;ll see only the raw text logs summary. To turn the summary fields back on, simply enable the toggle or start a new Logs Explorer session in a new tab.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eLogs Explorer also enables you to add your own \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields\" track-metadata-module=\"post\"\u003ecustom summary fields\u003c/a\u003e to the log lines so you can view what’s most important to you about the log lines. \u003c/p\u003e\u003cp\u003eWe’ve heard feedback that sometimes all that’s needed is the raw text of logs. To show raw text logs, we’ve added a toggle to turn off/on the default summary fields for your log results for the duration of your session. By turning off the default summary fields, you’ll see only the raw text logs summary. To turn the summary fields back on, simply enable the toggle or start a new Logs Explorer session in a new tab.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c68=\"\"\u003e\u003cdiv _ngcontent-c68=\"\" innerhtml=\"\u0026lt;h3\u0026gt;The road ahead\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We\u0026amp;#8217;re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven\u0026amp;#8217;t already, get started with the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/view/logs-explorer-interface\u0026#34;\u0026gt;Logs Explorer\u0026lt;/a\u0026gt; and join the discussion in our \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Cloud Operations page\u0026lt;/a\u0026gt; on the Google Cloud Community site.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eThe road ahead\u003c/h3\u003e\u003cp\u003eWe’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\" track-metadata-module=\"post\"\u003eLogs Explorer\u003c/a\u003e and join the discussion in our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003eCloud Operations page\u003c/a\u003e on the Google Cloud Community site.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c67=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen you’re troubleshooting an issue, finding the root cause often involves finding specific logs generated by infrastructure and application code. The faster you can find logs, the faster you can confirm or refute your hypothesis about the root cause and resolve the issue! Today, we’re pleased to announce a dramatically simpler way to find logs in Logs Explorer. \u003c/p\u003e\u003ch3\u003eMaking querying even easier!\u003c/h3\u003e\u003cp\u003eOver the past 2 years, we heard feedback that many users needed simple free text search to find their logs. We also heard that users wanted to build a query using the dropdown selectors. We took all that feedback to heart and made many critical changes to the Logs Explorer to address this feedback and make searching logs even easier.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSimple text search\u003c/b\u003e – a new simple text search box for global text searches\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAdvanced query\u003c/b\u003e – a new toggle to show/hide the Logging query language for the query\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDate/time picker\u003c/b\u003e– the date/time range picker is now a part of the query builder\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDate/time preferences\u003c/b\u003e – the date/time display now respects date/time preferences set in the Cloud Console settings\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDropdown selectors\u003c/b\u003e – prominently display the resource, logName and severity dropdown selectors\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDropdown selector state\u003c/b\u003e – maintain the state in the resource, logName, severity and free text search boxes whether building query via dropdown or by editing the Logging query language\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDefault summary fields\u003c/b\u003e – a new option to disable default summary fields for a more basic log view\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"01.logs_explorer_simple_mode_query.gif\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/01.logs_explorer_simple_mode_query.gif\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSimple text search\u003c/h3\u003e\u003cp\u003eThe new text search box performs global free text searches across your logs for the strings added to the text search box. For example, a simple “POST OR GET” will find any logs including the text “POST” or “GET” in any log field.  \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"02.simple_text_search.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/02.simple_text_search.1000066220000402.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAdditionally, you’ll see your query results highlighted both in the log summary line and the individual log entry itself. \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"03.log_results_text_highlight.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/03.log_results_text_highlight.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eShow/hide query toggle\u003c/h3\u003e\u003cp\u003eThe new features simplify the query experience for many users, but the Logs Explorer still needs to allow users to to write complex queries for advanced use cases. That’s why we added the \u003ci\u003eShow/hide\u003c/i\u003e query toggle which expands and closes the Logging query language behind the query. \u003c/p\u003e\u003cp\u003eYou can use the \u003ci\u003eShow/hide query\u003c/i\u003e toggle when the dropdowns just don’t cut it for your use case and you need to build conditional logic or regexes into your queries. You can update the Logging query language directly by selecting the \u003ci\u003eShow/hide query\u003c/i\u003e toggle.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"04.logs_explorer_simple_query_mode_show_query.gif\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/04.logs_explorer_simple_query_mode_show_query.gif\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eDate/time picker\u003c/h3\u003e\u003cp\u003eWe moved the date/time picker location to be featured prominently as the first item in the query builder which makes it easier to find. While this move represents a relocation in the Logs Explorer user interface, we have a series of improvements that the team is actively developing to make it even easier to find the right logs for a date/time range.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"05.date_time_range_selector.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/05.date_time_range_selector.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eDate/time display based on Cloud Console settings\u003c/h3\u003e\u003cp\u003eWhether you’re a developer, DevOps engineer, SRE, or anywhere in between, working with dates can be difficult because of the different representations. With this change, the \u003ci\u003eJump to time and Enter custom range\u003c/i\u003e options in the date/time range selector will now respect your date and time format preferences set in the Cloud Console settings. This means, if you select mm/dd/yyyy or dd/mm/yyyy in your Cloud Console settings, you’ll see the date/time options in that format. When you select a 24-hour time format or an AM/PM time format, you’ll see the time options in your selected format.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"06.date_time_preferences.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/06.date_time_preferences.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eResource, logName and severity dropdown selectors\u003c/h3\u003e\u003cp\u003eThe Logs Explorer now prominently displays the \u003ca href=\"https://cloud.google.com/logging/docs/view/building-queries#query-builder-menus\"\u003edropdown selectors\u003c/a\u003e for resources, logNames and severity. These dropdown selectors have also been improved so that they run the query each time a selection is made. This makes it easier to narrow logs quickly with each dropdown selection. Together, these changes make the dropdown selectors easier to find and more responsive.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"07.logs_explorer_simple_query_mode_resource_selectors.gif\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/07.logs_explorer_simple_query_mode_resource_selectors.gif\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eKeeping the Logging query language and dropdowns in sync\u003c/h3\u003e\u003cp\u003eWhen you use the resource, logName, severity dropdowns or add search text, Logs Explorer now builds a Logging query language query for you. For basic queries you don’t even need to look at the Logging query language. For more complex queries, you can edit the query language directly. \u003c/p\u003e\u003cp\u003eHere’s the interesting part: when you edit the query directly, the resource, logName, severity dropdowns and search text will be updated to match the Logging query language terms if they can be parsed and don’t include complex logical conditions. \u003c/p\u003e\u003cp\u003eFor example, if you use the \u003ci\u003eShow logs\u003c/i\u003e toggle to show the Logging query language and add the \u003ccode\u003eseverity=ERROR\u003c/code\u003e, the severity dropdown is updated to show that ERROR is selected.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"08.show_query_severity_selector.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/08.show_query_severity_selector.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eNext, if you select  “DEBUG” from the severity dropdown, the query is updated to \u003ccode\u003eseverity=(ERROR OR DEBUG)\u003c/code\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"09.severity_selectors.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/09.severity_selectors.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eMaintaining the query state regardless of whether you’re selecting from the dropdowns or typing in queries means there is one less detail to remember when you’re querying your logs.\u003c/p\u003e\u003ch3\u003eDisabling default summary fields for a basic log view\u003c/h3\u003e\u003cp\u003eThe Logs Explorer adds default \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields\"\u003esummary fields\u003c/a\u003e to the log results to highlight useful information and make it easy to take action directly from the log line. For example, on App Engine logs, the default summary field chips highlight the latency which can help you more easily filter logs.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"10.custom_summary_fields.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/10.custom_summary_fields.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eLogs Explorer also enables you to add your own \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface#add_summary_fields\"\u003ecustom summary fields\u003c/a\u003e to the log lines so you can view what’s most important to you about the log lines. \u003c/p\u003e\u003cp\u003eWe’ve heard feedback that sometimes all that’s needed is the raw text of logs. To show raw text logs, we’ve added a toggle to turn off/on the default summary fields for your log results for the duration of your session. By turning off the default summary fields, you’ll see only the raw text logs summary. To turn the summary fields back on, simply enable the toggle or start a new Logs Explorer session in a new tab.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"11.logs_explorer_simple_query_mode_hide_default_summary_fields.gif\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/11.logs_explorer_simple_query_mode_hide_default_summary_fields.gif\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eThe road ahead\u003c/h3\u003e\u003cp\u003eWe’re committed to making Logs Explorer the best place to troubleshoot your applications running on Google Cloud. Over the coming months, we have many more changes planned to make Logs Explorer both easier and more powerful for all users. If you haven’t already, get started with the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-explorer-interface\"\u003eLogs Explorer\u003c/a\u003e and join the discussion in our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Cloud-Operations/bd-p/cloud-operations\" target=\"_blank\"\u003eCloud Operations page\u003c/a\u003e on the Google Cloud Community site.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/operations/faster-debugging-with-traces-and-logs-together/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_logging_OUrfE4R.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eEnabling SRE best practices: new contextual traces in Cloud Logging\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eDevelopers can now view trace information for applications directly in Google Cloud Logging for faster debugging.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-04-25T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eCharles Baer\u003c/name\u003e\u003ctitle\u003eProduct Manager, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/databases/making-mongodb-on-google-cloud-even-more-flexible-with-pay-go/",
      "title": "Google Cloud and MongoDB Atlas expand their partnership",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cdiv _ngcontent-c74=\"\" innerhtml=\"\u0026lt;p\u0026gt;As the volume and velocity of data grows daily, business success depends on the ability to manage it effectively and transform it into actionable insights. Since 2019, \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/partners/mongodb-and-google-extend-partnership-drive-enterprise-cloud-modernization\u0026#34;\u0026gt;Google Cloud and MongoDB have worked together\u0026lt;/a\u0026gt; to give businesses the secure, global, and highly performant infrastructure, the sophisticated data intelligence, and the developer-centric tools they need to power modern, data-driven cloud applications. Our partnership with MongoDB continues to deliver richer yet simpler ways to lead with software.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;With MongoDB Atlas on Google Cloud, developers can build upon a solid foundation that enables them to work with data the way they want in support of global-scale applications. For example, \u0026lt;a href=\u0026#34;https://www.youtube.com/watch?v=vvBZ-LFOHko\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Forbes\u0026lt;/a\u0026gt;, a large business media brand, migrated its platform to Google Cloud and MongoDB Atlas in just six months. The company\u0026amp;#8217;s new cloud infrastructure helped the website scale to accommodate record-breaking growth even while making development more nimble. MongoDB\u0026amp;#8217;s document model meant developers could build new features quickly, easily incorporate changes, and better handle a growing diversity of data types. It also allowed for more powerful tools, such a machine-language trending story recommendation engine for journalists. Results have been impressive, including:\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;58% faster build time for new products and fixes\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Accelerated release cycle by 4x\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Reduced total cost of ownership by 25%\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;28% increase in subscriptions from new newsletters\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;The Forbes team is already looking ahead, with plans for improved personalization, loyalty, and management of first-party data. \u0026amp;#8220;Our decision to migrate to Google Cloud was made based on the toolset that it offers, scalability, and developer friendliness,\u0026amp;#8221; says Vadim Supitsky, Forbes CTO.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Over the course of the partnership between MongoDB and Google Cloud, we have continually added new benefits for our mutual customers, such as the ability to to integrate \u0026lt;a href=\u0026#34;https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro?utm_campaign=gcpatlaspro\u0026amp;amp;utm_source=linkedin\u0026amp;amp;utm_medium=organic_social\u0026#34;\u0026gt;MongoDB Atlas with Google Cloud\u0026lt;/a\u0026gt; products; leveraging \u0026lt;a href=\u0026#34;https://cloud.google.com/bigquery\u0026#34;\u0026gt;BigQuery\u0026lt;/a\u0026gt; to create a managed, serverless, scalable architecture; rich data connectivity; and flexible scaling. We\u0026amp;#8217;ve also made it easier to migrate MongoDB on-premises instances to MongoDB Atlas on Google Cloud.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Making MongoDB on Google Cloud even more flexible with Pay-Go\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;That\u0026amp;#8217;s why we\u0026amp;#8217;re excited to see\u0026amp;#160; yet another reason for companies to choose MongoDB Atlas on Google Cloud: A new pay-as-you-go option, available on the \u0026lt;a href=\u0026#34;https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service\u0026#34;\u0026gt;Google Cloud Marketplace\u0026lt;/a\u0026gt;. With this new offering, developers now have a simplified subscription experience, and enterprises have a simplified way to procure MongoDB in addition to privately negotiated offers already supported on the Google Cloud Marketplace. There are no up-front commitments required to use MongoDB Atlas on Google Cloud, and customers pay only for the resources they use and scale based on their needs. Here are just a few of the benefits of this new service:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Spin up a MongoDB Atlas cluster on the Google Cloud Console within a few minutes\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;No need for a separate credit card payment: You can use your Google Cloud Billing account for your MongoDB Atlas environment\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Receive a single bill for Google Cloud and MongoDB Atlas\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Apply Google Cloud committed spend to MongoDB transactions through Google Cloud Marketplace\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Get started with MongoDB Atlas with 512 MB of storage for free. Atlas free tier clusters are perfect for learning MongoDB or prototyping applications\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;We\u0026amp;#8217;re excited to see even further growth in our continuing partnership with three new developments: First, because resellers are such an important force, we are pleased to share that reseller partners can now make MongoDB Atlas available via the Google Cloud Marketplace. Second, MongoDB and Google Cloud have expanded our joint reach to 28 global regions after adding Toronto, Canada, and Santiago, Chile. And third, we\u0026amp;#8217;ve made a joint commitment to early stage companies through \u0026lt;a href=\u0026#34;https://www.mongodb.com/startups\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;MongoDB for Startups\u0026lt;/a\u0026gt; and the \u0026lt;a href=\u0026#34;https://cloud.google.com/startup\u0026#34;\u0026gt;Google for Startups Cloud Program\u0026lt;/a\u0026gt;. Pay-Go is great for startups because it\u0026amp;#8217;s easy to scale and you only pay for what you use. Similarly, MongoDB and Google are helping startups get off the ground with our respective programs, which both include credits and support.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Better together\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Your company\u0026amp;#8217;s transformation hinges on new cloud database capabilities. The first step to building all-new digital experiences is to select the operational database that will power your application and, in essence, run your business.The partnership between MongoDB and Google Cloud gives you the benefits of a modern database service in a tightly integrated, cloud-native way. Since the beginning of our collaborative journey, we have made significant enhancements to the experience for our mutual customers, and we continue to work toward providing a rich developer experience for MongoDB Atlas on Google Cloud.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/mongodb?utm_source=youtube\u0026amp;amp;utm_medium=unpaidsoc\u0026amp;amp;utm_campaign=fy21q3-googlecloud-web-data-description-no-brand-global\u0026amp;amp;utm_content=skyvine1016918617\u0026amp;amp;utm_term=-\u0026#34;\u0026gt;Discover how customers\u0026lt;/a\u0026gt; choose MongoDB and Google Cloud to power the future of customer innovation. For more information on how to get started, visit \u0026lt;a href=\u0026#34;https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service/\u0026#34;\u0026gt;MongoDB Atlas on Google Cloud Marketplace.\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eAs the volume and velocity of data grows daily, business success depends on the ability to manage it effectively and transform it into actionable insights. Since 2019, \u003ca href=\"https://cloud.google.com/blog/topics/partners/mongodb-and-google-extend-partnership-drive-enterprise-cloud-modernization\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/partners/mongodb-and-google-extend-partnership-drive-enterprise-cloud-modernization\" track-metadata-module=\"post\"\u003eGoogle Cloud and MongoDB have worked together\u003c/a\u003e to give businesses the secure, global, and highly performant infrastructure, the sophisticated data intelligence, and the developer-centric tools they need to power modern, data-driven cloud applications. Our partnership with MongoDB continues to deliver richer yet simpler ways to lead with software. \u003c/p\u003e\u003cp\u003eWith MongoDB Atlas on Google Cloud, developers can build upon a solid foundation that enables them to work with data the way they want in support of global-scale applications. For example, \u003ca href=\"https://www.youtube.com/watch?v=vvBZ-LFOHko\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://www.youtube.com\" track-metadata-module=\"post\"\u003eForbes\u003c/a\u003e, a large business media brand, migrated its platform to Google Cloud and MongoDB Atlas in just six months. The company’s new cloud infrastructure helped the website scale to accommodate record-breaking growth even while making development more nimble. MongoDB’s document model meant developers could build new features quickly, easily incorporate changes, and better handle a growing diversity of data types. It also allowed for more powerful tools, such a machine-language trending story recommendation engine for journalists. Results have been impressive, including:\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e58% faster build time for new products and fixes\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAccelerated release cycle by 4x\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eReduced total cost of ownership by 25%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e28% increase in subscriptions from new newsletters\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe Forbes team is already looking ahead, with plans for improved personalization, loyalty, and management of first-party data. “Our decision to migrate to Google Cloud was made based on the toolset that it offers, scalability, and developer friendliness,” says Vadim Supitsky, Forbes CTO.\u003c/p\u003e\u003cp\u003eOver the course of the partnership between MongoDB and Google Cloud, we have continually added new benefits for our mutual customers, such as the ability to to integrate \u003ca href=\"https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro?utm_campaign=gcpatlaspro\u0026amp;utm_source=linkedin\u0026amp;utm_medium=organic_social\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro?utm_campaign=gcpatlaspro\u0026amp;utm_source=linkedin\u0026amp;utm_medium=organic_social\" track-metadata-module=\"post\"\u003eMongoDB Atlas with Google Cloud\u003c/a\u003e products; leveraging \u003ca href=\"https://cloud.google.com/bigquery\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/bigquery\" track-metadata-module=\"post\"\u003eBigQuery\u003c/a\u003e to create a managed, serverless, scalable architecture; rich data connectivity; and flexible scaling. We’ve also made it easier to migrate MongoDB on-premises instances to MongoDB Atlas on Google Cloud. \u003c/p\u003e\u003ch3\u003eMaking MongoDB on Google Cloud even more flexible with Pay-Go\u003c/h3\u003e\u003cp\u003eThat’s why we’re excited to see  yet another reason for companies to choose MongoDB Atlas on Google Cloud: A new pay-as-you-go option, available on the \u003ca href=\"https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service\" track-metadata-module=\"post\"\u003eGoogle Cloud Marketplace\u003c/a\u003e. With this new offering, developers now have a simplified subscription experience, and enterprises have a simplified way to procure MongoDB in addition to privately negotiated offers already supported on the Google Cloud Marketplace. There are no up-front commitments required to use MongoDB Atlas on Google Cloud, and customers pay only for the resources they use and scale based on their needs. Here are just a few of the benefits of this new service:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSpin up a MongoDB Atlas cluster on the Google Cloud Console within a few minutes\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNo need for a separate credit card payment: You can use your Google Cloud Billing account for your MongoDB Atlas environment\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eReceive a single bill for Google Cloud and MongoDB Atlas \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eApply Google Cloud committed spend to MongoDB transactions through Google Cloud Marketplace\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGet started with MongoDB Atlas with 512 MB of storage for free. Atlas free tier clusters are perfect for learning MongoDB or prototyping applications\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’re excited to see even further growth in our continuing partnership with three new developments: First, because resellers are such an important force, we are pleased to share that reseller partners can now make MongoDB Atlas available via the Google Cloud Marketplace. Second, MongoDB and Google Cloud have expanded our joint reach to 28 global regions after adding Toronto, Canada, and Santiago, Chile. And third, we’ve made a joint commitment to early stage companies through \u003ca href=\"https://www.mongodb.com/startups\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://www.mongodb.com\" track-metadata-module=\"post\"\u003eMongoDB for Startups\u003c/a\u003e and the \u003ca href=\"https://cloud.google.com/startup\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/startup\" track-metadata-module=\"post\"\u003eGoogle for Startups Cloud Program\u003c/a\u003e. Pay-Go is great for startups because it’s easy to scale and you only pay for what you use. Similarly, MongoDB and Google are helping startups get off the ground with our respective programs, which both include credits and support.  \u003c/p\u003e\u003ch3\u003eBetter together\u003c/h3\u003e\u003cp\u003eYour company’s transformation hinges on new cloud database capabilities. The first step to building all-new digital experiences is to select the operational database that will power your application and, in essence, run your business.The partnership between MongoDB and Google Cloud gives you the benefits of a modern database service in a tightly integrated, cloud-native way. Since the beginning of our collaborative journey, we have made significant enhancements to the experience for our mutual customers, and we continue to work toward providing a rich developer experience for MongoDB Atlas on Google Cloud.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/mongodb?utm_source=youtube\u0026amp;utm_medium=unpaidsoc\u0026amp;utm_campaign=fy21q3-googlecloud-web-data-description-no-brand-global\u0026amp;utm_content=skyvine1016918617\u0026amp;utm_term=-\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/mongodb?utm_source=youtube\u0026amp;utm_medium=unpaidsoc\u0026amp;utm_campaign=fy21q3-googlecloud-web-data-description-no-brand-global\u0026amp;utm_content=skyvine1016918617\u0026amp;utm_term=-\" track-metadata-module=\"post\"\u003eDiscover how customers\u003c/a\u003e choose MongoDB and Google Cloud to power the future of customer innovation. For more information on how to get started, visit \u003ca href=\"https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service/\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service/\" track-metadata-module=\"post\"\u003eMongoDB Atlas on Google Cloud Marketplace.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAs the volume and velocity of data grows daily, business success depends on the ability to manage it effectively and transform it into actionable insights. Since 2019, \u003ca href=\"https://cloud.google.com/blog/topics/partners/mongodb-and-google-extend-partnership-drive-enterprise-cloud-modernization\"\u003eGoogle Cloud and MongoDB have worked together\u003c/a\u003e to give businesses the secure, global, and highly performant infrastructure, the sophisticated data intelligence, and the developer-centric tools they need to power modern, data-driven cloud applications. Our partnership with MongoDB continues to deliver richer yet simpler ways to lead with software. \u003c/p\u003e\u003cp\u003eWith MongoDB Atlas on Google Cloud, developers can build upon a solid foundation that enables them to work with data the way they want in support of global-scale applications. For example, \u003ca href=\"https://www.youtube.com/watch?v=vvBZ-LFOHko\" target=\"_blank\"\u003eForbes\u003c/a\u003e, a large business media brand, migrated its platform to Google Cloud and MongoDB Atlas in just six months. The company’s new cloud infrastructure helped the website scale to accommodate record-breaking growth even while making development more nimble. MongoDB’s document model meant developers could build new features quickly, easily incorporate changes, and better handle a growing diversity of data types. It also allowed for more powerful tools, such a machine-language trending story recommendation engine for journalists. Results have been impressive, including:\u003cbr/\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e58% faster build time for new products and fixes\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAccelerated release cycle by 4x\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eReduced total cost of ownership by 25%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e28% increase in subscriptions from new newsletters\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe Forbes team is already looking ahead, with plans for improved personalization, loyalty, and management of first-party data. “Our decision to migrate to Google Cloud was made based on the toolset that it offers, scalability, and developer friendliness,” says Vadim Supitsky, Forbes CTO.\u003c/p\u003e\u003cp\u003eOver the course of the partnership between MongoDB and Google Cloud, we have continually added new benefits for our mutual customers, such as the ability to to integrate \u003ca href=\"https://console.cloud.google.com/marketplace/product/mongodb/atlas-pro?utm_campaign=gcpatlaspro\u0026amp;utm_source=linkedin\u0026amp;utm_medium=organic_social\"\u003eMongoDB Atlas with Google Cloud\u003c/a\u003e products; leveraging \u003ca href=\"https://cloud.google.com/bigquery\"\u003eBigQuery\u003c/a\u003e to create a managed, serverless, scalable architecture; rich data connectivity; and flexible scaling. We’ve also made it easier to migrate MongoDB on-premises instances to MongoDB Atlas on Google Cloud. \u003c/p\u003e\u003ch3\u003eMaking MongoDB on Google Cloud even more flexible with Pay-Go\u003c/h3\u003e\u003cp\u003eThat’s why we’re excited to see  yet another reason for companies to choose MongoDB Atlas on Google Cloud: A new pay-as-you-go option, available on the \u003ca href=\"https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service\"\u003eGoogle Cloud Marketplace\u003c/a\u003e. With this new offering, developers now have a simplified subscription experience, and enterprises have a simplified way to procure MongoDB in addition to privately negotiated offers already supported on the Google Cloud Marketplace. There are no up-front commitments required to use MongoDB Atlas on Google Cloud, and customers pay only for the resources they use and scale based on their needs. Here are just a few of the benefits of this new service:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSpin up a MongoDB Atlas cluster on the Google Cloud Console within a few minutes\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNo need for a separate credit card payment: You can use your Google Cloud Billing account for your MongoDB Atlas environment\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eReceive a single bill for Google Cloud and MongoDB Atlas \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eApply Google Cloud committed spend to MongoDB transactions through Google Cloud Marketplace\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGet started with MongoDB Atlas with 512 MB of storage for free. Atlas free tier clusters are perfect for learning MongoDB or prototyping applications\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’re excited to see even further growth in our continuing partnership with three new developments: First, because resellers are such an important force, we are pleased to share that reseller partners can now make MongoDB Atlas available via the Google Cloud Marketplace. Second, MongoDB and Google Cloud have expanded our joint reach to 28 global regions after adding Toronto, Canada, and Santiago, Chile. And third, we’ve made a joint commitment to early stage companies through \u003ca href=\"https://www.mongodb.com/startups\" target=\"_blank\"\u003eMongoDB for Startups\u003c/a\u003e and the \u003ca href=\"https://cloud.google.com/startup\"\u003eGoogle for Startups Cloud Program\u003c/a\u003e. Pay-Go is great for startups because it’s easy to scale and you only pay for what you use. Similarly, MongoDB and Google are helping startups get off the ground with our respective programs, which both include credits and support.  \u003c/p\u003e\u003ch3\u003eBetter together\u003c/h3\u003e\u003cp\u003eYour company’s transformation hinges on new cloud database capabilities. The first step to building all-new digital experiences is to select the operational database that will power your application and, in essence, run your business.The partnership between MongoDB and Google Cloud gives you the benefits of a modern database service in a tightly integrated, cloud-native way. Since the beginning of our collaborative journey, we have made significant enhancements to the experience for our mutual customers, and we continue to work toward providing a rich developer experience for MongoDB Atlas on Google Cloud.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/mongodb?utm_source=youtube\u0026amp;utm_medium=unpaidsoc\u0026amp;utm_campaign=fy21q3-googlecloud-web-data-description-no-brand-global\u0026amp;utm_content=skyvine1016918617\u0026amp;utm_term=-\"\u003eDiscover how customers\u003c/a\u003e choose MongoDB and Google Cloud to power the future of customer innovation. For more information on how to get started, visit \u003ca href=\"https://console.cloud.google.com/marketplace/product/mongodb/mdb-atlas-self-service/\"\u003eMongoDB Atlas on Google Cloud Marketplace.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/databases/ochk-used-cloud-spanner-to-build-covid-19-vaccine-app/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_Databases.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eVaccinating a nation: Vaccination app delivery in 30 days with Cloud Spanner\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eOChK used Cloud Spanner to design and deploy an application to help vaccinate every citizen in Poland against COVID-19.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-04-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eRitika Suri\u003c/name\u003e\u003ctitle\u003eDirector Data Technology Partnerships, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/the-sre-book-turns-6/",
      "title": "The SRE book turns 6!",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c52=\"\"\u003e\u003cdiv _ngcontent-c52=\"\" innerhtml=\"\u0026lt;p\u0026gt;It\u0026#39;s hard to believe that it\u0026#39;s already been six years since we published \u0026lt;a href=\u0026#34;https://sre.google/sre-book/table-of-contents/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Site Reliability Engineering: How Google Runs Production Systems\u0026lt;/a\u0026gt; with O\u0026#39;Reilly Media. We\u0026#39;ve been both humbled and pleasantly surprised by how popular the book has been, and continues to be. You may already be familiar with the two related books Google published after the SRE Book became a bestseller: \u0026lt;a href=\u0026#34;https://sre.google/workbook/table-of-contents/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;The Site Reliability Workbook\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Building Secure and Reliable Systems\u0026lt;/a\u0026gt;. All three books are available for free at \u0026lt;a href=\u0026#34;http://sre.google/books\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;sre.google/books\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It\u0026#39;s perhaps harder to find and explore the numerous journal articles, longer format reports, blog posts, and trainings that Google SREs have published since 2016. Google SREs have also given dozens of talks at conferences about the topics covered in the SRE Book in the intervening years. While the content in the book remains largely evergreen, SRE is a dynamic field, and we\u0026#39;ve had a lot more to say as our practices have evolved and gained depth.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To make this body of work more discoverable, we\u0026#39;ve put together a compendium of this material, mapped by topic to each chapter of the book on sre.google: \u0026lt;a href=\u0026#34;https://sre.google/resources/book-update/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SRE Book Updates, by Topic\u0026lt;/a\u0026gt;. Here you\u0026#39;ll find dozens more resources on some of our most popular topics, such as SLOs, Monitoring and Alerting, Canarying, Incident Management and Postmortem Culture, and Training SREs. Please explore away!\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Of course, SREs have also spoken and written about topics beyond what\u0026#39;s covered in the SRE Book (for example: Machine Learning, Capacity Planning, Innovations, and Security and Privacy); stay tuned for a catalog of those resources.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eIt\u0026#39;s hard to believe that it\u0026#39;s already been six years since we published \u003ca href=\"https://sre.google/sre-book/table-of-contents/\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSite Reliability Engineering: How Google Runs Production Systems\u003c/a\u003e with O\u0026#39;Reilly Media. We\u0026#39;ve been both humbled and pleasantly surprised by how popular the book has been, and continues to be. You may already be familiar with the two related books Google published after the SRE Book became a bestseller: \u003ca href=\"https://sre.google/workbook/table-of-contents/\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eThe Site Reliability Workbook\u003c/a\u003e and \u003ca href=\"https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://static.googleusercontent.com\" track-metadata-module=\"post\"\u003eBuilding Secure and Reliable Systems\u003c/a\u003e. All three books are available for free at \u003ca href=\"http://sre.google/books\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"http://sre.google\" track-metadata-module=\"post\"\u003esre.google/books\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eIt\u0026#39;s perhaps harder to find and explore the numerous journal articles, longer format reports, blog posts, and trainings that Google SREs have published since 2016. Google SREs have also given dozens of talks at conferences about the topics covered in the SRE Book in the intervening years. While the content in the book remains largely evergreen, SRE is a dynamic field, and we\u0026#39;ve had a lot more to say as our practices have evolved and gained depth. \u003c/p\u003e\u003cp\u003eTo make this body of work more discoverable, we\u0026#39;ve put together a compendium of this material, mapped by topic to each chapter of the book on sre.google: \u003ca href=\"https://sre.google/resources/book-update/\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSRE Book Updates, by Topic\u003c/a\u003e. Here you\u0026#39;ll find dozens more resources on some of our most popular topics, such as SLOs, Monitoring and Alerting, Canarying, Incident Management and Postmortem Culture, and Training SREs. Please explore away!\u003c/p\u003e\u003cp\u003eOf course, SREs have also spoken and written about topics beyond what\u0026#39;s covered in the SRE Book (for example: Machine Learning, Capacity Planning, Innovations, and Security and Privacy); stay tuned for a catalog of those resources.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIt's hard to believe that it's already been six years since we published \u003ca href=\"https://sre.google/sre-book/table-of-contents/\" target=\"_blank\"\u003eSite Reliability Engineering: How Google Runs Production Systems\u003c/a\u003e with O'Reilly Media. We've been both humbled and pleasantly surprised by how popular the book has been, and continues to be. You may already be familiar with the two related books Google published after the SRE Book became a bestseller: \u003ca href=\"https://sre.google/workbook/table-of-contents/\" target=\"_blank\"\u003eThe Site Reliability Workbook\u003c/a\u003e and \u003ca href=\"https://static.googleusercontent.com/media/sre.google/en//static/pdf/building_secure_and_reliable_systems.pdf\" target=\"_blank\"\u003eBuilding Secure and Reliable Systems\u003c/a\u003e. All three books are available for free at \u003ca href=\"http://sre.google/books\" target=\"_blank\"\u003esre.google/books\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eIt's perhaps harder to find and explore the numerous journal articles, longer format reports, blog posts, and trainings that Google SREs have published since 2016. Google SREs have also given dozens of talks at conferences about the topics covered in the SRE Book in the intervening years. While the content in the book remains largely evergreen, SRE is a dynamic field, and we've had a lot more to say as our practices have evolved and gained depth. \u003c/p\u003e\u003cp\u003eTo make this body of work more discoverable, we've put together a compendium of this material, mapped by topic to each chapter of the book on sre.google: \u003ca href=\"https://sre.google/resources/book-update/\" target=\"_blank\"\u003eSRE Book Updates, by Topic\u003c/a\u003e. Here you'll find dozens more resources on some of our most popular topics, such as SLOs, Monitoring and Alerting, Canarying, Incident Management and Postmortem Culture, and Training SREs. Please explore away!\u003c/p\u003e\u003cp\u003eOf course, SREs have also spoken and written about topics beyond what's covered in the SRE Book (for example: Machine Learning, Capacity Planning, Innovations, and Security and Privacy); stay tuned for a catalog of those resources.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/prodcast.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eIntroducing the Google SRE Prodcast\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eDiscover Prodcast, Google’s Site Reliability Engineering Podcast. This limited-edition series explores fundamental topics in reliability ...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-04-19T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eBetsy Beyer\u003c/name\u003e\u003ctitle\u003eTechnical Writer for SRE\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/cloud-first/whats-new-cloud-native-apps/",
      "title": "What’s new in cloud-native apps?",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c45=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003einframod living.jpg\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003carticle-cta _nghost-c47=\"\"\u003e\u003cdiv _ngcontent-c47=\"\"\u003e\u003ch4 _ngcontent-c47=\"\"\u003e\u003cspan _ngcontent-c47=\"\"\u003eAnchoring on Containers\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c47=\"\"\u003e\u003cspan _ngcontent-c47=\"\"\u003eLearn why Google Cloud’s container offerings lead the market\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c47=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"forrester_container_feb2022\" track-metadata-eventdetail=\"https://cloud.google.com/resources/forrester-wave-container-platforms-report\" href=\"https://cloud.google.com/resources/forrester-wave-container-platforms-report\"\u003e\u003cspan _ngcontent-c47=\"\"\u003eDownload\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cp\u003eDevelopers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Apr 11 - Apr 15, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Listen to a Prodcast\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Google\u0026amp;#8217;s SRE team has launched a \u0026amp;#8220;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast\u0026#34;\u0026gt;Prodcast\u0026lt;/a\u0026gt;\u0026amp;#8221; focusing on concepts from its SRE book. Available from wherever you get your podcasts.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Run Apache Spark on a modern container base\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Dataproc, our managed version of Apache Spark, is now generally available on Google Kubernetes Engine (GKE), allowing you to create a Dataproc cluster and submit Spark jobs on a self-managed GKE cluster. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/infrastructure-modernization/running-spark-on-kubernetes-with-dataproc\u0026#34;\u0026gt;Read all about it\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Loads of new runtimes in App Engine and Cloud Functions\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Java, Ruby, Python and PHP developers, rejoice! You can now update or develop new App Engine apps and Cloud Functions using \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/new-java-ruby-python-php-runtimes\u0026#34;\u0026gt;Java 17, Ruby 3, Python 3.10 and PHP 8.1\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;BeReal shows you how modern app development is done\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Social media company BeReal discusses how it uses Google Cloud services including Firebase, Cloud Functions and GKE to \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/startups/bereal-creates-reality-based-social-media-using-google-cloud\u0026#34;\u0026gt;build its app\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Build fast without breaking things\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;In this three-part series, learn about the Supply-chain Levels for Software Artifacts (SLSA) framework designed to improve the integrity of your software packages and infrastructure. Start with, \u0026lt;a href=\u0026#34;https://security.googleblog.com/2022/04/how-to-slsa-part-1-basics.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;How to SLSA Part 1 - The Basics\u0026lt;/a\u0026gt;, then move on to \u0026lt;a href=\u0026#34;https://security.googleblog.com/2022/04/how-to-slsa-part-2-details.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;part 2\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://security.googleblog.com/2022/04/how-to-slsa-part-3-putting-it-all.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;part 3\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Apr 11 - Apr 15, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eListen to a Prodcast\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGoogle’s SRE team has launched a “\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast\" track-metadata-module=\"post\"\u003eProdcast\u003c/a\u003e” focusing on concepts from its SRE book. Available from wherever you get your podcasts. \u003c/p\u003e\u003cp\u003e\u003cb\u003eRun Apache Spark on a modern container base\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eDataproc, our managed version of Apache Spark, is now generally available on Google Kubernetes Engine (GKE), allowing you to create a Dataproc cluster and submit Spark jobs on a self-managed GKE cluster. \u003ca href=\"https://cloud.google.com/blog/products/infrastructure-modernization/running-spark-on-kubernetes-with-dataproc\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/infrastructure-modernization/running-spark-on-kubernetes-with-dataproc\" track-metadata-module=\"post\"\u003eRead all about it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eLoads of new runtimes in App Engine and Cloud Functions\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eJava, Ruby, Python and PHP developers, rejoice! You can now update or develop new App Engine apps and Cloud Functions using \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/new-java-ruby-python-php-runtimes\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/new-java-ruby-python-php-runtimes\" track-metadata-module=\"post\"\u003eJava 17, Ruby 3, Python 3.10 and PHP 8.1\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cb\u003eBeReal shows you how modern app development is done\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eSocial media company BeReal discusses how it uses Google Cloud services including Firebase, Cloud Functions and GKE to \u003ca href=\"https://cloud.google.com/blog/topics/startups/bereal-creates-reality-based-social-media-using-google-cloud\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/startups/bereal-creates-reality-based-social-media-using-google-cloud\" track-metadata-module=\"post\"\u003ebuild its app\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBuild fast without breaking things\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn this three-part series, learn about the Supply-chain Levels for Software Artifacts (SLSA) framework designed to improve the integrity of your software packages and infrastructure. Start with, \u003ca href=\"https://security.googleblog.com/2022/04/how-to-slsa-part-1-basics.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://security.googleblog.com\" track-metadata-module=\"post\"\u003eHow to SLSA Part 1 - The Basics\u003c/a\u003e, then move on to \u003ca href=\"https://security.googleblog.com/2022/04/how-to-slsa-part-2-details.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://security.googleblog.com\" track-metadata-module=\"post\"\u003epart 2\u003c/a\u003e and \u003ca href=\"https://security.googleblog.com/2022/04/how-to-slsa-part-3-putting-it-all.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://security.googleblog.com\" track-metadata-module=\"post\"\u003epart 3\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Apr 4 - Apr 8, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;How to migrate a container from a VM to Cloud Run\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;With Cloud Run, you can migrate a legacy VM to a container and save money \u0026amp;#8211; even if you don\u0026amp;#8217;t know Kubernetes. This \u0026lt;a href=\u0026#34;https://youtu.be/HKuUmzSpljU\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;video\u0026lt;/a\u0026gt; shows you how.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Receive Error Reporting notifications through Slack and Webhooks\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Error Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications\u0026#34;\u0026gt;blog\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cloud-native architecture is in the cards at NCR\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Earlier this year, NCR Authentic Cards talked about \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud\u0026#34;\u0026gt;how it built\u0026lt;/a\u0026gt; a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2\u0026#34;\u0026gt;part two of the migration story\u0026lt;/a\u0026gt;, taking a detailed look at all the components that went into the cloud-based architecture.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;How to easily share a service with Cloud Run\u0026amp;#160;\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Have you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates \u0026lt;a href=\u0026#34;https://coloring-page.lolo.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;an image processing service\u0026lt;/a\u0026gt; that generates coloring pages, then makes it available to others \u0026amp;#8212; all in under 200 lines of Python and JavaScript. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run\u0026#34;\u0026gt;Follow along in this tutorial\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Apr 4 - Apr 8, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eHow to migrate a container from a VM to Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith Cloud Run, you can migrate a legacy VM to a container and save money – even if you don’t know Kubernetes. This \u003ca href=\"https://youtu.be/HKuUmzSpljU\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://youtu.be\" track-metadata-module=\"post\"\u003evideo\u003c/a\u003e shows you how. \u003c/p\u003e\u003cp\u003e\u003cb\u003eReceive Error Reporting notifications through Slack and Webhooks\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eError Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications\" track-metadata-module=\"post\"\u003eblog\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud-native architecture is in the cards at NCR\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eEarlier this year, NCR Authentic Cards talked about \u003ca href=\"https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud\" track-metadata-module=\"post\"\u003ehow it built\u003c/a\u003e a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for \u003ca href=\"https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2\" track-metadata-module=\"post\"\u003epart two of the migration story\u003c/a\u003e, taking a detailed look at all the components that went into the cloud-based architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eHow to easily share a service with Cloud Run \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eHave you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates \u003ca href=\"https://coloring-page.lolo.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://coloring-page.lolo.dev\" track-metadata-module=\"post\"\u003ean image processing service\u003c/a\u003e that generates coloring pages, then makes it available to others — all in under 200 lines of Python and JavaScript. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run\" track-metadata-module=\"post\"\u003eFollow along in this tutorial\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 28 - Apr 1, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Another cool thing you can do with Cloud Functions\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\u0026#34;\u0026gt;shows you how\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Add custom severity levels to Cloud Monitoring alert policies\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Not all alerts are created equal. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\u0026#34;\u0026gt;In this blog post\u0026lt;/a\u0026gt;, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Learn how to use CPU allocation controls in Cloud Run\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Last fall, \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\u0026#34;\u0026gt;we added\u0026lt;/a\u0026gt; \u0026amp;#8220;always-on CPU\u0026amp;#8221; capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\u0026#34;\u0026gt;In this post\u0026lt;/a\u0026gt;, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app\u0026amp;#8217;s average user response latency by over 80%.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 28 - Apr 1, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eAnother cool thing you can do with Cloud Functions\u003cbr/\u003e\u003c/b\u003eGot data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial \u003ca href=\"https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\" track-metadata-module=\"post\"\u003eshows you how\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eAdd custom severity levels to Cloud Monitoring alert policies\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot all alerts are created equal. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\" track-metadata-module=\"post\"\u003eIn this blog post\u003c/a\u003e, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. \u003c/p\u003e\u003cp\u003e\u003cb\u003eLearn how to use CPU allocation controls in Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLast fall, \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\" track-metadata-module=\"post\"\u003ewe added\u003c/a\u003e “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\" track-metadata-module=\"post\"\u003eIn this post\u003c/a\u003e, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 21 - Mar 25, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Get Going with latest Go 1.18 release\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\u0026#34;\u0026gt;Learn more here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 21 - Mar 25, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eGet Going with latest Go 1.18 release\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. \u003ca href=\"https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\" track-metadata-module=\"post\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 14 - Mar 18, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Create EventArc triggers with Terraform\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\u0026#34;\u0026gt;shows you how\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Scaling to new markets with Cloud Run\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\u0026#34;\u0026gt;website architecture\u0026lt;/a\u0026gt; here.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;The serverless way to celebrate Pi Day\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\u0026#34;\u0026gt;calculate \u0026amp;#960;\u0026lt;/a\u0026gt; \u0026amp;#8212; serverlessly.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 14 - Mar 18, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eCreate EventArc triggers with Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\" track-metadata-module=\"post\"\u003eshows you how\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eScaling to new markets with Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eFrench publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its \u003ca href=\"https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\" track-metadata-module=\"post\"\u003ewebsite architecture\u003c/a\u003e here. \u003c/p\u003e\u003cp\u003e\u003cb\u003eThe serverless way to celebrate Pi Day\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\" track-metadata-module=\"post\"\u003ecalculate π\u003c/a\u003e — serverlessly.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 07 - Mar 11, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Rhode Island moves to Google Cloud-based job board\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\u0026#34;\u0026gt;how they did it\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Containerized microservices at Lowe\u0026amp;#8217;s\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Lowe\u0026amp;#8217;s already told us \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\u0026#34;\u0026gt;how they use SRE\u0026lt;/a\u0026gt;. They\u0026amp;#8217;re at it again, describing how they built an e-commerce website using a \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\u0026#34;\u0026gt;containerized microservices architecture and Kubernetes\u0026lt;/a\u0026gt;, with Istio for service mesh and Cloud Operations for good measure.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cruise AVs hit the road with Google Cloud services\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Autonomous Vehicle (AV) startup Cruise detailed how it\u0026amp;#8217;s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\u0026#34;\u0026gt;Read the guest post\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;L\u0026amp;#8217;Or\u0026amp;#233;al\u0026amp;#8217;s data analytics gets a makeover with serverless\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;We\u0026amp;#8217;re hurtling toward a \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\u0026#34;\u0026gt;programmable cloud\u0026lt;/a\u0026gt; \u0026amp;#8212; a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\u0026#34;\u0026gt;L\u0026amp;#8217;Or\u0026amp;#233;al is a great example\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Better telemetry for your Anthos clusters\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\u0026#34;\u0026gt;Anthos Service Mesh Dashboard\u0026lt;/a\u0026gt; is now available (public preview) on the \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\u0026#34;\u0026gt;Anthos clusters on Bare Metal\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\u0026#34;\u0026gt;Anthos clusters on VMware\u0026lt;/a\u0026gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Instrument your Java apps\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;With the new version of the \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\u0026#34;\u0026gt;Google Cloud Logging Java library\u0026lt;/a\u0026gt;, you can wire your application logs with more information \u0026amp;#8212; without adding a single line of code.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Visualize metrics from Cloud Spanner\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Building an app on top of Cloud Spanner but can\u0026amp;#8217;t assess how well it\u0026amp;#8217;s operating? The new \u0026lt;a href=\u0026#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;OpenTelemetery receiver for Cloud Spanner\u0026lt;/a\u0026gt; provides an easy way for you to process and visualize metrics from Cloud Spanner \u0026lt;a href=\u0026#34;https://cloud.google.com/spanner/docs/introspection\u0026#34;\u0026gt;System tables\u0026lt;/a\u0026gt;, and export these to the APM tool of your choice. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\u0026#34;\u0026gt;Read more here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 07 - Mar 11, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eRhode Island moves to Google Cloud-based job board\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhen the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\" track-metadata-module=\"post\"\u003ehow they did it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eContainerized microservices at Lowe’s\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLowe’s already told us \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\" track-metadata-module=\"post\"\u003ehow they use SRE\u003c/a\u003e. They’re at it again, describing how they built an e-commerce website using a \u003ca href=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\" track-metadata-module=\"post\"\u003econtainerized microservices architecture and Kubernetes\u003c/a\u003e, with Istio for service mesh and Cloud Operations for good measure.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCruise AVs hit the road with Google Cloud services\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eAutonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\" track-metadata-module=\"post\"\u003eRead the guest post\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eL’Oréal’s data analytics gets a makeover with serverless\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe’re hurtling toward a \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\" track-metadata-module=\"post\"\u003eprogrammable cloud\u003c/a\u003e — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u003ca href=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\" track-metadata-module=\"post\"\u003eL’Oréal is a great example\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBetter telemetry for your Anthos clusters\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\" track-metadata-module=\"post\"\u003eAnthos Service Mesh Dashboard\u003c/a\u003e is now available (public preview) on the \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\" track-metadata-module=\"post\"\u003eAnthos clusters on Bare Metal\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\" track-metadata-module=\"post\"\u003eAnthos clusters on VMware\u003c/a\u003e. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u003c/p\u003e\u003cp\u003e\u003cb\u003eInstrument your Java apps\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the new version of the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\" track-type=\"inline link\" track-name=\"31\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\" track-metadata-module=\"post\"\u003eGoogle Cloud Logging Java library\u003c/a\u003e, you can wire your application logs with more information — without adding a single line of code.\u003c/p\u003e\u003cp\u003e\u003cb\u003eVisualize metrics from Cloud Spanner\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBuilding an app on top of Cloud Spanner but can’t assess how well it’s operating? The new \u003ca href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\" target=\"_blank\" track-type=\"inline link\" track-name=\"32\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eOpenTelemetery receiver for Cloud Spanner\u003c/a\u003e provides an easy way for you to process and visualize metrics from Cloud Spanner \u003ca href=\"https://cloud.google.com/spanner/docs/introspection\" track-type=\"inline link\" track-name=\"33\" track-metadata-eventdetail=\"https://cloud.google.com/spanner/docs/introspection\" track-metadata-module=\"post\"\u003eSystem tables\u003c/a\u003e, and export these to the APM tool of your choice. \u003ca href=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\" track-type=\"inline link\" track-name=\"34\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\" track-metadata-module=\"post\"\u003eRead more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Feb 28 - Mar 4, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Introducing Cloud SDK\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;The rebranded \u0026lt;a href=\u0026#34;https://cloud.google.com/sdk\u0026#34;\u0026gt;Cloud SDK\u0026lt;/a\u0026gt; is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cloud CLI, meet Terraform\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Google Cloud CLI\u0026amp;#8217;s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google\u0026amp;#8217;s KRM declarative tooling, and is now \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\u0026#34;\u0026gt;available in preview\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Knative graduates to incubating project\u0026amp;#160;\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Congratulations to Knative, which has been \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\u0026#34;\u0026gt;accepted by the Cloud Native Computing Foundation\u0026lt;/a\u0026gt;, or CNCF, as an incubating project, enabling the next phase of serverless architecture.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;We manage Prometheus so you don\u0026amp;#8217;t have to\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus\u0026#34;\u0026gt;Google Cloud Managed Service for Prometheus\u0026lt;/a\u0026gt; is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\u0026#34;\u0026gt;Learn more here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Feb 28 - Mar 4, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eIntroducing Cloud SDK\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe rebranded \u003ca href=\"https://cloud.google.com/sdk\" track-type=\"inline link\" track-name=\"35\" track-metadata-eventdetail=\"https://cloud.google.com/sdk\" track-metadata-module=\"post\"\u003eCloud SDK\u003c/a\u003e is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more \u003ca href=\"https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\" track-type=\"inline link\" track-name=\"36\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud CLI, meet Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGoogle Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now \u003ca href=\"https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\" track-type=\"inline link\" track-name=\"37\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\" track-metadata-module=\"post\"\u003eavailable in preview\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eKnative graduates to incubating project \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eCongratulations to Knative, which has been \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\" track-type=\"inline link\" track-name=\"38\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\" track-metadata-module=\"post\"\u003eaccepted by the Cloud Native Computing Foundation\u003c/a\u003e, or CNCF, as an incubating project, enabling the next phase of serverless architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eWe manage Prometheus so you don’t have to\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\" track-type=\"inline link\" track-name=\"39\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\" track-metadata-module=\"post\"\u003eGoogle Cloud Managed Service for Prometheus\u003c/a\u003e is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\" track-type=\"inline link\" track-name=\"40\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\" track-metadata-module=\"post\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c48=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDevelopers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Apr 11 - Apr 15, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eListen to a Prodcast\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGoogle’s SRE team has launched a “\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast\"\u003eProdcast\u003c/a\u003e” focusing on concepts from its SRE book. Available from wherever you get your podcasts. \u003c/p\u003e\u003cp\u003e\u003cb\u003eRun Apache Spark on a modern container base\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eDataproc, our managed version of Apache Spark, is now generally available on Google Kubernetes Engine (GKE), allowing you to create a Dataproc cluster and submit Spark jobs on a self-managed GKE cluster. \u003ca href=\"https://cloud.google.com/blog/products/infrastructure-modernization/running-spark-on-kubernetes-with-dataproc\"\u003eRead all about it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eLoads of new runtimes in App Engine and Cloud Functions\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eJava, Ruby, Python and PHP developers, rejoice! You can now update or develop new App Engine apps and Cloud Functions using \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/new-java-ruby-python-php-runtimes\"\u003eJava 17, Ruby 3, Python 3.10 and PHP 8.1\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cb\u003eBeReal shows you how modern app development is done\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eSocial media company BeReal discusses how it uses Google Cloud services including Firebase, Cloud Functions and GKE to \u003ca href=\"https://cloud.google.com/blog/topics/startups/bereal-creates-reality-based-social-media-using-google-cloud\"\u003ebuild its app\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBuild fast without breaking things\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn this three-part series, learn about the Supply-chain Levels for Software Artifacts (SLSA) framework designed to improve the integrity of your software packages and infrastructure. Start with, \u003ca href=\"https://security.googleblog.com/2022/04/how-to-slsa-part-1-basics.html\" target=\"_blank\"\u003eHow to SLSA Part 1 - The Basics\u003c/a\u003e, then move on to \u003ca href=\"https://security.googleblog.com/2022/04/how-to-slsa-part-2-details.html\" target=\"_blank\"\u003epart 2\u003c/a\u003e and \u003ca href=\"https://security.googleblog.com/2022/04/how-to-slsa-part-3-putting-it-all.html\" target=\"_blank\"\u003epart 3\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/topics/sustainability/how-the-us-forest-service-uses-google-cloud/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/USFS.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003ePicture this: How the U.S. Forest Service uses Google Cloud tools to analyze a changing planet\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eFor over a decade, the U.S. Forest Service has been using Google Earth Engine and other Google Cloud tools to study our changing planet.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Apr 4 - Apr 8, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eHow to migrate a container from a VM to Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith Cloud Run, you can migrate a legacy VM to a container and save money – even if you don’t know Kubernetes. This \u003ca href=\"https://youtu.be/HKuUmzSpljU\" target=\"_blank\"\u003evideo\u003c/a\u003e shows you how. \u003c/p\u003e\u003cp\u003e\u003cb\u003eReceive Error Reporting notifications through Slack and Webhooks\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eError Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications\"\u003eblog\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud-native architecture is in the cards at NCR\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eEarlier this year, NCR Authentic Cards talked about \u003ca href=\"https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud\"\u003ehow it built\u003c/a\u003e a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for \u003ca href=\"https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2\"\u003epart two of the migration story\u003c/a\u003e, taking a detailed look at all the components that went into the cloud-based architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eHow to easily share a service with Cloud Run \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eHave you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates \u003ca href=\"https://coloring-page.lolo.dev/\" target=\"_blank\"\u003ean image processing service\u003c/a\u003e that generates coloring pages, then makes it available to others — all in under 200 lines of Python and JavaScript. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run\"\u003eFollow along in this tutorial\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/infrastructure/topaz-subsea-cable-connects-canada-and-asia/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Banner_Topaz_map_hero_Banner.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eIntroducing Topaz — the first subsea cable to connect Canada and Asia\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe Topaz subsea cable is the first fiber cable to connect Canada and Asia, and will provide better resiliency and lower latency for Goog...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 28 - Apr 1, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eAnother cool thing you can do with Cloud Functions\u003cbr/\u003e\u003c/b\u003eGot data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial \u003ca href=\"https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\"\u003eshows you how\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eAdd custom severity levels to Cloud Monitoring alert policies\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot all alerts are created equal. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\"\u003eIn this blog post\u003c/a\u003e, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. \u003c/p\u003e\u003cp\u003e\u003cb\u003eLearn how to use CPU allocation controls in Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLast fall, \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\"\u003ewe added\u003c/a\u003e “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\"\u003eIn this post\u003c/a\u003e, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_App_Dev_4.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGo 1.18 and Google Cloud: Go now with Google Cloud\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eGo 1.18 release and Google Cloud working better together.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 21 - Mar 25, 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eGet Going with latest Go 1.18 release\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. \u003ca href=\"https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 14 - Mar 18, 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eCreate EventArc triggers with Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\"\u003eshows you how\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eScaling to new markets with Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eFrench publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its \u003ca href=\"https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\"\u003ewebsite architecture\u003c/a\u003e here. \u003c/p\u003e\u003cp\u003e\u003cb\u003eThe serverless way to celebrate Pi Day\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\"\u003ecalculate π\u003c/a\u003e — serverlessly.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 07 - Mar 11, 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eRhode Island moves to Google Cloud-based job board\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhen the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\"\u003ehow they did it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eContainerized microservices at Lowe’s\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLowe’s already told us \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\"\u003ehow they use SRE\u003c/a\u003e. They’re at it again, describing how they built an e-commerce website using a \u003ca href=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\"\u003econtainerized microservices architecture and Kubernetes\u003c/a\u003e, with Istio for service mesh and Cloud Operations for good measure.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCruise AVs hit the road with Google Cloud services\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eAutonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\"\u003eRead the guest post\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eL’Oréal’s data analytics gets a makeover with serverless\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe’re hurtling toward a \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\"\u003eprogrammable cloud\u003c/a\u003e — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u003ca href=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\"\u003eL’Oréal is a great example\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBetter telemetry for your Anthos clusters\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\"\u003eAnthos Service Mesh Dashboard\u003c/a\u003e is now available (public preview) on the \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\"\u003eAnthos clusters on Bare Metal\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\"\u003eAnthos clusters on VMware\u003c/a\u003e. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u003c/p\u003e\u003cp\u003e\u003cb\u003eInstrument your Java apps\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the new version of the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\"\u003eGoogle Cloud Logging Java library\u003c/a\u003e, you can wire your application logs with more information — without adding a single line of code.\u003c/p\u003e\u003cp\u003e\u003cb\u003eVisualize metrics from Cloud Spanner\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBuilding an app on top of Cloud Spanner but can’t assess how well it’s operating? The new \u003ca href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\" target=\"_blank\"\u003eOpenTelemetery receiver for Cloud Spanner\u003c/a\u003e provides an easy way for you to process and visualize metrics from Cloud Spanner \u003ca href=\"https://cloud.google.com/spanner/docs/introspection\"\u003eSystem tables\u003c/a\u003e, and export these to the APM tool of your choice. \u003ca href=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\"\u003eRead more here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Feb 28 - Mar 4, 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntroducing Cloud SDK\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe rebranded \u003ca href=\"https://cloud.google.com/sdk\"\u003eCloud SDK\u003c/a\u003e is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more \u003ca href=\"https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud CLI, meet Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGoogle Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now \u003ca href=\"https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\"\u003eavailable in preview\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eKnative graduates to incubating project \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eCongratulations to Knative, which has been \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\"\u003eaccepted by the Cloud Native Computing Foundation\u003c/a\u003e, or CNCF, as an incubating project, enabling the next phase of serverless architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eWe manage Prometheus so you don’t have to\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eGoogle Cloud Managed Service for Prometheus\u003c/a\u003e is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/inframod_living_3.max-2200x2200.jpg",
      "date_published": "2022-04-15T20:00:00Z",
      "author": {
        "name": "\u003cname\u003eGoogle Cloud Content \u0026 Editorial \u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/discover-prodcast-the-site-reliability-engineering-podcast/",
      "title": "Introducing the Google SRE Prodcast",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;We\u0026#39;re excited to announce the launch of the Google SRE Prodcast, an interview-style podcast where Site Reliability Engineers at Google discuss key SRE concepts and share their experiences, advice, and strong opinions along the way.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Since early 2016, SREs at Google have been producing and listening to an internal version of the Prodcast. This year, we decided to think bigger and share insights from Google SREs with the wider SRE community.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The resulting Prodcast that we\u0026#39;re launching today is a series of conversations inspired by concepts from the \u0026lt;a href=\u0026#34;https://sre.google/sre-book/table-of-contents/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SRE Book\u0026lt;/a\u0026gt;. Over the course of nine episodes, domain experts explain, challenge, and reframe everything from SLOs to what it means to be an excellent SRE.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We had so much fun creating this series with our guests, and we hope you enjoy it. Check the SRE Prodcast out at \u0026lt;a href=\u0026#34;https://sre.google/prodcast\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;https://sre.google/prodcast\u0026lt;/a\u0026gt;!\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWe\u0026#39;re excited to announce the launch of the Google SRE Prodcast, an interview-style podcast where Site Reliability Engineers at Google discuss key SRE concepts and share their experiences, advice, and strong opinions along the way.\u003c/p\u003e\u003cp\u003eSince early 2016, SREs at Google have been producing and listening to an internal version of the Prodcast. This year, we decided to think bigger and share insights from Google SREs with the wider SRE community.\u003c/p\u003e\u003cp\u003eThe resulting Prodcast that we\u0026#39;re launching today is a series of conversations inspired by concepts from the \u003ca href=\"https://sre.google/sre-book/table-of-contents/\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSRE Book\u003c/a\u003e. Over the course of nine episodes, domain experts explain, challenge, and reframe everything from SLOs to what it means to be an excellent SRE.\u003c/p\u003e\u003cp\u003eWe had so much fun creating this series with our guests, and we hope you enjoy it. Check the SRE Prodcast out at \u003ca href=\"https://sre.google/prodcast\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003ehttps://sre.google/prodcast\u003c/a\u003e!\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe're excited to announce the launch of the Google SRE Prodcast, an interview-style podcast where Site Reliability Engineers at Google discuss key SRE concepts and share their experiences, advice, and strong opinions along the way.\u003c/p\u003e\u003cp\u003eSince early 2016, SREs at Google have been producing and listening to an internal version of the Prodcast. This year, we decided to think bigger and share insights from Google SREs with the wider SRE community.\u003c/p\u003e\u003cp\u003eThe resulting Prodcast that we're launching today is a series of conversations inspired by concepts from the \u003ca href=\"https://sre.google/sre-book/table-of-contents/\" target=\"_blank\"\u003eSRE Book\u003c/a\u003e. Over the course of nine episodes, domain experts explain, challenge, and reframe everything from SLOs to what it means to be an excellent SRE.\u003c/p\u003e\u003cp\u003eWe had so much fun creating this series with our guests, and we hope you enjoy it. Check the SRE Prodcast out at \u003ca href=\"https://sre.google/prodcast\" target=\"_blank\"\u003ehttps://sre.google/prodcast\u003c/a\u003e!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_aWHZoxD.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eAdd severity levels to your alert policies in Cloud Monitoring\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAdd static and dynamic severity levels to your alert policies for easier triaging and include these in notifications when sent to 3rd par...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/prodcast.max-2200x2200.jpg",
      "date_published": "2022-04-14T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eSRE Prodcast Team \u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/cloud-first/whats-new-cloud-native-apps/",
      "title": "What’s new in cloud-native apps?",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c61=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003einframod living.jpg\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003carticle-cta _nghost-c67=\"\"\u003e\u003cdiv _ngcontent-c67=\"\"\u003e\u003ch4 _ngcontent-c67=\"\"\u003e\u003cspan _ngcontent-c67=\"\"\u003eAnchoring on Containers\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c67=\"\"\u003e\u003cspan _ngcontent-c67=\"\"\u003eLearn why Google Cloud’s container offerings lead the market\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c67=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"forrester_container_feb2022\" track-metadata-eventdetail=\"https://cloud.google.com/resources/forrester-wave-container-platforms-report\" href=\"https://cloud.google.com/resources/forrester-wave-container-platforms-report\"\u003e\u003cspan _ngcontent-c67=\"\"\u003eDownload\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c69=\"\"\u003e\u003cp\u003eDevelopers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place. \u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c69=\"\"\u003e\u003cdiv _ngcontent-c69=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Apr 4 - Apr 8, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;How to migrate a container from a VM to Cloud Run\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;With Cloud Run, you can migrate a legacy VM to a container and save money \u0026amp;#8211; even if you don\u0026amp;#8217;t know Kubernetes. This \u0026lt;a href=\u0026#34;https://youtu.be/HKuUmzSpljU\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;video\u0026lt;/a\u0026gt; shows you how.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Receive Error Reporting notifications through Slack and Webhooks\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Error Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications\u0026#34;\u0026gt;blog\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cloud-native architecture is in the cards at NCR\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Earlier this year, NCR Authentic Cards talked about \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud\u0026#34;\u0026gt;how it built\u0026lt;/a\u0026gt; a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2\u0026#34;\u0026gt;part two of the migration story\u0026lt;/a\u0026gt;, taking a detailed look at all the components that went into the cloud-based architecture.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;How to easily share a service with Cloud Run\u0026amp;#160;\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Have you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates \u0026lt;a href=\u0026#34;https://coloring-page.lolo.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;an image processing service\u0026lt;/a\u0026gt; that generates coloring pages, then makes it available to others \u0026amp;#8212; all in under 200 lines of Python and JavaScript. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run\u0026#34;\u0026gt;Follow along in this tutorial\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Apr 4 - Apr 8, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eHow to migrate a container from a VM to Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith Cloud Run, you can migrate a legacy VM to a container and save money – even if you don’t know Kubernetes. This \u003ca href=\"https://youtu.be/HKuUmzSpljU\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://youtu.be\" track-metadata-module=\"post\"\u003evideo\u003c/a\u003e shows you how. \u003c/p\u003e\u003cp\u003e\u003cb\u003eReceive Error Reporting notifications through Slack and Webhooks\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eError Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications\" track-metadata-module=\"post\"\u003eblog\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud-native architecture is in the cards at NCR\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eEarlier this year, NCR Authentic Cards talked about \u003ca href=\"https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud\" track-metadata-module=\"post\"\u003ehow it built\u003c/a\u003e a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for \u003ca href=\"https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2\" track-metadata-module=\"post\"\u003epart two of the migration story\u003c/a\u003e, taking a detailed look at all the components that went into the cloud-based architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eHow to easily share a service with Cloud Run \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eHave you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates \u003ca href=\"https://coloring-page.lolo.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://coloring-page.lolo.dev\" track-metadata-module=\"post\"\u003ean image processing service\u003c/a\u003e that generates coloring pages, then makes it available to others — all in under 200 lines of Python and JavaScript. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run\" track-metadata-module=\"post\"\u003eFollow along in this tutorial\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c69=\"\"\u003e\u003cdiv _ngcontent-c69=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 28 - Apr 1, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Another cool thing you can do with Cloud Functions\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Got data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\u0026#34;\u0026gt;shows you how\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Add custom severity levels to Cloud Monitoring alert policies\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Not all alerts are created equal. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\u0026#34;\u0026gt;In this blog post\u0026lt;/a\u0026gt;, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Learn how to use CPU allocation controls in Cloud Run\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Last fall, \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\u0026#34;\u0026gt;we added\u0026lt;/a\u0026gt; \u0026amp;#8220;always-on CPU\u0026amp;#8221; capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\u0026#34;\u0026gt;In this post\u0026lt;/a\u0026gt;, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app\u0026amp;#8217;s average user response latency by over 80%.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 28 - Apr 1, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eAnother cool thing you can do with Cloud Functions\u003cbr/\u003e\u003c/b\u003eGot data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial \u003ca href=\"https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\" track-metadata-module=\"post\"\u003eshows you how\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eAdd custom severity levels to Cloud Monitoring alert policies\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot all alerts are created equal. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\" track-metadata-module=\"post\"\u003eIn this blog post\u003c/a\u003e, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. \u003c/p\u003e\u003cp\u003e\u003cb\u003eLearn how to use CPU allocation controls in Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLast fall, \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\" track-metadata-module=\"post\"\u003ewe added\u003c/a\u003e “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\" track-metadata-module=\"post\"\u003eIn this post\u003c/a\u003e, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c69=\"\"\u003e\u003cdiv _ngcontent-c69=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 21 - Mar 25, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Get Going with latest Go 1.18 release\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;With the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\u0026#34;\u0026gt;Learn more here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 21 - Mar 25, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eGet Going with latest Go 1.18 release\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. \u003ca href=\"https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\" track-metadata-module=\"post\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c69=\"\"\u003e\u003cdiv _ngcontent-c69=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 14 - Mar 18, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Create EventArc triggers with Terraform\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;In addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\u0026#34;\u0026gt;shows you how\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Scaling to new markets with Cloud Run\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;French publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\u0026#34;\u0026gt;website architecture\u0026lt;/a\u0026gt; here.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;The serverless way to celebrate Pi Day\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;In honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\u0026#34;\u0026gt;calculate \u0026amp;#960;\u0026lt;/a\u0026gt; \u0026amp;#8212; serverlessly.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 14 - Mar 18, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eCreate EventArc triggers with Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\" track-metadata-module=\"post\"\u003eshows you how\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eScaling to new markets with Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eFrench publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its \u003ca href=\"https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\" track-metadata-module=\"post\"\u003ewebsite architecture\u003c/a\u003e here. \u003c/p\u003e\u003cp\u003e\u003cb\u003eThe serverless way to celebrate Pi Day\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\" track-metadata-module=\"post\"\u003ecalculate π\u003c/a\u003e — serverlessly.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c69=\"\"\u003e\u003cdiv _ngcontent-c69=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 07 - Mar 11, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Rhode Island moves to Google Cloud-based job board\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\u0026#34;\u0026gt;how they did it\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Containerized microservices at Lowe\u0026amp;#8217;s\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Lowe\u0026amp;#8217;s already told us \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\u0026#34;\u0026gt;how they use SRE\u0026lt;/a\u0026gt;. They\u0026amp;#8217;re at it again, describing how they built an e-commerce website using a \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\u0026#34;\u0026gt;containerized microservices architecture and Kubernetes\u0026lt;/a\u0026gt;, with Istio for service mesh and Cloud Operations for good measure.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cruise AVs hit the road with Google Cloud services\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Autonomous Vehicle (AV) startup Cruise detailed how it\u0026amp;#8217;s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\u0026#34;\u0026gt;Read the guest post\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;L\u0026amp;#8217;Or\u0026amp;#233;al\u0026amp;#8217;s data analytics gets a makeover with serverless\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;We\u0026amp;#8217;re hurtling toward a \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\u0026#34;\u0026gt;programmable cloud\u0026lt;/a\u0026gt; \u0026amp;#8212; a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\u0026#34;\u0026gt;L\u0026amp;#8217;Or\u0026amp;#233;al is a great example\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Better telemetry for your Anthos clusters\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\u0026#34;\u0026gt;Anthos Service Mesh Dashboard\u0026lt;/a\u0026gt; is now available (public preview) on the \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\u0026#34;\u0026gt;Anthos clusters on Bare Metal\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\u0026#34;\u0026gt;Anthos clusters on VMware\u0026lt;/a\u0026gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Instrument your Java apps\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;With the new version of the \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\u0026#34;\u0026gt;Google Cloud Logging Java library\u0026lt;/a\u0026gt;, you can wire your application logs with more information \u0026amp;#8212; without adding a single line of code.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Visualize metrics from Cloud Spanner\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Building an app on top of Cloud Spanner but can\u0026amp;#8217;t assess how well it\u0026amp;#8217;s operating? The new \u0026lt;a href=\u0026#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;OpenTelemetery receiver for Cloud Spanner\u0026lt;/a\u0026gt; provides an easy way for you to process and visualize metrics from Cloud Spanner \u0026lt;a href=\u0026#34;https://cloud.google.com/spanner/docs/introspection\u0026#34;\u0026gt;System tables\u0026lt;/a\u0026gt;, and export these to the APM tool of your choice. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\u0026#34;\u0026gt;Read more here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 07 - Mar 11, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eRhode Island moves to Google Cloud-based job board\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhen the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\" track-metadata-module=\"post\"\u003ehow they did it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eContainerized microservices at Lowe’s\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLowe’s already told us \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\" track-metadata-module=\"post\"\u003ehow they use SRE\u003c/a\u003e. They’re at it again, describing how they built an e-commerce website using a \u003ca href=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\" track-metadata-module=\"post\"\u003econtainerized microservices architecture and Kubernetes\u003c/a\u003e, with Istio for service mesh and Cloud Operations for good measure.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCruise AVs hit the road with Google Cloud services\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eAutonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\" track-metadata-module=\"post\"\u003eRead the guest post\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eL’Oréal’s data analytics gets a makeover with serverless\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe’re hurtling toward a \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\" track-metadata-module=\"post\"\u003eprogrammable cloud\u003c/a\u003e — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u003ca href=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\" track-metadata-module=\"post\"\u003eL’Oréal is a great example\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBetter telemetry for your Anthos clusters\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\" track-metadata-module=\"post\"\u003eAnthos Service Mesh Dashboard\u003c/a\u003e is now available (public preview) on the \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\" track-metadata-module=\"post\"\u003eAnthos clusters on Bare Metal\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\" track-metadata-module=\"post\"\u003eAnthos clusters on VMware\u003c/a\u003e. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u003c/p\u003e\u003cp\u003e\u003cb\u003eInstrument your Java apps\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the new version of the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\" track-metadata-module=\"post\"\u003eGoogle Cloud Logging Java library\u003c/a\u003e, you can wire your application logs with more information — without adding a single line of code.\u003c/p\u003e\u003cp\u003e\u003cb\u003eVisualize metrics from Cloud Spanner\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBuilding an app on top of Cloud Spanner but can’t assess how well it’s operating? The new \u003ca href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\" target=\"_blank\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eOpenTelemetery receiver for Cloud Spanner\u003c/a\u003e provides an easy way for you to process and visualize metrics from Cloud Spanner \u003ca href=\"https://cloud.google.com/spanner/docs/introspection\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/spanner/docs/introspection\" track-metadata-module=\"post\"\u003eSystem tables\u003c/a\u003e, and export these to the APM tool of your choice. \u003ca href=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\" track-metadata-module=\"post\"\u003eRead more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c69=\"\"\u003e\u003cdiv _ngcontent-c69=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Feb 28 - Mar 4, 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Introducing Cloud SDK\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;The rebranded \u0026lt;a href=\u0026#34;https://cloud.google.com/sdk\u0026#34;\u0026gt;Cloud SDK\u0026lt;/a\u0026gt; is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cloud CLI, meet Terraform\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Google Cloud CLI\u0026amp;#8217;s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google\u0026amp;#8217;s KRM declarative tooling, and is now \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\u0026#34;\u0026gt;available in preview\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Knative graduates to incubating project\u0026amp;#160;\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Congratulations to Knative, which has been \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\u0026#34;\u0026gt;accepted by the Cloud Native Computing Foundation\u0026lt;/a\u0026gt;, or CNCF, as an incubating project, enabling the next phase of serverless architecture.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;We manage Prometheus so you don\u0026amp;#8217;t have to\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/managed-prometheus\u0026#34;\u0026gt;Google Cloud Managed Service for Prometheus\u0026lt;/a\u0026gt; is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\u0026#34;\u0026gt;Learn more here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Feb 28 - Mar 4, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eIntroducing Cloud SDK\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe rebranded \u003ca href=\"https://cloud.google.com/sdk\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/sdk\" track-metadata-module=\"post\"\u003eCloud SDK\u003c/a\u003e is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more \u003ca href=\"https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud CLI, meet Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGoogle Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now \u003ca href=\"https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\" track-metadata-module=\"post\"\u003eavailable in preview\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eKnative graduates to incubating project \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eCongratulations to Knative, which has been \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\" track-type=\"inline link\" track-name=\"31\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\" track-metadata-module=\"post\"\u003eaccepted by the Cloud Native Computing Foundation\u003c/a\u003e, or CNCF, as an incubating project, enabling the next phase of serverless architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eWe manage Prometheus so you don’t have to\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\" track-type=\"inline link\" track-name=\"32\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\" track-metadata-module=\"post\"\u003eGoogle Cloud Managed Service for Prometheus\u003c/a\u003e is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\" track-type=\"inline link\" track-name=\"33\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\" track-metadata-module=\"post\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c68=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDevelopers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Apr 4 - Apr 8, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eHow to migrate a container from a VM to Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith Cloud Run, you can migrate a legacy VM to a container and save money – even if you don’t know Kubernetes. This \u003ca href=\"https://youtu.be/HKuUmzSpljU\" target=\"_blank\"\u003evideo\u003c/a\u003e shows you how. \u003c/p\u003e\u003cp\u003e\u003cb\u003eReceive Error Reporting notifications through Slack and Webhooks\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eError Reporting can analyze, aggregate, and notify DevOps teams about crashes that happened in their cloud services, right to their preferred channels. Learn more in this \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications\"\u003eblog\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud-native architecture is in the cards at NCR\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eEarlier this year, NCR Authentic Cards talked about \u003ca href=\"https://cloud.google.com/blog/topics/partners/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud\"\u003ehow it built\u003c/a\u003e a transaction processing platform on Google Cloud. NCR and its consulting partner Opus Systems are back for \u003ca href=\"https://cloud.google.com/blog/topics/financial-services/how-ncr-and-opus-migrated-ncr-authentic-cards-to-google-cloud-pt2\"\u003epart two of the migration story\u003c/a\u003e, taking a detailed look at all the components that went into the cloud-based architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eHow to easily share a service with Cloud Run \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eHave you ever written a script that you wanted to make available to others? Cloud Run makes it easy to deploy a processing service quickly and easily. In this blog post, Developer Advocate Laurent Picard creates \u003ca href=\"https://coloring-page.lolo.dev/\" target=\"_blank\"\u003ean image processing service\u003c/a\u003e that generates coloring pages, then makes it available to others — all in under 200 lines of Python and JavaScript. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/deploy-coloring-page-generator-minutes-cloud-run\"\u003eFollow along in this tutorial\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/infrastructure/topaz-subsea-cable-connects-canada-and-asia/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Banner_Topaz_map_hero_Banner.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eIntroducing Topaz — the first subsea cable to connect Canada and Asia\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe Topaz subsea cable is the first fiber cable to connect Canada and Asia, and will provide better resiliency and lower latency for Goog...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 28 - Apr 1, 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eAnother cool thing you can do with Cloud Functions\u003cbr/\u003e\u003c/b\u003eGot data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial \u003ca href=\"https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\"\u003eshows you how\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eAdd custom severity levels to Cloud Monitoring alert policies\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot all alerts are created equal. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\"\u003eIn this blog post\u003c/a\u003e, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. \u003c/p\u003e\u003cp\u003e\u003cb\u003eLearn how to use CPU allocation controls in Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLast fall, \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\"\u003ewe added\u003c/a\u003e “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\"\u003eIn this post\u003c/a\u003e, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_App_Dev_4.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGo 1.18 and Google Cloud: Go now with Google Cloud\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eGo 1.18 release and Google Cloud working better together.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 21 - Mar 25, 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eGet Going with latest Go 1.18 release\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. \u003ca href=\"https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/serverless_2.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eScaling quickly to new markets with Cloud Run—a web modernization story\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eMoving from on-prem to cloud using serverless containers and PHP, a French news outlet more easily expands to reach new markets.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 14 - Mar 18, 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eCreate EventArc triggers with Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\"\u003eshows you how\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eScaling to new markets with Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eFrench publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its \u003ca href=\"https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\"\u003ewebsite architecture\u003c/a\u003e here. \u003c/p\u003e\u003cp\u003e\u003cb\u003eThe serverless way to celebrate Pi Day\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\"\u003ecalculate π\u003c/a\u003e — serverlessly.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 07 - Mar 11, 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eRhode Island moves to Google Cloud-based job board\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhen the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\"\u003ehow they did it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eContainerized microservices at Lowe’s\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLowe’s already told us \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\"\u003ehow they use SRE\u003c/a\u003e. They’re at it again, describing how they built an e-commerce website using a \u003ca href=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\"\u003econtainerized microservices architecture and Kubernetes\u003c/a\u003e, with Istio for service mesh and Cloud Operations for good measure.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCruise AVs hit the road with Google Cloud services\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eAutonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\"\u003eRead the guest post\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eL’Oréal’s data analytics gets a makeover with serverless\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe’re hurtling toward a \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\"\u003eprogrammable cloud\u003c/a\u003e — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u003ca href=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\"\u003eL’Oréal is a great example\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBetter telemetry for your Anthos clusters\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\"\u003eAnthos Service Mesh Dashboard\u003c/a\u003e is now available (public preview) on the \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\"\u003eAnthos clusters on Bare Metal\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\"\u003eAnthos clusters on VMware\u003c/a\u003e. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u003c/p\u003e\u003cp\u003e\u003cb\u003eInstrument your Java apps\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the new version of the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\"\u003eGoogle Cloud Logging Java library\u003c/a\u003e, you can wire your application logs with more information — without adding a single line of code.\u003c/p\u003e\u003cp\u003e\u003cb\u003eVisualize metrics from Cloud Spanner\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBuilding an app on top of Cloud Spanner but can’t assess how well it’s operating? The new \u003ca href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\" target=\"_blank\"\u003eOpenTelemetery receiver for Cloud Spanner\u003c/a\u003e provides an easy way for you to process and visualize metrics from Cloud Spanner \u003ca href=\"https://cloud.google.com/spanner/docs/introspection\"\u003eSystem tables\u003c/a\u003e, and export these to the APM tool of your choice. \u003ca href=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\"\u003eRead more here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Feb 28 - Mar 4, 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntroducing Cloud SDK\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe rebranded \u003ca href=\"https://cloud.google.com/sdk\"\u003eCloud SDK\u003c/a\u003e is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more \u003ca href=\"https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud CLI, meet Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGoogle Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now \u003ca href=\"https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\"\u003eavailable in preview\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eKnative graduates to incubating project \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eCongratulations to Knative, which has been \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\"\u003eaccepted by the Cloud Native Computing Foundation\u003c/a\u003e, or CNCF, as an incubating project, enabling the next phase of serverless architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eWe manage Prometheus so you don’t have to\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eGoogle Cloud Managed Service for Prometheus\u003c/a\u003e is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/inframod_living_3.max-2200x2200.jpg",
      "date_published": "2022-04-08T20:00:00Z",
      "author": {
        "name": "\u003cname\u003eGoogle Cloud Content \u0026 Editorial \u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/use-slack-and-webhooks-for-notifications/",
      "title": "Deliver exception messages through Slack and Webhooks for fast resolution",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c56=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e Alek Szilagyi \u003c/p\u003e\u003cp\u003e Software Engineer, Google Cloud \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e April 5, 2022 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c58=\"\"\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Building new applications is a lot of fun, but troubleshooting and fixing the crashes that can come with app development is not. While many organizations are fast adopting the DevOps model, there are still some legacy frameworks where developers and operations teams are separate. Developers build and submit apps to their ops team, who in turn deploy and maintain the production stack.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;A common issue that arises due to this workflow is the time it takes to find and resolve crashes. To help reduce the time it takes to find crashes, we recently introduced new \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available\u0026#34;\u0026gt;notification\u0026lt;/a\u0026gt; channels for our Alerting product. Building on that release, we\u0026amp;#8217;re happy to announce today that you can send Error Reporting notifications through both \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/notifications#slack\u0026#34;\u0026gt;Slack\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/notifications#webhooks\u0026#34;\u0026gt;Webhooks\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;What is Error Reporting?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Error Reporting can analyze, aggregate, and notify you about crashes in your running cloud services. It can synthesize this information from ingested logs in \u0026lt;a href=\u0026#34;https://cloud.google.com/logging\u0026#34;\u0026gt;Cloud Logging\u0026lt;/a\u0026gt; and has a \u0026lt;a href=\u0026#34;http://console.cloud.google.com/errors\u0026#34;\u0026gt;\u0026lt;i\u0026gt;\u0026lt;b\u0026gt;dedicated page\u0026lt;/b\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/a\u0026gt; that displays the details of the errors, including a histogram of occurrences, list of affected versions, request URL, and links to the request log.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eBuilding new applications is a lot of fun, but troubleshooting and fixing the crashes that can come with app development is not. While many organizations are fast adopting the DevOps model, there are still some legacy frameworks where developers and operations teams are separate. Developers build and submit apps to their ops team, who in turn deploy and maintain the production stack. \u003c/p\u003e\u003cp\u003eA common issue that arises due to this workflow is the time it takes to find and resolve crashes. To help reduce the time it takes to find crashes, we recently introduced new \u003ca href=\"https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available\" track-metadata-module=\"post\"\u003enotification\u003c/a\u003e channels for our Alerting product. Building on that release, we’re happy to announce today that you can send Error Reporting notifications through both \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#slack\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/notifications#slack\" track-metadata-module=\"post\"\u003eSlack\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#webhooks\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/notifications#webhooks\" track-metadata-module=\"post\"\u003eWebhooks\u003c/a\u003e. \u003c/p\u003e\u003ch3\u003eWhat is Error Reporting?\u003c/h3\u003e\u003cp\u003eError Reporting can analyze, aggregate, and notify you about crashes in your running cloud services. It can synthesize this information from ingested logs in \u003ca href=\"https://cloud.google.com/logging\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/logging\" track-metadata-module=\"post\"\u003eCloud Logging\u003c/a\u003e and has a \u003ca href=\"http://console.cloud.google.com/errors\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"http://console.cloud.google.com/errors\" track-metadata-module=\"post\"\u003e\u003ci\u003e\u003cb\u003ededicated page\u003c/b\u003e\u003c/i\u003e\u003c/a\u003e that displays the details of the errors, including a histogram of occurrences, list of affected versions, request URL, and links to the request log.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;h3\u0026gt;What are we launching?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Building on our recent launch of \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available\u0026#34;\u0026gt;alerting notification\u0026lt;/a\u0026gt; channels, today we are announcing an extension of \u0026lt;b\u0026gt;Error Reporting\u0026amp;#8217;s notification capabilities to include Slack and Webhooks\u0026lt;/b\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;With this launch, your teams can receive notifications about crashes directly into their configured Slack channel or preferred collaboration platform using webhooks.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Crash information can then be quickly swarmed, discussed, and resolved.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWhat are we launching?\u003c/h3\u003e\u003cp\u003eBuilding on our recent launch of \u003ca href=\"https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available\" track-metadata-module=\"post\"\u003ealerting notification\u003c/a\u003e channels, today we are announcing an extension of \u003cb\u003eError Reporting’s notification capabilities to include Slack and Webhooks\u003c/b\u003e. \u003c/p\u003e\u003cp\u003eWith this launch, your teams can receive notifications about crashes directly into their configured Slack channel or preferred collaboration platform using webhooks. \u003c/p\u003e\u003cp\u003eCrash information can then be quickly swarmed, discussed, and resolved.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;These new channels\u0026amp;#160; join our existing Error Reporting notification capabilities of email and the Google Cloud Console mobile app.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;What do I have to do to enable Error Reporting and these notifications?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Error Reporting is automatically enabled as soon as logs that contain error events like stack traces are ingested into Cloud Logging. Alternatively,\u0026amp;#160; you can \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/how-to\u0026#34;\u0026gt;self configure\u0026lt;/a\u0026gt; Error Reporting for a new project if you won\u0026amp;#8217;t be using Cloud Logging.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To configure the new notification channels, see documentation \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/notifications#create-channel\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt; for \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;Slack\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt; and \u0026lt;b\u0026gt;\u0026lt;i\u0026gt;Webhooks\u0026lt;/i\u0026gt;\u0026lt;/b\u0026gt;, or keep reading below:\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Enabling Slack\u0026lt;/h3\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;In Slack\u0026lt;/b\u0026gt;: Create a Slack workspace and channel at the \u0026lt;a href=\u0026#34;https://www.slack.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Slack site\u0026lt;/a\u0026gt;. Record the channel URL.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;In the Cloud Console, select \u0026lt;a href=\u0026#34;https://console.cloud.google.com/monitoring\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Monitoring\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click \u0026lt;b\u0026gt;Alerting\u0026lt;/b\u0026gt; and then click \u0026lt;b\u0026gt;Edit notification channels\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;In the \u0026lt;b\u0026gt;Slack\u0026lt;/b\u0026gt; section, click \u0026lt;b\u0026gt;Add new\u0026lt;/b\u0026gt; to open the Slack sign-in page:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Select your Slack workspace.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click \u0026lt;b\u0026gt;Allow\u0026lt;/b\u0026gt; to enable Cloud Monitoring access to your Slack workspace. This action takes you back to the Monitoring configuration page for your notification channel.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Enter the name of the Slack channel you want to use for notifications.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Enter a display name for the Slack notification channel.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;(Optional) To test the connection between Cloud Monitoring and your Slack workspace, click \u0026lt;b\u0026gt;Send test notification\u0026lt;/b\u0026gt;. If the connection is successful, then you see a message This is a test alert notification... in the Slack notification channel that you specified. Check the notification channel to confirm receipt.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;If the Slack channel you want to use for notifications is a private channel, then you must manually invite the Monitoring app to the channel:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Open Slack.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Go to the channel you specified as your Monitoring notification channel.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Invite the Monitoring app to the channel by entering and sending the following message in the channel:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;/invite @Google Cloud Monitoring\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Be sure you invite the Monitoring app to the private channel you specified when creating the notification channel in Monitoring. Inviting the Monitoring app to public channels is optional.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;h3\u0026gt;Enabling Webhooks\u0026lt;/h3\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;The webhook handler\u0026lt;/b\u0026gt;: Identify the public endpoint URL to receive webhook data from Monitoring.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;In the Cloud Console, select \u0026lt;a href=\u0026#34;https://console.cloud.google.com/monitoring\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Monitoring\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click \u0026lt;b\u0026gt;Alerting\u0026lt;/b\u0026gt; and then click \u0026lt;b\u0026gt;Edit notification channels\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;In the Webhook section, click Add new.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Complete the dialog.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click \u0026lt;b\u0026gt;Test Connection\u0026lt;/b\u0026gt; to send a test payload to the Webhook endpoint. You can go to the receiving endpoint to verify delivery.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click \u0026lt;b\u0026gt;Save\u0026lt;/b\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;h3\u0026gt;Webhook schema\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The Webhook schema structure for Error Reporting, is as follows:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Schema structure, version 1.0\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThese new channels  join our existing Error Reporting notification capabilities of email and the Google Cloud Console mobile app.\u003c/p\u003e\u003ch3\u003eWhat do I have to do to enable Error Reporting and these notifications?\u003c/h3\u003e\u003cp\u003eError Reporting is automatically enabled as soon as logs that contain error events like stack traces are ingested into Cloud Logging. Alternatively,  you can \u003ca href=\"https://cloud.google.com/error-reporting/docs/how-to\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/how-to\" track-metadata-module=\"post\"\u003eself configure\u003c/a\u003e Error Reporting for a new project if you won’t be using Cloud Logging.\u003c/p\u003e\u003cp\u003eTo configure the new notification channels, see documentation \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#create-channel\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/notifications#create-channel\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e for \u003cb\u003e\u003ci\u003eSlack\u003c/i\u003e\u003c/b\u003e and \u003cb\u003e\u003ci\u003eWebhooks\u003c/i\u003e\u003c/b\u003e, or keep reading below:\u003c/p\u003e\u003ch3\u003eEnabling Slack\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eIn Slack\u003c/b\u003e: Create a Slack workspace and channel at the \u003ca href=\"https://www.slack.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://www.slack.com\" track-metadata-module=\"post\"\u003eSlack site\u003c/a\u003e. Record the channel URL.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn the Cloud Console, select \u003ca href=\"https://console.cloud.google.com/monitoring\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://console.cloud.google.com/monitoring\" track-metadata-module=\"post\"\u003e\u003cb\u003eMonitoring\u003c/b\u003e\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eAlerting\u003c/b\u003e and then click \u003cb\u003eEdit notification channels\u003c/b\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn the \u003cb\u003eSlack\u003c/b\u003e section, click \u003cb\u003eAdd new\u003c/b\u003e to open the Slack sign-in page:\u003c/p\u003e\u003c/li\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eSelect your Slack workspace.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eAllow\u003c/b\u003e to enable Cloud Monitoring access to your Slack workspace. This action takes you back to the Monitoring configuration page for your notification channel.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEnter the name of the Slack channel you want to use for notifications.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEnter a display name for the Slack notification channel.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e(Optional) To test the connection between Cloud Monitoring and your Slack workspace, click \u003cb\u003eSend test notification\u003c/b\u003e. If the connection is successful, then you see a message This is a test alert notification... in the Slack notification channel that you specified. Check the notification channel to confirm receipt.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cli\u003e\u003cp\u003eIf the Slack channel you want to use for notifications is a private channel, then you must manually invite the Monitoring app to the channel:\u003c/p\u003e\u003c/li\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eOpen Slack.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGo to the channel you specified as your Monitoring notification channel.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eInvite the Monitoring app to the channel by entering and sending the following message in the channel:\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e/invite @Google Cloud Monitoring\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBe sure you invite the Monitoring app to the private channel you specified when creating the notification channel in Monitoring. Inviting the Monitoring app to public channels is optional.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/ol\u003e\u003ch3\u003eEnabling Webhooks\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eThe webhook handler\u003c/b\u003e: Identify the public endpoint URL to receive webhook data from Monitoring.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn the Cloud Console, select \u003ca href=\"https://console.cloud.google.com/monitoring\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://console.cloud.google.com/monitoring\" track-metadata-module=\"post\"\u003e\u003cb\u003eMonitoring\u003c/b\u003e\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eAlerting\u003c/b\u003e and then click \u003cb\u003eEdit notification channels\u003c/b\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn the Webhook section, click Add new.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eComplete the dialog.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eTest Connection\u003c/b\u003e to send a test payload to the Webhook endpoint. You can go to the receiving endpoint to verify delivery.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eSave\u003c/b\u003e.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eWebhook schema\u003c/h3\u003e\u003cp\u003eThe Webhook schema structure for Error Reporting, is as follows:\u003c/p\u003e\u003cp\u003e\u003cb\u003eSchema structure, version 1.0\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c61=\"\"\u003e\u003cpre _ngcontent-c61=\"\"\u003e  \u003ccode _ngcontent-c61=\"\"\u003e{\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;,\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  \u0026#34;subject\u0026#34;: string, description of the new or reopened error group.\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  \u0026#34;group_info\u0026#34;: {\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;project_id\u0026#34;: string, project that owns the error group.\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;detail_link\u0026#34;: string, link to the Error Reporting Details page for the error group.\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  },\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  \u0026#34;exception_info\u0026#34;: {\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;type\u0026#34;: string, type of the exception logged in the event.\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;message\u0026#34;: string, exception message for the event.\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  },\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  \u0026#34;event_info\u0026#34;: {\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;log_message\u0026#34;: string\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;request_method\u0026#34;: string\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;request_url\u0026#34;: string\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;referrer\u0026#34;: string\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;user_agent\u0026#34;: string\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;service\u0026#34;: string\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;version\u0026#34;: string\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e    \u0026#34;response_status\u0026#34;: string\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e  },\n\u003c/code\u003e\u003ccode _ngcontent-c61=\"\"\u003e}\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Basic authentication\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In addition to the webhook request sent by Cloud Monitoring, basic authentication utilizes the HTTP specification for the username and password. Cloud Monitoring requires your server to return a 401 response with the proper WWW-Authenticate header. For more information about basic authentication, see the following:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://www.ietf.org/rfc/rfc2617.txt\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;RFC Specification\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://en.wikipedia.org/wiki/Basic_access_authentication\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Basic authentication\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Token authentication\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Token Authentication requires a query string parameter in the endpoint URL and a key that the server expects to be secret between itself and Monitoring. The following is a sample URL that includes a token:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;https://www.myserver.com/stackdriver-hook?auth_token=1234-abcd\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If Monitoring posts an incident to the endpoint URL, your server can validate the attached token. This method of authentication can be more effective when used with SSL/TLS to encrypt the HTTP request, which can prevent snoopers from learning the token.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For an example server in Python, see this \u0026lt;a href=\u0026#34;https://gist.github.com/tschieggm/7604940\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;sample server\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Get Started Today\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;If you haven\u0026amp;#8217;t visited your Error Reporting console yet, \u0026lt;a href=\u0026#34;http://console.cloud.google.com/errors\u0026#34;\u0026gt;give it a try today\u0026lt;/a\u0026gt; and learn more about it in \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/viewing-errors\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;. When you\u0026amp;#8217;re ready to configure your notifications, consider \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/notifications#slack\u0026#34;\u0026gt;Slack\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/notifications#webhooks\u0026#34;\u0026gt;Webhooks\u0026lt;/a\u0026gt; if they fit into your current alerting and notification strategy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you have any questions or want to start a discussion with other Error Reporting users, visit the Cloud Operations section of the \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Google Cloud Community\u0026lt;/a\u0026gt; and post a discussion topic.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eBasic authentication\u003c/b\u003e\u003c/p\u003e\u003cp\u003eIn addition to the webhook request sent by Cloud Monitoring, basic authentication utilizes the HTTP specification for the username and password. Cloud Monitoring requires your server to return a 401 response with the proper WWW-Authenticate header. For more information about basic authentication, see the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.ietf.org/rfc/rfc2617.txt\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://www.ietf.org\" track-metadata-module=\"post\"\u003eRFC Specification\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Basic_access_authentication\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://en.wikipedia.org\" track-metadata-module=\"post\"\u003eBasic authentication\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cb\u003eToken authentication\u003c/b\u003e\u003c/p\u003e\u003cp\u003eToken Authentication requires a query string parameter in the endpoint URL and a key that the server expects to be secret between itself and Monitoring. The following is a sample URL that includes a token:\u003c/p\u003e\u003cp\u003ehttps://www.myserver.com/stackdriver-hook?auth_token=1234-abcd\u003c/p\u003e\u003cp\u003eIf Monitoring posts an incident to the endpoint URL, your server can validate the attached token. This method of authentication can be more effective when used with SSL/TLS to encrypt the HTTP request, which can prevent snoopers from learning the token.\u003c/p\u003e\u003cp\u003eFor an example server in Python, see this \u003ca href=\"https://gist.github.com/tschieggm/7604940\" target=\"_blank\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://gist.github.com\" track-metadata-module=\"post\"\u003esample server\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eGet Started Today\u003c/h3\u003e\u003cp\u003eIf you haven’t visited your Error Reporting console yet, \u003ca href=\"http://console.cloud.google.com/errors\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"http://console.cloud.google.com/errors\" track-metadata-module=\"post\"\u003egive it a try today\u003c/a\u003e and learn more about it in \u003ca href=\"https://cloud.google.com/error-reporting/docs/viewing-errors\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/viewing-errors\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e. When you’re ready to configure your notifications, consider \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#slack\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/notifications#slack\" track-metadata-module=\"post\"\u003eSlack\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#webhooks\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/notifications#webhooks\" track-metadata-module=\"post\"\u003eWebhooks\u003c/a\u003e if they fit into your current alerting and notification strategy.\u003c/p\u003e\u003cp\u003eIf you have any questions or want to start a discussion with other Error Reporting users, visit the Cloud Operations section of the \u003ca href=\"https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003eGoogle Cloud Community\u003c/a\u003e and post a discussion topic.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c59=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eBuilding new applications is a lot of fun, but troubleshooting and fixing the crashes that can come with app development is not. While many organizations are fast adopting the DevOps model, there are still some legacy frameworks where developers and operations teams are separate. Developers build and submit apps to their ops team, who in turn deploy and maintain the production stack. \u003c/p\u003e\u003cp\u003eA common issue that arises due to this workflow is the time it takes to find and resolve crashes. To help reduce the time it takes to find crashes, we recently introduced new \u003ca href=\"https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available\"\u003enotification\u003c/a\u003e channels for our Alerting product. Building on that release, we’re happy to announce today that you can send Error Reporting notifications through both \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#slack\"\u003eSlack\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#webhooks\"\u003eWebhooks\u003c/a\u003e. \u003c/p\u003e\u003ch3\u003eWhat is Error Reporting?\u003c/h3\u003e\u003cp\u003eError Reporting can analyze, aggregate, and notify you about crashes in your running cloud services. It can synthesize this information from ingested logs in \u003ca href=\"https://cloud.google.com/logging\"\u003eCloud Logging\u003c/a\u003e and has a \u003ca href=\"http://console.cloud.google.com/errors\"\u003e\u003ci\u003e\u003cb\u003ededicated page\u003c/b\u003e\u003c/i\u003e\u003c/a\u003e that displays the details of the errors, including a histogram of occurrences, list of affected versions, request URL, and links to the request log.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Error_Reporting_interface.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Error Reporting interface.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Error_Reporting_interface.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWhat are we launching?\u003c/h3\u003e\u003cp\u003eBuilding on our recent launch of \u003ca href=\"https://cloud.google.com/blog/products/operations/pub-sub-webook-and-slack-notifications-are-now-available\"\u003ealerting notification\u003c/a\u003e channels, today we are announcing an extension of \u003cb\u003eError Reporting’s notification capabilities to include Slack and Webhooks\u003c/b\u003e. \u003c/p\u003e\u003cp\u003eWith this launch, your teams can receive notifications about crashes directly into their configured Slack channel or preferred collaboration platform using webhooks. \u003c/p\u003e\u003cp\u003eCrash information can then be quickly swarmed, discussed, and resolved.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Slack_and_Webhooks.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Slack and Webhooks.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Slack_and_Webhooks.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThese new channels  join our existing Error Reporting notification capabilities of email and the Google Cloud Console mobile app.\u003c/p\u003e\u003ch3\u003eWhat do I have to do to enable Error Reporting and these notifications?\u003c/h3\u003e\u003cp\u003eError Reporting is automatically enabled as soon as logs that contain error events like stack traces are ingested into Cloud Logging. Alternatively,  you can \u003ca href=\"https://cloud.google.com/error-reporting/docs/how-to\"\u003eself configure\u003c/a\u003e Error Reporting for a new project if you won’t be using Cloud Logging.\u003c/p\u003e\u003cp\u003eTo configure the new notification channels, see documentation \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#create-channel\"\u003ehere\u003c/a\u003e for \u003cb\u003e\u003ci\u003eSlack\u003c/i\u003e\u003c/b\u003e and \u003cb\u003e\u003ci\u003eWebhooks\u003c/i\u003e\u003c/b\u003e, or keep reading below:\u003c/p\u003e\u003ch3\u003eEnabling Slack\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eIn Slack\u003c/b\u003e: Create a Slack workspace and channel at the \u003ca href=\"https://www.slack.com/\" target=\"_blank\"\u003eSlack site\u003c/a\u003e. Record the channel URL.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn the Cloud Console, select \u003ca href=\"https://console.cloud.google.com/monitoring\"\u003e\u003cb\u003eMonitoring\u003c/b\u003e\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eAlerting\u003c/b\u003e and then click \u003cb\u003eEdit notification channels\u003c/b\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn the \u003cb\u003eSlack\u003c/b\u003e section, click \u003cb\u003eAdd new\u003c/b\u003e to open the Slack sign-in page:\u003c/p\u003e\u003c/li\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eSelect your Slack workspace.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eAllow\u003c/b\u003e to enable Cloud Monitoring access to your Slack workspace. This action takes you back to the Monitoring configuration page for your notification channel.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEnter the name of the Slack channel you want to use for notifications.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eEnter a display name for the Slack notification channel.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e(Optional) To test the connection between Cloud Monitoring and your Slack workspace, click \u003cb\u003eSend test notification\u003c/b\u003e. If the connection is successful, then you see a message This is a test alert notification... in the Slack notification channel that you specified. Check the notification channel to confirm receipt.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cli\u003e\u003cp\u003eIf the Slack channel you want to use for notifications is a private channel, then you must manually invite the Monitoring app to the channel:\u003c/p\u003e\u003c/li\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eOpen Slack.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGo to the channel you specified as your Monitoring notification channel.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eInvite the Monitoring app to the channel by entering and sending the following message in the channel:\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e/invite @Google Cloud Monitoring\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBe sure you invite the Monitoring app to the private channel you specified when creating the notification channel in Monitoring. Inviting the Monitoring app to public channels is optional.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003c/ol\u003e\u003ch3\u003eEnabling Webhooks\u003c/h3\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eThe webhook handler\u003c/b\u003e: Identify the public endpoint URL to receive webhook data from Monitoring.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn the Cloud Console, select \u003ca href=\"https://console.cloud.google.com/monitoring\"\u003e\u003cb\u003eMonitoring\u003c/b\u003e\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eAlerting\u003c/b\u003e and then click \u003cb\u003eEdit notification channels\u003c/b\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn the Webhook section, click Add new.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eComplete the dialog.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eTest Connection\u003c/b\u003e to send a test payload to the Webhook endpoint. You can go to the receiving endpoint to verify delivery.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick \u003cb\u003eSave\u003c/b\u003e.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch3\u003eWebhook schema\u003c/h3\u003e\u003cp\u003eThe Webhook schema structure for Error Reporting, is as follows:\u003c/p\u003e\u003cp\u003e\u003cb\u003eSchema structure, version 1.0\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'{\\r\\n \"version\": \"1.0\",\\r\\n \"subject\": string, description of the new or reopened error group.\\r\\n \"group_info\": {\\r\\n \"project_id\": string, project that owns the error group.\\r\\n \"detail_link\": string, link to the Error Reporting Details page for the error group.\\r\\n },\\r\\n \"exception_info\": {\\r\\n \"type\": string, type of the exception logged in the event.\\r\\n \"message\": string, exception message for the event.\\r\\n },\\r\\n \"event_info\": {\\r\\n \"log_message\": string\\r\\n \"request_method\": string\\r\\n \"request_url\": string\\r\\n \"referrer\": string\\r\\n \"user_agent\": string\\r\\n \"service\": string\\r\\n \"version\": string\\r\\n \"response_status\": string\\r\\n },\\r\\n}'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eBasic authentication\u003c/b\u003e\u003c/p\u003e\u003cp\u003eIn addition to the webhook request sent by Cloud Monitoring, basic authentication utilizes the HTTP specification for the username and password. Cloud Monitoring requires your server to return a 401 response with the proper WWW-Authenticate header. For more information about basic authentication, see the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://www.ietf.org/rfc/rfc2617.txt\" target=\"_blank\"\u003eRFC Specification\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Basic_access_authentication\" target=\"_blank\"\u003eBasic authentication\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cb\u003eToken authentication\u003c/b\u003e\u003c/p\u003e\u003cp\u003eToken Authentication requires a query string parameter in the endpoint URL and a key that the server expects to be secret between itself and Monitoring. The following is a sample URL that includes a token:\u003c/p\u003e\u003cp\u003ehttps://www.myserver.com/stackdriver-hook?auth_token=1234-abcd\u003c/p\u003e\u003cp\u003eIf Monitoring posts an incident to the endpoint URL, your server can validate the attached token. This method of authentication can be more effective when used with SSL/TLS to encrypt the HTTP request, which can prevent snoopers from learning the token.\u003c/p\u003e\u003cp\u003eFor an example server in Python, see this \u003ca href=\"https://gist.github.com/tschieggm/7604940\" target=\"_blank\"\u003esample server\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eGet Started Today\u003c/h3\u003e\u003cp\u003eIf you haven’t visited your Error Reporting console yet, \u003ca href=\"http://console.cloud.google.com/errors\"\u003egive it a try today\u003c/a\u003e and learn more about it in \u003ca href=\"https://cloud.google.com/error-reporting/docs/viewing-errors\"\u003edocumentation\u003c/a\u003e. When you’re ready to configure your notifications, consider \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#slack\"\u003eSlack\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications#webhooks\"\u003eWebhooks\u003c/a\u003e if they fit into your current alerting and notification strategy.\u003c/p\u003e\u003cp\u003eIf you have any questions or want to start a discussion with other Error Reporting users, visit the Cloud Operations section of the \u003ca href=\"https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations\" target=\"_blank\"\u003eGoogle Cloud Community\u003c/a\u003e and post a discussion topic.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/operations/pub-sub-webook-and-slack-notifications-are-now-available/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Genric_GCP_upA1oyz.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eWebhook, Pub/Sub, and Slack Alerting notification channels launched\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnnouncing the general availability of the new Pub/Sub, Webhook, and Slack Notification channels.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-04-05T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlek Szilagyi\u003c/name\u003e\u003ctitle\u003eSoftware Engineer, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/vm-best-practices-app-observability-with-the-ops-agent/",
      "title": "Application observability made easier for Compute Engine",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen IT operators and architects begin their journey with Google Cloud, Day 0 observability needs tend to focus on infrastructure and aim to address questions about resource needs, a plan for scaling, and similar considerations. During this phase, developers and DevOps engineers also make a plan for how to get deep observability into the performance of third-party and open-source applications running on their \u003ca href=\"https://cloud.google.com/compute\"\u003eCompute Engine VMs\u003c/a\u003e. That’s why we recently launched a \u003ca href=\"https://cloud.google.com/monitoring/agent/integrations\"\u003ededicated UI\u003c/a\u003e under \u003cb\u003eMonitoring → Integrations\u003c/b\u003e that guides you to the available integrations that may be a good fit for your fleet, automatically installs relevant dashboards to help you get value faster, and shows you the integrations that are live on your VMs.   \u003c/p\u003e\u003ch3\u003eGetting up and running faster with application observability \u003c/h3\u003e\u003cp\u003eSince the Ops Agent \u003ca href=\"https://cloud.google.com/blog/products/operations/ops-agent-now-ga-and-it-includes-opentelemetry\"\u003eentered General Availability\u003c/a\u003e, we have added out of the box integrations with dozens of \u003ca href=\"https://cloud.google.com/monitoring/agent/ops-agent/third-party\"\u003epopular open source and licensed applications\u003c/a\u003e like databases, web servers, in-memory caches, event streaming, and application runtimes.  \u003c/p\u003e\u003cp\u003eThese integrations can be easily turned on within the Ops Agent on any VM, usually with a simple YAML configuration file update. Application-specific metrics and logs are sent to \u003ca href=\"https://cloud.google.com/products/operations\"\u003ecloud operations\u003c/a\u003e, providing deep visibility not just into infrastructure health indicators, but also into the application-specific telemetry that drives underlying infrastructure utilization. This data populates the analysis and query tools of \u003ca href=\"https://cloud.google.com/logging\"\u003eCloud Logging\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/monitoring\"\u003eCloud Monitoring\u003c/a\u003e like dashboards, Logs Explorer, alerts, SLOs, and more.    \u003c/p\u003e\u003ch3\u003e A single location for all integrations\u003c/h3\u003e\u003cp\u003eEven with easy configuration instructions, setting up an observability toolset with a view of dozens of third-party applications can be tricky. That’s why we created one UI that lists out all application integrations that we offer, along with instructions on how to get up and running quickly. With this UI, you can:  \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eFilter by integrations already installed on one or more VMs in your fleet, or those that are available but not yet installed on any VMs.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCheck at a glance which of your VMs have the Ops Agent installed already, and go through quick installation for VMs that will need the Ops Agent for application monitoring.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eQuickly identify lists of the specific metrics and log types collected for each integration so you know the telemetry collected.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLink directly to the setup instructions for each integration to save time searching through documentation.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eView samples and screenshots of “out of the box” dashboards and visualizations available with zero configuration, and have the dashboards automatically added to your ‘Integration Dashboards’ list based on what is live on your fleet. Links to the dashboards will be available in the Integrations UI, or by going to \u003cb\u003eMonitoring → Dashboards → Integrations\u003c/b\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWith a single click, choose “Copy Dashboard” to clone a given dashboard into your Custom list, and make modifications and edits specific to your applications and use cases.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Application_observability.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Application observability.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Application_observability.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eThe integrations UI makes it easy to browse available application integrations and learn more\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Application_observability.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Application observability.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Application_observability.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eEach integration lists telemetry collected and helps you get up and running with intuitive dashboards that you can later customize\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe also heard from our customers that you want a feedback loop that clearly shows which VMs have which integrations live. So, we added the following to our VM Instances Dashboard (located under \u003cb\u003eMonitoring → Dashboards → GCP → VM Instances\u003c/b\u003e):\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eClick into any VM to see whether the Ops Agent is installed, and the exact version running.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eView the “Integrations” chips to check which Integrations are currently collecting metrics and logs for each VM.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eView other infrastructure metrics and logs per VM, and quickly spot if there are any alerts or uptime checks set up for the VM or live events and incidents that require attention.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Application_observability.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"3 Application observability.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Application_observability.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eGet started today\u003c/h3\u003e\u003cp\u003eIn the coming months, we will add more integrations to the Ops Agent for more open source and licensed applications that you have told us are priorities. \u003c/p\u003e\u003cp\u003eTo get started with application observability today, make sure you are running the latest version of the \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent\"\u003eOps Agent\u003c/a\u003e on your GCE VMs, check out the instructions to add \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/third-party\"\u003ethird-party application integrations\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/monitoring/agent/integrations\"\u003echeck out the Integrations UI\u003c/a\u003e in the Monitoring section of the Google Cloud Console. \u003c/p\u003e\u003cp\u003eLastly, if you have feedback, want to ask us questions, or request another application integration, drop us a line on the \u003ca href=\"https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations\" target=\"_blank\"\u003eGoogle Cloud Community Cloud Ops\u003c/a\u003e area!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/operations/ops-agent-now-ga-and-it-includes-opentelemetry/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Blog_CloudMigration_D.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eThe Ops Agent is now GA and it leverages OpenTelemetry\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eToday, we’re happy to announce the General Availability of the new Ops Agent, which replaces both the Logging and Monitoring agents and s...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-04-04T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eJJ Roepke\u003c/name\u003e\u003ctitle\u003eSoftware Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/cloud-first/whats-new-cloud-native-apps/",
      "title": "What’s new in cloud-native apps?",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c53=\"\"\u003e\u003cdiv _ngcontent-c53=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Week of Mar 07 - Mar 11 2022\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Rhode Island moves to Google Cloud-based job board\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;When the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\u0026#34;\u0026gt;how they did it\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Containerized microservices at Lowe\u0026amp;#8217;s\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Lowe\u0026amp;#8217;s already told us \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\u0026#34;\u0026gt;how they use SRE\u0026lt;/a\u0026gt;. They\u0026amp;#8217;re at it again, describing how they built an e-commerce website using a \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\u0026#34;\u0026gt;containerized microservices architecture and Kubernetes\u0026lt;/a\u0026gt;, with Istio for service mesh and Cloud Operations for good measure.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cruise AVs hit the road with Google Cloud services\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Autonomous Vehicle (AV) startup Cruise detailed how it\u0026amp;#8217;s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\u0026#34;\u0026gt;Read the guest post\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;L\u0026amp;#8217;Or\u0026amp;#233;al\u0026amp;#8217;s data analytics gets a makeover with serverless\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;We\u0026amp;#8217;re hurtling toward a \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\u0026#34;\u0026gt;programmable cloud\u0026lt;/a\u0026gt; \u0026amp;#8212; a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\u0026#34;\u0026gt;L\u0026amp;#8217;Or\u0026amp;#233;al is a great example\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Better telemetry for your Anthos clusters\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\u0026#34;\u0026gt;Anthos Service Mesh Dashboard\u0026lt;/a\u0026gt; is now available (public preview) on the \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\u0026#34;\u0026gt;Anthos clusters on Bare Metal\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\u0026#34;\u0026gt;Anthos clusters on VMware\u0026lt;/a\u0026gt;. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Instrument your Java apps\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;With the new version of the \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\u0026#34;\u0026gt;Google Cloud Logging Java library\u0026lt;/a\u0026gt;, you can wire your application logs with more information \u0026amp;#8212; without adding a single line of code.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Visualize metrics from Cloud Spanner\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Building an app on top of Cloud Spanner but can\u0026amp;#8217;t assess how well it\u0026amp;#8217;s operating? The new \u0026lt;a href=\u0026#34;https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;OpenTelemetery receiver for Cloud Spanner\u0026lt;/a\u0026gt; provides an easy way for you to process and visualize metrics from Cloud Spanner \u0026lt;a href=\u0026#34;https://cloud.google.com/spanner/docs/introspection\u0026#34;\u0026gt;System tables\u0026lt;/a\u0026gt;, and export these to the APM tool of your choice. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\u0026#34;\u0026gt;Read more here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eWeek of Mar 07 - Mar 11 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eRhode Island moves to Google Cloud-based job board\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhen the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\" track-metadata-module=\"post\"\u003ehow they did it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eContainerized microservices at Lowe’s\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLowe’s already told us \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\" track-metadata-module=\"post\"\u003ehow they use SRE\u003c/a\u003e. They’re at it again, describing how they built an e-commerce website using a \u003ca href=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\" track-metadata-module=\"post\"\u003econtainerized microservices architecture and Kubernetes\u003c/a\u003e, with Istio for service mesh and Cloud Operations for good measure.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCruise AVs hit the road with Google Cloud services\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eAutonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\" track-metadata-module=\"post\"\u003eRead the guest post\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eL’Oréal’s data analytics gets a makeover with serverless\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe’re hurtling toward a \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\" track-metadata-module=\"post\"\u003eprogrammable cloud\u003c/a\u003e — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u003ca href=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\" track-metadata-module=\"post\"\u003eL’Oréal is a great example\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBetter telemetry for your Anthos clusters\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\" track-metadata-module=\"post\"\u003eAnthos Service Mesh Dashboard\u003c/a\u003e is now available (public preview) on the \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\" track-metadata-module=\"post\"\u003eAnthos clusters on Bare Metal\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\" track-metadata-module=\"post\"\u003eAnthos clusters on VMware\u003c/a\u003e. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u003c/p\u003e\u003cp\u003e\u003cb\u003eInstrument your Java apps\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the new version of the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\" track-metadata-module=\"post\"\u003eGoogle Cloud Logging Java library\u003c/a\u003e, you can wire your application logs with more information — without adding a single line of code.\u003c/p\u003e\u003cp\u003e\u003cb\u003eVisualize metrics from Cloud Spanner\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBuilding an app on top of Cloud Spanner but can’t assess how well it’s operating? The new \u003ca href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eOpenTelemetery receiver for Cloud Spanner\u003c/a\u003e provides an easy way for you to process and visualize metrics from Cloud Spanner \u003ca href=\"https://cloud.google.com/spanner/docs/introspection\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/spanner/docs/introspection\" track-metadata-module=\"post\"\u003eSystem tables\u003c/a\u003e, and export these to the APM tool of your choice. \u003ca href=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\" track-metadata-module=\"post\"\u003eRead more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDevelopers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 28 - Apr 1 2022\u003c/h3\u003e\u003cp\u003e\u003cb\u003eAnother cool thing you can do with Cloud Functions\u003cbr/\u003e\u003c/b\u003eGot data you want to ingest from Cloud Storage to BigQuery? Cloud Functions can help with that. This tutorial \u003ca href=\"https://cloud.google.com/blog/products/data-analytics/ingesting-data-into-bigquery-using-serverless-spark\"\u003eshows you how\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eAdd custom severity levels to Cloud Monitoring alert policies\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot all alerts are created equal. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts\"\u003eIn this blog post\u003c/a\u003e, learn how to add static and dynamic severity levels to a Cloud Monitoring alert policy, with enhanced notification channels including email, webhooks, Cloud Pub/Sub and PagerDuty. \u003c/p\u003e\u003cp\u003e\u003cb\u003eLearn how to use CPU allocation controls in Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLast fall, \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\"\u003ewe added\u003c/a\u003e “always-on CPU” capabilities to Cloud Run, making it a better fit for running background- and other asynchronous-processing tasks. \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/use-cloud-run-always-cpu-allocation-background-work\"\u003eIn this post\u003c/a\u003e, Developer Advocate Wesley Chun uses a weather alerting app to demonstrate how to use the feature, and along the way, reduces the app’s average user response latency by over 80%.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_App_Dev_4.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGo 1.18 and Google Cloud: Go now with Google Cloud\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eGo 1.18 release and Google Cloud working better together.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 21 - Mar 25 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eGet Going with latest Go 1.18 release\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the release of version 1.18, the Go programming language now includes support for generic code using parameterized types, integrated fuzz testing, and a new Go workspace mode that makes it simple to work with multiple modules. \u003ca href=\"https://cloud.google.com/blog/products/gcp/go-1-18-and-google-cloud-go-now-with-google-cloud\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/serverless_2.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eScaling quickly to new markets with Cloud Run—a web modernization story\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eMoving from on-prem to cloud using serverless containers and PHP, a French news outlet more easily expands to reach new markets.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 14 - Mar 18 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eCreate EventArc triggers with Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\"\u003eshows you how\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eScaling to new markets with Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eFrench publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its \u003ca href=\"https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\"\u003ewebsite architecture\u003c/a\u003e here. \u003c/p\u003e\u003cp\u003e\u003cb\u003eThe serverless way to celebrate Pi Day\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\"\u003ecalculate π\u003c/a\u003e — serverlessly.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/automotive.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eBuilding continuous integration \u0026amp; continuous delivery for autonomous vehicles on Google Cloud\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eCruise relies on a whole host of Google Cloud technologies to develop and test the tech that goes into its autonomous vehicles, or AVs.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 07 - Mar 11 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eRhode Island moves to Google Cloud-based job board\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhen the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\"\u003ehow they did it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eContainerized microservices at Lowe’s\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLowe’s already told us \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\"\u003ehow they use SRE\u003c/a\u003e. They’re at it again, describing how they built an e-commerce website using a \u003ca href=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\"\u003econtainerized microservices architecture and Kubernetes\u003c/a\u003e, with Istio for service mesh and Cloud Operations for good measure.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCruise AVs hit the road with Google Cloud services\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eAutonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\"\u003eRead the guest post\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eL’Oréal’s data analytics gets a makeover with serverless\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe’re hurtling toward a \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\"\u003eprogrammable cloud\u003c/a\u003e — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u003ca href=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\"\u003eL’Oréal is a great example\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBetter telemetry for your Anthos clusters\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\"\u003eAnthos Service Mesh Dashboard\u003c/a\u003e is now available (public preview) on the \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\"\u003eAnthos clusters on Bare Metal\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\"\u003eAnthos clusters on VMware\u003c/a\u003e. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u003c/p\u003e\u003cp\u003e\u003cb\u003eInstrument your Java apps\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the new version of the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\"\u003eGoogle Cloud Logging Java library\u003c/a\u003e, you can wire your application logs with more information — without adding a single line of code.\u003c/p\u003e\u003cp\u003e\u003cb\u003eVisualize metrics from Cloud Spanner\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBuilding an app on top of Cloud Spanner but can’t assess how well it’s operating? The new \u003ca href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\" target=\"_blank\"\u003eOpenTelemetery receiver for Cloud Spanner\u003c/a\u003e provides an easy way for you to process and visualize metrics from Cloud Spanner \u003ca href=\"https://cloud.google.com/spanner/docs/introspection\"\u003eSystem tables\u003c/a\u003e, and export these to the APM tool of your choice. \u003ca href=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\"\u003eRead more here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Feb 28 - Mar 4 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntroducing Cloud SDK\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe rebranded \u003ca href=\"https://cloud.google.com/sdk\"\u003eCloud SDK\u003c/a\u003e is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more \u003ca href=\"https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud CLI, meet Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGoogle Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now \u003ca href=\"https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\"\u003eavailable in preview\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eKnative graduates to incubating project \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eCongratulations to Knative, which has been \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\"\u003eaccepted by the Cloud Native Computing Foundation\u003c/a\u003e, or CNCF, as an incubating project, enabling the next phase of serverless architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eWe manage Prometheus so you don’t have to\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eGoogle Cloud Managed Service for Prometheus\u003c/a\u003e is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/inframod_living_3.max-2200x2200.jpg",
      "date_published": "2022-03-31T20:00:00Z",
      "author": {
        "name": "\u003cname\u003eGoogle Cloud Content \u0026 Editorial \u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/",
      "title": "Add severity levels to your alert policies in Cloud Monitoring",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;When you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we\u0026amp;#8217;re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty).\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/support/notification-options\u0026#34;\u0026gt;notification channels\u0026lt;/a\u0026gt; have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Below, we\u0026#39;ll walk through examples of \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/alerts/labels\u0026#34;\u0026gt;how to add\u0026lt;/a\u0026gt; static and dynamic severity levels to an Alert Policy.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Create user labels to support static severity levels\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;When you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt; to see how to add user labels to alert policies via the Alert Policy API.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;INFO\u0026lt;/code\u0026gt; when CPU utilization is between 70% and 80%\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;WARNING\u0026lt;/code\u0026gt; when CPU utilization is between 80% and 90%\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;CRITICAL\u0026lt;/code\u0026gt; when CPU utilization is above 90%\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;To accomplish this, you can create three separate alert policies with user labels defined as below:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Create alert policy (\u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt;) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label \u0026lt;code\u0026gt;Severity\u0026lt;/code\u0026gt; with value \u0026lt;code\u0026gt;CRITICAL\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;\u0026amp;#34;userLabels\u0026amp;#34;: {\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026amp;#160;\u0026amp;#160;\u0026amp;#160;\u0026amp;#160;\u0026lt;code\u0026gt;\u0026amp;#8220;Severity\u0026amp;#8221;: \u0026amp;#8220;CRITICAL\u0026amp;#8221;,\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026lt;code\u0026gt;}\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Create a second policy (\u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt;) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label \u0026lt;code\u0026gt;Severity\u0026lt;/code\u0026gt; with value \u0026lt;code\u0026gt;WARNING\u0026lt;/code\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;\u0026amp;#34;userLabels\u0026amp;#34;: {\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026amp;#160; \u0026amp;#160;\u0026amp;#160;\u0026lt;code\u0026gt;\u0026amp;#8220;Severity\u0026amp;#8221;: \u0026amp;#8220;WARNING\u0026amp;#8221;,\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026lt;code\u0026gt;}\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Create a third policy (\u0026lt;code\u0026gt;C\u0026lt;/code\u0026gt;) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label \u0026lt;code\u0026gt;Severity\u0026lt;/code\u0026gt; with value \u0026lt;code\u0026gt;INFO\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;\u0026amp;#34;userLabels\u0026amp;#34;: {\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026amp;#160; \u0026amp;#160;\u0026amp;#160;\u0026lt;code\u0026gt;\u0026amp;#8220;Severity\u0026amp;#8221;: \u0026amp;#8220;INFO\u0026amp;#8221;,\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026lt;code\u0026gt;}\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In this scenario, when the CPU utilization crosses a threshold of 90% policies \u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt;, and \u0026lt;code\u0026gt;C\u0026lt;/code\u0026gt; will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy \u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt; will close, but the incidents from policies \u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;C\u0026lt;/code\u0026gt; will remain open. If the CPU utilization falls even further down to 75%, the incident from policy \u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt; will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Use MQL to create dynamic severity levels\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Alert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label \u0026lt;code\u0026gt;Severity\u0026lt;/code\u0026gt; with value:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;INFO\u0026lt;/code\u0026gt; below a threshold of 80%,\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;WARNING\u0026lt;/code\u0026gt; below a threshold of 90%, and\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;CRITICAL\u0026lt;/code\u0026gt; above a threshold of 90%.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;If you\u0026#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/mql/alerts\u0026#34;\u0026gt;utilize MQL to create alert policies\u0026lt;/a\u0026gt; with dynamic custom metric labels that will be embedded in the incident. Via MQL \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/mql/reference#map\u0026#34;\u0026gt;map\u0026lt;/a\u0026gt;, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Take the sample MQL query below:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWhen you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we’re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty). \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/support/notification-options\" track-metadata-module=\"post\"\u003enotification channels\u003c/a\u003e have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.\u003c/p\u003e\u003cp\u003eBelow, we\u0026#39;ll walk through examples of \u003ca href=\"https://cloud.google.com/monitoring/alerts/labels\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/alerts/labels\" track-metadata-module=\"post\"\u003ehow to add\u003c/a\u003e static and dynamic severity levels to an Alert Policy. \u003c/p\u003e\u003ch3\u003eCreate user labels to support static severity levels\u003c/h3\u003e\u003cp\u003eWhen you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the \u003ca href=\"https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e to see how to add user labels to alert policies via the Alert Policy API. \u003c/p\u003e\u003cp\u003eLet’s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eINFO\u003c/code\u003e when CPU utilization is between 70% and 80%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eWARNING\u003c/code\u003e when CPU utilization is between 80% and 90%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eCRITICAL\u003c/code\u003e when CPU utilization is above 90%\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo accomplish this, you can create three separate alert policies with user labels defined as below:\u003c/p\u003e\u003cp\u003eCreate alert policy (\u003ccode\u003eA\u003c/code\u003e) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eCRITICAL\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003ccode\u003e\u0026#34;userLabels\u0026#34;: {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “CRITICAL”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eCreate a second policy (\u003ccode\u003eB\u003c/code\u003e) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eWARNING\u003c/code\u003e. \u003c/p\u003e\u003cp\u003e\u003ccode\u003e\u0026#34;userLabels\u0026#34;: {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “WARNING”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eCreate a third policy (\u003ccode\u003eC\u003c/code\u003e) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eINFO\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003ccode\u003e\u0026#34;userLabels\u0026#34;: {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “INFO”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eIn this scenario, when the CPU utilization crosses a threshold of 90% policies \u003ccode\u003eA\u003c/code\u003e, \u003ccode\u003eB\u003c/code\u003e, and \u003ccode\u003eC\u003c/code\u003e will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy \u003ccode\u003eA\u003c/code\u003e will close, but the incidents from policies \u003ccode\u003eB\u003c/code\u003e and \u003ccode\u003eC\u003c/code\u003e will remain open. If the CPU utilization falls even further down to 75%, the incident from policy \u003ccode\u003eB\u003c/code\u003e will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.\u003c/p\u003e\u003ch3\u003eUse MQL to create dynamic severity levels\u003c/h3\u003e\u003cp\u003eAlert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label \u003ccode\u003eSeverity\u003c/code\u003e with value:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eINFO\u003c/code\u003e below a threshold of 80%,\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eWARNING\u003c/code\u003e below a threshold of 90%, and\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eCRITICAL\u003c/code\u003e above a threshold of 90%.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you\u0026#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can \u003ca href=\"https://cloud.google.com/monitoring/mql/alerts\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/mql/alerts\" track-metadata-module=\"post\"\u003eutilize MQL to create alert policies\u003c/a\u003e with dynamic custom metric labels that will be embedded in the incident. Via MQL \u003ca href=\"https://cloud.google.com/monitoring/mql/reference#map\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/mql/reference#map\" track-metadata-module=\"post\"\u003emap\u003c/a\u003e, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.\u003c/p\u003e\u003cp\u003eTake the sample MQL query below:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we’re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty). \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options\"\u003enotification channels\u003c/a\u003e have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.\u003c/p\u003e\u003cp\u003eBelow, we'll walk through examples of \u003ca href=\"https://cloud.google.com/monitoring/alerts/labels\"\u003ehow to add\u003c/a\u003e static and dynamic severity levels to an Alert Policy. \u003c/p\u003e\u003ch3\u003eCreate user labels to support static severity levels\u003c/h3\u003e\u003cp\u003eWhen you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the \u003ca href=\"https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies\"\u003edocumentation\u003c/a\u003e to see how to add user labels to alert policies via the Alert Policy API. \u003c/p\u003e\u003cp\u003eLet’s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eINFO\u003c/code\u003e when CPU utilization is between 70% and 80%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eWARNING\u003c/code\u003e when CPU utilization is between 80% and 90%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eCRITICAL\u003c/code\u003e when CPU utilization is above 90%\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo accomplish this, you can create three separate alert policies with user labels defined as below:\u003c/p\u003e\u003cp\u003eCreate alert policy (\u003ccode\u003eA\u003c/code\u003e) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eCRITICAL\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003ccode\u003e\"userLabels\": {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “CRITICAL”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eCreate a second policy (\u003ccode\u003eB\u003c/code\u003e) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eWARNING\u003c/code\u003e. \u003c/p\u003e\u003cp\u003e\u003ccode\u003e\"userLabels\": {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “WARNING”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eCreate a third policy (\u003ccode\u003eC\u003c/code\u003e) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eINFO\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003ccode\u003e\"userLabels\": {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “INFO”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eIn this scenario, when the CPU utilization crosses a threshold of 90% policies \u003ccode\u003eA\u003c/code\u003e, \u003ccode\u003eB\u003c/code\u003e, and \u003ccode\u003eC\u003c/code\u003e will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy \u003ccode\u003eA\u003c/code\u003e will close, but the incidents from policies \u003ccode\u003eB\u003c/code\u003e and \u003ccode\u003eC\u003c/code\u003e will remain open. If the CPU utilization falls even further down to 75%, the incident from policy \u003ccode\u003eB\u003c/code\u003e will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.\u003c/p\u003e\u003ch3\u003eUse MQL to create dynamic severity levels\u003c/h3\u003e\u003cp\u003eAlert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label \u003ccode\u003eSeverity\u003c/code\u003e with value:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eINFO\u003c/code\u003e below a threshold of 80%,\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eWARNING\u003c/code\u003e below a threshold of 90%, and\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eCRITICAL\u003c/code\u003e above a threshold of 90%.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you'd like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can \u003ca href=\"https://cloud.google.com/monitoring/mql/alerts\"\u003eutilize MQL to create alert policies\u003c/a\u003e with dynamic custom metric labels that will be embedded in the incident. Via MQL \u003ca href=\"https://cloud.google.com/monitoring/mql/reference#map\"\u003emap\u003c/a\u003e, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.\u003c/p\u003e\u003cp\u003eTake the sample MQL query below:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"fetch gce_instance\\r\\n| metric 'compute.googleapis.com/instance/cpu/utilization'\\r\\n| filter (metadata.user_labels.env == 'prod') \u0026amp;\u0026amp; (resource.zone =~ 'asia.*')\\r\\n| group_by sliding(5m), [value_utilization_mean: mean(value.utilization)]\\r\\n| map\\r\\n add[\\r\\n Severity:\\r\\n if(val() \u0026gt; 90 '%', 'CRITICAL',\\r\\n if(val() \u0026gt;= 80 '%' \u0026amp;\u0026amp; val() \u0026lt;= 90 '%', 'WARNING', 'INFO'))]\\r\\n| condition val() \u0026gt; 70 '%'\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn this example, an incident will be created any time CPU utilization is above a threshold of 70%. If the value is between 70-80%, the incident will contain a metric label called \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eINFO\u003c/code\u003e. If the value is between 80-90%, the metric label \u003ccode\u003eSeverity\u003c/code\u003e will have value WARNING, and if the value is above 90%, the label \u003ccode\u003eSeverity\u003c/code\u003e will have value \u003ccode\u003eCRITICAL\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eIn the above scenario, if the CPU utilization value starts at 92%, incident A will be created with severity level \u003ccode\u003eCRITICAL\u003c/code\u003e. If the utilization value then drops down to 73%, a new incident \u003ccode\u003eB\u003c/code\u003e will be opened with severity level \u003ccode\u003eINFO\u003c/code\u003e. Incident \u003ccode\u003eA\u003c/code\u003e, however, will remain open. If the value jumps to 82%, a new incident \u003ccode\u003eC\u003c/code\u003e will open with severity level \u003ccode\u003eWARNING\u003c/code\u003e and incidents \u003ccode\u003eA\u003c/code\u003e and \u003ccode\u003eB\u003c/code\u003e will remain open. If \u003ca href=\"https://cloud.google.com/monitoring/alerts/incidents-events#closing\"\u003eauto-close\u003c/a\u003e is configured in your policy with a duration of 30 minutes, incident `A` will auto-close 30 minutes after incident `B` starts, and incident `B` will auto-close 30 minutes after incident `C` starts.  If the value drops below 70%, all incidents will close. \u003c/p\u003e\u003cp\u003eIn order to ensure the alert policy only has one incident open at a time with the correct corresponding label, and to avoid waiting for incidents to auto-close as in the example above, set \u003ca href=\"https://cloud.google.com/monitoring/alerts/concepts-indepth#partial-metric-data\"\u003eevaluationMissingData\u003c/a\u003e to \u003ccode\u003eEVALUATION_MISSING_DATA_INACTIVE\u003c/code\u003e in your API request. This field tells the Alert Policy how to handle situations when the metric stream has sparse or missing data, so the incident can be closed appropriately as needed. If you are making your MQL alert policy in the UI, select the \u003ccode\u003eMissing data points treated as values that do not violate the policy condition\u003c/code\u003e button in the \u003ccode\u003eAdvanced Options\u003c/code\u003e dropdown in the \u003ccode\u003eConfigure Trigger\u003c/code\u003e section:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_trigger.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Monitoring trigger.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_trigger.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen \u003ccode\u003eEVALUATION_MISSING_DATA_INACTIVE\u003c/code\u003e is specified in the above scenario, incident \u003ccode\u003eA\u003c/code\u003e will close once incident\u003ccode\u003eB\u003c/code\u003e is created, and incident \u003ccode\u003eB\u003c/code\u003e will close once incident \u003ccode\u003eC\u003c/code\u003e is created.\u003c/p\u003e\u003ch3\u003eSeverity Labels in Notification Channels\u003c/h3\u003e\u003cp\u003eIf you send \u003ca href=\"https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.notificationChannels#resource:-notificationchannel\"\u003enotifications\u003c/a\u003e to a third-party service like \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options#creating_channels\"\u003ePagerDuty, Webhooks, or Pub/Sub\u003c/a\u003e then you can parse the JSON payload and route the notification according to its severity so that critical information is not missed by your team. \u003c/p\u003e\u003cp\u003eIf you utilize alert policy user labels, these will appear as an object on the notification with the key \u003ccode\u003epolicy_user_labels\u003c/code\u003e i.e.:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'\"policy_user_labels\": {\\r\\n \"Severity\": \"CRITICAL\",\\r\\n}'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you utilize metric labels via MQL, these will appear as an object with key \u003ccode\u003elabels\u003c/code\u003e nested in an object with key \u003ccode\u003emetric\u003c/code\u003e i.e.:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'\"metric\": {\\r\\n \"displayName\": \"Some Display Name\",\\r\\n \"labels\": {\\r\\n \"instance_name\": \"some_instance_name\",\\r\\n \"Severity\": \"CRITICAL\"\\r\\n },\\r\\n }'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eGet Started Today\u003c/h3\u003e\u003cp\u003eAlerts can be configured on nearly any metric, log, or trace (or the absence of that data) that is captured in \u003ca href=\"https://cloud.google.com/products/operations\"\u003eGoogle Cloud’s operations suite\u003c/a\u003e. Severity levels give you and your teams an additional way to cut through noise to find the issues that you know will have the most positive impact when resolved. Check out this video on \u003ca href=\"https://youtu.be/4RgJjx4IxMs\" target=\"_blank\"\u003elog alerts\u003c/a\u003e as part of our Observability in-depth video series and if you have questions, feature requests, or just want to read topics from other customers who are using Cloud Alerting, visit our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations\" target=\"_blank\"\u003eGoogle Cloud Community site\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/operations/pub-sub-webook-and-slack-notifications-are-now-available/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Genric_GCP_upA1oyz.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eWebhook, Pub/Sub, and Slack Alerting notification channels launched\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnnouncing the general availability of the new Pub/Sub, Webhook, and Slack Notification channels.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-03-29T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlizah Lalani\u003c/name\u003e\u003ctitle\u003eSoftware Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/identity-security/automate-public-certificate-lifecycle-management-via--acme-client-api/",
      "title": "Automate Public Certificates Lifecycle Management via RFC 8555 (ACME)",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;It is that simple. Just schedule this task to run periodically and you will now be automatically acquiring and maintaining the TLS certificates for the associated workload.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;For Kubernetes based workloads\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;If you are using Kubernetes, thanks to \u0026lt;a href=\u0026#34;https://cert-manager.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;cert-manager\u0026lt;/a\u0026gt; (another ACME client), it is just as easy. Simply specify the ACME url and \u0026lt;a href=\u0026#34;https://cert-manager.io/docs/configuration/acme/#external-account-bindings\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;External Account Binding\u0026lt;/a\u0026gt; details in your configuration. Your ACME client will ensure you always have an up to date certificate for your Kubernetes deployment.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Announcing the Private Preview\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We have heard loud and clear that our customers want to use a unified solution for managing their HTTPS certificates which is why we have launched this offering today.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Using this service and Google Trust Services means you will get the same industry leading \u0026lt;a href=\u0026#34;https://security.googleblog.com/2021/03/google-https-and-device-compatibility.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;device compatibility\u0026lt;/a\u0026gt; we use for services like YouTube and Google search for your own products and services.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;FAQ\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We know you might have some questions about this release so here are our answers to the most frequent questions we hear:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;How can I get access?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;You can request access to this Private Preview using \u0026lt;a href=\u0026#34;https://docs.google.com/forms/d/1Euhflb5CXpuLik8czElhyAloTZJZobar4086dmlPqXA/viewform?ts=620a6854\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;this sign up form\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;How long are the certificates you issue good for?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;By default all certificates issued by Google Trust Services are good for up to 90 days; however, ACME allows for clients to request certificates with different validity periods. Using this capability we allow the requestor to get certificates that are good for as little as 1 day, though we would not recommend using anything less than 3 days due to concerns over \u0026lt;a href=\u0026#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46359.pdf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;clock skew\u0026lt;/a\u0026gt; and certificate validity overlap.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;What forms of domain control verification do you support?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;The ACME protocol defines several mechanisms for domain control verification and we support three of them, they include : TLS-ALPN-01, HTTP-01, and DNS-01. \u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Each of these have different scenarios where their use makes the most sense, for example TLS-ALPN-01 might make sense in cases where HTTPS is not used and the requestor does not have access to dynamically update DNS records. Choose the mechanism that fits your use case best.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Do you support email based domain control verification?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;No we do not.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Do you issue wildcard certificates?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Yes we do. Please note, as with other Certificate Authorities you must currently use\u0026amp;#160; DNS based domain control verification to get a wildcard certificate.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Do you issue certificates for punycode encoded Unicode domain names?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Not at this time.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Do you issue certificates containing IP addresses?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Yes we do; however, this is currently limited to customers who control an IANA assigned IP address block. Contact your sales representative for more information.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Can I use ACME to get private certificates from Cloud CA Service?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Yes, but not directly. Our partner \u0026lt;a href=\u0026#34;https://smallstep.com/blog/private-acme-server/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SmallStep\u0026lt;/a\u0026gt; created an ACME Registration Authority (RA) that can be used to get certificates from the \u0026lt;a href=\u0026#34;https://cloud.google.com/certificate-authority-service\u0026#34;\u0026gt;Cloud CA Service\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;What algorithms and key lengths do you support?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;We support issuing both ECC and RSA certificates. For more information see our \u0026lt;a href=\u0026#34;https://pki.goog/repository/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Certificate Practice Statement\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://pki.goog/repository/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;CA Certificate Repository\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Do you offer certificates from a pure ECC based certificate chain?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Not at this time.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;What root certificates do you use?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;We list all of our root certificates and intermediate certificates \u0026lt;a href=\u0026#34;https://pki.goog/repository/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt; and we do change which ones we use from time to time.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It is important to also note that we send the appropriate intermediate certificates with every certificate request via the \u0026lt;a href=\u0026#34;https://tools.ietf.org/html/rfc8555#section-7.4.2\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;ACME protocol. \u0026lt;/a\u0026gt;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Why should I use Google Trust Services instead of another certificate authority?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;There are multiple good ACME CAs you may use.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We envision a world where those that deploy SSL use a number of ACME based certificate authorities to enable sites to continue to operate without downtime when one provider has availability issues. If you need a large number of certificates or guarantees on geographic diversity, the GTS CA may be an especially good fit.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It is our hope that by making this service available to cloud customers they will be able to get the benefit of that robustness, and reduce latency for workloads terminating TLS within Google Cloud.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;You recently announced Certificate Manager, is this an alternative to that offering?\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;No it is not. This extends Certificate Manager so that workloads that choose to terminate TLS on their own are able to get certificates from the same CA we use when we manage your certificates for you.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It is our hope that with this ACME API, you will be able to simplify your HTTPS certificate lifecycle management for your workloads.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eIt is that simple. Just schedule this task to run periodically and you will now be automatically acquiring and maintaining the TLS certificates for the associated workload.\u003c/p\u003e\u003ch3\u003eFor Kubernetes based workloads\u003c/h3\u003e\u003cp\u003eIf you are using Kubernetes, thanks to \u003ca href=\"https://cert-manager.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cert-manager.io\" track-metadata-module=\"post\"\u003ecert-manager\u003c/a\u003e (another ACME client), it is just as easy. Simply specify the ACME url and \u003ca href=\"https://cert-manager.io/docs/configuration/acme/#external-account-bindings\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cert-manager.io\" track-metadata-module=\"post\"\u003eExternal Account Binding\u003c/a\u003e details in your configuration. Your ACME client will ensure you always have an up to date certificate for your Kubernetes deployment.\u003c/p\u003e\u003ch3\u003eAnnouncing the Private Preview\u003c/h3\u003e\u003cp\u003eWe have heard loud and clear that our customers want to use a unified solution for managing their HTTPS certificates which is why we have launched this offering today.\u003c/p\u003e\u003cp\u003eUsing this service and Google Trust Services means you will get the same industry leading \u003ca href=\"https://security.googleblog.com/2021/03/google-https-and-device-compatibility.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://security.googleblog.com\" track-metadata-module=\"post\"\u003edevice compatibility\u003c/a\u003e we use for services like YouTube and Google search for your own products and services.\u003c/p\u003e\u003ch3\u003eFAQ\u003c/h3\u003e\u003cp\u003eWe know you might have some questions about this release so here are our answers to the most frequent questions we hear:\u003c/p\u003e\u003cp\u003e\u003cb\u003eHow can I get access?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eYou can request access to this Private Preview using \u003ca href=\"https://docs.google.com/forms/d/1Euhflb5CXpuLik8czElhyAloTZJZobar4086dmlPqXA/viewform?ts=620a6854\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://docs.google.com\" track-metadata-module=\"post\"\u003ethis sign up form\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cb\u003eHow long are the certificates you issue good for?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBy default all certificates issued by Google Trust Services are good for up to 90 days; however, ACME allows for clients to request certificates with different validity periods. Using this capability we allow the requestor to get certificates that are good for as little as 1 day, though we would not recommend using anything less than 3 days due to concerns over \u003ca href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46359.pdf\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://static.googleusercontent.com\" track-metadata-module=\"post\"\u003eclock skew\u003c/a\u003e and certificate validity overlap.\u003c/p\u003e\u003cp\u003e\u003cb\u003eWhat forms of domain control verification do you support?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe ACME protocol defines several mechanisms for domain control verification and we support three of them, they include : TLS-ALPN-01, HTTP-01, and DNS-01. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eEach of these have different scenarios where their use makes the most sense, for example TLS-ALPN-01 might make sense in cases where HTTPS is not used and the requestor does not have access to dynamically update DNS records. Choose the mechanism that fits your use case best.\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you support email based domain control verification?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNo we do not.\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you issue wildcard certificates?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eYes we do. Please note, as with other Certificate Authorities you must currently use  DNS based domain control verification to get a wildcard certificate.\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you issue certificates for punycode encoded Unicode domain names?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot at this time.\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you issue certificates containing IP addresses?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eYes we do; however, this is currently limited to customers who control an IANA assigned IP address block. Contact your sales representative for more information.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCan I use ACME to get private certificates from Cloud CA Service?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eYes, but not directly. Our partner \u003ca href=\"https://smallstep.com/blog/private-acme-server/\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://smallstep.com\" track-metadata-module=\"post\"\u003eSmallStep\u003c/a\u003e created an ACME Registration Authority (RA) that can be used to get certificates from the \u003ca href=\"https://cloud.google.com/certificate-authority-service\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/certificate-authority-service\" track-metadata-module=\"post\"\u003eCloud CA Service\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cb\u003eWhat algorithms and key lengths do you support?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe support issuing both ECC and RSA certificates. For more information see our \u003ca href=\"https://pki.goog/repository/\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://pki.goog\" track-metadata-module=\"post\"\u003eCertificate Practice Statement\u003c/a\u003e and \u003ca href=\"https://pki.goog/repository/\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://pki.goog\" track-metadata-module=\"post\"\u003eCA Certificate Repository\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you offer certificates from a pure ECC based certificate chain?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot at this time.\u003c/p\u003e\u003cp\u003e\u003cb\u003eWhat root certificates do you use?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe list all of our root certificates and intermediate certificates \u003ca href=\"https://pki.goog/repository/\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://pki.goog\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e and we do change which ones we use from time to time. \u003c/p\u003e\u003cp\u003eIt is important to also note that we send the appropriate intermediate certificates with every certificate request via the \u003ca href=\"https://tools.ietf.org/html/rfc8555#section-7.4.2\" target=\"_blank\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://tools.ietf.org\" track-metadata-module=\"post\"\u003eACME protocol. \u003c/a\u003e \u003c/p\u003e\u003cp\u003e\u003cb\u003eWhy should I use Google Trust Services instead of another certificate authority?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThere are multiple good ACME CAs you may use. \u003c/p\u003e\u003cp\u003eWe envision a world where those that deploy SSL use a number of ACME based certificate authorities to enable sites to continue to operate without downtime when one provider has availability issues. If you need a large number of certificates or guarantees on geographic diversity, the GTS CA may be an especially good fit.\u003c/p\u003e\u003cp\u003eIt is our hope that by making this service available to cloud customers they will be able to get the benefit of that robustness, and reduce latency for workloads terminating TLS within Google Cloud. \u003c/p\u003e\u003cp\u003e\u003cb\u003eYou recently announced Certificate Manager, is this an alternative to that offering?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNo it is not. This extends Certificate Manager so that workloads that choose to terminate TLS on their own are able to get certificates from the same CA we use when we manage your certificates for you.\u003c/p\u003e\u003cp\u003eIt is our hope that with this ACME API, you will be able to simplify your HTTPS certificate lifecycle management for your workloads.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe’re excited to announce an enhancement of our preview of Certificate Manager which allows Google Cloud customers to acquire public certificates for their workloads that terminate TLS directly or for their cross-cloud and on-premise workloads. This is accomplished via the Automatic Certificate Management Environment (\u003ca href=\"https://datatracker.ietf.org/doc/html/rfc8555\" target=\"_blank\"\u003eACME\u003c/a\u003e) protocol which is the same protocol used by Certificate Authorities  to enable seamless automatic lifecycle management of TLS certificates.\u003c/p\u003e\u003cp\u003eThese certificates come from Google Trust Services, the same Certificate Authority (CA) we use by default when we manage certificates on your behalf with the Global External HTTPS Load Balancer. By using the same CA for managed certificates and unmanaged certificates you are assured that both scenarios work equally well across the entire spectrum of devices that use your services.\u003c/p\u003e\u003cp\u003eThis also enables Cloud Customers to get a more reliable TLS deployment. It does so by enabling one common certificate lifecycle management story based on ACME to be used without a single point of failure (relying just on one certificate authority). In other words, it is now possible to freely load balance or fail over to multiple ACME CAs with confidence. \u003c/p\u003e\u003ch3\u003eHow do I use it ? \u003c/h3\u003e\u003cp\u003eTo use this feature you will need an API key so you can use a feature in ACME called \u003ca href=\"https://tools.ietf.org/html/rfc8555#section-7.3.4\" target=\"_blank\"\u003eExternal Account Binding\u003c/a\u003e. This enables us to associate your certificate requests to your Google Cloud account and allows us to impose rate limits on a per customer basis. You may easily get an API key using the following commands:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'$ gcloud config set project \u0026lt;project ID\u0026gt;\\r\\n$ gcloud projects add-iam-policy-binding project-foo \\\\\\r\\n --member=user:ca-manager@example.net \\\\\\r\\n --role=roles/publicca.externalAccountKeyCreator\\r\\n# Request a key:\\r\\n$ gcloud alpha publicca external-account-keys create'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eEach ACME implementation differs slightly on how you specify this API key but as an example with the popular\u003ca href=\"https://certbot.eff.org/\" target=\"_blank\"\u003eCertbot ACME\u003c/a\u003e client the configuration looks something like this:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'certbot -d \u0026lt;domain.com\u0026gt; --server https://dv.acme-v02.api.pki.goog/directory --eab-kid \u0026lt;EAB_KEY_ID\u0026gt; --eab-hmac-key \u0026lt;EAB_HMAC_KEY\u0026gt; --standalone --preferred-challenges dns certonly'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIt is that simple. Just schedule this task to run periodically and you will now be automatically acquiring and maintaining the TLS certificates for the associated workload.\u003c/p\u003e\u003ch3\u003eFor Kubernetes based workloads\u003c/h3\u003e\u003cp\u003eIf you are using Kubernetes, thanks to \u003ca href=\"https://cert-manager.io/\" target=\"_blank\"\u003ecert-manager\u003c/a\u003e (another ACME client), it is just as easy. Simply specify the ACME url and \u003ca href=\"https://cert-manager.io/docs/configuration/acme/#external-account-bindings\" target=\"_blank\"\u003eExternal Account Binding\u003c/a\u003e details in your configuration. Your ACME client will ensure you always have an up to date certificate for your Kubernetes deployment.\u003c/p\u003e\u003ch3\u003eAnnouncing the Private Preview\u003c/h3\u003e\u003cp\u003eWe have heard loud and clear that our customers want to use a unified solution for managing their HTTPS certificates which is why we have launched this offering today.\u003c/p\u003e\u003cp\u003eUsing this service and Google Trust Services means you will get the same industry leading \u003ca href=\"https://security.googleblog.com/2021/03/google-https-and-device-compatibility.html\" target=\"_blank\"\u003edevice compatibility\u003c/a\u003e we use for services like YouTube and Google search for your own products and services.\u003c/p\u003e\u003ch3\u003eFAQ\u003c/h3\u003e\u003cp\u003eWe know you might have some questions about this release so here are our answers to the most frequent questions we hear:\u003c/p\u003e\u003cp\u003e\u003cb\u003eHow can I get access?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eYou can request access to this Private Preview using \u003ca href=\"https://docs.google.com/forms/d/1Euhflb5CXpuLik8czElhyAloTZJZobar4086dmlPqXA/viewform?ts=620a6854\" target=\"_blank\"\u003ethis sign up form\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cb\u003eHow long are the certificates you issue good for?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBy default all certificates issued by Google Trust Services are good for up to 90 days; however, ACME allows for clients to request certificates with different validity periods. Using this capability we allow the requestor to get certificates that are good for as little as 1 day, though we would not recommend using anything less than 3 days due to concerns over \u003ca href=\"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46359.pdf\" target=\"_blank\"\u003eclock skew\u003c/a\u003e and certificate validity overlap.\u003c/p\u003e\u003cp\u003e\u003cb\u003eWhat forms of domain control verification do you support?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe ACME protocol defines several mechanisms for domain control verification and we support three of them, they include : TLS-ALPN-01, HTTP-01, and DNS-01. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003eEach of these have different scenarios where their use makes the most sense, for example TLS-ALPN-01 might make sense in cases where HTTPS is not used and the requestor does not have access to dynamically update DNS records. Choose the mechanism that fits your use case best.\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you support email based domain control verification?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNo we do not.\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you issue wildcard certificates?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eYes we do. Please note, as with other Certificate Authorities you must currently use  DNS based domain control verification to get a wildcard certificate.\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you issue certificates for punycode encoded Unicode domain names?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot at this time.\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you issue certificates containing IP addresses?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eYes we do; however, this is currently limited to customers who control an IANA assigned IP address block. Contact your sales representative for more information.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCan I use ACME to get private certificates from Cloud CA Service?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eYes, but not directly. Our partner \u003ca href=\"https://smallstep.com/blog/private-acme-server/\" target=\"_blank\"\u003eSmallStep\u003c/a\u003e created an ACME Registration Authority (RA) that can be used to get certificates from the \u003ca href=\"https://cloud.google.com/certificate-authority-service\"\u003eCloud CA Service\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cb\u003eWhat algorithms and key lengths do you support?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe support issuing both ECC and RSA certificates. For more information see our \u003ca href=\"https://pki.goog/repository/\" target=\"_blank\"\u003eCertificate Practice Statement\u003c/a\u003e and \u003ca href=\"https://pki.goog/repository/\" target=\"_blank\"\u003eCA Certificate Repository\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eDo you offer certificates from a pure ECC based certificate chain?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNot at this time.\u003c/p\u003e\u003cp\u003e\u003cb\u003eWhat root certificates do you use?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe list all of our root certificates and intermediate certificates \u003ca href=\"https://pki.goog/repository/\" target=\"_blank\"\u003ehere\u003c/a\u003e and we do change which ones we use from time to time. \u003c/p\u003e\u003cp\u003eIt is important to also note that we send the appropriate intermediate certificates with every certificate request via the \u003ca href=\"https://tools.ietf.org/html/rfc8555#section-7.4.2\" target=\"_blank\"\u003eACME protocol.\u003c/a\u003e \u003c/p\u003e\u003cp\u003e\u003cb\u003eWhy should I use Google Trust Services instead of another certificate authority?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThere are multiple good ACME CAs you may use. \u003c/p\u003e\u003cp\u003eWe envision a world where those that deploy SSL use a number of ACME based certificate authorities to enable sites to continue to operate without downtime when one provider has availability issues. If you need a large number of certificates or guarantees on geographic diversity, the GTS CA may be an especially good fit.\u003c/p\u003e\u003cp\u003eIt is our hope that by making this service available to cloud customers they will be able to get the benefit of that robustness, and reduce latency for workloads terminating TLS within Google Cloud. \u003c/p\u003e\u003cp\u003e\u003cb\u003eYou recently announced Certificate Manager, is this an alternative to that offering?\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eNo it is not. This extends Certificate Manager so that workloads that choose to terminate TLS on their own are able to get certificates from the same CA we use when we manage your certificates for you.\u003c/p\u003e\u003cp\u003eIt is our hope that with this ACME API, you will be able to simplify your HTTPS certificate lifecycle management for your workloads.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/identity-security/simplify-saas-scale-tls-certificate-management/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/google_cloud_security.0999040819990817.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eIntroducing Certificate Manager to simplify SaaS scale TLS and certificate management\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eCloud Certificate Manager lets our users acquire and manage TLS certificates for use with Cloud Load Balancing.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-03-29T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eBabi Seal\u003c/name\u003e\u003ctitle\u003eProduct Manager, Load Balancing\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/devops-best-practices-add-severity-levels-to-alerts/",
      "title": "Add severity levels to your alert policies in Cloud Monitoring",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c77=\"\"\u003e\u003cdiv _ngcontent-c77=\"\" innerhtml=\"\u0026lt;p\u0026gt;When you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we\u0026amp;#8217;re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty).\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/support/notification-options\u0026#34;\u0026gt;notification channels\u0026lt;/a\u0026gt; have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Below, we\u0026#39;ll walk through examples of \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/alerts/labels\u0026#34;\u0026gt;how to add\u0026lt;/a\u0026gt; static and dynamic severity levels to an Alert Policy.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Create user labels to support static severity levels\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;When you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt; to see how to add user labels to alert policies via the Alert Policy API.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;INFO\u0026lt;/code\u0026gt; when CPU utilization is between 70% and 80%\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;WARNING\u0026lt;/code\u0026gt; when CPU utilization is between 80% and 90%\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;CRITICAL\u0026lt;/code\u0026gt; when CPU utilization is above 90%\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;To accomplish this, you can create three separate alert policies with user labels defined as below:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Create alert policy (\u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt;) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label \u0026lt;code\u0026gt;Severity\u0026lt;/code\u0026gt; with value \u0026lt;code\u0026gt;CRITICAL\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;\u0026amp;#34;userLabels\u0026amp;#34;: {\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026amp;#160;\u0026amp;#160;\u0026amp;#160;\u0026amp;#160;\u0026lt;code\u0026gt;\u0026amp;#8220;Severity\u0026amp;#8221;: \u0026amp;#8220;CRITICAL\u0026amp;#8221;,\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026lt;code\u0026gt;}\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Create a second policy (\u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt;) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label \u0026lt;code\u0026gt;Severity\u0026lt;/code\u0026gt; with value \u0026lt;code\u0026gt;WARNING\u0026lt;/code\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;\u0026amp;#34;userLabels\u0026amp;#34;: {\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026amp;#160; \u0026amp;#160;\u0026amp;#160;\u0026lt;code\u0026gt;\u0026amp;#8220;Severity\u0026amp;#8221;: \u0026amp;#8220;WARNING\u0026amp;#8221;,\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026lt;code\u0026gt;}\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Create a third policy (\u0026lt;code\u0026gt;C\u0026lt;/code\u0026gt;) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label \u0026lt;code\u0026gt;Severity\u0026lt;/code\u0026gt; with value \u0026lt;code\u0026gt;INFO\u0026lt;/code\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;\u0026amp;#34;userLabels\u0026amp;#34;: {\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026amp;#160; \u0026amp;#160;\u0026amp;#160;\u0026lt;code\u0026gt;\u0026amp;#8220;Severity\u0026amp;#8221;: \u0026amp;#8220;INFO\u0026amp;#8221;,\u0026lt;/code\u0026gt;\u0026lt;br\u0026gt;\u0026lt;code\u0026gt;}\u0026lt;/code\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In this scenario, when the CPU utilization crosses a threshold of 90% policies \u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt;, and \u0026lt;code\u0026gt;C\u0026lt;/code\u0026gt; will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy \u0026lt;code\u0026gt;A\u0026lt;/code\u0026gt; will close, but the incidents from policies \u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;C\u0026lt;/code\u0026gt; will remain open. If the CPU utilization falls even further down to 75%, the incident from policy \u0026lt;code\u0026gt;B\u0026lt;/code\u0026gt; will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Use MQL to create dynamic severity levels\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Alert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label \u0026lt;code\u0026gt;Severity\u0026lt;/code\u0026gt; with value:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;INFO\u0026lt;/code\u0026gt; below a threshold of 80%,\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;WARNING\u0026lt;/code\u0026gt; below a threshold of 90%, and\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;code\u0026gt;CRITICAL\u0026lt;/code\u0026gt; above a threshold of 90%.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;If you\u0026#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/mql/alerts\u0026#34;\u0026gt;utilize MQL to create alert policies\u0026lt;/a\u0026gt; with dynamic custom metric labels that will be embedded in the incident. Via MQL \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/mql/reference#map\u0026#34;\u0026gt;map\u0026lt;/a\u0026gt;, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Take the sample MQL query below:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWhen you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we’re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty). \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/support/notification-options\" track-metadata-module=\"post\"\u003enotification channels\u003c/a\u003e have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.\u003c/p\u003e\u003cp\u003eBelow, we\u0026#39;ll walk through examples of \u003ca href=\"https://cloud.google.com/monitoring/alerts/labels\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/alerts/labels\" track-metadata-module=\"post\"\u003ehow to add\u003c/a\u003e static and dynamic severity levels to an Alert Policy. \u003c/p\u003e\u003ch3\u003eCreate user labels to support static severity levels\u003c/h3\u003e\u003cp\u003eWhen you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the \u003ca href=\"https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e to see how to add user labels to alert policies via the Alert Policy API. \u003c/p\u003e\u003cp\u003eLet’s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eINFO\u003c/code\u003e when CPU utilization is between 70% and 80%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eWARNING\u003c/code\u003e when CPU utilization is between 80% and 90%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eCRITICAL\u003c/code\u003e when CPU utilization is above 90%\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo accomplish this, you can create three separate alert policies with user labels defined as below:\u003c/p\u003e\u003cp\u003eCreate alert policy (\u003ccode\u003eA\u003c/code\u003e) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eCRITICAL\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003ccode\u003e\u0026#34;userLabels\u0026#34;: {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “CRITICAL”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eCreate a second policy (\u003ccode\u003eB\u003c/code\u003e) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eWARNING\u003c/code\u003e. \u003c/p\u003e\u003cp\u003e\u003ccode\u003e\u0026#34;userLabels\u0026#34;: {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “WARNING”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eCreate a third policy (\u003ccode\u003eC\u003c/code\u003e) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eINFO\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003ccode\u003e\u0026#34;userLabels\u0026#34;: {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “INFO”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eIn this scenario, when the CPU utilization crosses a threshold of 90% policies \u003ccode\u003eA\u003c/code\u003e, \u003ccode\u003eB\u003c/code\u003e, and \u003ccode\u003eC\u003c/code\u003e will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy \u003ccode\u003eA\u003c/code\u003e will close, but the incidents from policies \u003ccode\u003eB\u003c/code\u003e and \u003ccode\u003eC\u003c/code\u003e will remain open. If the CPU utilization falls even further down to 75%, the incident from policy \u003ccode\u003eB\u003c/code\u003e will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.\u003c/p\u003e\u003ch3\u003eUse MQL to create dynamic severity levels\u003c/h3\u003e\u003cp\u003eAlert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label \u003ccode\u003eSeverity\u003c/code\u003e with value:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eINFO\u003c/code\u003e below a threshold of 80%,\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eWARNING\u003c/code\u003e below a threshold of 90%, and\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eCRITICAL\u003c/code\u003e above a threshold of 90%.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you\u0026#39;d like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can \u003ca href=\"https://cloud.google.com/monitoring/mql/alerts\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/mql/alerts\" track-metadata-module=\"post\"\u003eutilize MQL to create alert policies\u003c/a\u003e with dynamic custom metric labels that will be embedded in the incident. Via MQL \u003ca href=\"https://cloud.google.com/monitoring/mql/reference#map\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/mql/reference#map\" track-metadata-module=\"post\"\u003emap\u003c/a\u003e, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.\u003c/p\u003e\u003cp\u003eTake the sample MQL query below:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen you are dealing with a situation that fires a bevy of alerts, do you instinctively know which alerts are the most pressing? Severity levels are an important concept in alerting to aid you and your team in properly assessing which notifications should be prioritized. You can use these levels to focus on the issues deemed most critical for your operations and triage through the noise. Today, we’re happy to announce that you can create custom severity levels on your alert policies and have this data included in your notifications for more effective alerting and integration with downstream third-party services (e.g. Webhook, Cloud Pub/Sub, PagerDuty). \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options\"\u003enotification channels\u003c/a\u003e have been enhanced to accept this data - including Email, Webhooks, Cloud Pub/Sub, and PagerDuty - with planned support for Slack at a later time. This enables further automation/customization based on importance wherever the notifications are consumed.\u003c/p\u003e\u003cp\u003eBelow, we'll walk through examples of \u003ca href=\"https://cloud.google.com/monitoring/alerts/labels\"\u003ehow to add\u003c/a\u003e static and dynamic severity levels to an Alert Policy. \u003c/p\u003e\u003ch3\u003eCreate user labels to support static severity levels\u003c/h3\u003e\u003cp\u003eWhen you add user labels on an alert policy, they will appear on every notification and incident generated by that alert policy. Refer to the \u003ca href=\"https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies\"\u003edocumentation\u003c/a\u003e to see how to add user labels to alert policies via the Alert Policy API. \u003c/p\u003e\u003cp\u003eLet’s walk through an example: suppose you want to configure Alert Policies that notify you when the CPU utilization crosses a particular threshold. Further, you want the notifications to indicate the following severity levels:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eINFO\u003c/code\u003e when CPU utilization is between 70% and 80%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eWARNING\u003c/code\u003e when CPU utilization is between 80% and 90%\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eCRITICAL\u003c/code\u003e when CPU utilization is above 90%\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eTo accomplish this, you can create three separate alert policies with user labels defined as below:\u003c/p\u003e\u003cp\u003eCreate alert policy (\u003ccode\u003eA\u003c/code\u003e) which triggers when the CPU utilization is above 90%, and includes the following user labels: any incident generated by this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eCRITICAL\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003ccode\u003e\"userLabels\": {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “CRITICAL”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eCreate a second policy (\u003ccode\u003eB\u003c/code\u003e) which triggers when resource CPU utilization is above 80%, and includes the following user labels: any incident generated on this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eWARNING\u003c/code\u003e. \u003c/p\u003e\u003cp\u003e\u003ccode\u003e\"userLabels\": {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “WARNING”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eCreate a third policy (\u003ccode\u003eC\u003c/code\u003e) which triggers when resource CPU utilization is above 70%, and includes the following user labels: any incident generated on this policy will include a label \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eINFO\u003c/code\u003e.\u003c/p\u003e\u003cp\u003e\u003ccode\u003e\"userLabels\": {\u003c/code\u003e\u003cbr/\u003e    \u003ccode\u003e“Severity”: “INFO”,\u003c/code\u003e\u003cbr/\u003e\u003ccode\u003e}\u003c/code\u003e\u003c/p\u003e\u003cp\u003eIn this scenario, when the CPU utilization crosses a threshold of 90% policies \u003ccode\u003eA\u003c/code\u003e, \u003ccode\u003eB\u003c/code\u003e, and \u003ccode\u003eC\u003c/code\u003e will trigger alerts. If the CPU utilization falls back down to 85%, the incident from policy \u003ccode\u003eA\u003c/code\u003e will close, but the incidents from policies \u003ccode\u003eB\u003c/code\u003e and \u003ccode\u003eC\u003c/code\u003e will remain open. If the CPU utilization falls even further down to 75%, the incident from policy \u003ccode\u003eB\u003c/code\u003e will close, and the incident from policy C will remain open. If the CPU utilization drops down to 40%, incidents generated by all three policies will automatically close.\u003c/p\u003e\u003ch3\u003eUse MQL to create dynamic severity levels\u003c/h3\u003e\u003cp\u003eAlert policy user labels are static in nature, meaning you cannot dynamically apply user labels based on a changing threshold. As shown earlier, you need to create three separate alert policies to generate notifications that contain user label \u003ccode\u003eSeverity\u003c/code\u003e with value:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eINFO\u003c/code\u003e below a threshold of 80%,\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eWARNING\u003c/code\u003e below a threshold of 90%, and\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ccode\u003eCRITICAL\u003c/code\u003e above a threshold of 90%.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you'd like to dynamically apply the severity level based on threshold within a single alert policy, you can use MQL. You can \u003ca href=\"https://cloud.google.com/monitoring/mql/alerts\"\u003eutilize MQL to create alert policies\u003c/a\u003e with dynamic custom metric labels that will be embedded in the incident. Via MQL \u003ca href=\"https://cloud.google.com/monitoring/mql/reference#map\"\u003emap\u003c/a\u003e, you can specify what threshold level should result in which severity label. This means you can accomplish the above scenario of three severity levels based on threshold by creating only one alert policy.\u003c/p\u003e\u003cp\u003eTake the sample MQL query below:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"fetch gce_instance\\r\\n| metric 'compute.googleapis.com/instance/cpu/utilization'\\r\\n| filter (metadata.user_labels.env == 'prod') \u0026amp;\u0026amp; (resource.zone =~ 'asia.*')\\r\\n| group_by sliding(5m), [value_utilization_mean: mean(value.utilization)]\\r\\n| map\\r\\n add[\\r\\n Severity:\\r\\n if(val() \u0026gt; 90 '%', 'CRITICAL',\\r\\n if(val() \u0026gt;= 80 '%' \u0026amp;\u0026amp; val() \u0026lt;= 90 '%', 'WARNING', 'INFO'))]\\r\\n| condition val() \u0026gt; 70 '%'\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn this example, an incident will be created any time CPU utilization is above a threshold of 70%. If the value is between 70-80%, the incident will contain a metric label called \u003ccode\u003eSeverity\u003c/code\u003e with value \u003ccode\u003eINFO\u003c/code\u003e. If the value is between 80-90%, the metric label \u003ccode\u003eSeverity\u003c/code\u003e will have value WARNING, and if the value is above 90%, the label \u003ccode\u003eSeverity\u003c/code\u003e will have value \u003ccode\u003eCRITICAL\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eIn the above scenario, if the CPU utilization value starts at 92%, incident A will be created with severity level \u003ccode\u003eCRITICAL\u003c/code\u003e. If the utilization value then drops down to 73%, a new incident \u003ccode\u003eB\u003c/code\u003e will be opened with severity level \u003ccode\u003eINFO\u003c/code\u003e. Incident \u003ccode\u003eA\u003c/code\u003e, however, will remain open. If the value jumps to 82%, a new incident \u003ccode\u003eC\u003c/code\u003e will open with severity level \u003ccode\u003eWARNING\u003c/code\u003e and incidents \u003ccode\u003eA\u003c/code\u003e and \u003ccode\u003eB\u003c/code\u003e will remain open. If \u003ca href=\"https://cloud.google.com/monitoring/alerts/incidents-events#closing\"\u003eauto-close\u003c/a\u003e is configured in your policy with a duration of 30 minutes, incident `A` will auto-close 30 minutes after incident `B` starts, and incident `B` will auto-close 30 minutes after incident `C` starts.  If the value drops below 70%, all incidents will close. \u003c/p\u003e\u003cp\u003eIn order to ensure the alert policy only has one incident open at a time with the correct corresponding label, and to avoid waiting for incidents to auto-close as in the example above, set \u003ca href=\"https://cloud.google.com/monitoring/alerts/concepts-indepth#partial-metric-data\"\u003eevaluationMissingData\u003c/a\u003e to \u003ccode\u003eEVALUATION_MISSING_DATA_INACTIVE\u003c/code\u003e in your API request. This field tells the Alert Policy how to handle situations when the metric stream has sparse or missing data, so the incident can be closed appropriately as needed. If you are making your MQL alert policy in the UI, select the \u003ccode\u003eMissing data points treated as values that do not violate the policy condition\u003c/code\u003e button in the \u003ccode\u003eAdvanced Options\u003c/code\u003e dropdown in the \u003ccode\u003eConfigure Trigger\u003c/code\u003e section:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_trigger.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Monitoring trigger.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_trigger.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen \u003ccode\u003eEVALUATION_MISSING_DATA_INACTIVE\u003c/code\u003e is specified in the above scenario, incident \u003ccode\u003eA\u003c/code\u003e will close once incident\u003ccode\u003eB\u003c/code\u003e is created, and incident \u003ccode\u003eB\u003c/code\u003e will close once incident \u003ccode\u003eC\u003c/code\u003e is created.\u003c/p\u003e\u003ch3\u003eSeverity Labels in Notification Channels\u003c/h3\u003e\u003cp\u003eIf you send \u003ca href=\"https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.notificationChannels#resource:-notificationchannel\"\u003enotifications\u003c/a\u003e to a third-party service like \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options#creating_channels\"\u003ePagerDuty, Webhooks, or Pub/Sub\u003c/a\u003e then you can parse the JSON payload and route the notification according to its severity so that critical information is not missed by your team. \u003c/p\u003e\u003cp\u003eIf you utilize alert policy user labels, these will appear as an object on the notification with the key \u003ccode\u003epolicy_user_labels\u003c/code\u003e i.e.:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'\"policy_user_labels\": {\\r\\n \"Severity\": \"CRITICAL\",\\r\\n}'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you utilize metric labels via MQL, these will appear as an object with key \u003ccode\u003elabels\u003c/code\u003e nested in an object with key \u003ccode\u003emetric\u003c/code\u003e i.e.:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'\"metric\": {\\r\\n \"displayName\": \"Some Display Name\",\\r\\n \"labels\": {\\r\\n \"instance_name\": \"some_instance_name\",\\r\\n \"Severity\": \"CRITICAL\"\\r\\n },\\r\\n }'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eGet Started Today\u003c/h3\u003e\u003cp\u003eAlerts can be configured on nearly any metric, log, or trace (or the absence of that data) that is captured in \u003ca href=\"https://cloud.google.com/products/operations\"\u003eGoogle Cloud’s operations suite\u003c/a\u003e. Severity levels give you and your teams an additional way to cut through noise to find the issues that you know will have the most positive impact when resolved. Check out this video on \u003ca href=\"https://youtu.be/4RgJjx4IxMs\" target=\"_blank\"\u003elog alerts\u003c/a\u003e as part of our Observability in-depth video series and if you have questions, feature requests, or just want to read topics from other customers who are using Cloud Alerting, visit our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations\" target=\"_blank\"\u003eGoogle Cloud Community site\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/operations/pub-sub-webook-and-slack-notifications-are-now-available/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Genric_GCP_upA1oyz.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eWebhook, Pub/Sub, and Slack Alerting notification channels launched\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnnouncing the general availability of the new Pub/Sub, Webhook, and Slack Notification channels.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2022-03-29T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlizah Lalani\u003c/name\u003e\u003ctitle\u003eSoftware Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/cloud-first/whats-new-cloud-native-apps/",
      "title": "What’s new in cloud-native apps?",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDevelopers and IT operations pros of all stripes come to Google Cloud to build modern, cloud-first and cloud-native applications. Here’s the latest from Google Cloud on everything app dev, containers, Kubernetes, DevOps, serverless and open source, all in one place.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 14 - Mar 18 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eCreate EventArc triggers with Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn addition to the Google Cloud Console or gcloud, you can also use a Terraform resource to create an Eventarc trigger. Mete Atamel \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/creating-eventarc-triggers-terraform\"\u003eshows you how\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eScaling to new markets with Cloud Run\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eFrench publisher Les Echos Le Parisien Annonces switched from dedicated on-prem infrastructure to Cloud Run to supplement its main news site with regional variations. Les Echos shares its \u003ca href=\"https://cloud.google.com/blog/products/serverless/paris-based-news-organization-expands-markets-with-serverless-containers-and-php-cms\"\u003ewebsite architecture\u003c/a\u003e here. \u003c/p\u003e\u003cp\u003e\u003cb\u003eThe serverless way to celebrate Pi Day\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eIn honor of Pi Day, Google Cloud Developer Advocate Emma Haruka Iwao shows you how to use the new Cloud Functions (2nd gen) to \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/celebrating-pi-day-cloud-functions\"\u003ecalculate π\u003c/a\u003e — serverlessly.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/automotive.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eBuilding continuous integration \u0026amp; continuous delivery for autonomous vehicles on Google Cloud\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eCruise relies on a whole host of Google Cloud technologies to develop and test the tech that goes into its autonomous vehicles, or AVs.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Mar 07 - Mar 11 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eRhode Island moves to Google Cloud-based job board\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWhen the pandemic hit, the State of Rhode Island moved its workforce development operations entirely online on a foundation of Google Workspace and Google Cloud resources, including Firestore, Cloud Functions, and Kubernetes, among others. Check out \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/how-rhode-island-created-virtual-career-center\"\u003ehow they did it\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eContainerized microservices at Lowe’s\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eLowe’s already told us \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\"\u003ehow they use SRE\u003c/a\u003e. They’re at it again, describing how they built an e-commerce website using a \u003ca href=\"https://cloud.google.com/blog/topics/retail/how-google-cloud-services-helped-lowes-transform-ecommerce\"\u003econtainerized microservices architecture and Kubernetes\u003c/a\u003e, with Istio for service mesh and Cloud Operations for good measure.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCruise AVs hit the road with Google Cloud services\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eAutonomous Vehicle (AV) startup Cruise detailed how it’s using data analytics and machine learning on a foundation of Google Kubernetes Engine (GKE) and other services to develop and test its self-driving cars. \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform\"\u003eRead the guest post\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eL’Oréal’s data analytics gets a makeover with serverless\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWe’re hurtling toward a \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-the-programmable-cloud\"\u003eprogrammable cloud\u003c/a\u003e — a world where developers use cloud-native serverless tools like Cloud Functions to quickly prototype and build powerful, data-driven business insights. \u003ca href=\"https://cloud.google.com/blog/products/serverless/loreal-combines-google-cloud-serverless-and-data-offerings\"\u003eL’Oréal is a great example\u003c/a\u003e.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eBetter telemetry for your Anthos clusters\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/service-mesh/docs/observability/explore-dashboard\"\u003eAnthos Service Mesh Dashboard\u003c/a\u003e is now available (public preview) on the \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/latest\"\u003eAnthos clusters on Bare Metal\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/on-prem/1.10\"\u003eAnthos clusters on VMware\u003c/a\u003e. Now, you can get out-of-the-box telemetry dashboards to see a services-first view of your application on the Cloud Console.\u003c/p\u003e\u003cp\u003e\u003cb\u003eInstrument your Java apps\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eWith the new version of the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features\"\u003eGoogle Cloud Logging Java library\u003c/a\u003e, you can wire your application logs with more information — without adding a single line of code.\u003c/p\u003e\u003cp\u003e\u003cb\u003eVisualize metrics from Cloud Spanner\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eBuilding an app on top of Cloud Spanner but can’t assess how well it’s operating? The new \u003ca href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/googlecloudspannerreceiver\" target=\"_blank\"\u003eOpenTelemetery receiver for Cloud Spanner\u003c/a\u003e provides an easy way for you to process and visualize metrics from Cloud Spanner \u003ca href=\"https://cloud.google.com/spanner/docs/introspection\"\u003eSystem tables\u003c/a\u003e, and export these to the APM tool of your choice. \u003ca href=\"https://cloud.google.com/blog/products/databases/consume-spanner-metrics-using-opentelemetery\"\u003eRead more here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/knative.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eCongratulations Knative on becoming part of the CNCF\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eKnative enters the CNCF as an incubating project\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eWeek of Feb 28 - Mar 4 2022\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntroducing Cloud SDK\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eThe rebranded \u003ca href=\"https://cloud.google.com/sdk\"\u003eCloud SDK\u003c/a\u003e is a collection of all the libraries and tools (including Google Cloud CLI) you need to interact with Google Cloud products and services. Learn more \u003ca href=\"https://cloud.google.com/blog/products/application-development/redesigning-the-cloud-sdk-cli-for-easier-development\"\u003ehere\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eCloud CLI, meet Terraform\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eGoogle Cloud CLI’s new Declarative Export for Terraform allows you to export the current state of your Google Cloud infrastructure into a descriptive file compatible with Terraform (HCL) or Google’s KRM declarative tooling, and is now \u003ca href=\"https://cloud.google.com/blog/products/application-development/google-cloud-cli-declarative-export-preview\"\u003eavailable in preview\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003cb\u003eKnative graduates to incubating project \u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003eCongratulations to Knative, which has been \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/knative-enters-the-cncf-as-an-incubating-project\"\u003eaccepted by the Cloud Native Computing Foundation\u003c/a\u003e, or CNCF, as an incubating project, enabling the next phase of serverless architecture. \u003c/p\u003e\u003cp\u003e\u003cb\u003eWe manage Prometheus so you don’t have to\u003c/b\u003e\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eGoogle Cloud Managed Service for Prometheus\u003c/a\u003e is now generally available! Get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes\"\u003eLearn more here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/inframod_living_3.jpg",
      "date_published": "2022-03-18T20:00:00Z",
      "author": {
        "name": "\u003cname\u003eGoogle Cloud Content \u0026 Editorial \u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-java-client-library-new-features/",
      "title": "Get more insights from your Java applications logs",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cdiv _ngcontent-c74=\"\" innerhtml=\"\u0026lt;p\u0026gt;Today it is even easier to capture logs in your Java applications. Developers can get more data with their application logs using a new version of the Cloud Logging client library for Java. The library populates the current executing context implicitly with every ingested log entry. Read this if you want to learn how to get HTTP requests and tracing information and additional metadata in your logs without writing a single line of code.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;There are three ways to ingest log data into Google Cloud Logging:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Develop a proprietary solution that directly calls the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/api-overview\u0026#34;\u0026gt;Logging API\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Leverage logging capabilities of the Google Cloud managed environments like \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#overview\u0026#34;\u0026gt;GKE\u0026lt;/a\u0026gt; or install Google Cloud \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/agent\u0026#34;\u0026gt;Ops agent\u0026lt;/a\u0026gt; and print your application logs to stdout and stderr.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Use Google Cloud \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/libraries\u0026#34;\u0026gt;Logging client library\u0026lt;/a\u0026gt; in one of many supported programming languages.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;The library provides you with ready to use boilerplate constructs built following the best practices of using Logging API. Java applications can use the Google Cloud Logging library to ingest logs using the integrations with \u0026lt;a href=\u0026#34;https://docs.oracle.com/javase/10/core/java-logging-overview.htm\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Java Logging\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://logback.qos.ch/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Logback framework\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you are new to using Google Logging client libraries for Java, follow the steps to \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/setup/java\u0026#34;\u0026gt;set up Cloud Logging for Java\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/libraries\u0026#34;\u0026gt;get started\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the version 3.6 release of the the Logging client library for Java you get many long demanding features including automatic population of the metadata about the environment\u0026#39;s resource supporting Cloud Run and Cloud Functions, HTTP request contextual information, tracing correlation that enables displaying grouped log entries in Logs Explorer and more. This release of the library is composed of the three packages:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;google-cloud-logging\u0026lt;/a\u0026gt; -- provides the hand-written layer above Cloud Logging API and the integration with legacy \u0026lt;a href=\u0026#34;https://docs.oracle.com/javase/10/core/java-logging-overview.htm\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Java Logging\u0026lt;/a\u0026gt; solution.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging-logback\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;google-cloud-logging-logback\u0026lt;/a\u0026gt; is the integration with the Logback framework and ingests logs using the google-cloud-logging package.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging-servlet-initializer\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;google-cloud-logging-servlet-initializer\u0026lt;/a\u0026gt; is a new addition to the library; it provides integration with servlet-based Web applications.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;The features are available in the versions \u0026amp;#8805;3.6.3 and \u0026amp;#8805;0.123.3-alpha of the google-cloud-logging and google-cloud-logging-logback packages respectively.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you are using \u0026lt;a href=\u0026#34;https://maven.apache.org/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Maven\u0026lt;/a\u0026gt;, update the packages\u0026#39; versions in the pom.xml:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eToday it is even easier to capture logs in your Java applications. Developers can get more data with their application logs using a new version of the Cloud Logging client library for Java. The library populates the current executing context implicitly with every ingested log entry. Read this if you want to learn how to get HTTP requests and tracing information and additional metadata in your logs without writing a single line of code.\u003c/p\u003e\u003cp\u003eThere are three ways to ingest log data into Google Cloud Logging:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDevelop a proprietary solution that directly calls the \u003ca href=\"https://cloud.google.com/logging/docs/reference/api-overview\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/api-overview\" track-metadata-module=\"post\"\u003eLogging API\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLeverage logging capabilities of the Google Cloud managed environments like \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#overview\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#overview\" track-metadata-module=\"post\"\u003eGKE\u003c/a\u003e or install Google Cloud \u003ca href=\"https://cloud.google.com/monitoring/agent\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/agent\" track-metadata-module=\"post\"\u003eOps agent\u003c/a\u003e and print your application logs to stdout and stderr.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse Google Cloud \u003ca href=\"https://cloud.google.com/logging/docs/reference/libraries\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/libraries\" track-metadata-module=\"post\"\u003eLogging client library\u003c/a\u003e in one of many supported programming languages.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe library provides you with ready to use boilerplate constructs built following the best practices of using Logging API. Java applications can use the Google Cloud Logging library to ingest logs using the integrations with \u003ca href=\"https://docs.oracle.com/javase/10/core/java-logging-overview.htm\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://docs.oracle.com\" track-metadata-module=\"post\"\u003eJava Logging\u003c/a\u003e and \u003ca href=\"https://logback.qos.ch/\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://logback.qos.ch\" track-metadata-module=\"post\"\u003eLogback framework\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIf you are new to using Google Logging client libraries for Java, follow the steps to \u003ca href=\"https://cloud.google.com/logging/docs/setup/java\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/setup/java\" track-metadata-module=\"post\"\u003eset up Cloud Logging for Java\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/reference/libraries\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/libraries\" track-metadata-module=\"post\"\u003eget started\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIn the version 3.6 release of the the Logging client library for Java you get many long demanding features including automatic population of the metadata about the environment\u0026#39;s resource supporting Cloud Run and Cloud Functions, HTTP request contextual information, tracing correlation that enables displaying grouped log entries in Logs Explorer and more. This release of the library is composed of the three packages:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://github.com/googleapis/java-logging\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003egoogle-cloud-logging\u003c/a\u003e -- provides the hand-written layer above Cloud Logging API and the integration with legacy \u003ca href=\"https://docs.oracle.com/javase/10/core/java-logging-overview.htm\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://docs.oracle.com\" track-metadata-module=\"post\"\u003eJava Logging\u003c/a\u003e solution.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://github.com/googleapis/java-logging-logback\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003egoogle-cloud-logging-logback\u003c/a\u003e is the integration with the Logback framework and ingests logs using the google-cloud-logging package.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://github.com/googleapis/java-logging-servlet-initializer\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003egoogle-cloud-logging-servlet-initializer\u003c/a\u003e is a new addition to the library; it provides integration with servlet-based Web applications.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe features are available in the versions ≥3.6.3 and ≥0.123.3-alpha of the google-cloud-logging and google-cloud-logging-logback packages respectively.\u003c/p\u003e\u003cp\u003eIf you are using \u003ca href=\"https://maven.apache.org/\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://maven.apache.org\" track-metadata-module=\"post\"\u003eMaven\u003c/a\u003e, update the packages\u0026#39; versions in the pom.xml:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003e\u0026lt;dependency\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;groupId\u0026gt;com.google.cloud\u0026lt;/groupId\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;artifactId\u0026gt;google-cloud-logging\u0026lt;/artifactId\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;version\u0026gt;3.6.3\u0026lt;/version\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e\u0026lt;/dependency\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e\u0026lt;dependency\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;groupId\u0026gt;com.google.cloud\u0026lt;/groupId\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;artifactId\u0026gt;google-cloud-logging-logback\u0026lt;/artifactId\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;version\u0026gt;0.123.3-alpha\u0026lt;/version\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e\u0026lt;/dependency\u0026gt;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cp\u003eIf you are using \u003ca href=\"https://gradle.org/\" target=\"_blank\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://gradle.org\" track-metadata-module=\"post\"\u003eGradle\u003c/a\u003e, , update your dependencies:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003eimplementation \u0026#39;com.google.cloud:google-cloud-logging:3.6.3\u0026#39;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eimplementation \u0026#39;com.google.cloud:google-cloud-logging-logback:0.123.3-alpha\u0026#39;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cdiv _ngcontent-c74=\"\" innerhtml=\"\u0026lt;p\u0026gt;You can use the official Google Cloud BOM version 0.167.0 that includes the new releases of the packages.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;What is new\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The Java library inserts structured information about the executing environment including resource types, HTTP request metadata, tracing and more. Using the library you can write your payloads in one of the three formats:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A text provided as a Java string\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A JSON object provided as an instance of \u0026lt;a href=\u0026#34;https://docs.oracle.com/javase/8/docs/api/java/util/Map.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Map\u0026amp;lt;String, ?\u0026amp;gt;\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Struct\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Struct\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A protobuf object provided as an instance of \u0026lt;a href=\u0026#34;https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Any\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Any\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;You can use the structured logs with enhanced filtering in Logs Explorer to observe and troubleshoot their applications. The Logs Explorer uses structured logs to establish \u0026lt;a href=\u0026#34;https://cloud.google.com/trace/docs/trace-log-integration\u0026#34;\u0026gt;correlations\u0026lt;/a\u0026gt; between traces and logs and to group together logs that belong to the same transaction. The correlated \u0026amp;#34;child\u0026amp;#34; logs are displayed \u0026amp;#34;under\u0026amp;#34; the entry of the \u0026amp;#34;parent\u0026amp;#34; log:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eYou can use the official Google Cloud BOM version 0.167.0 that includes the new releases of the packages.\u003c/p\u003e\u003ch3\u003eWhat is new\u003c/h3\u003e\u003cp\u003eThe Java library inserts structured information about the executing environment including resource types, HTTP request metadata, tracing and more. Using the library you can write your payloads in one of the three formats:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eA text provided as a Java string\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA JSON object provided as an instance of \u003ca href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Map.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://docs.oracle.com\" track-metadata-module=\"post\"\u003eMap\u0026lt;String, ?\u0026gt;\u003c/a\u003e or \u003ca href=\"https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Struct\" target=\"_blank\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://developers.google.com\" track-metadata-module=\"post\"\u003eStruct\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA protobuf object provided as an instance of \u003ca href=\"https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Any\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://developers.google.com\" track-metadata-module=\"post\"\u003eAny\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can use the structured logs with enhanced filtering in Logs Explorer to observe and troubleshoot their applications. The Logs Explorer uses structured logs to establish \u003ca href=\"https://cloud.google.com/trace/docs/trace-log-integration\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/trace/docs/trace-log-integration\" track-metadata-module=\"post\"\u003ecorrelations\u003c/a\u003e between traces and logs and to group together logs that belong to the same transaction. The correlated \u0026#34;child\u0026#34; logs are displayed \u0026#34;under\u0026#34; the entry of the \u0026#34;parent\u0026#34; log:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cp\u003eWith the previous versions of the Logging library you had to write code to explicitly populate these fields. For example, developers that use Logback framework had to write a code like below to populate the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace\" track-metadata-module=\"post\"\u003etrace\u003c/a\u003e field of the ingested logs:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003e// . . .\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eString traceInfo = request.getHeader(\u0026#34;x-cloud-trace-context\u0026#34;);\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eTraceLoggingEventEnhancer.setCurrentTraceId(traceInfo);\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e// . . .\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cdiv _ngcontent-c74=\"\" innerhtml=\"\u0026lt;p\u0026gt;And to invoke this code at the beginning of each transaction.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The new features of the Logging library makes implementing the population logic unnecessary. The new version of the library supports automatic population of following log entry fields:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource\u0026#34;\u0026gt;resource\u0026lt;/a\u0026gt; \u0026amp;#8210; describes the resource type and its attributes where the application is running. Along with GCE instances, it supports Google Cloud managed services such as GKE, AppEngine (both Standard and Flexible), Cloud Run and Cloud Functions.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.http_request\u0026#34;\u0026gt;httpRequest\u0026lt;/a\u0026gt; \u0026amp;#8210; captures info about HTTP requests from the current application\u0026#39;s context. The context is defined per-thread and can be populated both explicitly in the application code or implicitly from the \u0026lt;a href=\u0026#34;https://docs.oracle.com/cd/E26180_01/Platform.94/ATGProgGuide/html/s0801requesthandlingwithservletpipeli01.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Jakarta servlet requests pipeline\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace\u0026#34;\u0026gt;trace\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.span_id\u0026#34;\u0026gt;spanId\u0026lt;/a\u0026gt; \u0026amp;#8210; reads the tracing data from the HTTP request header. The tracing data assists in correlating multiple logs that belong to the same transaction.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.source_location\u0026#34;\u0026gt;sourceLocation\u0026lt;/a\u0026gt; \u0026amp;#8210; stores info about the class and method names as well as the line of code where the application called the log ingestion method. The library retrieves the data by traversing the trace stack up until the first entry that is not part of the Logging library code or the system package.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;What is left to you is to set the payload and relevant payload\u0026#39;s metadata labels. The only field in the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\u0026#34;\u0026gt;log entry\u0026lt;/a\u0026gt; that the library does not automatically populate now is the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.operation\u0026#34;\u0026gt;operation\u0026lt;/a\u0026gt; field.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Disable information auto-population in log entries\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;You have full control over the auto-population functionality. The auto-population is enabled by default for your convenience. But in certain scenarios it can be desirable to disable it. For example, if your application is log intensive and has a narrow bandwidth, you may want to disable the auto-population in order to save the connection\u0026#39;s bandwidth for the application communication.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you are ingesting logs using the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/libraries#write_standard_logs\u0026#34;\u0026gt;write()\u0026lt;/a\u0026gt; method of the Logging interface\u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Logging.java#L1209\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;,\u0026lt;/a\u0026gt; you can configure the LoggingOptions argument to disable the auto-population:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eAnd to invoke this code at the beginning of each transaction.\u003c/p\u003e\u003cp\u003eThe new features of the Logging library makes implementing the population logic unnecessary. The new version of the library supports automatic population of following log entry fields:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource\" track-metadata-module=\"post\"\u003eresource\u003c/a\u003e ‒ describes the resource type and its attributes where the application is running. Along with GCE instances, it supports Google Cloud managed services such as GKE, AppEngine (both Standard and Flexible), Cloud Run and Cloud Functions.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.http_request\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.http_request\" track-metadata-module=\"post\"\u003ehttpRequest\u003c/a\u003e ‒ captures info about HTTP requests from the current application\u0026#39;s context. The context is defined per-thread and can be populated both explicitly in the application code or implicitly from the \u003ca href=\"https://docs.oracle.com/cd/E26180_01/Platform.94/ATGProgGuide/html/s0801requesthandlingwithservletpipeli01.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://docs.oracle.com\" track-metadata-module=\"post\"\u003eJakarta servlet requests pipeline\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace\" track-metadata-module=\"post\"\u003etrace\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.span_id\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.span_id\" track-metadata-module=\"post\"\u003espanId\u003c/a\u003e ‒ reads the tracing data from the HTTP request header. The tracing data assists in correlating multiple logs that belong to the same transaction.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.source_location\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.source_location\" track-metadata-module=\"post\"\u003esourceLocation\u003c/a\u003e ‒ stores info about the class and method names as well as the line of code where the application called the log ingestion method. The library retrieves the data by traversing the trace stack up until the first entry that is not part of the Logging library code or the system package.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhat is left to you is to set the payload and relevant payload\u0026#39;s metadata labels. The only field in the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\" track-metadata-module=\"post\"\u003elog entry\u003c/a\u003e that the library does not automatically populate now is the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.operation\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.operation\" track-metadata-module=\"post\"\u003eoperation\u003c/a\u003e field.\u003c/p\u003e\u003ch3\u003eDisable information auto-population in log entries\u003c/h3\u003e\u003cp\u003eYou have full control over the auto-population functionality. The auto-population is enabled by default for your convenience. But in certain scenarios it can be desirable to disable it. For example, if your application is log intensive and has a narrow bandwidth, you may want to disable the auto-population in order to save the connection\u0026#39;s bandwidth for the application communication.\u003c/p\u003e\u003cp\u003eIf you are ingesting logs using the \u003ca href=\"https://cloud.google.com/logging/docs/reference/libraries#write_standard_logs\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/libraries#write_standard_logs\" track-metadata-module=\"post\"\u003ewrite()\u003c/a\u003e method of the Logging interface\u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Logging.java#L1209\" target=\"_blank\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e,\u003c/a\u003e you can configure the LoggingOptions argument to disable the auto-population:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003eLoggingOptions options = LoggingOptions.newBuilder()\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e    .setAutoPopulateMetadata(false).build();\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eLogging logging = options.getService();\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cp\u003eIf you are using Java Logging, you can disable auto population by adding the following to your logging.properties file:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003ecom.google.cloud.logging.LoggingHandler.autoPopulateMetadata=false\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cp\u003eIf you are using Logback framework, you can disable auto population by adding the following to your Logback configuration:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003e\u0026lt;autoPopulateMetadata\u0026gt;false\u0026lt;/autoPopulateMetadata\u0026gt;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cdiv _ngcontent-c74=\"\" innerhtml=\"\u0026lt;h3\u0026gt;How the current context is populated\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Rich query and display capabilities of Log Explorer such as displaying correlated logs use the log entries\u0026#39; fields such as httpRequest and trace. The new version of the library uses the \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Context\u0026lt;/a\u0026gt; class to store the information about the HTTP request and tracing data in the current application context. The context\u0026#39;s scope is per thread. Before the library ingests logs into Cloud Logging, it reads the HTTP request and tracing information from the current context and sets the respective fields in the log entries. The fields are populated only if the caller did not explicitly provide values in these fields. Using the \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/ContextHandler.java\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;ContextHandler\u0026lt;/a\u0026gt; class you can setup the HTTP request and tracing data of the current context:\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eHow the current context is populated\u003c/h3\u003e\u003cp\u003eRich query and display capabilities of Log Explorer such as displaying correlated logs use the log entries\u0026#39; fields such as httpRequest and trace. The new version of the library uses the \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java\" target=\"_blank\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eContext\u003c/a\u003e class to store the information about the HTTP request and tracing data in the current application context. The context\u0026#39;s scope is per thread. Before the library ingests logs into Cloud Logging, it reads the HTTP request and tracing information from the current context and sets the respective fields in the log entries. The fields are populated only if the caller did not explicitly provide values in these fields. Using the \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/ContextHandler.java\" target=\"_blank\" track-type=\"inline link\" track-name=\"31\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eContextHandler\u003c/a\u003e class you can setup the HTTP request and tracing data of the current context:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003eimport com.google.cloud.logging.HttpRequest;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e// . . .\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eHttpRequest request;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e// . . .\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eContextHandler ctxHandler = new ContextHandler();\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eContext ctx = Context.newBuilder()\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e        .setRequest(request)\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e        .setTraceId(traceId)\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e        .setSpanId(spanId)\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e        .build();\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003ectxHandler.setCurrentContext(ctx);\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cp\u003eAfter the context is set all logs that will be ingested in the same scope as the context will be populated with the HTTP request and tracing information that was set in the current context. The \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java\" target=\"_blank\" track-type=\"inline link\" track-name=\"32\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eContext\u003c/a\u003e class can setup the HTTP request using partial data such as URL or request method:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003eimport com.google.cloud.logging.HttpRequest.RequestMethod;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e// . . .\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eContextHandler ctxHandler = new ContextHandler();\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003eContext ctx = Context.newBuilder()\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e        .setRequestUrl(\u0026#34;https://example.com/info\u0026#34;)\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e        .setRequestMethod(RequestMethod.GET);\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e        .build();\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003ectxHandler.setCurrentContext(ctx);\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cdiv _ngcontent-c74=\"\" innerhtml=\"\u0026lt;p\u0026gt;The builder of the Context class also supports setting the tracing information from the parsed values of the \u0026lt;a href=\u0026#34;https://cloud.google.com/trace/docs/setup#force-trace\u0026#34;\u0026gt;Google tracing context\u0026lt;/a\u0026gt; and\u0026amp;#160; \u0026lt;a href=\u0026#34;https://www.w3.org/TR/trace-context/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;W3C tracing context\u0026lt;/a\u0026gt; strings using the methods \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L116\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;loadCloudTraceContext()\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L149\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;loadW3CTraceParentContext()\u0026lt;/a\u0026gt; respectively.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Implementation of the context population can be a complex task. Java Web servers support asynchronous execution of the request handlers. To manage the context in the right scope may require in-depth knowledge of specific implementation details about each Web server. The new version of the Logging library provides a simple way to automate the process of the current context management, saving you the effort of implementing the code by themselves. The automation supports all Web servers that are based on the Jakarta servlets such as \u0026lt;a href=\u0026#34;https://tomcat.apache.org/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Tomcat\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://www.eclipse.org/jetty/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Jetty\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://undertow.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Undertow\u0026lt;/a\u0026gt;. The current implementation supports Jakarta servlets version \u0026amp;#8805; 4.0.4. The implementation is added to the new \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging-servlet-initializer\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;google-cloud-logging-servlet-initializer\u0026lt;/a\u0026gt; package. All that you have to do to enable automatic capturing of the current context is to add the package to your application.\u0026lt;/p\u0026gt;If you are using \u0026lt;a href=\u0026#34;https://maven.apache.org/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Maven\u0026lt;/a\u0026gt; add the following to your pom.xml:\"\u003e\u003cp\u003eThe builder of the Context class also supports setting the tracing information from the parsed values of the \u003ca href=\"https://cloud.google.com/trace/docs/setup#force-trace\" track-type=\"inline link\" track-name=\"33\" track-metadata-eventdetail=\"https://cloud.google.com/trace/docs/setup#force-trace\" track-metadata-module=\"post\"\u003eGoogle tracing context\u003c/a\u003e and  \u003ca href=\"https://www.w3.org/TR/trace-context/\" target=\"_blank\" track-type=\"inline link\" track-name=\"34\" track-metadata-eventdetail=\"https://www.w3.org\" track-metadata-module=\"post\"\u003eW3C tracing context\u003c/a\u003e strings using the methods \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L116\" target=\"_blank\" track-type=\"inline link\" track-name=\"35\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eloadCloudTraceContext()\u003c/a\u003e and \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L149\" target=\"_blank\" track-type=\"inline link\" track-name=\"36\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eloadW3CTraceParentContext()\u003c/a\u003e respectively.\u003c/p\u003e\u003cp\u003eImplementation of the context population can be a complex task. Java Web servers support asynchronous execution of the request handlers. To manage the context in the right scope may require in-depth knowledge of specific implementation details about each Web server. The new version of the Logging library provides a simple way to automate the process of the current context management, saving you the effort of implementing the code by themselves. The automation supports all Web servers that are based on the Jakarta servlets such as \u003ca href=\"https://tomcat.apache.org/\" target=\"_blank\" track-type=\"inline link\" track-name=\"37\" track-metadata-eventdetail=\"https://tomcat.apache.org\" track-metadata-module=\"post\"\u003eTomcat\u003c/a\u003e, \u003ca href=\"https://www.eclipse.org/jetty/\" target=\"_blank\" track-type=\"inline link\" track-name=\"38\" track-metadata-eventdetail=\"https://www.eclipse.org\" track-metadata-module=\"post\"\u003eJetty\u003c/a\u003e or \u003ca href=\"https://undertow.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"39\" track-metadata-eventdetail=\"https://undertow.io\" track-metadata-module=\"post\"\u003eUndertow\u003c/a\u003e. The current implementation supports Jakarta servlets version ≥ 4.0.4. The implementation is added to the new \u003ca href=\"https://github.com/googleapis/java-logging-servlet-initializer\" target=\"_blank\" track-type=\"inline link\" track-name=\"40\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003egoogle-cloud-logging-servlet-initializer\u003c/a\u003e package. All that you have to do to enable automatic capturing of the current context is to add the package to your application.\u003c/p\u003e\u003cp\u003eIf you are using \u003ca href=\"https://maven.apache.org/\" target=\"_blank\" track-type=\"inline link\" track-name=\"41\" track-metadata-eventdetail=\"https://maven.apache.org\" track-metadata-module=\"post\"\u003eMaven\u003c/a\u003e add the following to your pom.xml:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003e\u0026lt;dependency\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;groupId\u0026gt;com.google.cloud\u0026lt;/groupId\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;artifactId\u0026gt;google-cloud-logging-servlet-initializer\u0026lt;/artifactId\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;version\u0026gt;0.1.7-alpha\u0026lt;/version\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e  \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;\n\u003c/code\u003e\u003ccode _ngcontent-c75=\"\"\u003e\u0026lt;/dependency\u0026gt;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cp\u003eIf you are using \u003ca href=\"https://gradle.org/\" target=\"_blank\" track-type=\"inline link\" track-name=\"42\" track-metadata-eventdetail=\"https://gradle.org\" track-metadata-module=\"post\"\u003eGradle\u003c/a\u003e, add the following to your dependencies:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003eimplementation \u0026#39;com.google.cloud:google-cloud-logging-servlet-initializer:0.1.7-alpha\u0026#39;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cdiv _ngcontent-c74=\"\" innerhtml=\"\u0026lt;p\u0026gt;The added package uses the Java\u0026#39;s \u0026lt;a href=\u0026#34;https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Service Provider Interface\u0026lt;/a\u0026gt; to register the \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging-servlet-initializer/blob/5076b0cc81fd1c0c3b39c6add17a0c25c38c7ece/src/main/java/com/google/cloud/logging/servlet/ContextCaptureInitializer.java\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;ContextCaptureInitializer\u0026lt;/a\u0026gt; class which integrates into the servlet pipeline to capture information about current HTTP requests. The information is parsed to populate the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#HttpRequest\u0026#34;\u0026gt;HttpRequest\u0026lt;/a\u0026gt; structure. It also parses the request\u0026#39;s headers to retrieve tracing information. It supports \u0026amp;#34;\u0026lt;code\u0026gt;x-cloud-trace-context\u0026lt;/code\u0026gt;\u0026amp;#34; (\u0026lt;a href=\u0026#34;https://cloud.google.com/trace/docs/setup#force-trace\u0026#34;\u0026gt;Google tracing context\u0026lt;/a\u0026gt;) and \u0026amp;#34;traceparent\u0026amp;#34; (\u0026lt;a href=\u0026#34;https://www.w3.org/TR/trace-context/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;W3C tracing context\u0026lt;/a\u0026gt;) headers.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Use Logging library with logging agents\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Many applications utilize logging capabilities of the Google Cloud managed services. The applications output their logs to \u0026lt;code\u0026gt;stdout\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;stderr\u0026lt;/code\u0026gt;, and the logs are ingested into Cloud Logging by \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/agent\u0026#34;\u0026gt;Logging agents\u0026lt;/a\u0026gt; or the Cloud managed services with the logging agent capabilities. This approach benefits from asynchronous log processing that does not consume application resources. The drawback of the approach is that if you want to populate fields in the structured logs or provide the structured payload, they have to format their output following the special \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/structured-logging#special-payload-fields\u0026#34;\u0026gt;Json format\u0026lt;/a\u0026gt; that the logging agents can parse. Also, while the logging agents can detect and populate the resource information about the managed environment, they cannot help with auto population of other fields of the log entry such as traceId or sourceLocation.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The new release of the Logging library for Java introduces the support for logging agents in both of its Java Logging and Logback integrations. Now the library\u0026#39;s users can instruct the appropriate handler to redirect the log writing to stdout instead of Logging API.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you are using Java Logging, add the following to your logging.properties file:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThe added package uses the Java\u0026#39;s \u003ca href=\"https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"43\" track-metadata-eventdetail=\"https://docs.oracle.com\" track-metadata-module=\"post\"\u003eService Provider Interface\u003c/a\u003e to register the \u003ca href=\"https://github.com/googleapis/java-logging-servlet-initializer/blob/5076b0cc81fd1c0c3b39c6add17a0c25c38c7ece/src/main/java/com/google/cloud/logging/servlet/ContextCaptureInitializer.java\" target=\"_blank\" track-type=\"inline link\" track-name=\"44\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eContextCaptureInitializer\u003c/a\u003e class which integrates into the servlet pipeline to capture information about current HTTP requests. The information is parsed to populate the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#HttpRequest\" track-type=\"inline link\" track-name=\"45\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#HttpRequest\" track-metadata-module=\"post\"\u003eHttpRequest\u003c/a\u003e structure. It also parses the request\u0026#39;s headers to retrieve tracing information. It supports \u0026#34;\u003ccode\u003ex-cloud-trace-context\u003c/code\u003e\u0026#34; (\u003ca href=\"https://cloud.google.com/trace/docs/setup#force-trace\" track-type=\"inline link\" track-name=\"46\" track-metadata-eventdetail=\"https://cloud.google.com/trace/docs/setup#force-trace\" track-metadata-module=\"post\"\u003eGoogle tracing context\u003c/a\u003e) and \u0026#34;traceparent\u0026#34; (\u003ca href=\"https://www.w3.org/TR/trace-context/\" target=\"_blank\" track-type=\"inline link\" track-name=\"47\" track-metadata-eventdetail=\"https://www.w3.org\" track-metadata-module=\"post\"\u003eW3C tracing context\u003c/a\u003e) headers.\u003c/p\u003e\u003ch3\u003eUse Logging library with logging agents\u003c/h3\u003e\u003cp\u003eMany applications utilize logging capabilities of the Google Cloud managed services. The applications output their logs to \u003ccode\u003estdout\u003c/code\u003e and \u003ccode\u003estderr\u003c/code\u003e, and the logs are ingested into Cloud Logging by \u003ca href=\"https://cloud.google.com/logging/docs/agent\" track-type=\"inline link\" track-name=\"48\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/agent\" track-metadata-module=\"post\"\u003eLogging agents\u003c/a\u003e or the Cloud managed services with the logging agent capabilities. This approach benefits from asynchronous log processing that does not consume application resources. The drawback of the approach is that if you want to populate fields in the structured logs or provide the structured payload, they have to format their output following the special \u003ca href=\"https://cloud.google.com/logging/docs/structured-logging#special-payload-fields\" track-type=\"inline link\" track-name=\"49\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/structured-logging#special-payload-fields\" track-metadata-module=\"post\"\u003eJson format\u003c/a\u003e that the logging agents can parse. Also, while the logging agents can detect and populate the resource information about the managed environment, they cannot help with auto population of other fields of the log entry such as traceId or sourceLocation.\u003c/p\u003e\u003cp\u003eThe new release of the Logging library for Java introduces the support for logging agents in both of its Java Logging and Logback integrations. Now the library\u0026#39;s users can instruct the appropriate handler to redirect the log writing to stdout instead of Logging API.\u003c/p\u003e\u003cp\u003eIf you are using Java Logging, add the following to your logging.properties file:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003ecom.google.cloud.logging.LoggingHandler.redirectToStdout=true\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cp\u003eIf you are using Logback, add the following to the Logback configuration:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c75=\"\"\u003e\u003cpre _ngcontent-c75=\"\"\u003e  \u003ccode _ngcontent-c75=\"\"\u003e\u0026lt;redirectToStdout\u0026gt;true\u0026lt;/redirectToStdout\u0026gt;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c74=\"\"\u003e\u003cdiv _ngcontent-c74=\"\" innerhtml=\"\u0026lt;p\u0026gt;By default, both \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LoggingHandler.java\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;LoggingHandler\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging-logback/blob/6ce7950d4fe380c6f4f785660af80373c136352a/src/main/java/com/google/cloud/logging/logback/LoggingAppender.java\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;LoggingAppender\u0026lt;/a\u0026gt; write logs by calling the Logging API. You have to add the above configurations to make them utilize the logging agents for the log ingestion.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Some limitations of using Logging Agents\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;When configuring the library\u0026#39;s Java Logging handler or Logback adapter to redirect log writing to stdout, you should be aware of the constraints that the use of logging agents implies.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud managed services (e.g. GKE) automatically install logging agents in the resources that they provision. For example, a GKE cluster has a logging agent installed in each worker node (GCE instance) of the cluster. As a result, logging agents are constrained with the resource they run and do not support customization of the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource\u0026#34;\u0026gt;resource\u0026lt;/a\u0026gt; field of the ingested log entries.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Additionally, the \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name\u0026#34;\u0026gt;logName\u0026lt;/a\u0026gt; of all ingested logs is defined by the agent and cannot be changed*. It means that the application cannot define the log name or where the log entry will be stored (a.k.a. \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LogDestinationName.java\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;log\u0026#39;s destination name\u0026lt;/a\u0026gt;).\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If it is essential for you to define a custom resource type or to control to which project the logs will be routed and/or the log name, you should not redirect the log writing to standard output.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;* It is possible to customize the log name (but not the destination) by \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/agent/logging/configuration#configure\u0026#34;\u0026gt;customizing the Logging agent\u0026#39;s configuration\u0026lt;/a\u0026gt; in GCE instances by defining the name as the \u0026amp;#34;tag\u0026amp;#34;.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;What is next\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Let\u0026#39;s recap the benefits of upgrading your logging client to the latest version.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Use the new Logging library if you need log correlation capabilities of Log Explorer or forward Cloud Logging structured logs to external solutions and use the data in the auto-populated fields.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Use the \u0026lt;a href=\u0026#34;https://github.com/googleapis/java-logging-servlet-initializer\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;google-cloud-logging-servlet-initializer\u0026lt;/a\u0026gt; package to automate the context management if you run a request based application that uses Jakarta servlets. Note that it will not work with legacy Java EE servlets or Web servers that are not based on Java servlets such as \u0026lt;a href=\u0026#34;https://netty.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Netty\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you run your application in the Google Cloud serverless environments like Cloud Run or Cloud Functions, consider using Java Logging or Logback with the configuration that redirects formatted logs to standard output like it is described in the previous section. Leveraging logging agents for ingesting logs resolves some reliability problems about asynchronous log ingestion such as \u0026lt;a href=\u0026#34;https://cloud.google.com/run/docs/configuring/cpu-allocation\u0026#34;\u0026gt;CPU throttling\u0026lt;/a\u0026gt; on Cloud Run or no grace period in Cloud Functions.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eBy default, both \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LoggingHandler.java\" target=\"_blank\" track-type=\"inline link\" track-name=\"50\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eLoggingHandler\u003c/a\u003e and \u003ca href=\"https://github.com/googleapis/java-logging-logback/blob/6ce7950d4fe380c6f4f785660af80373c136352a/src/main/java/com/google/cloud/logging/logback/LoggingAppender.java\" target=\"_blank\" track-type=\"inline link\" track-name=\"51\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eLoggingAppender\u003c/a\u003e write logs by calling the Logging API. You have to add the above configurations to make them utilize the logging agents for the log ingestion.\u003c/p\u003e\u003ch3\u003eSome limitations of using Logging Agents\u003c/h3\u003e\u003cp\u003eWhen configuring the library\u0026#39;s Java Logging handler or Logback adapter to redirect log writing to stdout, you should be aware of the constraints that the use of logging agents implies.\u003c/p\u003e\u003cp\u003eGoogle Cloud managed services (e.g. GKE) automatically install logging agents in the resources that they provision. For example, a GKE cluster has a logging agent installed in each worker node (GCE instance) of the cluster. As a result, logging agents are constrained with the resource they run and do not support customization of the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource\" track-type=\"inline link\" track-name=\"52\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource\" track-metadata-module=\"post\"\u003eresource\u003c/a\u003e field of the ingested log entries.\u003c/p\u003e\u003cp\u003eAdditionally, the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name\" track-type=\"inline link\" track-name=\"53\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name\" track-metadata-module=\"post\"\u003elogName\u003c/a\u003e of all ingested logs is defined by the agent and cannot be changed*. It means that the application cannot define the log name or where the log entry will be stored (a.k.a. \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LogDestinationName.java\" target=\"_blank\" track-type=\"inline link\" track-name=\"54\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003elog\u0026#39;s destination name\u003c/a\u003e).\u003c/p\u003e\u003cp\u003eIf it is essential for you to define a custom resource type or to control to which project the logs will be routed and/or the log name, you should not redirect the log writing to standard output.\u003c/p\u003e\u003cp\u003e* It is possible to customize the log name (but not the destination) by \u003ca href=\"https://cloud.google.com/logging/docs/agent/logging/configuration#configure\" track-type=\"inline link\" track-name=\"55\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/agent/logging/configuration#configure\" track-metadata-module=\"post\"\u003ecustomizing the Logging agent\u0026#39;s configuration\u003c/a\u003e in GCE instances by defining the name as the \u0026#34;tag\u0026#34;.\u003c/p\u003e\u003ch3\u003eWhat is next\u003c/h3\u003e\u003cp\u003eLet\u0026#39;s recap the benefits of upgrading your logging client to the latest version.\u003c/p\u003e\u003cp\u003eUse the new Logging library if you need log correlation capabilities of Log Explorer or forward Cloud Logging structured logs to external solutions and use the data in the auto-populated fields.\u003c/p\u003e\u003cp\u003eUse the \u003ca href=\"https://github.com/googleapis/java-logging-servlet-initializer\" target=\"_blank\" track-type=\"inline link\" track-name=\"56\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003egoogle-cloud-logging-servlet-initializer\u003c/a\u003e package to automate the context management if you run a request based application that uses Jakarta servlets. Note that it will not work with legacy Java EE servlets or Web servers that are not based on Java servlets such as \u003ca href=\"https://netty.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"57\" track-metadata-eventdetail=\"https://netty.io\" track-metadata-module=\"post\"\u003eNetty\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIf you run your application in the Google Cloud serverless environments like Cloud Run or Cloud Functions, consider using Java Logging or Logback with the configuration that redirects formatted logs to standard output like it is described in the previous section. Leveraging logging agents for ingesting logs resolves some reliability problems about asynchronous log ingestion such as \u003ca href=\"https://cloud.google.com/run/docs/configuring/cpu-allocation\" track-type=\"inline link\" track-name=\"58\" track-metadata-eventdetail=\"https://cloud.google.com/run/docs/configuring/cpu-allocation\" track-metadata-module=\"post\"\u003eCPU throttling\u003c/a\u003e on Cloud Run or no grace period in Cloud Functions.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eToday it is even easier to capture logs in your Java applications. Developers can get more data with their application logs using a new version of the Cloud Logging client library for Java. The library populates the current executing context implicitly with every ingested log entry. Read this if you want to learn how to get HTTP requests and tracing information and additional metadata in your logs without writing a single line of code.\u003c/p\u003e\u003cp\u003eThere are three ways to ingest log data into Google Cloud Logging:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDevelop a proprietary solution that directly calls the \u003ca href=\"https://cloud.google.com/logging/docs/reference/api-overview\"\u003eLogging API\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLeverage logging capabilities of the Google Cloud managed environments like \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-logs#overview\"\u003eGKE\u003c/a\u003e or install Google Cloud \u003ca href=\"https://cloud.google.com/monitoring/agent\"\u003eOps agent\u003c/a\u003e and print your application logs to stdout and stderr.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse Google Cloud \u003ca href=\"https://cloud.google.com/logging/docs/reference/libraries\"\u003eLogging client library\u003c/a\u003e in one of many supported programming languages.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe library provides you with ready to use boilerplate constructs built following the best practices of using Logging API. Java applications can use the Google Cloud Logging library to ingest logs using the integrations with \u003ca href=\"https://docs.oracle.com/javase/10/core/java-logging-overview.htm\" target=\"_blank\"\u003eJava Logging\u003c/a\u003e and \u003ca href=\"https://logback.qos.ch/\" target=\"_blank\"\u003eLogback framework\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIf you are new to using Google Logging client libraries for Java, follow the steps to \u003ca href=\"https://cloud.google.com/logging/docs/setup/java\"\u003eset up Cloud Logging for Java\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/reference/libraries\"\u003eget started\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIn the version 3.6 release of the the Logging client library for Java you get many long demanding features including automatic population of the metadata about the environment's resource supporting Cloud Run and Cloud Functions, HTTP request contextual information, tracing correlation that enables displaying grouped log entries in Logs Explorer and more. This release of the library is composed of the three packages:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://github.com/googleapis/java-logging\" target=\"_blank\"\u003egoogle-cloud-logging\u003c/a\u003e -- provides the hand-written layer above Cloud Logging API and the integration with legacy \u003ca href=\"https://docs.oracle.com/javase/10/core/java-logging-overview.htm\" target=\"_blank\"\u003eJava Logging\u003c/a\u003e solution.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://github.com/googleapis/java-logging-logback\" target=\"_blank\"\u003egoogle-cloud-logging-logback\u003c/a\u003e is the integration with the Logback framework and ingests logs using the google-cloud-logging package.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://github.com/googleapis/java-logging-servlet-initializer\" target=\"_blank\"\u003egoogle-cloud-logging-servlet-initializer\u003c/a\u003e is a new addition to the library; it provides integration with servlet-based Web applications.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe features are available in the versions ≥3.6.3 and ≥0.123.3-alpha of the google-cloud-logging and google-cloud-logging-logback packages respectively.\u003c/p\u003e\u003cp\u003eIf you are using \u003ca href=\"https://maven.apache.org/\" target=\"_blank\"\u003eMaven\u003c/a\u003e, update the packages' versions in the pom.xml:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'\u0026lt;dependency\u0026gt;\\r\\n \u0026lt;groupId\u0026gt;com.google.cloud\u0026lt;/groupId\u0026gt;\\r\\n \u0026lt;artifactId\u0026gt;google-cloud-logging\u0026lt;/artifactId\u0026gt;\\r\\n \u0026lt;version\u0026gt;3.6.3\u0026lt;/version\u0026gt;\\r\\n\u0026lt;/dependency\u0026gt;\\r\\n\u0026lt;dependency\u0026gt;\\r\\n \u0026lt;groupId\u0026gt;com.google.cloud\u0026lt;/groupId\u0026gt;\\r\\n \u0026lt;artifactId\u0026gt;google-cloud-logging-logback\u0026lt;/artifactId\u0026gt;\\r\\n \u0026lt;version\u0026gt;0.123.3-alpha\u0026lt;/version\u0026gt;\\r\\n\u0026lt;/dependency\u0026gt;'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you are using \u003ca href=\"https://gradle.org/\" target=\"_blank\"\u003eGradle\u003c/a\u003e, , update your dependencies:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"implementation 'com.google.cloud:google-cloud-logging:3.6.3'\\r\\nimplementation 'com.google.cloud:google-cloud-logging-logback:0.123.3-alpha'\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou can use the official Google Cloud BOM version 0.167.0 that includes the new releases of the packages.\u003c/p\u003e\u003ch3\u003eWhat is new\u003c/h3\u003e\u003cp\u003eThe Java library inserts structured information about the executing environment including resource types, HTTP request metadata, tracing and more. Using the library you can write your payloads in one of the three formats:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eA text provided as a Java string\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA JSON object provided as an instance of \u003ca href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Map.html\" target=\"_blank\"\u003eMap\u0026lt;String, ?\u0026gt;\u003c/a\u003e or \u003ca href=\"https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Struct\" target=\"_blank\"\u003eStruct\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA protobuf object provided as an instance of \u003ca href=\"https://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/Any\" target=\"_blank\"\u003eAny\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can use the structured logs with enhanced filtering in Logs Explorer to observe and troubleshoot their applications. The Logs Explorer uses structured logs to establish \u003ca href=\"https://cloud.google.com/trace/docs/trace-log-integration\"\u003ecorrelations\u003c/a\u003e between traces and logs and to group together logs that belong to the same transaction. The correlated \"child\" logs are displayed \"under\" the entry of the \"parent\" log:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"Grouped logs display in Logs Explorer.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Grouped_logs_display_in_Logs_Explorer.max-1000x1000.jpg\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eGrouped logs display in Logs Explorer\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWith the previous versions of the Logging library you had to write code to explicitly populate these fields. For example, developers that use Logback framework had to write a code like below to populate the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace\"\u003etrace\u003c/a\u003e field of the ingested logs:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'// . . .\\r\\nString traceInfo = request.getHeader(\"x-cloud-trace-context\");\\r\\nTraceLoggingEventEnhancer.setCurrentTraceId(traceInfo);\\r\\n// . . .'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAnd to invoke this code at the beginning of each transaction.\u003c/p\u003e\u003cp\u003eThe new features of the Logging library makes implementing the population logic unnecessary. The new version of the library supports automatic population of following log entry fields:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource\"\u003eresource\u003c/a\u003e ‒ describes the resource type and its attributes where the application is running. Along with GCE instances, it supports Google Cloud managed services such as GKE, AppEngine (both Standard and Flexible), Cloud Run and Cloud Functions.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.http_request\"\u003ehttpRequest\u003c/a\u003e ‒ captures info about HTTP requests from the current application's context. The context is defined per-thread and can be populated both explicitly in the application code or implicitly from the \u003ca href=\"https://docs.oracle.com/cd/E26180_01/Platform.94/ATGProgGuide/html/s0801requesthandlingwithservletpipeli01.html\" target=\"_blank\"\u003eJakarta servlet requests pipeline\u003c/a\u003e.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.trace\"\u003etrace\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.span_id\"\u003espanId\u003c/a\u003e ‒ reads the tracing data from the HTTP request header. The tracing data assists in correlating multiple logs that belong to the same transaction.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.source_location\"\u003esourceLocation\u003c/a\u003e ‒ stores info about the class and method names as well as the line of code where the application called the log ingestion method. The library retrieves the data by traversing the trace stack up until the first entry that is not part of the Logging library code or the system package.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhat is left to you is to set the payload and relevant payload's metadata labels. The only field in the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\"\u003elog entry\u003c/a\u003e that the library does not automatically populate now is the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.operation\"\u003eoperation\u003c/a\u003e field.\u003c/p\u003e\u003ch3\u003eDisable information auto-population in log entries\u003c/h3\u003e\u003cp\u003eYou have full control over the auto-population functionality. The auto-population is enabled by default for your convenience. But in certain scenarios it can be desirable to disable it. For example, if your application is log intensive and has a narrow bandwidth, you may want to disable the auto-population in order to save the connection's bandwidth for the application communication.\u003c/p\u003e\u003cp\u003eIf you are ingesting logs using the \u003ca href=\"https://cloud.google.com/logging/docs/reference/libraries#write_standard_logs\"\u003ewrite()\u003c/a\u003e method of the Logging interface\u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Logging.java#L1209\" target=\"_blank\"\u003e,\u003c/a\u003e you can configure the LoggingOptions argument to disable the auto-population:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'LoggingOptions options = LoggingOptions.newBuilder()\\r\\n .setAutoPopulateMetadata(false).build();\\r\\nLogging logging = options.getService();'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you are using Java Logging, you can disable auto population by adding the following to your logging.properties file:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'com.google.cloud.logging.LoggingHandler.autoPopulateMetadata=false'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you are using Logback framework, you can disable auto population by adding the following to your Logback configuration:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'\u0026lt;autoPopulateMetadata\u0026gt;false\u0026lt;/autoPopulateMetadata\u0026gt;'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eHow the current context is populated\u003c/h3\u003e\u003cp\u003eRich query and display capabilities of Log Explorer such as displaying correlated logs use the log entries' fields such as httpRequest and trace. The new version of the library uses the \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java\" target=\"_blank\"\u003eContext\u003c/a\u003e class to store the information about the HTTP request and tracing data in the current application context. The context's scope is per thread. Before the library ingests logs into Cloud Logging, it reads the HTTP request and tracing information from the current context and sets the respective fields in the log entries. The fields are populated only if the caller did not explicitly provide values in these fields. Using the \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/ContextHandler.java\" target=\"_blank\"\u003eContextHandler\u003c/a\u003e class you can setup the HTTP request and tracing data of the current context:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'import com.google.cloud.logging.HttpRequest;\\r\\n// . . .\\r\\nHttpRequest request;\\r\\n// . . .\\r\\nContextHandler ctxHandler = new ContextHandler();\\r\\nContext ctx = Context.newBuilder()\\r\\n .setRequest(request)\\r\\n .setTraceId(traceId)\\r\\n .setSpanId(spanId)\\r\\n .build();\\r\\nctxHandler.setCurrentContext(ctx);'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAfter the context is set all logs that will be ingested in the same scope as the context will be populated with the HTTP request and tracing information that was set in the current context. The \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java\" target=\"_blank\"\u003eContext\u003c/a\u003e class can setup the HTTP request using partial data such as URL or request method:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'import com.google.cloud.logging.HttpRequest.RequestMethod;\\r\\n// . . .\\r\\nContextHandler ctxHandler = new ContextHandler();\\r\\nContext ctx = Context.newBuilder()\\r\\n .setRequestUrl(\"https://example.com/info\")\\r\\n .setRequestMethod(RequestMethod.GET);\\r\\n .build();\\r\\nctxHandler.setCurrentContext(ctx);'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe builder of the Context class also supports setting the tracing information from the parsed values of the \u003ca href=\"https://cloud.google.com/trace/docs/setup#force-trace\"\u003eGoogle tracing context\u003c/a\u003e and  \u003ca href=\"https://www.w3.org/TR/trace-context/\" target=\"_blank\"\u003eW3C tracing context\u003c/a\u003e strings using the methods \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L116\" target=\"_blank\"\u003eloadCloudTraceContext()\u003c/a\u003e and \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/Context.java#L149\" target=\"_blank\"\u003eloadW3CTraceParentContext()\u003c/a\u003e respectively.\u003c/p\u003e\u003cp\u003eImplementation of the context population can be a complex task. Java Web servers support asynchronous execution of the request handlers. To manage the context in the right scope may require in-depth knowledge of specific implementation details about each Web server. The new version of the Logging library provides a simple way to automate the process of the current context management, saving you the effort of implementing the code by themselves. The automation supports all Web servers that are based on the Jakarta servlets such as \u003ca href=\"https://tomcat.apache.org/\" target=\"_blank\"\u003eTomcat\u003c/a\u003e, \u003ca href=\"https://www.eclipse.org/jetty/\" target=\"_blank\"\u003eJetty\u003c/a\u003e or \u003ca href=\"https://undertow.io/\" target=\"_blank\"\u003eUndertow\u003c/a\u003e. The current implementation supports Jakarta servlets version ≥ 4.0.4. The implementation is added to the new \u003ca href=\"https://github.com/googleapis/java-logging-servlet-initializer\" target=\"_blank\"\u003egoogle-cloud-logging-servlet-initializer\u003c/a\u003e package. All that you have to do to enable automatic capturing of the current context is to add the package to your application.\u003c/p\u003eIf you are using \u003ca href=\"https://maven.apache.org/\" target=\"_blank\"\u003eMaven\u003c/a\u003e add the following to your pom.xml:\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'\u0026lt;dependency\u0026gt;\\r\\n \u0026lt;groupId\u0026gt;com.google.cloud\u0026lt;/groupId\u0026gt;\\r\\n \u0026lt;artifactId\u0026gt;google-cloud-logging-servlet-initializer\u0026lt;/artifactId\u0026gt;\\r\\n \u0026lt;version\u0026gt;0.1.7-alpha\u0026lt;/version\u0026gt;\\r\\n \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;\\r\\n\u0026lt;/dependency\u0026gt;'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you are using \u003ca href=\"https://gradle.org/\" target=\"_blank\"\u003eGradle\u003c/a\u003e, add the following to your dependencies:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u\"implementation 'com.google.cloud:google-cloud-logging-servlet-initializer:0.1.7-alpha'\"), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe added package uses the Java's \u003ca href=\"https://docs.oracle.com/javase/tutorial/sound/SPI-intro.html\" target=\"_blank\"\u003eService Provider Interface\u003c/a\u003e to register the \u003ca href=\"https://github.com/googleapis/java-logging-servlet-initializer/blob/5076b0cc81fd1c0c3b39c6add17a0c25c38c7ece/src/main/java/com/google/cloud/logging/servlet/ContextCaptureInitializer.java\" target=\"_blank\"\u003eContextCaptureInitializer\u003c/a\u003e class which integrates into the servlet pipeline to capture information about current HTTP requests. The information is parsed to populate the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#HttpRequest\"\u003eHttpRequest\u003c/a\u003e structure. It also parses the request's headers to retrieve tracing information. It supports \"\u003ccode\u003ex-cloud-trace-context\u003c/code\u003e\" (\u003ca href=\"https://cloud.google.com/trace/docs/setup#force-trace\"\u003eGoogle tracing context\u003c/a\u003e) and \"traceparent\" (\u003ca href=\"https://www.w3.org/TR/trace-context/\" target=\"_blank\"\u003eW3C tracing context\u003c/a\u003e) headers.\u003c/p\u003e\u003ch3\u003eUse Logging library with logging agents\u003c/h3\u003e\u003cp\u003eMany applications utilize logging capabilities of the Google Cloud managed services. The applications output their logs to \u003ccode\u003estdout\u003c/code\u003e and \u003ccode\u003estderr\u003c/code\u003e, and the logs are ingested into Cloud Logging by \u003ca href=\"https://cloud.google.com/logging/docs/agent\"\u003eLogging agents\u003c/a\u003e or the Cloud managed services with the logging agent capabilities. This approach benefits from asynchronous log processing that does not consume application resources. The drawback of the approach is that if you want to populate fields in the structured logs or provide the structured payload, they have to format their output following the special \u003ca href=\"https://cloud.google.com/logging/docs/structured-logging#special-payload-fields\"\u003eJson format\u003c/a\u003e that the logging agents can parse. Also, while the logging agents can detect and populate the resource information about the managed environment, they cannot help with auto population of other fields of the log entry such as traceId or sourceLocation.\u003c/p\u003e\u003cp\u003eThe new release of the Logging library for Java introduces the support for logging agents in both of its Java Logging and Logback integrations. Now the library's users can instruct the appropriate handler to redirect the log writing to stdout instead of Logging API.\u003c/p\u003e\u003cp\u003eIf you are using Java Logging, add the following to your logging.properties file:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'com.google.cloud.logging.LoggingHandler.redirectToStdout=true'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIf you are using Logback, add the following to the Logback configuration:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdl\u003e\u003cdt\u003ecode_block\u003c/dt\u003e\u003cdd\u003e[StructValue([(u'code', u'\u0026lt;redirectToStdout\u0026gt;true\u0026lt;/redirectToStdout\u0026gt;'), (u'language', u'')])]\u003c/dd\u003e\u003c/dl\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eBy default, both \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LoggingHandler.java\" target=\"_blank\"\u003eLoggingHandler\u003c/a\u003e and \u003ca href=\"https://github.com/googleapis/java-logging-logback/blob/6ce7950d4fe380c6f4f785660af80373c136352a/src/main/java/com/google/cloud/logging/logback/LoggingAppender.java\" target=\"_blank\"\u003eLoggingAppender\u003c/a\u003e write logs by calling the Logging API. You have to add the above configurations to make them utilize the logging agents for the log ingestion.\u003c/p\u003e\u003ch3\u003eSome limitations of using Logging Agents\u003c/h3\u003e\u003cp\u003eWhen configuring the library's Java Logging handler or Logback adapter to redirect log writing to stdout, you should be aware of the constraints that the use of logging agents implies.\u003c/p\u003e\u003cp\u003eGoogle Cloud managed services (e.g. GKE) automatically install logging agents in the resources that they provision. For example, a GKE cluster has a logging agent installed in each worker node (GCE instance) of the cluster. As a result, logging agents are constrained with the resource they run and do not support customization of the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.resource\"\u003eresource\u003c/a\u003e field of the ingested log entries.\u003c/p\u003e\u003cp\u003eAdditionally, the \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name\"\u003elogName\u003c/a\u003e of all ingested logs is defined by the agent and cannot be changed*. It means that the application cannot define the log name or where the log entry will be stored (a.k.a. \u003ca href=\"https://github.com/googleapis/java-logging/blob/e24e099ae3cb5780bf122c64de6fde76b880b5d6/google-cloud-logging/src/main/java/com/google/cloud/logging/LogDestinationName.java\" target=\"_blank\"\u003elog's destination name\u003c/a\u003e).\u003c/p\u003e\u003cp\u003eIf it is essential for you to define a custom resource type or to control to which project the logs will be routed and/or the log name, you should not redirect the log writing to standard output.\u003c/p\u003e\u003cp\u003e* It is possible to customize the log name (but not the destination) by \u003ca href=\"https://cloud.google.com/logging/docs/agent/logging/configuration#configure\"\u003ecustomizing the Logging agent's configuration\u003c/a\u003e in GCE instances by defining the name as the \"tag\".\u003c/p\u003e\u003ch3\u003eWhat is next\u003c/h3\u003e\u003cp\u003eLet's recap the benefits of upgrading your logging client to the latest version.\u003c/p\u003e\u003cp\u003eUse the new Logging library if you need log correlation capabilities of Log Explorer or forward Cloud Logging structured logs to external solutions and use the data in the auto-populated fields.\u003c/p\u003e\u003cp\u003eUse the \u003ca href=\"https://github.com/googleapis/java-logging-servlet-initializer\" target=\"_blank\"\u003egoogle-cloud-logging-servlet-initializer\u003c/a\u003e package to automate the context management if you run a request based application that uses Jakarta servlets. Note that it will not work with legacy Java EE servlets or Web servers that are not based on Java servlets such as \u003ca href=\"https://netty.io/\" target=\"_blank\"\u003eNetty\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIf you run your application in the Google Cloud serverless environments like Cloud Run or Cloud Functions, consider using Java Logging or Logback with the configuration that redirects formatted logs to standard output like it is described in the previous section. Leveraging logging agents for ingesting logs resolves some reliability problems about asynchronous log ingestion such as \u003ca href=\"https://cloud.google.com/run/docs/configuring/cpu-allocation\"\u003eCPU throttling\u003c/a\u003e on Cloud Run or no grace period in Cloud Functions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-logging-python-client-library-v3-0-0-release/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/logging.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGetting Started with Google Cloud Logging Python v3.0.0\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eLearn how to manage your app's Python logs and related metadata using Google Cloud client libraries.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Java_applications_logs.max-2200x2200.jpg",
      "date_published": "2022-03-07T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eLeonid Yankulin\u003c/name\u003e\u003ctitle\u003eDeveloper Relations Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/easy-managed-prometheus-metrics-service-for-kubernetes/",
      "title": "Google Cloud Managed Service for Prometheus is now generally available",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe are excited to announce that Google Cloud \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eManaged Service for Prometheus\u003c/a\u003e is now generally available! Now you can get all the benefits of open source-compatible monitoring with the ease of use of Google-scale managed services.\u003c/p\u003e\u003cp\u003eThe rapid adoption of managed platforms and services across the cloud computing industry has shown that fewer and fewer organizations want to invest developer time into managing infrastructure. Google Cloud was recently recognized as a Leader in \u003ca href=\"https://cloud.google.com/resources/forrester-wave-container-platforms-report\"\u003eThe Forrester Wave™: Public Cloud Container Platforms, Q1 2022\u003c/a\u003e and in the report the authors note: “Large firms are seeking enterprise container platforms that accelerate and simplify the development and operations of cloud-native apps with resiliency, manageability, and observability via full-stack cloud-native capabilities.” \u003c/p\u003e\u003cp\u003eGetting a better picture of the \u003ca href=\"https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals\" target=\"_blank\"\u003efour golden signals\u003c/a\u003e of monitoring, as laid out in Google’s SRE book, means you have to capture metrics. In both self-managed Kubernetes and Google Kubernetes Engine (GKE) environments, the de-facto standard monitoring technology is \u003ca href=\"http://prometheus.io\" target=\"_blank\"\u003ePrometheus\u003c/a\u003e, an open source metrics collection and alerting tool. While Prometheus works great out-of-the-box for smaller deployments, running Prometheus at scale creates some uniquely difficult challenges.\u003c/p\u003e\u003cp\u003eMuch like you might use GKE because you prefer to not manage your own Kubernetes infrastructure, Managed Service for Prometheus is here for those who prefer to not manage their own Prometheus infrastructure. Focus your developers’ efforts on building features for your customers, as opposed to focusing efforts on operations toil that merely keeps the lights on.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-pull_quote\"\u003e\u003cdiv class=\"uni-pull-quote h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003cdiv class=\"uni-pull-quote__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cdiv class=\"uni-pull-quote__inner-wrapper h-c-copy h-c-copy\"\u003e\u003cq class=\"uni-pull-quote__text\"\u003eSince adopting the service, we've been able to really streamline our Prometheus management, and we've highly enjoyed simplifying our operations by bringing together more data sources into a single pane of glass,\" says Jonathan Campos, CTO and VP of Engineering at Alto. \"And since we don't have to worry about managing historical data, we've been able to reduce our cluster storage from 1TB down to 50GB while extending our Prometheus data retention period from 7 days to 2 years.\u003c/q\u003e\u003c/div\u003e\u003c/div\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eBenefits of using Managed Service for Prometheus  \u003c/h3\u003e\u003cp\u003e\u003cb\u003eTwo-year retention of all metrics, included in the price\u003c/b\u003e: Manually sharding long-term storage is a major pain point of running your own Prometheus-compatible aggregator. Thanks to the global scale and scalability of \u003ca href=\"https://research.google/pubs/pub50652/\" target=\"_blank\"\u003eMonarch\u003c/a\u003e, the system that powers not just \u003ca href=\"https://cloud.google.com/monitoring\"\u003eCloud Monitoring\u003c/a\u003e but all monitoring at Google, long-term storage of Managed Service for Prometheus metrics is easy for us. That benefit gets passed along to you as a two-year retention policy, for all metrics, at no additional charge.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost-effective monitoring\u003c/b\u003e: Switching from open source to a managed service always brings the fear of increased costs, but the pricing model for this service is straightforward and predictable. Charging on a per-sample basis means you pay only while your containers are alive and sending metrics data, taking the worry out of using features like Horizontal Pod Autoscaling that frequently scale containers up and down. Managed Service for Prometheus also provides other \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls\"\u003ecost control and cost reducing measures\u003c/a\u003e such as a reduced charge for sparse histograms, a fee structure that charges less for longer sampling periods, and the ability to only send locally pre-aggregated data.\u003c/p\u003e\u003cp\u003e\u003cb\u003eEasy cost identification and attribution\u003c/b\u003e: Within Cloud Monitoring, you can easily \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls#identify-cost-sources\"\u003eseparate out your Prometheus ingestion volume by metric name and namespace\u003c/a\u003e. This allows you to identify which metrics contribute the most to your bill, determine what team’s namespace is responsible for sending those metrics, and take action to reduce your costs.\u003c/p\u003e\u003cp\u003e\u003cb\u003eNo changes needed to existing querying or alerting workflows\u003c/b\u003e: You can choose to \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged\"\u003ereuse your existing Prometheus collection deployment\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\"\u003eswitch to our managed collection\u003c/a\u003e. In either case you can keep using the same Grafana dashboards and alert configs you’re using today. PromQL compatibility for dashboarding and alerting means that your existing incident creation and investigation workflows will continue to work as before.  \u003c/p\u003e\u003cp\u003e\u003cb\u003eViewing Prometheus metrics and\u003c/b\u003e\u003ca href=\"https://cloud.google.com/blog/products/operations/in-depth-explanation-of-operational-metrics-at-google-cloud\"\u003e\u003cb\u003eGoogle Cloud system metrics\u003c/b\u003e\u003c/a\u003e \u003cb\u003etogether\u003c/b\u003e: Many organizations try, but struggle to simplify their operations by building a “single pane of glass” for all their metric sources. Because our service is built on the same technology and backend as Cloud Monitoring, your Prometheus metrics can be used with the dashboarding, alerting, and SLO monitoring available within Cloud Monitoring. Chart your Prometheus metrics right alongside your \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/managing-metrics\"\u003eGKE metrics\u003c/a\u003e, your load balancer metrics, \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp\"\u003eand more\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Google_Cloud_Managed_Service_for_Prometh.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Google Cloud Managed Service for Prometheus.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Google_Cloud_Managed_Service_for_Prometh.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eGoogle Cloud Managed Service for Prometheus maintains compatibility with the rich ecosystem of open source tools and services used to analyze, query, and visualize Prometheus metrics\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eRead more from customers who have used Managed Service for Prometheus on \u003ca href=\"https://cloud.google.com/managed-prometheus#section-3\"\u003eour website\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eWhat’s coming up next\u003c/h3\u003e\u003cp\u003eGeneral availability is just the beginning! We have many more great Managed Service for Prometheus features on our roadmap, including:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003ePromQL querying of \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_gcp\"\u003efree GCP system metrics\u003c/a\u003e available in Cloud Monitoring, including \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_kubernetes\"\u003eGKE\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_anthos\"\u003eAnthos\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/monitoring/api/metrics_istio\"\u003eIstio\u003c/a\u003e metrics, so you can chart these metrics right alongside your Prometheus metrics in Grafana\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRecommended default collection configs for commonly-used exporters, such as \u003ca href=\"https://github.com/kubernetes/kube-state-metrics\" target=\"_blank\"\u003ekube-state-metrics\u003c/a\u003e and \u003ca href=\"https://github.com/prometheus/node_exporter\" target=\"_blank\"\u003enode-exporter\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOptimized network performance for on-prem clusters\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDebugging tools, such as a targets discovery and health page\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAnd more!\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eHow to get started\u003c/h3\u003e\u003cp\u003eSetting up Managed Service for Prometheus is straightforward. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eFor those \u003cb\u003estarting from scratch\u003c/b\u003e or for those who want a \u003cb\u003emore fully-managed experience\u003c/b\u003e, you can deploy \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-managed\"\u003emanaged collectors\u003c/a\u003e in any Kubernetes cluster by using the GKE UI in Cloud Console, the \u003ccode\u003egcloud\u003c/code\u003e CLI, or \u003ccode\u003ekubectl\u003c/code\u003e. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFor those with \u003cb\u003eexisting Prometheus deployments\u003c/b\u003e, you can \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged\"\u003ekeep using your existing configuration\u003c/a\u003e by just swapping out your Prometheus binary with the Managed Service for Prometheus binary. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSee \u003ca href=\"https://cloud.google.com/stackdriver/docs/managed-prometheus\"\u003eManaged Service for Prometheus documentation\u003c/a\u003e to get started. You can also check out \u003ca href=\"https://www.youtube.com/watch?v=X4qAEa8_JxQ\" target=\"_blank\"\u003ethis video\u003c/a\u003e which walks you through a few different ways to set up the service:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_kubectl_commands.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 kubectl commands.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_kubectl_commands.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eA screenshot from the video, showing kubectl commands used when setting up Managed Service for Prometheus\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eLastly, if you have questions, feature requests, or just want to read topics from other customers who are using Google Cloud Managed Service for Prometheus and Google Cloud’s operations suite, visit our \u003ca href=\"https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations\" target=\"_blank\"\u003eGoogle Cloud Community site\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/operations/introducing-google-cloud-managed-service-for-prometheus/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Prometheus.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGet planet-scale monitoring with Managed Service for Prometheus\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eGoogle Cloud's managed Prometheus monitoring solution, offering collection, storage, and global querying of Prometheus metrics at scale.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Prometheus_HCKF6h9.jpg",
      "date_published": "2022-03-02T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eLee Yanco\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/application-exceptions-surfaced-automatically/",
      "title": "Quickly troubleshoot application errors with Error Reporting",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c27=\"\"\u003e\u003cdiv _ngcontent-c27=\"\" innerhtml=\"\u0026lt;p\u0026gt;Are you familiar with the \u0026lt;a href=\u0026#34;https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;four golden signals\u0026lt;/a\u0026gt; of Site Reliability Engineering (SRE): latency, traffic, errors, and saturation? Whether you\u0026amp;#8217;re a developer or an operator, you\u0026amp;#8217;ve likely been responsible for collecting, storing, or analyzing the data associated with these concepts. Much of this data is captured in application and infrastructure logs, which provide a rich history of what is happening behind the scenes in your workloads.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Getting insights from your logs to track those four golden signals can become unruly very quickly as the application scales up, hindering the ability for your developers and operations teams to identify when and where errors are occurring. If you fail to set up your monitoring and logging systems correctly, your Mean Time to Recovery (MTTR) from service impacting events can be impacted.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud provides guidance on what to think about when deciding how to set up your logging, monitoring, and alert systems in the operational excellence section of the \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging\u0026#34;\u0026gt;Cloud Architecture Framework\u0026lt;/a\u0026gt;. Google Cloud also provides managed services as part of the \u0026lt;a href=\u0026#34;https://cloud.google.com/products/operations\u0026#34;\u0026gt;operations suite\u0026lt;/a\u0026gt; to automate collection, storage and analysis of the four golden signals. Cloud Error Reporting is one such service.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Error Reporting - Speed up your MTTR with zero effort\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/\u0026#34;\u0026gt;Error Reporting\u0026lt;/a\u0026gt; automatically captures exceptions found in logs ingested by Cloud Logging from the following languages: \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/setup/go\u0026#34;\u0026gt;Go\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/setup/java\u0026#34;\u0026gt;Java\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/setup/nodejs\u0026#34;\u0026gt;Node.js\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/setup/php\u0026#34;\u0026gt;PHP\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/setup/python\u0026#34;\u0026gt;Python\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/setup/ruby\u0026#34;\u0026gt;Ruby\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/setup/dotnet\u0026#34;\u0026gt;.NET\u0026lt;/a\u0026gt;, aggregates them, and then \u0026lt;a href=\u0026#34;https://cloud.google.com/error-reporting/docs/notifications\u0026#34;\u0026gt;notifies you\u0026lt;/a\u0026gt; of their existence. The service intelligently groups together the errors that it finds and makes them available in a \u0026lt;a href=\u0026#34;http://console.cloud.google.com/errors\u0026#34;\u0026gt;dedicated dashboard\u0026lt;/a\u0026gt;. The dashboard displays the details of the exception including a histogram of occurrences, list of affected versions, request URL and links to the request log, meaning you can get to the affected resource immediately, with just one click!\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eAre you familiar with the \u003ca href=\"https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003efour golden signals\u003c/a\u003e of Site Reliability Engineering (SRE): latency, traffic, errors, and saturation? Whether you’re a developer or an operator, you’ve likely been responsible for collecting, storing, or analyzing the data associated with these concepts. Much of this data is captured in application and infrastructure logs, which provide a rich history of what is happening behind the scenes in your workloads.  \u003c/p\u003e\u003cp\u003eGetting insights from your logs to track those four golden signals can become unruly very quickly as the application scales up, hindering the ability for your developers and operations teams to identify when and where errors are occurring. If you fail to set up your monitoring and logging systems correctly, your Mean Time to Recovery (MTTR) from service impacting events can be impacted.\u003c/p\u003e\u003cp\u003eGoogle Cloud provides guidance on what to think about when deciding how to set up your logging, monitoring, and alert systems in the operational excellence section of the \u003ca href=\"https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging\" track-metadata-module=\"post\"\u003eCloud Architecture Framework\u003c/a\u003e. Google Cloud also provides managed services as part of the \u003ca href=\"https://cloud.google.com/products/operations\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/products/operations\" track-metadata-module=\"post\"\u003eoperations suite\u003c/a\u003e to automate collection, storage and analysis of the four golden signals. Cloud Error Reporting is one such service.\u003c/p\u003e\u003ch3\u003eError Reporting - Speed up your MTTR with zero effort\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/error-reporting/\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/\" track-metadata-module=\"post\"\u003eError Reporting\u003c/a\u003e automatically captures exceptions found in logs ingested by Cloud Logging from the following languages: \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/go\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/setup/go\" track-metadata-module=\"post\"\u003eGo\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/java\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/setup/java\" track-metadata-module=\"post\"\u003eJava\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/nodejs\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/setup/nodejs\" track-metadata-module=\"post\"\u003eNode.js\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/php\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/setup/php\" track-metadata-module=\"post\"\u003ePHP\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/python\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/setup/python\" track-metadata-module=\"post\"\u003ePython\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/ruby\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/setup/ruby\" track-metadata-module=\"post\"\u003eRuby\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/dotnet\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/setup/dotnet\" track-metadata-module=\"post\"\u003e.NET\u003c/a\u003e, aggregates them, and then \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/error-reporting/docs/notifications\" track-metadata-module=\"post\"\u003enotifies you\u003c/a\u003e of their existence. The service intelligently groups together the errors that it finds and makes them available in a \u003ca href=\"http://console.cloud.google.com/errors\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"http://console.cloud.google.com/errors\" track-metadata-module=\"post\"\u003ededicated dashboard\u003c/a\u003e. The dashboard displays the details of the exception including a histogram of occurrences, list of affected versions, request URL and links to the request log, meaning you can get to the affected resource immediately, with just one click!\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAre you familiar with the \u003ca href=\"https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals\" target=\"_blank\"\u003efour golden signals\u003c/a\u003e of Site Reliability Engineering (SRE): latency, traffic, errors, and saturation? Whether you’re a developer or an operator, you’ve likely been responsible for collecting, storing, or analyzing the data associated with these concepts. Much of this data is captured in application and infrastructure logs, which provide a rich history of what is happening behind the scenes in your workloads.  \u003c/p\u003e\u003cp\u003eGetting insights from your logs to track those four golden signals can become unruly very quickly as the application scales up, hindering the ability for your developers and operations teams to identify when and where errors are occurring. If you fail to set up your monitoring and logging systems correctly, your Mean Time to Recovery (MTTR) from service impacting events can be impacted.\u003c/p\u003e\u003cp\u003eGoogle Cloud provides guidance on what to think about when deciding how to set up your logging, monitoring, and alert systems in the operational excellence section of the \u003ca href=\"https://cloud.google.com/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging\"\u003eCloud Architecture Framework\u003c/a\u003e. Google Cloud also provides managed services as part of the \u003ca href=\"https://cloud.google.com/products/operations\"\u003eoperations suite\u003c/a\u003e to automate collection, storage and analysis of the four golden signals. Cloud Error Reporting is one such service.\u003c/p\u003e\u003ch3\u003eError Reporting - Speed up your MTTR with zero effort\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/error-reporting/\"\u003eError Reporting\u003c/a\u003e automatically captures exceptions found in logs ingested by Cloud Logging from the following languages: \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/go\"\u003eGo\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/java\"\u003eJava\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/nodejs\"\u003eNode.js\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/php\"\u003ePHP\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/python\"\u003ePython\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/ruby\"\u003eRuby\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/error-reporting/docs/setup/dotnet\"\u003e.NET\u003c/a\u003e, aggregates them, and then \u003ca href=\"https://cloud.google.com/error-reporting/docs/notifications\"\u003enotifies you\u003c/a\u003e of their existence. The service intelligently groups together the errors that it finds and makes them available in a \u003ca href=\"http://console.cloud.google.com/errors\"\u003ededicated dashboard\u003c/a\u003e. The dashboard displays the details of the exception including a histogram of occurrences, list of affected versions, request URL and links to the request log, meaning you can get to the affected resource immediately, with just one click!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Homepage_of_the_Error_Reporting_service.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 Homepage of the Error Reporting service.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Homepage_of_the_Error_Reporting_service.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eHomepage of the Error Reporting service\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eHow can Error Reporting help your organization today?\u003c/p\u003e\u003cp\u003eError Reporting helps focus your most valuable resource (i.e Developer attention) on the potential source of exceptions that are impacting your workloads. With the notifications and embedded links, exceptions can quickly be resolved before they impact your customers and bottom line.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Error_Reporting_value.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"2 Error Reporting value.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Error_Reporting_value.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eError Reporting value\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhat do you have to do to enable Error Reporting?\u003c/p\u003e\u003cp\u003eError Reporting is automatically enabled as soon as logs that contain error events like stack traces are ingested into Cloud Logging or when you use the API to \u003ca href=\"https://cloud.google.com/error-reporting/docs/how-to\"\u003eself configure\u003c/a\u003e a service to capture exceptions.\u003c/p\u003e\u003cp\u003eWhen you use Google Kubernetes Engine and our serverless offerings, application logs written to stdout or stderr will appear automatically in Cloud Logging, and therefore Error Reporting will automatically start analyzing them. To capture logs from applications running on VMs in Google Compute Engine, you will need to \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent\"\u003einstall the Ops Agent\u003c/a\u003e. From there, app logs will be captured in Cloud Logging and exceptions will flow through to Error Reporting. \u003c/p\u003e\u003ch3\u003eGet started today\u003c/h3\u003e\u003cp\u003eTo view available error events, visit the \u003ca href=\"http://console.cloud.google.com/errors\"\u003eError Reporting page\u003c/a\u003e in the Google Cloud Console. You can find it in the left navigation panel or by searching in the search bar at the top of the console.\u003c/p\u003e\u003cp\u003eIf you have any questions or want to start a discussion with other Error Reporting users, visit the Cloud Operations section of the \u003ca href=\"https://www.googlecloudcommunity.com/gc/Google-Cloud-s-operations-suite/bd-p/cloud-operations\" target=\"_blank\"\u003eGoogle Cloud Community\u003c/a\u003e and post a discussion topic.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/operations/faster-debugging-with-traces-and-logs-together/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_logging_OUrfE4R.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eEnabling SRE best practices: new contextual traces in Cloud Logging\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eDevelopers can now view trace information for applications directly in Google Cloud Logging for faster debugging.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/error_reporting.max-2200x2200.jpg",
      "date_published": "2022-02-28T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eJohn Day\u003c/name\u003e\u003ctitle\u003eProduct Marketing Manager, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-logging-python-client-library-v3-0-0-release/",
      "title": "Getting Started with Google Cloud Logging Python v3.0.0",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c34=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003elogging.jpg\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003carticle-cta _nghost-c55=\"\"\u003e\u003cdiv _ngcontent-c55=\"\"\u003e\u003ch4 _ngcontent-c55=\"\"\u003e\u003cspan _ngcontent-c55=\"\"\u003eTry Google Cloud\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c55=\"\"\u003e\u003cspan _ngcontent-c55=\"\"\u003eStart building on Google Cloud with $300 in free credits and 20+ always free products.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c55=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"free trial\" track-metadata-eventdetail=\"https://cloud.google.com/free/\" href=\"https://cloud.google.com/free/\"\u003e\u003cspan _ngcontent-c55=\"\"\u003eFree Trial\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c57=\"\"\u003e\u003cdiv _ngcontent-c57=\"\" innerhtml=\"\u0026lt;p\u0026gt;We\u0026amp;#8217;re excited to announce the release of a major update to the Google Cloud Python logging library.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;v3.0.0 makes it even easier for Python developers to send and read logs from Google Cloud, providing real-time insights into what is happening in your application.\u0026amp;#160; If you\u0026amp;#8217;re a Python developer working with Google Cloud, now is a great time to try out Cloud Logging!\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you\u0026#39;re unfamiliar with the `google-cloud-logging` library, getting started is simple. First, download the library using pip:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWe’re excited to announce the release of a major update to the Google Cloud Python logging library. \u003c/p\u003e\u003cp\u003ev3.0.0 makes it even easier for Python developers to send and read logs from Google Cloud, providing real-time insights into what is happening in your application.  If you’re a Python developer working with Google Cloud, now is a great time to try out Cloud Logging!\u003c/p\u003e\u003cp\u003eIf you\u0026#39;re unfamiliar with the `google-cloud-logging` library, getting started is simple. First, download the library using pip:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c58=\"\"\u003e\u003cpre _ngcontent-c58=\"\"\u003e  \u003ccode _ngcontent-c58=\"\"\u003e$ pip install \u0026#34;google-cloud-logging\u0026gt;=3.0.0\u0026#34;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c57=\"\"\u003e\u003cp\u003eNow, you can set up the client library to work with Python\u0026#39;s built-in `logging` library. Doing this will make it so that all your standard Python log statements will start sending data to Google Cloud:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c58=\"\"\u003e\u003cpre _ngcontent-c58=\"\"\u003e  \u003ccode _ngcontent-c58=\"\"\u003e# set up the Google Cloud Logging python client library\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003eimport google.cloud.logging\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003eclient = google.cloud.logging.Client()\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003eclient.setup_logging()\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e# use Python\u0026#39;s standard logging library to send logs to GCP\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003eimport logging\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003elogging.warning(\u0026#34;Hello World\u0026#34;)\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c58=\"\"\u003e\u003cpre _ngcontent-c58=\"\"\u003e  \u003ccode _ngcontent-c58=\"\"\u003eimport google.cloud.logging\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003eclient = google.cloud.logging.Client()\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003elogger = client.logger(name=\u0026#34;log_id\u0026#34;)\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003eclient.list_entries(max_size=5) # read logs from GCP\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003elogger.log(\u0026#34;hello world\u0026#34;, resource={\u0026#34;type\u0026#34;:\u0026#34;global\u0026#34;, \u0026#34;labels\u0026#34;:{}}) # write log to GCP\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c57=\"\"\u003e\u003cdiv _ngcontent-c57=\"\" innerhtml=\"\u0026lt;p\u0026gt;Here are some of the main features of the \u0026lt;a href=\u0026#34;https://github.com/googleapis/python-logging/blob/eac5e2db83f83b24962524fd9e0d7afa09e2785b/UPGRADING.md\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;new release\u0026lt;/a\u0026gt;:\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Support More Cloud Environments\u0026lt;/h3\u0026gt;\"\u003e\u003cp\u003eHere are some of the main features of the \u003ca href=\"https://github.com/googleapis/python-logging/blob/eac5e2db83f83b24962524fd9e0d7afa09e2785b/UPGRADING.md\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003enew release\u003c/a\u003e:\u003c/p\u003e\u003ch3\u003eSupport More Cloud Environments\u003c/h3\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c57=\"\"\u003e\u003cdiv _ngcontent-c57=\"\" innerhtml=\"\u0026lt;p\u0026gt;Previous versions of google-cloud-logging supported only\u0026lt;a href=\u0026#34;https://cloud.google.com/appengine\u0026#34;\u0026gt; App Engine\u0026lt;/a\u0026gt; and\u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt; Kubernetes Engine\u0026lt;/a\u0026gt;. Users reported that the library would occasionally drop logs on serverless environments like Cloud Run and Cloud Functions. This was because the library would send logs in batches over the network. When a serverless environment would spin down, unsent batches could be lost.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;v3.0.0 fixes this issue by making use of GCP\u0026amp;#8217;s built in\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/structured-logging\u0026#34;\u0026gt; structured JSON logging functionality\u0026lt;/a\u0026gt; on supported environments (GKE, Cloud Run, or Cloud Functions). If the library detects it is running on an environment that supports structured logging, it will automatically make use of the new\u0026lt;a href=\u0026#34;https://github.com/googleapis/python-logging/blob/v3.0.0/google/cloud/logging_v2/handlers/structured_log.py\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; StructuredLogHandler\u0026lt;/a\u0026gt;, which writes logs as JSON strings printed to standard out. Google Cloud\u0026amp;#8217;s built-in agents will then parse the logs and deliver them to Cloud Logging, even if the code that produced the logs has spun down.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Structured Logging is more reliable on serverless environments, and it allows us to support all major GCP compute environments in v3.0.0. Still, if you would prefer to send logs over the network as before, you can manually set up the library with a \u0026lt;a href=\u0026#34;https://github.com/googleapis/python-logging/blob/d86be6cf83c3f3b91c4fc0b2e0666b0ca1d7e248/google/cloud/logging_v2/handlers/handlers.py#L118\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;CloudLoggingHandler\u0026lt;/a\u0026gt; instance:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003ePrevious versions of google-cloud-logging supported only\u003ca href=\"https://cloud.google.com/appengine\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/appengine\" track-metadata-module=\"post\"\u003e App Engine\u003c/a\u003e and\u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003e Kubernetes Engine\u003c/a\u003e. Users reported that the library would occasionally drop logs on serverless environments like Cloud Run and Cloud Functions. This was because the library would send logs in batches over the network. When a serverless environment would spin down, unsent batches could be lost.\u003c/p\u003e\u003cp\u003ev3.0.0 fixes this issue by making use of GCP’s built in\u003ca href=\"https://cloud.google.com/logging/docs/structured-logging\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/structured-logging\" track-metadata-module=\"post\"\u003e structured JSON logging functionality\u003c/a\u003e on supported environments (GKE, Cloud Run, or Cloud Functions). If the library detects it is running on an environment that supports structured logging, it will automatically make use of the new\u003ca href=\"https://github.com/googleapis/python-logging/blob/v3.0.0/google/cloud/logging_v2/handlers/structured_log.py\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e StructuredLogHandler\u003c/a\u003e, which writes logs as JSON strings printed to standard out. Google Cloud’s built-in agents will then parse the logs and deliver them to Cloud Logging, even if the code that produced the logs has spun down. \u003c/p\u003e\u003cp\u003eStructured Logging is more reliable on serverless environments, and it allows us to support all major GCP compute environments in v3.0.0. Still, if you would prefer to send logs over the network as before, you can manually set up the library with a \u003ca href=\"https://github.com/googleapis/python-logging/blob/d86be6cf83c3f3b91c4fc0b2e0666b0ca1d7e248/google/cloud/logging_v2/handlers/handlers.py#L118\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eCloudLoggingHandler\u003c/a\u003e instance:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c58=\"\"\u003e\u003cpre _ngcontent-c58=\"\"\u003e  \u003ccode _ngcontent-c58=\"\"\u003efrom google.cloud.logging.handlers import CloudLoggingHandler\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003efrom google.cloud.logging_v2.handlers import setup_logging\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e# explicitly set up a CloudLoggingHandler to send logs over the network\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003ehandler = CloudLoggingHandler(client)\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003esetup_logging(handler)\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003eimport logging\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003elogging.warning(“Hello World”)\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c57=\"\"\u003e\u003cdiv _ngcontent-c57=\"\" innerhtml=\"\u0026lt;p\u0026gt;When you troubleshoot your application, it can be useful to have as much information about the environment as possible captured in your application logs. `google-cloud-logging` attempts to help in this process by detecting and attaching metadata about your environment to each log message. The following fields are currently supported:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;`\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource\u0026#34;\u0026gt;resource\u0026lt;/a\u0026gt;`: The Google Cloud resource the log originated from\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;for example, Functions, GKE, or Cloud Run\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;`\u0026lt;a href=\u0026#34;http://httprequest\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;httpRequest\u0026lt;/a\u0026gt;`: Information about an HTTP request in the log\u0026amp;#8217;s context\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Flask and Django are currently supported\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;`\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogEntrySourceLocation\u0026#34;\u0026gt;sourceLocation\u0026lt;/a\u0026gt;` : File, line, and function names\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\u0026#34;\u0026gt;trace\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\u0026#34;\u0026gt;spanId\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\u0026#34;\u0026gt;traceSampled\u0026lt;/a\u0026gt;: \u0026lt;a href=\u0026#34;https://medium.com/r/?url=https%3A%2F%2Fcloud.google.com%2Ftrace\u0026#34;\u0026gt;Cloud Trace\u0026lt;/a\u0026gt; metadata\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Supports \u0026lt;a href=\u0026#34;https://cloud.google.com/trace/docs/setup#force-trace\u0026#34;\u0026gt;X-Cloud-Trace-Context\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://www.w3.org/TR/trace-context/#traceparent-header\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;w3c transparent\u0026lt;/a\u0026gt; trace formats\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;The library will make an attempt to populate this data whenever possible, but any of these fields can also be explicitly set by developers using the library.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWhen you troubleshoot your application, it can be useful to have as much information about the environment as possible captured in your application logs. `google-cloud-logging` attempts to help in this process by detecting and attaching metadata about your environment to each log message. The following fields are currently supported:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e`\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource\" track-metadata-module=\"post\"\u003eresource\u003c/a\u003e`: The Google Cloud resource the log originated from \u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003efor example, Functions, GKE, or Cloud Run\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003e`\u003ca href=\"http://httprequest\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"http://httprequest\" track-metadata-module=\"post\"\u003ehttpRequest\u003c/a\u003e`: Information about an HTTP request in the log’s context\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eFlask and Django are currently supported\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003e`\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogEntrySourceLocation\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogEntrySourceLocation\" track-metadata-module=\"post\"\u003esourceLocation\u003c/a\u003e` : File, line, and function names\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\" track-metadata-module=\"post\"\u003etrace\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\" track-metadata-module=\"post\"\u003espanId\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\" track-metadata-module=\"post\"\u003etraceSampled\u003c/a\u003e: \u003ca href=\"https://medium.com/r/?url=https%3A%2F%2Fcloud.google.com%2Ftrace\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://medium.com\" track-metadata-module=\"post\"\u003eCloud Trace\u003c/a\u003e metadata\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSupports \u003ca href=\"https://cloud.google.com/trace/docs/setup#force-trace\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/trace/docs/setup#force-trace\" track-metadata-module=\"post\"\u003eX-Cloud-Trace-Context\u003c/a\u003e and \u003ca href=\"https://www.w3.org/TR/trace-context/#traceparent-header\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://www.w3.org\" track-metadata-module=\"post\"\u003ew3c transparent\u003c/a\u003e trace formats\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003eThe library will make an attempt to populate this data whenever possible, but any of these fields can also be explicitly set by developers using the library.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c58=\"\"\u003e\u003cpre _ngcontent-c58=\"\"\u003e  \u003ccode _ngcontent-c58=\"\"\u003elogging.info(\u0026#34;hello\u0026#34;, extra={\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e    \u0026#34;labels\u0026#34;: {\u0026#34;foo\u0026#34;: \u0026#34;bar\u0026#34;},\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e    \u0026#34;http_request\u0026#34;: {\u0026#34;requestUrl\u0026#34;: \u0026#34;localhost\u0026#34;},\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e    \u0026#34;trace\u0026#34;: \u0026#34;01234\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e})\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c57=\"\"\u003e\u003cp _ngcontent-c57=\"\" innerhtml=\"\u0026lt;h3\u0026gt;JSON Support in Standard Library Integration\u0026lt;/h3\u0026gt;\"\u003e\u003ch3\u003eJSON Support in Standard Library Integration\u003c/h3\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c58=\"\"\u003e\u003cpre _ngcontent-c58=\"\"\u003e  \u003ccode _ngcontent-c58=\"\"\u003eimport logging\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003eimport json\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003edata_dict = {\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003elogging.info(json.dumps(data_dict))\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c57=\"\"\u003e\u003cp\u003e2. Pass a `json_fields` dictionary using Python logging\u0026#39;s `extra` argument:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c58=\"\"\u003e\u003cpre _ngcontent-c58=\"\"\u003e  \u003ccode _ngcontent-c58=\"\"\u003eimport logging\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003edata_dict = {\u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;}\n\u003c/code\u003e\u003ccode _ngcontent-c58=\"\"\u003elogging.info(\u0026#34;message field\u0026#34;, extra={\u0026#34;json_fields\u0026#34;: data_dict})\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c57=\"\"\u003e\u003cdiv _ngcontent-c57=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Next Steps\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;With version v3.0.0, the Google Cloud Logging Python library now supports more compute environments, detects more helpful metadata, and provides more thorough support for JSON logs. Along with these major features, there are also user-experience improvements like a new \u0026lt;a href=\u0026#34;https://googleapis.dev/python/logging/latest/UPGRADING.html#new-logger-log-method-316\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;log method\u0026lt;/a\u0026gt; and more \u0026lt;a href=\u0026#34;https://googleapis.dev/python/logging/latest/UPGRADING.html#more-permissive-arguments-422\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;permissive argument parsing\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you want to learn more about the latest release, these changes and others are described in more detail in the \u0026lt;a href=\u0026#34;https://googleapis.dev/python/logging/latest/UPGRADING.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;v3.0.0 Migration Guide\u0026lt;/a\u0026gt;. If you\u0026amp;#8217;re new to the library, check out the \u0026lt;a href=\u0026#34;https://googleapis.dev/python/logging/latest/index.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;google-cloud-logging user guide\u0026lt;/a\u0026gt;. If you want to learn more about observability on GCP in general, you can spin up test environments using \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox\u0026#34;\u0026gt;Cloud Ops Sandbox\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, if you have any feedback about the latest release, have new feature requests, or would like to make any contributions, feel free to open issues on \u0026lt;a href=\u0026#34;https://github.com/googleapis/python-logging\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;our GitHub repo\u0026lt;/a\u0026gt;. The Google Cloud Logging libraries are open source software, and we welcome new contributors!\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eNext Steps\u003c/h3\u003e\u003cp\u003eWith version v3.0.0, the Google Cloud Logging Python library now supports more compute environments, detects more helpful metadata, and provides more thorough support for JSON logs. Along with these major features, there are also user-experience improvements like a new \u003ca href=\"https://googleapis.dev/python/logging/latest/UPGRADING.html#new-logger-log-method-316\" target=\"_blank\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://googleapis.dev\" track-metadata-module=\"post\"\u003elog method\u003c/a\u003e and more \u003ca href=\"https://googleapis.dev/python/logging/latest/UPGRADING.html#more-permissive-arguments-422\" target=\"_blank\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://googleapis.dev\" track-metadata-module=\"post\"\u003epermissive argument parsing\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eIf you want to learn more about the latest release, these changes and others are described in more detail in the \u003ca href=\"https://googleapis.dev/python/logging/latest/UPGRADING.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://googleapis.dev\" track-metadata-module=\"post\"\u003ev3.0.0 Migration Guide\u003c/a\u003e. If you’re new to the library, check out the \u003ca href=\"https://googleapis.dev/python/logging/latest/index.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://googleapis.dev\" track-metadata-module=\"post\"\u003egoogle-cloud-logging user guide\u003c/a\u003e. If you want to learn more about observability on GCP in general, you can spin up test environments using \u003ca href=\"https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox\" track-metadata-module=\"post\"\u003eCloud Ops Sandbox\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFinally, if you have any feedback about the latest release, have new feature requests, or would like to make any contributions, feel free to open issues on \u003ca href=\"https://github.com/googleapis/python-logging\" target=\"_blank\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eour GitHub repo\u003c/a\u003e. The Google Cloud Logging libraries are open source software, and we welcome new contributors!\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c56=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe’re excited to announce the release of a major update to the Google Cloud Python logging library. \u003c/p\u003e\u003cp\u003ev3.0.0 makes it even easier for Python developers to send and read logs from Google Cloud, providing real-time insights into what is happening in your application.  If you’re a Python developer working with Google Cloud, now is a great time to try out Cloud Logging!\u003c/p\u003e\u003cp\u003eIf you're unfamiliar with the `google-cloud-logging` library, getting started is simple. First, download the library using pip:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eNow, you can set up the client library to work with Python's built-in `logging` library. Doing this will make it so that all your standard Python log statements will start sending data to Google Cloud:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe recommend \u003ca href=\"https://googleapis.dev/python/logging/latest/std-lib-integration.html\" target=\"_blank\"\u003eusing the standard Python `logging` interface\u003c/a\u003e for log creation, as demonstrated above. However, if you need access to other \u003ca href=\"https://cloud.google.com/logging\"\u003eGoogle Cloud Logging features\u003c/a\u003e (reading logs, managing \u003ca href=\"https://cloud.google.com/logging/docs/export/configure_export_v2\"\u003elog sinks\u003c/a\u003e, etc), you can \u003ca href=\"https://googleapis.dev/python/logging/latest/direct-lib-usage.html\" target=\"_blank\"\u003euse `google.cloud.logging` directly\u003c/a\u003e:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eHere are some of the main features of the \u003ca href=\"https://github.com/googleapis/python-logging/blob/eac5e2db83f83b24962524fd9e0d7afa09e2785b/UPGRADING.md\" target=\"_blank\"\u003enew release\u003c/a\u003e:\u003c/p\u003e\u003ch3\u003eSupport More Cloud Environments\u003c/h3\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_jAhSLdi.0999065319990470.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003ePrevious versions of google-cloud-logging supported only\u003ca href=\"https://cloud.google.com/appengine\"\u003eApp Engine\u003c/a\u003e and\u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eKubernetes Engine\u003c/a\u003e. Users reported that the library would occasionally drop logs on serverless environments like Cloud Run and Cloud Functions. This was because the library would send logs in batches over the network. When a serverless environment would spin down, unsent batches could be lost.\u003c/p\u003e\u003cp\u003ev3.0.0 fixes this issue by making use of GCP’s built in\u003ca href=\"https://cloud.google.com/logging/docs/structured-logging\"\u003estructured JSON logging functionality\u003c/a\u003e on supported environments (GKE, Cloud Run, or Cloud Functions). If the library detects it is running on an environment that supports structured logging, it will automatically make use of the new\u003ca href=\"https://github.com/googleapis/python-logging/blob/v3.0.0/google/cloud/logging_v2/handlers/structured_log.py\" target=\"_blank\"\u003eStructuredLogHandler\u003c/a\u003e, which writes logs as JSON strings printed to standard out. Google Cloud’s built-in agents will then parse the logs and deliver them to Cloud Logging, even if the code that produced the logs has spun down. \u003c/p\u003e\u003cp\u003eStructured Logging is more reliable on serverless environments, and it allows us to support all major GCP compute environments in v3.0.0. Still, if you would prefer to send logs over the network as before, you can manually set up the library with a \u003ca href=\"https://github.com/googleapis/python-logging/blob/d86be6cf83c3f3b91c4fc0b2e0666b0ca1d7e248/google/cloud/logging_v2/handlers/handlers.py#L118\" target=\"_blank\"\u003eCloudLoggingHandler\u003c/a\u003e instance:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eMetadata Autodetection\u003c/h3\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_om8Pxs0.0999064919990643.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen you troubleshoot your application, it can be useful to have as much information about the environment as possible captured in your application logs. `google-cloud-logging` attempts to help in this process by detecting and attaching metadata about your environment to each log message. The following fields are currently supported:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e`\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource\"\u003eresource\u003c/a\u003e`: The Google Cloud resource the log originated from \u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003efor example, Functions, GKE, or Cloud Run\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003e`\u003ca href=\"http://httprequest\" target=\"_blank\"\u003ehttpRequest\u003c/a\u003e`: Information about an HTTP request in the log’s context\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eFlask and Django are currently supported\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003e`\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry#LogEntrySourceLocation\"\u003esourceLocation\u003c/a\u003e` : File, line, and function names\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\"\u003etrace\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\"\u003espanId\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry\"\u003etraceSampled\u003c/a\u003e: \u003ca href=\"https://medium.com/r/?url=https%3A%2F%2Fcloud.google.com%2Ftrace\"\u003eCloud Trace\u003c/a\u003e metadata\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eSupports \u003ca href=\"https://cloud.google.com/trace/docs/setup#force-trace\"\u003eX-Cloud-Trace-Context\u003c/a\u003e and \u003ca href=\"https://www.w3.org/TR/trace-context/#traceparent-header\" target=\"_blank\"\u003ew3c transparent\u003c/a\u003e trace formats\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/ul\u003e\u003cp\u003eThe library will make an attempt to populate this data whenever possible, but any of these fields can also be explicitly set by developers using the library.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eJSON Support in Standard Library Integration\u003c/h3\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_wcTDIdW.0999064519990884.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eGoogle Cloud Logging supports both\u003ca href=\"https://cloud.google.com/logging/docs/structured-logging\"\u003estring and JSON payloads\u003c/a\u003e for LogEntries, but up until now,\u003ca href=\"https://googleapis.dev/python/logging/latest/std-lib-integration.html\" target=\"_blank\"\u003ethe Python standard library integration\u003c/a\u003e could only send logs with string payloads.\u003c/p\u003e\u003cp\u003eIn `google-cloud-logging` v3,  you can log JSON data in two ways:\u003c/p\u003e\u003cp\u003e1. Log a JSON-parsable string:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e2. Pass a `json_fields` dictionary using Python logging's `extra` argument:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eNext Steps\u003c/h3\u003e\u003cp\u003eWith version v3.0.0, the Google Cloud Logging Python library now supports more compute environments, detects more helpful metadata, and provides more thorough support for JSON logs. Along with these major features, there are also user-experience improvements like a new \u003ca href=\"https://googleapis.dev/python/logging/latest/UPGRADING.html#new-logger-log-method-316\" target=\"_blank\"\u003elog method\u003c/a\u003e and more \u003ca href=\"https://googleapis.dev/python/logging/latest/UPGRADING.html#more-permissive-arguments-422\" target=\"_blank\"\u003epermissive argument parsing\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eIf you want to learn more about the latest release, these changes and others are described in more detail in the \u003ca href=\"https://googleapis.dev/python/logging/latest/UPGRADING.html\" target=\"_blank\"\u003ev3.0.0 Migration Guide\u003c/a\u003e. If you’re new to the library, check out the \u003ca href=\"https://googleapis.dev/python/logging/latest/index.html\" target=\"_blank\"\u003egoogle-cloud-logging user guide\u003c/a\u003e. If you want to learn more about observability on GCP in general, you can spin up test environments using \u003ca href=\"https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox\"\u003eCloud Ops Sandbox\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFinally, if you have any feedback about the latest release, have new feature requests, or would like to make any contributions, feel free to open issues on \u003ca href=\"https://github.com/googleapis/python-logging\" target=\"_blank\"\u003eour GitHub repo\u003c/a\u003e. The Google Cloud Logging libraries are open source software, and we welcome new contributors!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Public-Sector-Momentum.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eTake the first step toward SRE with Cloud Operations Sandbox\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eSpin up the Cloud Operations Sandbox to see how Google’s logging, monitoring, tracing, profiling and debugging can kickstart your SRE pra...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/logging.max-2200x2200.jpg",
      "date_published": "2022-02-07T18:00:00Z",
      "author": {
        "name": "\u003cname\u003eDaniel Sanche\u003c/name\u003e\u003ctitle\u003eDeveloper Programs Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/identity-security/simplify-saas-scale-tls-certificate-management/",
      "title": "Introducing Certificate Manager to simplify SaaS scale TLS and certificate management",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;We\u0026amp;#8217;re excited to announce the public preview of Certificate Manager and its integration with External HTTPS Load Balancing. Certificate Manager enables you to use External HTTPS Load Balancing with as many certificates or domains as you need. You can bring your own TLS certificates and keys if you have an existing certificate lifecycle management solution you\u0026#39;d like to use with Google Cloud, or enjoy the convenience of our fully Managed TLS offerings.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Extend the security and performance of the Google network to your customers\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Certificate Manager brings support for multiple certificates per customer. When\u0026amp;#160; coupled with our \u0026lt;a href=\u0026#34;https://cloud.google.com/load-balancing/docs/load-balancing-overview\u0026#34;\u0026gt;global anycast load balancing solution\u0026lt;/a\u0026gt; with automated autoscaling and failover, you now have a powerful platform for building robust SaaS and PaaS offerings. This enables custom domain support for your customers with the lowest latency and the highest level of availability.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://il.linkedin.com/in/alonkochba\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Alon Kochba\u0026lt;/a\u0026gt;, the head of web performance at website-building service Wix, explained how the new features lighten their workload.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. GCP\u0026#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to rely on our own custom solution for terminating SSL,\u0026amp;#8221; Kochba said.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Customers who switch to External HTTPS Load Balancing can also now protect their SaaS users from \u0026lt;a href=\u0026#34;https://cloud.google.com/armor/docs/adaptive-protection-overview\u0026#34;\u0026gt;denial of service attacks\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/owasp-top-ten-mitigation\u0026#34;\u0026gt;OWASP Top 10 risks\u0026lt;/a\u0026gt;, and other common Web attacks by adopting \u0026lt;a href=\u0026#34;https://cloud.google.com/armor\u0026#34;\u0026gt;Cloud Armor\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;DNS authorization\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;This release also now enables you to provision your Google-managed certificates with DNS-based authorizations and have them ready to use before your load-balancing production environment is fully set up. This will help streamline the migration process to Google Cloud, for example. To create a DNS authorization, use the following command:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWe’re excited to announce the public preview of Certificate Manager and its integration with External HTTPS Load Balancing. Certificate Manager enables you to use External HTTPS Load Balancing with as many certificates or domains as you need. You can bring your own TLS certificates and keys if you have an existing certificate lifecycle management solution you\u0026#39;d like to use with Google Cloud, or enjoy the convenience of our fully Managed TLS offerings.  \u003c/p\u003e\u003ch3\u003eExtend the security and performance of the Google network to your customers\u003c/h3\u003e\u003cp\u003eCertificate Manager brings support for multiple certificates per customer. When  coupled with our \u003ca href=\"https://cloud.google.com/load-balancing/docs/load-balancing-overview\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/load-balancing/docs/load-balancing-overview\" track-metadata-module=\"post\"\u003eglobal anycast load balancing solution\u003c/a\u003e with automated autoscaling and failover, you now have a powerful platform for building robust SaaS and PaaS offerings. This enables custom domain support for your customers with the lowest latency and the highest level of availability. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://il.linkedin.com/in/alonkochba\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://il.linkedin.com\" track-metadata-module=\"post\"\u003eAlon Kochba\u003c/a\u003e, the head of web performance at website-building service Wix, explained how the new features lighten their workload. \u003c/p\u003e\u003cp\u003e“As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. GCP\u0026#39;s Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to rely on our own custom solution for terminating SSL,” Kochba said.\u003c/p\u003e\u003cp\u003eCustomers who switch to External HTTPS Load Balancing can also now protect their SaaS users from \u003ca href=\"https://cloud.google.com/armor/docs/adaptive-protection-overview\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/armor/docs/adaptive-protection-overview\" track-metadata-module=\"post\"\u003edenial of service attacks\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/architecture/owasp-top-ten-mitigation\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/owasp-top-ten-mitigation\" track-metadata-module=\"post\"\u003eOWASP Top 10 risks\u003c/a\u003e, and other common Web attacks by adopting \u003ca href=\"https://cloud.google.com/armor\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/armor\" track-metadata-module=\"post\"\u003eCloud Armor\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eDNS authorization\u003c/h3\u003e\u003cp\u003eThis release also now enables you to provision your Google-managed certificates with DNS-based authorizations and have them ready to use before your load-balancing production environment is fully set up. This will help streamline the migration process to Google Cloud, for example. To create a DNS authorization, use the following command:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe’re excited to announce the public preview of Certificate Manager and its integration with External HTTPS Load Balancing. Certificate Manager enables you to use External HTTPS Load Balancing with as many certificates or domains as you need. You can bring your own TLS certificates and keys if you have an existing certificate lifecycle management solution you'd like to use with Google Cloud, or enjoy the convenience of our fully Managed TLS offerings.  \u003c/p\u003e\u003ch3\u003eExtend the security and performance of the Google network to your customers\u003c/h3\u003e\u003cp\u003eCertificate Manager brings support for multiple certificates per customer. When  coupled with our \u003ca href=\"https://cloud.google.com/load-balancing/docs/load-balancing-overview\"\u003eglobal anycast load balancing solution\u003c/a\u003e with automated autoscaling and failover, you now have a powerful platform for building robust SaaS and PaaS offerings. This enables custom domain support for your customers with the lowest latency and the highest level of availability. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://il.linkedin.com/in/alonkochba\" target=\"_blank\"\u003eAlon Kochba\u003c/a\u003e, the head of web performance at website-building service Wix, explained how the new features lighten their workload. \u003c/p\u003e\u003cp\u003e“As a SaaS product, we need to terminate SSL for millions of custom domains and certificates. GCP's Certificate Manager and External HTTPS Load Balancing lets us do this at the edge, close to the clients, without having to rely on our own custom solution for terminating SSL,” Kochba said.\u003c/p\u003e\u003cp\u003eCustomers who switch to External HTTPS Load Balancing can also now protect their SaaS users from \u003ca href=\"https://cloud.google.com/armor/docs/adaptive-protection-overview\"\u003edenial of service attacks\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/architecture/owasp-top-ten-mitigation\"\u003eOWASP Top 10 risks\u003c/a\u003e, and other common Web attacks by adopting \u003ca href=\"https://cloud.google.com/armor\"\u003eCloud Armor\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eDNS authorization\u003c/h3\u003e\u003cp\u003eThis release also now enables you to provision your Google-managed certificates with DNS-based authorizations and have them ready to use before your load-balancing production environment is fully set up. This will help streamline the migration process to Google Cloud, for example. To create a DNS authorization, use the following command:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis command returns the CNAME record for \u003ccode\u003e_acme-challenge.\u003c/code\u003e\u003ca href=\"http://www.example.com\" target=\"_blank\"\u003e\u003ccode\u003eexample.com\u003c/code\u003e\u003c/a\u003e that you must \u003ca href=\"https://cloud.devsite.corp.google.com/certificate-manager/docs/certificates#cert-dns-auth\" target=\"_blank\"\u003eadd to your DNS configuration\u003c/a\u003e in the DNS zone of the target domain. This CNAME record points to a special Google Cloud domain, e.g.: \"\u003ccode\u003e534959-1a8a-40cf-90b6-b1f5f8d22517.2.authorize.certificatemanager.goog\u003c/code\u003e” that is used  to verify domain ownership.\u003c/p\u003e\u003cp\u003eWhen you request a certificate based on the above authorization, Cloud Certificate Manager will work with the Certificate Authority automatically to get and later renew your certificate for that domain.\u003c/p\u003e\u003ch3\u003eWildcard support\u003c/h3\u003e\u003cp\u003eThis \u003cb\u003eDNS-based domain control authorization\u003c/b\u003e also allows us to bring you support for \u003cb\u003ewildcard certificates\u003c/b\u003e. To configure the use of wildcard certificates you first must configure the DNS authorization as we’ve indicated above. Once that has been completed, you can configure the use of a wildcard certificate using the following command. Our example below is for a top-level registered domain and its wildcard subdomains.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eMonitoring for Certificate Expiration\u003c/h3\u003e\u003cp\u003eAnother new feature that will be enabled with this product  is the ability to monitor certificate expiration with \u003ca href=\"https://cloud.google.com/products/operations\"\u003eGoogle Cloud Logging\u003c/a\u003e.  Cloud Logging creates a record of certificate expiration, uses the `\u003ccode\u003ecertificatemanager.googleapis.com/Project\u003c/code\u003e` monitored resource, and is represented by the following message:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe log message is delivered every hour and contains a sample of the certificates being close to expiry or already expired.\u003c/p\u003e\u003ch3\u003ePricing\u003c/h3\u003e\u003cp\u003eThe best part is that there’s no additional charge to use the Certificate Manager for the first 100 certificates. To use more than 100 certificates with the management tools, we will charge on a per-certificate, per-month pricing structure. This empowers you to scale up to as many certificates as you need, and as cost-effectively as possible. The pricing will be enabled when the solution goes to General Availability.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"Certificate Manager pricing.png\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Certificate_Manager_prici.0999064919990490.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIt is our hope that these \u003ca href=\"https://cloud.google.com/certificate-manager/docs/how-it-works\"\u003enew features\u003c/a\u003e, combined with the programmability offered by \u003ca href=\"https://cloud.google.com/certificate-manager/docs/overview\"\u003eCertificate Manager\u003c/a\u003e, will enable you to simplify the way you deploy HTTPS and offer a more scalable and secure service to your customers.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/identity-security/10-questions-to-help-boards-safely-maximize-cloud-opportunities/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/cybersecurity_action_team_jl2RU0c.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e10 questions to help boards safely maximize cloud opportunities\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThese 10 questions from a new Google Cloud whitepaper will help boards of directors safely guide their organizations through cloud migrat...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/google_cloud_security.0999040819990817.max-2000x2000.jpg",
      "date_published": "2022-01-31T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eBabi Seal\u003c/name\u003e\u003ctitle\u003eProduct Manager, Load Balancing\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-now-ga/",
      "title": "Google Cloud Deploy, now GA, makes it easier to do continuous delivery to GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003cpromo-banner-block _nghost-c45=\"\"\u003e\u003c/promo-banner-block\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003eGoogle Cloud Deploy\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e Victor Szalvay \u003c/p\u003e\u003cp\u003e Product Manager \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e January 25, 2022 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c47=\"\"\u003e\u003cdiv _ngcontent-c47=\"\"\u003e\u003ch4 _ngcontent-c47=\"\"\u003e\u003cspan _ngcontent-c47=\"\"\u003eTry Google Cloud\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c47=\"\"\u003e\u003cspan _ngcontent-c47=\"\"\u003eStart building on Google Cloud with $300 in free credits and 20+ always free products.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c47=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"free trial\" track-metadata-eventdetail=\"https://cloud.google.com/free/\" href=\"https://cloud.google.com/free/\"\u003e\u003cspan _ngcontent-c47=\"\"\u003eFree Trial\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;Effective software delivery \u0026amp;#8212; usually achieved via continuous integration (CI) and continuous delivery (CD) \u0026amp;#8212; is a top priority for many product development teams. It\u0026amp;#8217;s easy to understand why: the \u0026lt;a href=\u0026#34;https://cloud.google.com/devops/state-of-devops\u0026#34;\u0026gt;2021 State of DevOps report\u0026lt;/a\u0026gt; found that elite performers of software delivery deployed code much more frequently than low performers, with three times fewer change-related failures. Teams who excel at modern software delivery operational practices were also 1.8 times more likely to report better business outcomes.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;You need great tools to do software delivery effectively. Without capable tooling, teams have to design, maintain, and scale their software delivery solutions on their own, which can be difficult given the breadth of continuous delivery\u0026amp;#8217;s flow control, security and audit, and integration requirements. Deploying container image artifacts adds further complexity, particularly in Kubernetes environments.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eEffective software delivery — usually achieved via continuous integration (CI) and continuous delivery (CD) — is a top priority for many product development teams. It’s easy to understand why: the \u003ca href=\"https://cloud.google.com/devops/state-of-devops\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/devops/state-of-devops\" track-metadata-module=\"post\"\u003e2021 State of DevOps report\u003c/a\u003e found that elite performers of software delivery deployed code much more frequently than low performers, with three times fewer change-related failures. Teams who excel at modern software delivery operational practices were also 1.8 times more likely to report better business outcomes.\u003c/p\u003e\u003cp\u003eYou need great tools to do software delivery effectively. Without capable tooling, teams have to design, maintain, and scale their software delivery solutions on their own, which can be difficult given the breadth of continuous delivery’s flow control, security and audit, and integration requirements. Deploying container image artifacts adds further complexity, particularly in Kubernetes environments.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;Building on your feedback and Google\u0026amp;#8217;s own best practices, we\u0026amp;#8217;ve been working on software delivery tooling that helps you meet your continuous delivery goals \u0026amp;#8212; especially with respect to \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine (GKE)\u0026lt;/a\u0026gt; environments. Today, we are pleased to announce the general availability of \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Solving for continuous delivery challenges\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;While designing Google Cloud Deploy, we talked to a number of customers to better understand the challenges they face doing continuous delivery to GKE. While a handful of themes emerged, three stood out: cost of ownership, security and audit, and measurement.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cost of ownership\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As shared in our \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke\u0026#34;\u0026gt;Preview launch post\u0026lt;/a\u0026gt; this past September, the operational cost of Kubernetes continuous delivery can be very high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, collecting key metrics, and staying current \u0026amp;#8212; to say nothing of maintenance \u0026amp;#8212; is resource-intensive and takes time away from the core business.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy also provides structure. \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\u0026#34;\u0026gt;Delivery pipelines\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology\u0026#34;\u0026gt;targets\u0026lt;/a\u0026gt; are defined \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/config-files\u0026#34;\u0026gt;declaratively\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/pipeline-instances\u0026#34;\u0026gt;retained with each release\u0026lt;/a\u0026gt;. That means if your delivery pipeline changes, the release\u0026amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eBuilding on your feedback and Google’s own best practices, we’ve been working on software delivery tooling that helps you meet your continuous delivery goals — especially with respect to \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine (GKE)\u003c/a\u003e environments. Today, we are pleased to announce the general availability of \u003ca href=\"https://cloud.google.com/deploy/\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable. \u003c/p\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003c/h3\u003e\u003cp\u003eWhile designing Google Cloud Deploy, we talked to a number of customers to better understand the challenges they face doing continuous delivery to GKE. While a handful of themes emerged, three stood out: cost of ownership, security and audit, and measurement.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eAs shared in our \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke\" track-metadata-module=\"post\"\u003ePreview launch post\u003c/a\u003e this past September, the operational cost of Kubernetes continuous delivery can be very high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, collecting key metrics, and staying current — to say nothing of maintenance — is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-metadata-module=\"post\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology\" track-metadata-module=\"post\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/config-files\" track-metadata-module=\"post\"\u003edeclaratively\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-metadata-module=\"post\"\u003eretained with each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;Whether or not you already have a continuous delivery capability, you likely already have continuous integration, approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;embraces the GKE delivery tooling ecosystems\u0026lt;/a\u0026gt; in three ways: \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating-ci\u0026#34;\u0026gt;connectivity to CI systems\u0026lt;/a\u0026gt;, support for leading configuration (\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#render\u0026#34;\u0026gt;rendering\u0026lt;/a\u0026gt;) tooling, and \u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Pub/Sub\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\u0026#34;\u0026gt;notifications\u0026lt;/a\u0026gt; to enable \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;related software delivery tooling\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;While looking for a Continuous Delivery solution we considered ArgoCD and Spinnaker, however we chose Google Cloud Deploy because it is a managed service, provided proper CD primitives and integrated seamlessly with our GKE clusters. It has empowered every team member to safely and reliably promote their code from commit all the way through to production.\u0026amp;#8221;\u0026amp;#8212;Jonathan Sokolowski, DevOps Engineer, Search.io\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;A variety of GKE roles and personas interact with continuous delivery processes. DevOps engineers are focused on release promotion and rollback decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy\u0026amp;#8217;s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWhether or not you already have a continuous delivery capability, you likely already have continuous integration, approval and/or operation workflows, and other systems that intersect with your software delivery practices. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: \u003ca href=\"https://cloud.google.com/deploy/docs/integrating-ci\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating-ci\" track-metadata-module=\"post\"\u003econnectivity to CI systems\u003c/a\u003e, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#render\" track-metadata-module=\"post\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003ePub/Sub\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-metadata-module=\"post\"\u003enotifications\u003c/a\u003e to enable \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003erelated software delivery tooling\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003ci\u003e“While looking for a Continuous Delivery solution we considered ArgoCD and Spinnaker, however we chose Google Cloud Deploy because it is a managed service, provided proper CD primitives and integrated seamlessly with our GKE clusters. It has empowered every team member to safely and reliably promote their code from commit all the way through to production.”—Jonathan Sokolowski, DevOps Engineer, Search.io\u003c/i\u003e\u003c/p\u003e\u003cp\u003eA variety of GKE roles and personas interact with continuous delivery processes. DevOps engineers are focused on release promotion and rollback decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Security and control\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy\u0026amp;#8217;s security foundations strengthen secure software supply chain practices through delivery flow control and auditability.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lots of different users interact with a software delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn\u0026amp;#8217;t always mean you can create release candidates, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Throughout, Google Cloud Deploy enables fine-grained \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/iam-roles-permissions\u0026#34;\u0026gt;restriction through IAM\u0026lt;/a\u0026gt;, with \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/securing/iam\u0026#34;\u0026gt;discrete access control\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment\u0026#34;\u0026gt;execution-level security\u0026lt;/a\u0026gt;. Google Cloud Deploy also supports \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment#deploying_to_a_private_cluster_on_a_network\u0026#34;\u0026gt;deploying to private GKE clusters\u0026lt;/a\u0026gt; and\u0026amp;#160; \u0026lt;a href=\u0026#34;https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy\u0026#34;\u0026gt;Virtual Private Cloud (VPC) Service Controls\u0026lt;/a\u0026gt; (currently in Beta) to respect security perimeters. For safeguards against unwanted approvals, you can take advantage of flow management features such as release \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/deploying-application#promoting_a_release\u0026#34;\u0026gt;promotion\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#rolling_back_a_deployment\u0026#34;\u0026gt;rollback\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\u0026#34;\u0026gt;approvals\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u0026lt;a href=\u0026#34;https://cloud.google.com/audit-logs\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/audit-logs\u0026#34;\u0026gt;audits\u0026lt;/a\u0026gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline. You can also create Google Cloud Deploy pipelines in \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/regions\u0026#34;\u0026gt;supported locations\u0026lt;/a\u0026gt; to better conform with your business needs.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Measurement\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Great tooling is only part of an effective software delivery strategy \u0026amp;#8212; you also need to know what metrics you need to measure, how, and why. By making it easier to measure software delivery performance, Google Cloud Deploy helps teams focus on software delivery optimization and achieve their desired business outcomes.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy collects and makes available \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/metrics\u0026#34;\u0026gt;built in metrics\u0026lt;/a\u0026gt; about delivery pipelines. These include deployment history and success, and also the DORA metric \u0026amp;#8216;deployment frequency.\u0026amp;#8217;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eSecurity and control\u003c/b\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy’s security foundations strengthen secure software supply chain practices through delivery flow control and auditability.\u003c/p\u003e\u003cp\u003eLots of different users interact with a software delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create release candidates, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-metadata-module=\"post\"\u003erestriction through IAM\u003c/a\u003e, with \u003ca href=\"https://cloud.google.com/deploy/docs/securing/iam\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/securing/iam\" track-metadata-module=\"post\"\u003ediscrete access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment\" track-metadata-module=\"post\"\u003eexecution-level security\u003c/a\u003e. Google Cloud Deploy also supports \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment#deploying_to_a_private_cluster_on_a_network\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment#deploying_to_a_private_cluster_on_a_network\" track-metadata-module=\"post\"\u003edeploying to private GKE clusters\u003c/a\u003e and  \u003ca href=\"https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy\" track-metadata-module=\"post\"\u003eVirtual Private Cloud (VPC) Service Controls\u003c/a\u003e (currently in Beta) to respect security perimeters. For safeguards against unwanted approvals, you can take advantage of flow management features such as release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application#promoting_a_release\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application#promoting_a_release\" track-metadata-module=\"post\"\u003epromotion\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#rolling_back_a_deployment\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#rolling_back_a_deployment\" track-metadata-module=\"post\"\u003erollback\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-metadata-module=\"post\"\u003eapprovals\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/audit-logs\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/audit-logs\" track-metadata-module=\"post\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline. You can also create Google Cloud Deploy pipelines in \u003ca href=\"https://cloud.google.com/deploy/docs/regions\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/regions\" track-metadata-module=\"post\"\u003esupported locations\u003c/a\u003e to better conform with your business needs.\u003c/p\u003e\u003cp\u003e\u003cb\u003eMeasurement\u003c/b\u003e\u003c/p\u003e\u003cp\u003eGreat tooling is only part of an effective software delivery strategy — you also need to know what metrics you need to measure, how, and why. By making it easier to measure software delivery performance, Google Cloud Deploy helps teams focus on software delivery optimization and achieve their desired business outcomes.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy collects and makes available \u003ca href=\"https://cloud.google.com/deploy/docs/metrics\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/metrics\" track-metadata-module=\"post\"\u003ebuilt in metrics\u003c/a\u003e about delivery pipelines. These include deployment history and success, and also the DORA metric ‘deployment frequency.’\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c49=\"\"\u003e\u003cdiv _ngcontent-c49=\"\" innerhtml=\"\u0026lt;p\u0026gt;Monitoring your deployed resources is another way to measure the effectiveness of your software delivery processes. To aid monitoring, Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/labels-annotations#automatic_labels_from\u0026#34;\u0026gt;automatically labels deployed Kubernetes resources\u0026lt;/a\u0026gt;, making it easier to associate\u0026amp;#160; your delivery pipelines with application performance. You can integrate application monitoring further using the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/api/reference/rest\u0026#34;\u0026gt;Google Cloud Deploy API\u0026lt;/a\u0026gt;, so you can automatically promote code if it is stable and roll it back if an anomaly is detected.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The future\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it\u0026amp;#8217;s our hope that Google Cloud Deploy will help you implement complete CI/CD pipelines. And we\u0026amp;#8217;re just getting started! Stay tuned as we introduce exciting new capabilities and features to Google Cloud Deploy in the months to come.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the meantime, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;product page\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/quickstart-basic\u0026#34;\u0026gt;quickstart\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt;. Finally, If you have feedback on Google Cloud Deploy, you can \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;join the conversation\u0026lt;/a\u0026gt;. We look forward to hearing from you!\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eMonitoring your deployed resources is another way to measure the effectiveness of your software delivery processes. To aid monitoring, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/labels-annotations#automatic_labels_from\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/labels-annotations#automatic_labels_from\" track-metadata-module=\"post\"\u003eautomatically labels deployed Kubernetes resources\u003c/a\u003e, making it easier to associate  your delivery pipelines with application performance. You can integrate application monitoring further using the \u003ca href=\"https://cloud.google.com/deploy/docs/api/reference/rest\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/api/reference/rest\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy API\u003c/a\u003e, so you can automatically promote code if it is stable and roll it back if an anomaly is detected. \u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you implement complete CI/CD pipelines. And we’re just getting started! Stay tuned as we introduce exciting new capabilities and features to Google Cloud Deploy in the months to come. \u003c/p\u003e\u003cp\u003eIn the meantime, check out the \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-type=\"inline link\" track-name=\"31\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-metadata-module=\"post\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\" track-type=\"inline link\" track-name=\"32\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\" track-type=\"inline link\" track-name=\"33\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c48=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eEffective software delivery — usually achieved via continuous integration (CI) and continuous delivery (CD) — is a top priority for many product development teams. It’s easy to understand why: the \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003e2021 State of DevOps report\u003c/a\u003e found that elite performers of software delivery deployed code much more frequently than low performers, with three times fewer change-related failures. Teams who excel at modern software delivery operational practices were also 1.8 times more likely to report better business outcomes.\u003c/p\u003e\u003cp\u003eYou need great tools to do software delivery effectively. Without capable tooling, teams have to design, maintain, and scale their software delivery solutions on their own, which can be difficult given the breadth of continuous delivery’s flow control, security and audit, and integration requirements. Deploying container image artifacts adds further complexity, particularly in Kubernetes environments.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_cloud_deploy.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"1 cloud deploy.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_cloud_deploy.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eBuilding on your feedback and Google’s own best practices, we’ve been working on software delivery tooling that helps you meet your continuous delivery goals — especially with respect to \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine (GKE)\u003c/a\u003e environments. Today, we are pleased to announce the general availability of \u003ca href=\"https://cloud.google.com/deploy/\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable. \u003c/p\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003c/h3\u003e\u003cp\u003eWhile designing Google Cloud Deploy, we talked to a number of customers to better understand the challenges they face doing continuous delivery to GKE. While a handful of themes emerged, three stood out: cost of ownership, security and audit, and measurement.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eAs shared in our \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke\"\u003ePreview launch post\u003c/a\u003e this past September, the operational cost of Kubernetes continuous delivery can be very high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, collecting key metrics, and staying current — to say nothing of maintenance — is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003eretained with each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"4 Delivery pipeline metrics.gif\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/4_Delivery_pipeline_metrics.gif\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003ePromoting a release\u003c/i\u003e\u003cbr/\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhether or not you already have a continuous delivery capability, you likely already have continuous integration, approval and/or operation workflows, and other systems that intersect with your software delivery practices. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: \u003ca href=\"https://cloud.google.com/deploy/docs/integrating-ci\"\u003econnectivity to CI systems\u003c/a\u003e, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003enotifications\u003c/a\u003e to enable \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003erelated software delivery tooling\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003ci\u003e“While looking for a Continuous Delivery solution we considered ArgoCD and Spinnaker, however we chose Google Cloud Deploy because it is a managed service, provided proper CD primitives and integrated seamlessly with our GKE clusters. It has empowered every team member to safely and reliably promote their code from commit all the way through to production.”—Jonathan Sokolowski, DevOps Engineer, Search.io\u003c/i\u003e\u003c/p\u003e\u003cp\u003eA variety of GKE roles and personas interact with continuous delivery processes. DevOps engineers are focused on release promotion and rollback decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"3 Deployment approvals.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Deployment_approvals.max-1000x1000.jpg\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eDeployment approvals\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and control\u003c/b\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy’s security foundations strengthen secure software supply chain practices through delivery flow control and auditability.\u003c/p\u003e\u003cp\u003eLots of different users interact with a software delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create release candidates, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003erestriction through IAM\u003c/a\u003e, with \u003ca href=\"https://cloud.google.com/deploy/docs/securing/iam\"\u003ediscrete access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. Google Cloud Deploy also supports \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment#deploying_to_a_private_cluster_on_a_network\"\u003edeploying to private GKE clusters\u003c/a\u003e and  \u003ca href=\"https://cloud.google.com/vpc-service-controls/docs/supported-products#table_deploy\"\u003eVirtual Private Cloud (VPC) Service Controls\u003c/a\u003e (currently in Beta) to respect security perimeters. For safeguards against unwanted approvals, you can take advantage of flow management features such as release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application#promoting_a_release\"\u003epromotion\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#rolling_back_a_deployment\"\u003erollback\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline. You can also create Google Cloud Deploy pipelines in \u003ca href=\"https://cloud.google.com/deploy/docs/regions\"\u003esupported locations\u003c/a\u003e to better conform with your business needs.\u003c/p\u003e\u003cp\u003e\u003cb\u003eMeasurement\u003c/b\u003e\u003c/p\u003e\u003cp\u003eGreat tooling is only part of an effective software delivery strategy — you also need to know what metrics you need to measure, how, and why. By making it easier to measure software delivery performance, Google Cloud Deploy helps teams focus on software delivery optimization and achieve their desired business outcomes.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy collects and makes available \u003ca href=\"https://cloud.google.com/deploy/docs/metrics\"\u003ebuilt in metrics\u003c/a\u003e about delivery pipelines. These include deployment history and success, and also the DORA metric ‘deployment frequency.’\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"2 Promoting a release.gif\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/2_Promoting_a_release.gif\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003eDelivery pipeline metrics\u003c/i\u003e\u003cbr/\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eMonitoring your deployed resources is another way to measure the effectiveness of your software delivery processes. To aid monitoring, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/labels-annotations#automatic_labels_from\"\u003eautomatically labels deployed Kubernetes resources\u003c/a\u003e, making it easier to associate  your delivery pipelines with application performance. You can integrate application monitoring further using the \u003ca href=\"https://cloud.google.com/deploy/docs/api/reference/rest\"\u003eGoogle Cloud Deploy API\u003c/a\u003e, so you can automatically promote code if it is stable and roll it back if an anomaly is detected. \u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you implement complete CI/CD pipelines. And we’re just getting started! Stay tuned as we introduce exciting new capabilities and features to Google Cloud Deploy in the months to come. \u003c/p\u003e\u003cp\u003eIn the meantime, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eIntroducing Google Cloud Deploy: Managed continuous delivery to GKE\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe new Google Cloud Deploy managed services makes it easier to do continuous delivery to Google Kubernetes Engine, and soon, Anthos.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/blog_post_header.max-2200x2200.jpg",
      "date_published": "2022-01-25T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eVictor Szalvay\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/implementing-google-cloud-devops-for-your-cloud-native-organization/",
      "title": "DevOps for tech companies and startups: Learn from over 32,000 professionals on how to drive success with Google Cloud’s DORA research",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c16=\"\"\u003e\u003cdiv _ngcontent-c16=\"\" innerhtml=\"\u0026lt;p\u0026gt;Many technology-driven organizations and startups use \u0026lt;a href=\u0026#34;https://cloud.google.com/devops\u0026#34;\u0026gt;DevOps\u0026lt;/a\u0026gt; as a business enabler, allowing them to bring ideas to market quickly, increase developer productivity, and increase their customer base. Adopting DevOps workflows can drive success but many companies continue to struggle with how to get started or optimize the DevOps tools they currently have incorporated.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud\u0026amp;#8217;s DevOps Research and Assessment (\u0026lt;a href=\u0026#34;https://cloud.google.com/devops\u0026#34;\u0026gt;DORA\u0026lt;/a\u0026gt;) team helps organizations to deploy faster, scale on demand, and balance costs by providing essential tools and resources for you to succeed. Over the past seven years, our DORA team has surveyed more than 32,000 professionals worldwide via our yearly \u0026lt;a href=\u0026#34;https://cloud.google.com/devops/state-of-devops\u0026#34;\u0026gt;Accelerate State of DevOps reports\u0026lt;/a\u0026gt; (SODR). As the largest and longest-running research of its kind, the SODR provides data-driven industry insights that examine the capabilities and practices that drive software delivery and operational and organizational performance - no matter the industry.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Read below to learn more from our DORA team about how and why your organization should focus on DevOps this year:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Benchmark your team, identify improvement opportunities\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Our \u0026lt;a href=\u0026#34;https://inthecloud.withgoogle.com/devops-quick-check/dl-cd.html?utm_source=google_owned_website\u0026amp;amp;utm_medium=et\u0026amp;amp;utm_campaign=FY20-Q3-global-demandgen-website-other-gcp_gtm_cost_amp_devops_quick_check_mc\u0026amp;amp;utm_content=app_mod_lp_cta\u0026amp;amp;utm_term=-\u0026amp;amp;_ga=2.131280267.562104682.1627912649-1057981864.1627522111\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DevOps Quick Check\u0026lt;/a\u0026gt; is based on DORA research and allows companies to gauge their DevOps implementation with just five multiple choice questions. The DevOps Quick Check uses the \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance\u0026#34;\u0026gt;four key DevOps metrics\u0026lt;/a\u0026gt; to help your team assess your current performance, compare your performance to others in your industry, and identify which capabilities are most likely to impact your performance. Teams in every industry recognize the value of quickly taking code from development to production and are looking for ways to improve their organizations\u0026#39; agility. Compromising stability is not an option. The data from the research program shows that speed and stability go hand-in-hand, in fact elite performing teams are nearly twice as likely to have increased software delivery performance and achieve 6570x faster lead-time-to-deploy changes.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Increasing developer productivity\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Along with driving software delivery performance, DevOps helps to increase developer productivity by reducing burnout - something that is a top priority for many organizations. According to the SODR, 89% of respondents worked from home during the pandemic but teams with a\u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture\u0026#34;\u0026gt; generative team culture,\u0026lt;/a\u0026gt; with people who had feelings of inclusion and belonging within their team, were half as likely to experience burnout. A generative culture along with easy to use \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/devops/devops-tech-teams-empowered-to-choose-tools\u0026#34;\u0026gt;developer tools \u0026lt;/a\u0026gt;help to streamline developers\u0026amp;#8217; workflows and the process of working with cloud infrastructure.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Developer tools that keep developers focused on what they do best, writing code,\u0026amp;#160; is vital to increase developer productivity. Companies should utilize tools that ensure developers spend as little time as possible containerizing applications while increasing automation are paramount.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We commonly see developers leverage \u0026lt;a href=\u0026#34;https://docs.docker.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Docker\u0026lt;/a\u0026gt; to bring modern applications to life, and there are many benefits of doing so, especially in terms of portability. However, using Docker increases the operational burden on developers. Whereas those that leverage \u0026lt;a href=\u0026#34;https://www.youtube.com/watch?v=suhCr5W_bFc\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Buildpacks\u0026lt;/a\u0026gt; are able to reduce operational burden while supporting enterprise operators who manage apps at scale. They are able to do this because Buildpacks allows code to go straight from source to production, in addition to making it easier to meet security and compliance requirements without developer intervention.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Plus when developers want to speed up the deployment process with flexibility they have the option to leverage \u0026lt;a href=\u0026#34;https://cloud.google.com/run\u0026#34;\u0026gt;Cloud Run\u0026lt;/a\u0026gt;, our fully managed serverless platform offering, which fully supports the use of Buildpacks. Below are some benefits developers and operators can see from Cloud Run:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Increased Productivity\u0026lt;/b\u0026gt;: make it easier for developers to onboard more quickly and deploy faster\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;i\u0026gt;Hiring\u0026lt;/i\u0026gt;: choose a platform that makes all developers productive. Serverless platforms, like Cloud Run, help developers who know fewer programming languages become\u0026amp;#160; productive without the need to also be an IT expert\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Decreased Time to Market\u0026lt;/b\u0026gt;: accelerate software releases and value creation to customers that ultimately increase revenue and customer loyalty\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Decreased Cost\u0026lt;/b\u0026gt;: Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously - so you only pay for what you use\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;Whether you choose Cloud Run or another offering - it\u0026amp;#8217;s not just which cloud infrastructure you choose, but how you implement cloud services that really matters. This is especially important when it comes to being able to scale quickly and efficiently. In the SODR, we found that Elite performers were 3.5 times more likely to have met all essential \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\u0026#34;\u0026gt;NIST cloud characteristics\u0026lt;/a\u0026gt;. In terms of scaling, the characteristic of rapid elasticity is key to rapidly scaling outward or inward with demand. In other words, it is very important that your capabilities can be elastically provisioned and released - so no matter how much you grow, your customers always have access to your services.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The Business Case for DevOps\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Maybe you\u0026#39;re convinced that achieving better speed and stability will help your team but how do you convince your boss?\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\u0026#34;\u0026gt;ROI of DevOps Transformation\u0026lt;/a\u0026gt; provides IT and business decision makers an industry backed, data-driven foundational basis for measuring their investment in DevOps. We found that money saved from DevOps transformation varies from $10M to $259M a year with a return on investment of approximately 30 days. You can use the metrics you provided in the \u0026lt;a href=\u0026#34;https://www.devops-research.com/quickcheck.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DevOps Quick Check\u0026lt;/a\u0026gt; along with some additional information to get an estimate on your team\u0026#39;s potential return.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Google Cloud DevOps Awards\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Using a collection of these resources along with an objective assessment of how your team is doing, your organization will be able to get quick insights into improvement areas. Change your work by improving these capabilities to deliver more innovation to your customers and improve the speed and stability of your software delivery.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Now that we have shared some of our DevOps best practices with you, we would love to hear about how you are transforming your organization with DevOps. During our awards celebration in March we will be celebrating how our most advanced teams are using DevOps, so tell us about the positive impact that DevOps has had on your teams, customers, and organization. \u0026lt;a href=\u0026#34;https://cloud.google.com/awards/devops\u0026#34;\u0026gt;Enter your submission for the Google Cloud DevOps Awards\u0026lt;/a\u0026gt; today. But don\u0026#39;t delay! The deadline for submissions is January 31, 2022.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;What questions do you have about incorporating DevOps practices into your daily work? \u0026lt;a href=\u0026#34;https://inthecloud.withgoogle.com/born-digital/dl-cd.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Reach out to our experts at Google Cloud.\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eMany technology-driven organizations and startups use \u003ca href=\"https://cloud.google.com/devops\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/devops\" track-metadata-module=\"post\"\u003eDevOps\u003c/a\u003e as a business enabler, allowing them to bring ideas to market quickly, increase developer productivity, and increase their customer base. Adopting DevOps workflows can drive success but many companies continue to struggle with how to get started or optimize the DevOps tools they currently have incorporated.\u003c/p\u003e\u003cp\u003eGoogle Cloud’s DevOps Research and Assessment (\u003ca href=\"https://cloud.google.com/devops\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/devops\" track-metadata-module=\"post\"\u003eDORA\u003c/a\u003e) team helps organizations to deploy faster, scale on demand, and balance costs by providing essential tools and resources for you to succeed. Over the past seven years, our DORA team has surveyed more than 32,000 professionals worldwide via our yearly \u003ca href=\"https://cloud.google.com/devops/state-of-devops\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/devops/state-of-devops\" track-metadata-module=\"post\"\u003eAccelerate State of DevOps reports\u003c/a\u003e (SODR). As the largest and longest-running research of its kind, the SODR provides data-driven industry insights that examine the capabilities and practices that drive software delivery and operational and organizational performance - no matter the industry. \u003c/p\u003e\u003cp\u003eRead below to learn more from our DORA team about how and why your organization should focus on DevOps this year: \u003c/p\u003e\u003ch3\u003eBenchmark your team, identify improvement opportunities\u003c/h3\u003e\u003cp\u003eOur \u003ca href=\"https://inthecloud.withgoogle.com/devops-quick-check/dl-cd.html?utm_source=google_owned_website\u0026amp;utm_medium=et\u0026amp;utm_campaign=FY20-Q3-global-demandgen-website-other-gcp_gtm_cost_amp_devops_quick_check_mc\u0026amp;utm_content=app_mod_lp_cta\u0026amp;utm_term=-\u0026amp;_ga=2.131280267.562104682.1627912649-1057981864.1627522111\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://inthecloud.withgoogle.com\" track-metadata-module=\"post\"\u003eDevOps Quick Check\u003c/a\u003e is based on DORA research and allows companies to gauge their DevOps implementation with just five multiple choice questions. The DevOps Quick Check uses the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance\" track-metadata-module=\"post\"\u003efour key DevOps metrics\u003c/a\u003e to help your team assess your current performance, compare your performance to others in your industry, and identify which capabilities are most likely to impact your performance. Teams in every industry recognize the value of quickly taking code from development to production and are looking for ways to improve their organizations\u0026#39; agility. Compromising stability is not an option. The data from the research program shows that speed and stability go hand-in-hand, in fact elite performing teams are nearly twice as likely to have increased software delivery performance and achieve 6570x faster lead-time-to-deploy changes.\u003c/p\u003e\u003ch3\u003eIncreasing developer productivity\u003c/h3\u003e\u003cp\u003eAlong with driving software delivery performance, DevOps helps to increase developer productivity by reducing burnout - something that is a top priority for many organizations. According to the SODR, 89% of respondents worked from home during the pandemic but teams with a\u003ca href=\"https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture\" track-metadata-module=\"post\"\u003e generative team culture,\u003c/a\u003e with people who had feelings of inclusion and belonging within their team, were half as likely to experience burnout. A generative culture along with easy to use \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-teams-empowered-to-choose-tools\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/devops/devops-tech-teams-empowered-to-choose-tools\" track-metadata-module=\"post\"\u003edeveloper tools \u003c/a\u003ehelp to streamline developers’ workflows and the process of working with cloud infrastructure.\u003c/p\u003e\u003cp\u003eDeveloper tools that keep developers focused on what they do best, writing code,  is vital to increase developer productivity. Companies should utilize tools that ensure developers spend as little time as possible containerizing applications while increasing automation are paramount. \u003c/p\u003e\u003cp\u003eWe commonly see developers leverage \u003ca href=\"https://docs.docker.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://docs.docker.com\" track-metadata-module=\"post\"\u003eDocker\u003c/a\u003e to bring modern applications to life, and there are many benefits of doing so, especially in terms of portability. However, using Docker increases the operational burden on developers. Whereas those that leverage \u003ca href=\"https://www.youtube.com/watch?v=suhCr5W_bFc\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://www.youtube.com\" track-metadata-module=\"post\"\u003eBuildpacks\u003c/a\u003e are able to reduce operational burden while supporting enterprise operators who manage apps at scale. They are able to do this because Buildpacks allows code to go straight from source to production, in addition to making it easier to meet security and compliance requirements without developer intervention. \u003c/p\u003e\u003cp\u003ePlus when developers want to speed up the deployment process with flexibility they have the option to leverage \u003ca href=\"https://cloud.google.com/run\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/run\" track-metadata-module=\"post\"\u003eCloud Run\u003c/a\u003e, our fully managed serverless platform offering, which fully supports the use of Buildpacks. Below are some benefits developers and operators can see from Cloud Run:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eIncreased Productivity\u003c/b\u003e: make it easier for developers to onboard more quickly and deploy faster \u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003ci\u003eHiring\u003c/i\u003e: choose a platform that makes all developers productive. Serverless platforms, like Cloud Run, help developers who know fewer programming languages become  productive without the need to also be an IT expert \u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDecreased Time to Market\u003c/b\u003e: accelerate software releases and value creation to customers that ultimately increase revenue and customer loyalty\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDecreased Cost\u003c/b\u003e: Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously - so you only pay for what you use\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eWhether you choose Cloud Run or another offering - it’s not just which cloud infrastructure you choose, but how you implement cloud services that really matters. This is especially important when it comes to being able to scale quickly and efficiently. In the SODR, we found that Elite performers were 3.5 times more likely to have met all essential \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\" track-metadata-module=\"post\"\u003eNIST cloud characteristics\u003c/a\u003e. In terms of scaling, the characteristic of rapid elasticity is key to rapidly scaling outward or inward with demand. In other words, it is very important that your capabilities can be elastically provisioned and released - so no matter how much you grow, your customers always have access to your services. \u003c/p\u003e\u003ch3\u003eThe Business Case for DevOps\u003c/h3\u003e\u003cp\u003eMaybe you\u0026#39;re convinced that achieving better speed and stability will help your team but how do you convince your boss?\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\" track-metadata-module=\"post\"\u003eROI of DevOps Transformation\u003c/a\u003e provides IT and business decision makers an industry backed, data-driven foundational basis for measuring their investment in DevOps. We found that money saved from DevOps transformation varies from $10M to $259M a year with a return on investment of approximately 30 days. You can use the metrics you provided in the \u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDevOps Quick Check\u003c/a\u003e along with some additional information to get an estimate on your team\u0026#39;s potential return.\u003c/p\u003e\u003ch3\u003eGoogle Cloud DevOps Awards\u003c/h3\u003e\u003cp\u003eUsing a collection of these resources along with an objective assessment of how your team is doing, your organization will be able to get quick insights into improvement areas. Change your work by improving these capabilities to deliver more innovation to your customers and improve the speed and stability of your software delivery.\u003c/p\u003e\u003cp\u003eNow that we have shared some of our DevOps best practices with you, we would love to hear about how you are transforming your organization with DevOps. During our awards celebration in March we will be celebrating how our most advanced teams are using DevOps, so tell us about the positive impact that DevOps has had on your teams, customers, and organization. \u003ca href=\"https://cloud.google.com/awards/devops\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/awards/devops\" track-metadata-module=\"post\"\u003eEnter your submission for the Google Cloud DevOps Awards\u003c/a\u003e today. But don\u0026#39;t delay! The deadline for submissions is January 31, 2022.\u003c/p\u003e\u003cp\u003eWhat questions do you have about incorporating DevOps practices into your daily work? \u003ca href=\"https://inthecloud.withgoogle.com/born-digital/dl-cd.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://inthecloud.withgoogle.com\" track-metadata-module=\"post\"\u003eReach out to our experts at Google Cloud.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eMany technology-driven organizations and startups use \u003ca href=\"https://cloud.google.com/devops\"\u003eDevOps\u003c/a\u003e as a business enabler, allowing them to bring ideas to market quickly, increase developer productivity, and increase their customer base. Adopting DevOps workflows can drive success but many companies continue to struggle with how to get started or optimize the DevOps tools they currently have incorporated.\u003c/p\u003e\u003cp\u003eGoogle Cloud’s DevOps Research and Assessment (\u003ca href=\"https://cloud.google.com/devops\"\u003eDORA\u003c/a\u003e) team helps organizations to deploy faster, scale on demand, and balance costs by providing essential tools and resources for you to succeed. Over the past seven years, our DORA team has surveyed more than 32,000 professionals worldwide via our yearly \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003eAccelerate State of DevOps reports\u003c/a\u003e (SODR). As the largest and longest-running research of its kind, the SODR provides data-driven industry insights that examine the capabilities and practices that drive software delivery and operational and organizational performance - no matter the industry. \u003c/p\u003e\u003cp\u003eRead below to learn more from our DORA team about how and why your organization should focus on DevOps this year: \u003c/p\u003e\u003ch3\u003eBenchmark your team, identify improvement opportunities\u003c/h3\u003e\u003cp\u003eOur \u003ca href=\"https://inthecloud.withgoogle.com/devops-quick-check/dl-cd.html?utm_source=google_owned_website\u0026amp;utm_medium=et\u0026amp;utm_campaign=FY20-Q3-global-demandgen-website-other-gcp_gtm_cost_amp_devops_quick_check_mc\u0026amp;utm_content=app_mod_lp_cta\u0026amp;utm_term=-\u0026amp;_ga=2.131280267.562104682.1627912649-1057981864.1627522111\" target=\"_blank\"\u003eDevOps Quick Check\u003c/a\u003e is based on DORA research and allows companies to gauge their DevOps implementation with just five multiple choice questions. The DevOps Quick Check uses the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance\"\u003efour key DevOps metrics\u003c/a\u003e to help your team assess your current performance, compare your performance to others in your industry, and identify which capabilities are most likely to impact your performance. Teams in every industry recognize the value of quickly taking code from development to production and are looking for ways to improve their organizations' agility. Compromising stability is not an option. The data from the research program shows that speed and stability go hand-in-hand, in fact elite performing teams are nearly twice as likely to have increased software delivery performance and achieve 6570x faster lead-time-to-deploy changes.\u003c/p\u003e\u003ch3\u003eIncreasing developer productivity\u003c/h3\u003e\u003cp\u003eAlong with driving software delivery performance, DevOps helps to increase developer productivity by reducing burnout - something that is a top priority for many organizations. According to the SODR, 89% of respondents worked from home during the pandemic but teams with a\u003ca href=\"https://cloud.google.com/architecture/devops/devops-culture-westrum-organizational-culture\"\u003egenerative team culture,\u003c/a\u003e with people who had feelings of inclusion and belonging within their team, were half as likely to experience burnout. A generative culture along with easy to use \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-teams-empowered-to-choose-tools\"\u003edeveloper tools\u003c/a\u003ehelp to streamline developers’ workflows and the process of working with cloud infrastructure.\u003c/p\u003e\u003cp\u003eDeveloper tools that keep developers focused on what they do best, writing code,  is vital to increase developer productivity. Companies should utilize tools that ensure developers spend as little time as possible containerizing applications while increasing automation are paramount. \u003c/p\u003e\u003cp\u003eWe commonly see developers leverage \u003ca href=\"https://docs.docker.com/\" target=\"_blank\"\u003eDocker\u003c/a\u003e to bring modern applications to life, and there are many benefits of doing so, especially in terms of portability. However, using Docker increases the operational burden on developers. Whereas those that leverage \u003ca href=\"https://www.youtube.com/watch?v=suhCr5W_bFc\" target=\"_blank\"\u003eBuildpacks\u003c/a\u003e are able to reduce operational burden while supporting enterprise operators who manage apps at scale. They are able to do this because Buildpacks allows code to go straight from source to production, in addition to making it easier to meet security and compliance requirements without developer intervention. \u003c/p\u003e\u003cp\u003ePlus when developers want to speed up the deployment process with flexibility they have the option to leverage \u003ca href=\"https://cloud.google.com/run\"\u003eCloud Run\u003c/a\u003e, our fully managed serverless platform offering, which fully supports the use of Buildpacks. Below are some benefits developers and operators can see from Cloud Run:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eIncreased Productivity\u003c/b\u003e: make it easier for developers to onboard more quickly and deploy faster \u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003ci\u003eHiring\u003c/i\u003e: choose a platform that makes all developers productive. Serverless platforms, like Cloud Run, help developers who know fewer programming languages become  productive without the need to also be an IT expert \u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDecreased Time to Market\u003c/b\u003e: accelerate software releases and value creation to customers that ultimately increase revenue and customer loyalty\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDecreased Cost\u003c/b\u003e: Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously - so you only pay for what you use\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eWhether you choose Cloud Run or another offering - it’s not just which cloud infrastructure you choose, but how you implement cloud services that really matters. This is especially important when it comes to being able to scale quickly and efficiently. In the SODR, we found that Elite performers were 3.5 times more likely to have met all essential \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\"\u003eNIST cloud characteristics\u003c/a\u003e. In terms of scaling, the characteristic of rapid elasticity is key to rapidly scaling outward or inward with demand. In other words, it is very important that your capabilities can be elastically provisioned and released - so no matter how much you grow, your customers always have access to your services. \u003c/p\u003e\u003ch3\u003eThe Business Case for DevOps\u003c/h3\u003e\u003cp\u003eMaybe you're convinced that achieving better speed and stability will help your team but how do you convince your boss?\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\"\u003eROI of DevOps Transformation\u003c/a\u003e provides IT and business decision makers an industry backed, data-driven foundational basis for measuring their investment in DevOps. We found that money saved from DevOps transformation varies from $10M to $259M a year with a return on investment of approximately 30 days. You can use the metrics you provided in the \u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\"\u003eDevOps Quick Check\u003c/a\u003e along with some additional information to get an estimate on your team's potential return.\u003c/p\u003e\u003ch3\u003eGoogle Cloud DevOps Awards\u003c/h3\u003e\u003cp\u003eUsing a collection of these resources along with an objective assessment of how your team is doing, your organization will be able to get quick insights into improvement areas. Change your work by improving these capabilities to deliver more innovation to your customers and improve the speed and stability of your software delivery.\u003c/p\u003e\u003cp\u003eNow that we have shared some of our DevOps best practices with you, we would love to hear about how you are transforming your organization with DevOps. During our awards celebration in March we will be celebrating how our most advanced teams are using DevOps, so tell us about the positive impact that DevOps has had on your teams, customers, and organization. \u003ca href=\"https://cloud.google.com/awards/devops\"\u003eEnter your submission for the Google Cloud DevOps Awards\u003c/a\u003e today. But don't delay! The deadline for submissions is January 31, 2022.\u003c/p\u003e\u003cp\u003eWhat questions do you have about incorporating DevOps practices into your daily work? \u003ca href=\"https://inthecloud.withgoogle.com/born-digital/dl-cd.html\" target=\"_blank\"\u003eReach out to our experts at Google Cloud.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_C_Rnd3_n7MW7mI.max-2200x2200.jpg",
      "date_published": "2022-01-18T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eFer De Oliveira\u003c/name\u003e\u003ctitle\u003eHead, Serverless Scale Specialist, NorthAM\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/databases/understanding-firestore-performance-with-key-visualizer/",
      "title": "Understanding Firestore performance with Key Visualizer",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c16=\"\"\u003e\u003cdiv _ngcontent-c16=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;http://cloud/blog/topics/developers-practitioners/all-you-need-know-about-firestore-cheatsheet\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Firestore\u0026lt;/a\u0026gt; is a serverless, scalable, NoSQL document database. It is ideal for rapid and flexible web and mobile application development, and uniquely supports real-time client device syncing to the database.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To get the best performance out of \u0026lt;a href=\u0026#34;https://cloud.google.com/firestore\u0026#34;\u0026gt;Firestore\u0026lt;/a\u0026gt;, while also making the most out of Firestore\u0026#39;s automatic scaling and load balancing features, you need to make sure the data layout of your application allows requests to be processed optimally, particularly as your user traffic increases. There are some subtleties to be aware of when it comes to what could happen when traffic ramps up, and to help make this easier to identify, we\u0026amp;#8217;re announcing the General Availability of \u0026lt;a href=\u0026#34;https://cloud.google.com/firestore/docs/key-visualizer\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Key Visualizer\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;, an interactive, performance monitoring tool for Firestore.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Key Visualizer generates visual reports based on Firestore documents accessed over time, that will help you understand and optimize the access patterns of your database, as well as troubleshoot performance issues. With Key Visualizer, you can iteratively design a data model or improve your existing application\u0026amp;#8217;s data usage pattern.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Tip: While Key Visualizer can be used with production databases, it\u0026amp;#8217;s best to identify performance issues prior to rolling out changes in production. Consider running application load tests with Firestore in a pre-production environment, and using Key Visualizer to identify potential issues.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Viewing a visualization\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The Key Visualizer tool is available to all Firestore customers. Visualizations are generated at every hour boundary, covering data for the preceding two hours. Visualizations are generated as long as overall database traffic during a selected period meets the \u0026lt;a href=\u0026#34;https://cloud.google.com/firestore/docs/key-visualizer#scan_eligibility\u0026#34;\u0026gt;scan eligibility\u0026lt;/a\u0026gt; criteria.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To get an overview of activity using Key Visualizer, first select a two-hour time period and review the heatmap for the \u0026amp;#34;Total ops/s\u0026amp;#34; metric. This view estimates the number of operations per second and how they are distributed across your database. Total ops/s is an estimated sum of write, lookup, and query operations averaged by seconds.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Firestore automatically scales using a technique called range sharding. When using Firestore, you model data in the form of documents stored in hierarchies of collections. The collection hierarchy and document ID is translated to a single key for each document. Documents are logically stored and ordered lexicographically by this key. We use the term \u0026amp;#34;key range\u0026amp;#34; to refer to a range of keys. The full key range is then automatically split up as-needed, driven by storage and traffic load, and served by many replicated servers inside of Firestore.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The following example of Key Visualizer visualization shows a heatmap where there are some major differences in the usage pattern across the database. The X-axis is time, and the Y-axis is the key range for your database, broken down into buckets by traffic.\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Ranges shown in dark colors have little or no activity.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Ranges in bright colors have significantly more activity. In the example below, you can see the \u0026amp;#34;Bar\u0026amp;#34; and \u0026amp;#34;Qux\u0026amp;#34; collections going beyond 50 operations per second for some period of time.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\"\u003e\u003cp\u003e\u003ca href=\"http://cloud/blog/topics/developers-practitioners/all-you-need-know-about-firestore-cheatsheet\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"http://cloud\" track-metadata-module=\"post\"\u003eFirestore\u003c/a\u003e is a serverless, scalable, NoSQL document database. It is ideal for rapid and flexible web and mobile application development, and uniquely supports real-time client device syncing to the database.\u003c/p\u003e\u003cp\u003eTo get the best performance out of \u003ca href=\"https://cloud.google.com/firestore\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/firestore\" track-metadata-module=\"post\"\u003eFirestore\u003c/a\u003e, while also making the most out of Firestore\u0026#39;s automatic scaling and load balancing features, you need to make sure the data layout of your application allows requests to be processed optimally, particularly as your user traffic increases. There are some subtleties to be aware of when it comes to what could happen when traffic ramps up, and to help make this easier to identify, we’re announcing the General Availability of \u003ca href=\"https://cloud.google.com/firestore/docs/key-visualizer\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/firestore/docs/key-visualizer\" track-metadata-module=\"post\"\u003e\u003cb\u003eKey Visualizer\u003c/b\u003e\u003c/a\u003e, an interactive, performance monitoring tool for Firestore.\u003c/p\u003e\u003cp\u003eKey Visualizer generates visual reports based on Firestore documents accessed over time, that will help you understand and optimize the access patterns of your database, as well as troubleshoot performance issues. With Key Visualizer, you can iteratively design a data model or improve your existing application’s data usage pattern.\u003c/p\u003e\u003cp\u003eTip: While Key Visualizer can be used with production databases, it’s best to identify performance issues prior to rolling out changes in production. Consider running application load tests with Firestore in a pre-production environment, and using Key Visualizer to identify potential issues.\u003c/p\u003e\u003ch3\u003eViewing a visualization\u003c/h3\u003e\u003cp\u003eThe Key Visualizer tool is available to all Firestore customers. Visualizations are generated at every hour boundary, covering data for the preceding two hours. Visualizations are generated as long as overall database traffic during a selected period meets the \u003ca href=\"https://cloud.google.com/firestore/docs/key-visualizer#scan_eligibility\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/firestore/docs/key-visualizer#scan_eligibility\" track-metadata-module=\"post\"\u003escan eligibility\u003c/a\u003e criteria.\u003c/p\u003e\u003cp\u003eTo get an overview of activity using Key Visualizer, first select a two-hour time period and review the heatmap for the \u0026#34;Total ops/s\u0026#34; metric. This view estimates the number of operations per second and how they are distributed across your database. Total ops/s is an estimated sum of write, lookup, and query operations averaged by seconds.\u003c/p\u003e\u003cp\u003eFirestore automatically scales using a technique called range sharding. When using Firestore, you model data in the form of documents stored in hierarchies of collections. The collection hierarchy and document ID is translated to a single key for each document. Documents are logically stored and ordered lexicographically by this key. We use the term \u0026#34;key range\u0026#34; to refer to a range of keys. The full key range is then automatically split up as-needed, driven by storage and traffic load, and served by many replicated servers inside of Firestore.\u003c/p\u003e\u003cp\u003eThe following example of Key Visualizer visualization shows a heatmap where there are some major differences in the usage pattern across the database. The X-axis is time, and the Y-axis is the key range for your database, broken down into buckets by traffic.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eRanges shown in dark colors have little or no activity.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRanges in bright colors have significantly more activity. In the example below, you can see the \u0026#34;Bar\u0026#34; and \u0026#34;Qux\u0026#34; collections going beyond 50 operations per second for some period of time.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ca href=\"http://cloud/blog/topics/developers-practitioners/all-you-need-know-about-firestore-cheatsheet\" target=\"_blank\"\u003eFirestore\u003c/a\u003e is a serverless, scalable, NoSQL document database. It is ideal for rapid and flexible web and mobile application development, and uniquely supports real-time client device syncing to the database.\u003c/p\u003e\u003cp\u003eTo get the best performance out of \u003ca href=\"https://cloud.google.com/firestore\"\u003eFirestore\u003c/a\u003e, while also making the most out of Firestore's automatic scaling and load balancing features, you need to make sure the data layout of your application allows requests to be processed optimally, particularly as your user traffic increases. There are some subtleties to be aware of when it comes to what could happen when traffic ramps up, and to help make this easier to identify, we’re announcing the General Availability of \u003ca href=\"https://cloud.google.com/firestore/docs/key-visualizer\"\u003e\u003cb\u003eKey Visualizer\u003c/b\u003e\u003c/a\u003e, an interactive, performance monitoring tool for Firestore.\u003c/p\u003e\u003cp\u003eKey Visualizer generates visual reports based on Firestore documents accessed over time, that will help you understand and optimize the access patterns of your database, as well as troubleshoot performance issues. With Key Visualizer, you can iteratively design a data model or improve your existing application’s data usage pattern.\u003c/p\u003e\u003cp\u003eTip: While Key Visualizer can be used with production databases, it’s best to identify performance issues prior to rolling out changes in production. Consider running application load tests with Firestore in a pre-production environment, and using Key Visualizer to identify potential issues.\u003c/p\u003e\u003ch3\u003eViewing a visualization\u003c/h3\u003e\u003cp\u003eThe Key Visualizer tool is available to all Firestore customers. Visualizations are generated at every hour boundary, covering data for the preceding two hours. Visualizations are generated as long as overall database traffic during a selected period meets the \u003ca href=\"https://cloud.google.com/firestore/docs/key-visualizer#scan_eligibility\"\u003escan eligibility\u003c/a\u003e criteria.\u003c/p\u003e\u003cp\u003eTo get an overview of activity using Key Visualizer, first select a two-hour time period and review the heatmap for the \"Total ops/s\" metric. This view estimates the number of operations per second and how they are distributed across your database. Total ops/s is an estimated sum of write, lookup, and query operations averaged by seconds.\u003c/p\u003e\u003cp\u003eFirestore automatically scales using a technique called range sharding. When using Firestore, you model data in the form of documents stored in hierarchies of collections. The collection hierarchy and document ID is translated to a single key for each document. Documents are logically stored and ordered lexicographically by this key. We use the term \"key range\" to refer to a range of keys. The full key range is then automatically split up as-needed, driven by storage and traffic load, and served by many replicated servers inside of Firestore.\u003c/p\u003e\u003cp\u003eThe following example of Key Visualizer visualization shows a heatmap where there are some major differences in the usage pattern across the database. The X-axis is time, and the Y-axis is the key range for your database, broken down into buckets by traffic.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eRanges shown in dark colors have little or no activity.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRanges in bright colors have significantly more activity. In the example below, you can see the \"Bar\" and \"Qux\" collections going beyond 50 operations per second for some period of time.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"1 Firestore.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Firestore.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAdditional methods of interpreting Key Visualizer visualizations are detailed in our \u003ca href=\"https://cloud.google.com/firestore/docs/keyvis-patterns\"\u003edocumentation\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eBesides the total number of operations, Key Visualizer also provides views with metrics for ops per second, average latency, and tail latency, where traffic is broken down for writes and deletes, lookups, and queries. This capability allows you to identify issues with your data layout or poorly balanced traffic that may be contributing to increased latencies.\u003c/p\u003e\u003ch3\u003eHotspots and heatmap patterns\u003c/h3\u003e\u003cp\u003eKey Visualizer gives you insight into how your traffic is distributed, and lets you understand if latency increases correlate with a hotspot, thus providing you with information to determine what parts of your application need to change. We refer to a \"hotspot\" as a case where traffic is poorly balanced across the database's keyspace. For the best performance, requests should be distributed evenly across a keyspace. The effect of a hotspot can vary, but typically hotspotting causes higher latency and in some cases, even failed operations.\u003c/p\u003e\u003cp\u003eFirestore automatically splits a key range into smaller pieces and distributes the work of serving traffic to more servers when needed. However, this has some limitations. Splitting storage and load takes time, and ramping up traffic too fast may cause hotspots while the service adjusts. The best practice is to distribute operations across the key range, while \u003ca href=\"https://cloud.google.com/firestore/docs/best-practices#ramping_up_traffic\"\u003eramping up traffic\u003c/a\u003e on a cold database with 500 operations per second, and then increasing traffic by up to 50% every 5 minutes. This is called the \"500/50/5\" rule, and allows you to rapidly warm up a cold database safely. For example, ramping to 1,000,000 ops/s can be achieved in under two hours.\u003c/p\u003e\u003cp\u003eFirestore can automatically split a key range until it is serving a single document using a dedicated set of replicated servers. Once this threshold is hit, Firestore is unable to create further splits beyond a single document. As a result, high and sustained volumes of concurrent operations on a single document may result in elevated latencies. You can observe these high latencies using Key Visualizer’s average and tail latency metrics. If you encounter sustained high latencies on a single document, you should consider modifying your data model to split or replicate the data across multiple documents.\u003c/p\u003e\u003cp\u003eKey Visualizer will also help you identify additional traffic patterns:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph_with_image\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cfigure class=\"article-image--wrap-small \"\u003e\u003cimg alt=\"2 Firestore.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Firestore.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eEvenly distributed usage\u003c/b\u003e: If a heatmap shows a fine-grained mix of dark and bright colors, then reads and writes are evenly distributed throughout the database. This heatmap represents an effective usage pattern for Firestore, and no additional action is required.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph_with_image\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cfigure class=\"article-image--wrap-small \"\u003e\u003cimg alt=\"3 Firestore.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_Firestore.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSequential Keys\u003c/b\u003e: A heatmap with a single bright diagonal line can indicate a special hotspotting case where the database is using strictly increasing or decreasing keys (document IDs). Sequential keys are an anti-pattern in Firestore, which will result in elevated latency especially at higher operations per second. In this case, the document IDs that are generated and utilized should be randomized. To learn more, see the \u003ca href=\"https://cloud.devsite.corp.google.com/firestore/docs/best-practices#high_read_write_and_delete_rates_to_a_narrow_document_range\"\u003ebest practices page\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph_with_image\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cfigure class=\"article-image--wrap-small \"\u003e\u003cimg alt=\"4 Firestore.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_Firestore.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSudden traffic increase\u003c/b\u003e: A heatmap with a key range that suddenly changes from dark to bright indicates a sudden spike in load. If the load increase isn’t well distributed across the key range, and exceeds the 500/50/5 rule best practice, the database can experience elevated latency in the operations. In this case, the data layout should be modified to reflect a better distribution of usage and traffic across the keyspace.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eNext steps\u003c/h3\u003e\u003cp\u003eFirestore Key Visualizer is a performance monitoring tool available to administrators and developers to better understand how their applications interact with Firestore. With this launch, Firestore joins our family of Cloud-native databases, including \u003ca href=\"https://cloud.google.com/spanner\"\u003eCloud Spanner\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/bigtable\"\u003eCloud Bigtable\u003c/a\u003e, in offering Key Visualizer to its customers. You can get started with Firestore Key Visualizer for free, from the \u003ca href=\"https://console.cloud.google.com/firestore/key-visualizer\"\u003eCloud Console\u003c/a\u003e.\u003c/p\u003e\u003chr/\u003e\u003cp\u003e\u003ci\u003eAcknowledgement\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eSpecial thanks to Minh Nguyen, Lead Product Manager for Firestore, for contributing to this post.\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/cloud_firestore.max-2200x2200.jpg",
      "date_published": "2022-01-18T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eAmarnath Mullick\u003c/name\u003e\u003ctitle\u003eTech Lead and Engineering Manager for Firestore Performance\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/apply-now-for-the-google-cloud-devops-awards/",
      "title": "The Google Cloud DevOps Awards: Final call for submissions!",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;DevOps continues to be a major business accelerator for our customers and we continually see success from customers applying DevOps Research and Assessment (\u0026lt;a href=\u0026#34;https://www.devops-research.com/research.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DORA\u0026lt;/a\u0026gt;) principles and findings to their organization. This is why the first annual \u0026lt;a href=\u0026#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\u0026#34;\u0026gt;DevOps Awards \u0026lt;/a\u0026gt;is targeted to recognize customers shaping the future of DevOps with DORA. Share your inspirational story, supported by examples of business transformation and operational excellence, today.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;With inputs from over 32,000 professionals worldwide and seven years of research, the \u0026lt;a href=\u0026#34;https://cloud.google.com/devops\u0026#34;\u0026gt;Accelerate State of DevOps Report\u0026lt;/a\u0026gt; is the largest and longest running DevOps research of its kind. The different categories of \u0026lt;a href=\u0026#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\u0026#34;\u0026gt;DevOps Awards\u0026lt;/a\u0026gt; map closely to the practices and capabilities that drive high performance, as identified by the \u0026lt;a href=\u0026#34;https://cloud.google.com/devops/state-of-devops/\u0026#34;\u0026gt;report\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Organizations, irrespective of their\u0026amp;#160; size, industry, and region are able to apply to one or all ten categories. Please find the categories and their descriptions below:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Optimizing for Speed without sacrificing stability\u0026lt;/b\u0026gt;: This award recognizes one\u0026amp;#160; Google Cloud customer that has driven improvements in speed without sacrificing quality.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Embracing easy-to-use tools to improve remote productivity\u0026lt;/b\u0026gt;: The research showcases how high performing engineers are 1.5 times more likely to have easy to-use tools. To be eligible for this award, share your stories on how easy to use DevOps tools have helped you improve engineer productivity.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Mastering effective disaster recovery\u0026lt;/b\u0026gt;: This award winner will be awarded to demonstrate how a robust, well-tested\u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/dr-scenarios-planning-guide\u0026#34;\u0026gt; disaster recovery (DR)\u0026lt;/a\u0026gt; plan can\u0026amp;#160; protect business operations.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Leveraging loosely coupled architecture\u0026lt;/b\u0026gt;: This award recognizes one customer that successfully transitioned from a tightly coupled \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/devops/devops-tech-architecture\u0026#34;\u0026gt;architecture\u0026lt;/a\u0026gt; to service-oriented and microservice architectures.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Unleashing the full power of the Cloud\u0026lt;/b\u0026gt;: This award recognizes a Google Cloud customer leveraging all five capabilities of cloud computing to improve software delivery and organizational performance. Specifically, these five capabilities include:\u0026amp;#160;\u0026lt;br\u0026gt;- On demand self-service\u0026lt;br\u0026gt;- Broad network access\u0026lt;br\u0026gt;- Measured service\u0026lt;br\u0026gt;- Rapid elasticity\u0026lt;br\u0026gt;- Resource pooling.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Read more about the \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\u0026#34;\u0026gt;five essential characteristics of cloud computing\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Most improved documentation quality\u0026lt;/b\u0026gt;: This award recognizes one customer that has successfully integrated documentation into their DevOps workflow using Google Cloud Platform tools.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Reducing burnout during COVID-19\u0026lt;/b\u0026gt;: We will recognize one customer that implemented effective processes to improve work/life balance, foster a healthy DevOps culture, and ultimately prevent burnout.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Utilizing IT operations to drive informed business decisions\u0026lt;/b\u0026gt;: This award will go to one customer that employed DevOps best practices to break down silos between development and operations teams.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Driving inclusion and diversity in DevOps\u0026lt;/b\u0026gt;: To highlight the importance of a diverse organization, this award honors one Google Cloud customer that:\u0026amp;#160;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;Prioritizes diversity and inclusion initiatives for their organization to transform and strengthen their business.\u0026amp;#160;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;-or\u0026lt;br\u0026gt;\u0026lt;br\u0026gt;Creates unique solutions to help build a more diverse, inclusive, and accessible workplace for your customer, leading to higher levels of engagement, productivity, and innovation.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Accelerating DevOps with DORA\u0026lt;/b\u0026gt;: This award recognizes one customer that has successfully integrated the most DORA practices and capabilities into their workflow using Google Cloud Platform tools.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;This is your chance to show your innovation globally and become a role model for the industry to improve. Winners will receive invitations to roundtables and discussions, press materials, website and social badges, special announcements and even a trophy award.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We are excited to see all your great submissions. Applications are open until January 31st, so apply for what best suits your company and stay tuned for our awards show in February 2022!\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For more information on the awards visit our \u0026lt;a href=\u0026#34;https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\u0026#34;\u0026gt;webpage \u0026lt;/a\u0026gt;and check out \u0026lt;a href=\u0026#34;https://services.google.com/fh/files/misc/2021_devops_awards_guidebook.pdf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;The Google Cloud DevOps Awards Guidebook\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\" _nghost-c60=\"\"\u003e\u003cp\u003eDevOps continues to be a major business accelerator for our customers and we continually see success from customers applying DevOps Research and Assessment (\u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDORA\u003c/a\u003e) principles and findings to their organization. This is why the first annual \u003ca href=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\" track-metadata-module=\"post\"\u003eDevOps Awards \u003c/a\u003eis targeted to recognize customers shaping the future of DevOps with DORA. Share your inspirational story, supported by examples of business transformation and operational excellence, today. \u003c/p\u003e\u003cp\u003eWith inputs from over 32,000 professionals worldwide and seven years of research, the \u003ca href=\"https://cloud.google.com/devops\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/devops\" track-metadata-module=\"post\"\u003eAccelerate State of DevOps Report\u003c/a\u003e is the largest and longest running DevOps research of its kind. The different categories of \u003ca href=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\" track-metadata-module=\"post\"\u003eDevOps Awards\u003c/a\u003e map closely to the practices and capabilities that drive high performance, as identified by the \u003ca href=\"https://cloud.google.com/devops/state-of-devops/\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/devops/state-of-devops/\" track-metadata-module=\"post\"\u003ereport\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eOrganizations, irrespective of their  size, industry, and region are able to apply to one or all ten categories. Please find the categories and their descriptions below:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eOptimizing for Speed without sacrificing stability\u003c/b\u003e: This award recognizes one  Google Cloud customer that has driven improvements in speed without sacrificing quality. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eEmbracing easy-to-use tools to improve remote productivity\u003c/b\u003e: The research showcases how high performing engineers are 1.5 times more likely to have easy to-use tools. To be eligible for this award, share your stories on how easy to use DevOps tools have helped you improve engineer productivity.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eMastering effective disaster recovery\u003c/b\u003e: This award winner will be awarded to demonstrate how a robust, well-tested\u003ca href=\"https://cloud.google.com/architecture/dr-scenarios-planning-guide\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/dr-scenarios-planning-guide\" track-metadata-module=\"post\"\u003e disaster recovery (DR)\u003c/a\u003e plan can  protect business operations.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eLeveraging loosely coupled architecture\u003c/b\u003e: This award recognizes one customer that successfully transitioned from a tightly coupled \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-architecture\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/devops/devops-tech-architecture\" track-metadata-module=\"post\"\u003earchitecture\u003c/a\u003e to service-oriented and microservice architectures.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eUnleashing the full power of the Cloud\u003c/b\u003e: This award recognizes a Google Cloud customer leveraging all five capabilities of cloud computing to improve software delivery and organizational performance. Specifically, these five capabilities include: \u003cbr/\u003e- On demand self-service\u003cbr/\u003e- Broad network access\u003cbr/\u003e- Measured service\u003cbr/\u003e- Rapid elasticity\u003cbr/\u003e- Resource pooling.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eRead more about the \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\" track-metadata-module=\"post\"\u003efive essential characteristics of cloud computing\u003c/a\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eMost improved documentation quality\u003c/b\u003e: This award recognizes one customer that has successfully integrated documentation into their DevOps workflow using Google Cloud Platform tools.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eReducing burnout during COVID-19\u003c/b\u003e: We will recognize one customer that implemented effective processes to improve work/life balance, foster a healthy DevOps culture, and ultimately prevent burnout.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eUtilizing IT operations to drive informed business decisions\u003c/b\u003e: This award will go to one customer that employed DevOps best practices to break down silos between development and operations teams.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cdiv\u003e\u003cp\u003e\u003cb\u003eDriving inclusion and diversity in DevOps\u003c/b\u003e: To highlight the importance of a diverse organization, this award honors one Google Cloud customer that: \u003c/p\u003e\u003cp\u003ePrioritizes diversity and inclusion initiatives for their organization to transform and strengthen their business. \u003c/p\u003e\u003cp\u003e-or\u003c/p\u003e\u003cp\u003eCreates unique solutions to help build a more diverse, inclusive, and accessible workplace for your customer, leading to higher levels of engagement, productivity, and innovation.\u003c/p\u003e\u003c/div\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAccelerating DevOps with DORA\u003c/b\u003e: This award recognizes one customer that has successfully integrated the most DORA practices and capabilities into their workflow using Google Cloud Platform tools.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis is your chance to show your innovation globally and become a role model for the industry to improve. Winners will receive invitations to roundtables and discussions, press materials, website and social badges, special announcements and even a trophy award.\u003c/p\u003e\u003cp\u003eWe are excited to see all your great submissions. Applications are open until January 31st, so apply for what best suits your company and stay tuned for our awards show in February 2022!\u003c/p\u003e\u003cp\u003eFor more information on the awards visit our \u003ca href=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\" track-metadata-module=\"post\"\u003ewebpage \u003c/a\u003eand check out \u003ca href=\"https://services.google.com/fh/files/misc/2021_devops_awards_guidebook.pdf\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://services.google.com\" track-metadata-module=\"post\"\u003eThe Google Cloud DevOps Awards Guidebook\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDevOps continues to be a major business accelerator for our customers and we continually see success from customers applying DevOps Research and Assessment (\u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\"\u003eDORA\u003c/a\u003e) principles and findings to their organization. This is why the first annual \u003ca href=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\"\u003eDevOps Awards\u003c/a\u003eis targeted to recognize customers shaping the future of DevOps with DORA. Share your inspirational story, supported by examples of business transformation and operational excellence, today. \u003c/p\u003e\u003cp\u003eWith inputs from over 32,000 professionals worldwide and seven years of research, the \u003ca href=\"https://cloud.google.com/devops\"\u003eAccelerate State of DevOps Report\u003c/a\u003e is the largest and longest running DevOps research of its kind. The different categories of \u003ca href=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\"\u003eDevOps Awards\u003c/a\u003e map closely to the practices and capabilities that drive high performance, as identified by the \u003ca href=\"https://cloud.google.com/devops/state-of-devops/\"\u003ereport\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eOrganizations, irrespective of their  size, industry, and region are able to apply to one or all ten categories. Please find the categories and their descriptions below:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eOptimizing for Speed without sacrificing stability\u003c/b\u003e: This award recognizes one  Google Cloud customer that has driven improvements in speed without sacrificing quality. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eEmbracing easy-to-use tools to improve remote productivity\u003c/b\u003e: The research showcases how high performing engineers are 1.5 times more likely to have easy to-use tools. To be eligible for this award, share your stories on how easy to use DevOps tools have helped you improve engineer productivity.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eMastering effective disaster recovery\u003c/b\u003e: This award winner will be awarded to demonstrate how a robust, well-tested\u003ca href=\"https://cloud.google.com/architecture/dr-scenarios-planning-guide\"\u003edisaster recovery (DR)\u003c/a\u003e plan can  protect business operations.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eLeveraging loosely coupled architecture\u003c/b\u003e: This award recognizes one customer that successfully transitioned from a tightly coupled \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-architecture\"\u003earchitecture\u003c/a\u003e to service-oriented and microservice architectures.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eUnleashing the full power of the Cloud\u003c/b\u003e: This award recognizes a Google Cloud customer leveraging all five capabilities of cloud computing to improve software delivery and organizational performance. Specifically, these five capabilities include: \u003cbr/\u003e- On demand self-service\u003cbr/\u003e- Broad network access\u003cbr/\u003e- Measured service\u003cbr/\u003e- Rapid elasticity\u003cbr/\u003e- Resource pooling.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eRead more about the \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\"\u003efive essential characteristics of cloud computing\u003c/a\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eMost improved documentation quality\u003c/b\u003e: This award recognizes one customer that has successfully integrated documentation into their DevOps workflow using Google Cloud Platform tools.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eReducing burnout during COVID-19\u003c/b\u003e: We will recognize one customer that implemented effective processes to improve work/life balance, foster a healthy DevOps culture, and ultimately prevent burnout.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eUtilizing IT operations to drive informed business decisions\u003c/b\u003e: This award will go to one customer that employed DevOps best practices to break down silos between development and operations teams.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDriving inclusion and diversity in DevOps\u003c/b\u003e: To highlight the importance of a diverse organization, this award honors one Google Cloud customer that: \u003cbr/\u003e\u003cbr/\u003ePrioritizes diversity and inclusion initiatives for their organization to transform and strengthen their business. \u003cbr/\u003e\u003cbr/\u003e-or\u003cbr/\u003e\u003cbr/\u003eCreates unique solutions to help build a more diverse, inclusive, and accessible workplace for your customer, leading to higher levels of engagement, productivity, and innovation.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAccelerating DevOps with DORA\u003c/b\u003e: This award recognizes one customer that has successfully integrated the most DORA practices and capabilities into their workflow using Google Cloud Platform tools.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis is your chance to show your innovation globally and become a role model for the industry to improve. Winners will receive invitations to roundtables and discussions, press materials, website and social badges, special announcements and even a trophy award.\u003c/p\u003e\u003cp\u003eWe are excited to see all your great submissions. Applications are open until January 31st, so apply for what best suits your company and stay tuned for our awards show in February 2022!\u003c/p\u003e\u003cp\u003eFor more information on the awards visit our \u003ca href=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\"\u003ewebpage\u003c/a\u003eand check out \u003ca href=\"https://services.google.com/fh/files/misc/2021_devops_awards_guidebook.pdf\" target=\"_blank\"\u003eThe Google Cloud DevOps Awards Guidebook\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/devops_awards.gif",
      "date_published": "2022-01-11T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eBrenna Washington\u003c/name\u003e\u003ctitle\u003eProduct Marketing Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/application-development/a-cloud-built-for-developers-2021-year-in-review/",
      "title": "A cloud built for developers — 2021 year in review",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv _ngcontent-c77=\"\" innerhtml=\"\u0026lt;p\u0026gt;2021 was a seminal year for software developers. Every company accelerated their digital and online efforts, while simultaneously moving to remote development. Innovation by driving developer productivity was top of mind for nearly every IT executive we spoke to. Many asked us about Alphabet\u0026#39;s long track record of innovation. From Google search to Waymo\u0026amp;#8217;s driverless cars,\u0026amp;#160; is there a secret to developing the next big thing?\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The answer is simple: 10X thinking. Look for solutions that help customers drive 10X improvements, through a series of smaller increments that compound to a large impact over time. At Google Cloud, we follow a similar philosophy to help our customers become innovative technology companies. In recent times, we\u0026amp;#8217;ve worked closely with partners, customers, and developers on services that help unlock 10X improvements in developer productivity.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Six years ago, we introduced a managed Kubernetes service, Google Kubernetes Engine (GKE). This year, we added \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\u0026#34;\u0026gt;GKE Autopilot\u0026lt;/a\u0026gt;, which revolutionized Kubernetes management by eliminating all node management operations. Likewise, our \u0026lt;a href=\u0026#34;https://cloud.google.com/run\u0026#34;\u0026gt;Cloud Run\u0026lt;/a\u0026gt; serverless platform was the first service of its kind, allowing developers to go beyond running small bits of code and run full applications in a serverless environment. From September 2020 to September 2021, Cloud Run deployments more than quadrupled. More recently, we co-founded the \u0026lt;a href=\u0026#34;https://openssf.org/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Open Source Security Foundation\u0026lt;/a\u0026gt; and began working on secure continuous Integration and delivery (CI/CD) services a year or so ahead of the cybersecurity threats that made it to headlines.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Here are the top developer challenges that customers asked us to solve in 2021:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Driving distributed developer productivity\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Securing the software supply chain\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Simplifying running of cloud-native applications\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Read on for more insights.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Driving distributed developer productivity\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;A critical prerequisite for innovation is time. Investments in developer productivity free developers to work on the important things. Traditionally, developers have spent hours downloading and installing tools to their local environments, updating them with the latest versions, or dependencies. \u0026lt;a href=\u0026#34;https://cloud.google.com/shell/docs/editor-overview\u0026#34;\u0026gt;Cloud Shell Editor\u0026lt;/a\u0026gt; is a full remote development environment with a growing set of built in security capabilities. It comes with developer tools pre-installed, including MySql, Kubernetes, Docker, minikube, Skaffold, etc. Developers just needed a web browser and internet connection to be productive. Developers now have access to \u0026lt;a href=\u0026#34;https://cloud.google.com/shell/docs/cloud-shell-tutorials/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt; right from Cloud Shell Editor, and can try code samples directly in \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/run-code-samples-directly-google-cloud-documentation\u0026#34;\u0026gt;our documentation\u0026lt;/a\u0026gt;. Additionally, with support for \u0026lt;a href=\u0026#34;https://cloud.google.com/run/docs/building/containers\u0026#34;\u0026gt;buildpacks\u0026lt;/a\u0026gt;, developers can create container images \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command\u0026#34;\u0026gt;directly from source code\u0026lt;/a\u0026gt;, without knowing anything about docker or containers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Securing the software supply chain\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Software supply chain vulnerabilities had far reaching consequences in 2021, with events such as \u0026lt;a href=\u0026#34;https://blogs.microsoft.com/on-the-issues/2020/12/17/cyberattacks-cybersecurity-solarwinds-fireeye/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SolarWinds\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://www.mimecast.com/blog/important-update-from-mimecast/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Mimecast/Microsoft Exchange\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://logging.apache.org/log4j/2.x/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Log4j \u0026lt;/a\u0026gt;affecting businesses, daily life, and entire \u0026lt;a href=\u0026#34;https://www.cyberscoop.com/dhs-cyber-alert-subpoena-us/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;governments\u0026lt;/a\u0026gt;. President Biden even issued an \u0026lt;a href=\u0026#34;https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;executive order\u0026lt;/a\u0026gt; to strengthen software supply-chain security standards.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Solving the software supply chain problem requires players across industries to work together. This is why we co-founded the\u0026lt;a href=\u0026#34;https://openssf.org/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; Open Source Security Foundation\u0026lt;/a\u0026gt; (Open SSF). We also proposed \u0026lt;a href=\u0026#34;https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SLSA\u0026lt;/a\u0026gt;, an industry-wide framework for maintaining the integrity of software artifacts throughout the software supply chain.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Open source, with its complex dependency trees, continues to remain a prime target for exploitation. In fact, an estimated \u0026lt;a href=\u0026#34;https://venturebeat.com/2021/04/13/synopsys-84-of-codebases-contain-an-open-source-vulnerability/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;84% of commercial code bases\u0026lt;/a\u0026gt; have at least one open source vulnerability. Today, developers can use our tools such as \u0026lt;a href=\u0026#34;https://github.com/ossf/allstar\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Allstar GitHub App\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://opensource.googleblog.com/2020/11/security-scorecards-for-open-source.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;open source security score cards\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://deps.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Open Source Insights\u0026lt;/a\u0026gt; to implement security best practices, determine a risk score for open source projects, and visualize a project\u0026#39;s deep dependencies. And several of these same\u0026amp;#160; kinds of open-source innovations are available out of the box to Google Cloud customers. Here are a few examples:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Detailed \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/identity-security/recommendations-for-apache-log4j2-vulnerability\u0026#34;\u0026gt;recommendations\u0026lt;/a\u0026gt; to help mitigate the Apache Log4j vulnerability.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://cloud.google.com/container-analysis/docs/java-overview\u0026#34;\u0026gt;Java scanning feature\u0026lt;/a\u0026gt; of Google Cloud \u0026lt;a href=\u0026#34;https://cloud.google.com/container-analysis/docs/container-scanning-overview\u0026#34;\u0026gt;On-Demand Scanning\u0026lt;/a\u0026gt;, which can be quite handy for developers to identify Linux-based container images that use an impacted version of Log4j. On-Demand Scanning can be used with no charge until December 31, 2021.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cloud Build, our serverless CI/CD service, offers \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework\u0026#34;\u0026gt;SLSA Level 1 compliance\u0026lt;/a\u0026gt; by default. This verifiable build provenance lets you trace a binary to the source code to prevent tampering and prove that the code you\u0026amp;#8217;re running is the code you think you\u0026amp;#8217;re running.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cloud Build\u0026amp;#8217;s new \u0026lt;a href=\u0026#34;https://cloud.google.com/build/docs/securing-builds/use-provenance-and-binary-authorization\u0026#34;\u0026gt;build integrity feature\u0026lt;/a\u0026gt; improves on this by automatically generating digital signatures, which can be validated before deployment by \u0026lt;a href=\u0026#34;https://cloud.google.com/binary-authorization\u0026#34;\u0026gt;Binary Authorization\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Simplifying running cloud-native applications\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Innovation is rarely a straight road, there are many wrong turns along the way. Developers need a cost effective runtime, a way to run experiments and fail forward fast. That\u0026#39;s why \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\u0026#34;\u0026gt;GKE Autopilot\u0026lt;/a\u0026gt; takes GKE, the \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/gartner-solution-scorecard-for-kubernetes-analyst-report\u0026#34;\u0026gt;most\u0026lt;/a\u0026gt; mature Kubernetes service on the market and further simplifies Kubernetes operations by providing a managed control and data plane, an optimized configuration out-of-the-box, automated scalability, health checks and repairs, and pay-for-use pricing.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;With GKE Autopilot, we can do more with our business. We can continue developing and upgrading our products, rather than focusing on fine-tuning infrastructure.\u0026amp;#8221;\u0026amp;#8212;\u0026lt;b\u0026gt;Jun Sakata, Software Engineer, Site Reliability, \u0026lt;a href=\u0026#34;https://cloud.google.com/customers/ubie\u0026#34;\u0026gt;Ubie\u0026lt;/a\u0026gt;\u0026amp;#160;\u0026lt;/b\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Simpler still is no cluster all. Cloud Run provides developers the freedom to run services from code or container images with no cluster or VM to manage. At the same time, it provides a hypervisor grade secure sandbox environment and several built in DevOps capabilities such as, \u0026lt;a href=\u0026#34;https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration\u0026#34;\u0026gt;multi-versioned deployments, gradual rollouts and rollbacks\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build\u0026#34;\u0026gt;GitHub\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/run/docs/building/containers\u0026#34;\u0026gt;Cloud Build\u0026lt;/a\u0026gt; integrations. This is ideal for web and mobile application development. In 2021, with \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/the-next-big-evolution-in-cloud-computing\u0026#34;\u0026gt;additions\u0026lt;/a\u0026gt; like \u0026lt;a href=\u0026#34;https://cloud.google.com/run/docs/about-concurrency\u0026#34;\u0026gt;higher per-instance concurrency\u0026lt;/a\u0026gt;, new \u0026lt;a href=\u0026#34;https://cloud.google.com/run/docs/configuring/cpu-allocation\u0026#34;\u0026gt;CPU allocation controls\u0026lt;/a\u0026gt;, and support for \u0026lt;a href=\u0026#34;https://cloud.google.com/run/docs/deploying\u0026#34;\u0026gt;standard Docker images\u0026lt;/a\u0026gt;, the benefits of serverless can now be expanded to a wider range of workloads, including legacy ones. Additionally, with newer cost controls along with billing flexibility like \u0026lt;a href=\u0026#34;https://cloud.google.com/run/cud\u0026#34;\u0026gt;committed use contracts\u0026lt;/a\u0026gt; and features like \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\u0026#34;\u0026gt;always-on CPU\u0026lt;/a\u0026gt;, it\u0026amp;#8217;s possible to run more steady-state pattern workloads cost effectively in a serverless environment.\u0026amp;#160; Best of all, thanks to improvements like these, organizations using Cloud Run have reported reduction in \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report\u0026#34;\u0026gt;developer recruiting costs by 40%\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Cloud Run is also the first platform to provide developers the option to optimize their carbon footprint.\u0026amp;#160; With the news self-service \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/sustainability/google-cloud-region-picker-helps-you-make-the-green-choice\u0026#34;\u0026gt;Region Picker\u0026lt;/a\u0026gt; you can choose the data center region with the lowest gross carbon cost on which to run your Cloud Run workloads. Further, with just one click, \u0026lt;a href=\u0026#34;https://cloud.google.com/carbon-footprint\u0026#34;\u0026gt;Google Cloud Carbon Footprint \u0026lt;/a\u0026gt;gives you access to the energy-related emissions data for external carbon disclosures.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;With Cloud Run, we only need half the people to manage our systems as compared to before\u0026amp;#8221; \u0026lt;b\u0026gt;Google Cloud Platform Architect, \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report\u0026#34;\u0026gt;Cosmetics\u0026lt;/a\u0026gt;\u0026amp;#160;\u0026lt;/b\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;Cloud Run is one of the easiest services on Google Cloud Platform you can deploy to. It\u0026amp;#8217;s just super simple.\u0026amp;#8221; \u0026lt;b\u0026gt;CTO, \u0026lt;/b\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/resources/forrester-cloudrun-benefits-report\u0026#34;\u0026gt;\u0026lt;b\u0026gt;Healthcare SaaS\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you want to give Cloud Run and associated Cloud Functions a try, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/serverless-hackathon\u0026#34;\u0026gt;Easy as Pie Serverless Hackathon\u0026lt;/a\u0026gt;, which offers \u0026amp;#160;over $20,000 USD in cash prizes.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;2022: More to come\u0026amp;#160;\u0026amp;#160;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;2021 brought simplification and greater attention to developer productivity. It is essential that developers continue to operate at even higher levels of the stack, without worrying about infrastructure, security, compliance and integrations. This is the Northstar for 2022. In 2022, look for Google Cloud to co-innovate with our ISV partners, developers, and SecOps team to bring you the 10X innovation you need from the cloud that is built for developers.\u0026lt;/p\u0026gt;\" _nghost-c77=\"\"\u003e\u003cp\u003e2021 was a seminal year for software developers. Every company accelerated their digital and online efforts, while simultaneously moving to remote development. Innovation by driving developer productivity was top of mind for nearly every IT executive we spoke to. Many asked us about Alphabet\u0026#39;s long track record of innovation. From Google search to Waymo’s driverless cars,  is there a secret to developing the next big thing? \u003c/p\u003e\u003cp\u003eThe answer is simple: 10X thinking. Look for solutions that help customers drive 10X improvements, through a series of smaller increments that compound to a large impact over time. At Google Cloud, we follow a similar philosophy to help our customers become innovative technology companies. In recent times, we’ve worked closely with partners, customers, and developers on services that help unlock 10X improvements in developer productivity. \u003c/p\u003e\u003cp\u003eSix years ago, we introduced a managed Kubernetes service, Google Kubernetes Engine (GKE). This year, we added \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\" track-metadata-module=\"post\"\u003eGKE Autopilot\u003c/a\u003e, which revolutionized Kubernetes management by eliminating all node management operations. Likewise, our \u003ca href=\"https://cloud.google.com/run\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/run\" track-metadata-module=\"post\"\u003eCloud Run\u003c/a\u003e serverless platform was the first service of its kind, allowing developers to go beyond running small bits of code and run full applications in a serverless environment. From September 2020 to September 2021, Cloud Run deployments more than quadrupled. More recently, we co-founded the \u003ca href=\"https://openssf.org/\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://openssf.org\" track-metadata-module=\"post\"\u003eOpen Source Security Foundation\u003c/a\u003e and began working on secure continuous Integration and delivery (CI/CD) services a year or so ahead of the cybersecurity threats that made it to headlines. \u003c/p\u003e\u003cp\u003eHere are the top developer challenges that customers asked us to solve in 2021: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDriving distributed developer productivity\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSecuring the software supply chain\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSimplifying running of cloud-native applications \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eRead on for more insights. \u003c/p\u003e\u003ch3\u003eDriving distributed developer productivity\u003c/h3\u003e\u003cp\u003eA critical prerequisite for innovation is time. Investments in developer productivity free developers to work on the important things. Traditionally, developers have spent hours downloading and installing tools to their local environments, updating them with the latest versions, or dependencies. \u003ca href=\"https://cloud.google.com/shell/docs/editor-overview\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/shell/docs/editor-overview\" track-metadata-module=\"post\"\u003eCloud Shell Editor\u003c/a\u003e is a full remote development environment with a growing set of built in security capabilities. It comes with developer tools pre-installed, including MySql, Kubernetes, Docker, minikube, Skaffold, etc. Developers just needed a web browser and internet connection to be productive. Developers now have access to \u003ca href=\"https://cloud.google.com/shell/docs/cloud-shell-tutorials/tutorials\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/shell/docs/cloud-shell-tutorials/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e right from Cloud Shell Editor, and can try code samples directly in \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/run-code-samples-directly-google-cloud-documentation\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/run-code-samples-directly-google-cloud-documentation\" track-metadata-module=\"post\"\u003eour documentation\u003c/a\u003e. Additionally, with support for \u003ca href=\"https://cloud.google.com/run/docs/building/containers\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/run/docs/building/containers\" track-metadata-module=\"post\"\u003ebuildpacks\u003c/a\u003e, developers can create container images \u003ca href=\"https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command\" track-metadata-module=\"post\"\u003edirectly from source code\u003c/a\u003e, without knowing anything about docker or containers. \u003c/p\u003e\u003ch3\u003eSecuring the software supply chain\u003c/h3\u003e\u003cp\u003eSoftware supply chain vulnerabilities had far reaching consequences in 2021, with events such as \u003ca href=\"https://blogs.microsoft.com/on-the-issues/2020/12/17/cyberattacks-cybersecurity-solarwinds-fireeye/\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://blogs.microsoft.com\" track-metadata-module=\"post\"\u003eSolarWinds\u003c/a\u003e, \u003ca href=\"https://www.mimecast.com/blog/important-update-from-mimecast/\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://www.mimecast.com\" track-metadata-module=\"post\"\u003eMimecast/Microsoft Exchange\u003c/a\u003e, and \u003ca href=\"https://logging.apache.org/log4j/2.x/\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://logging.apache.org\" track-metadata-module=\"post\"\u003eLog4j \u003c/a\u003eaffecting businesses, daily life, and entire \u003ca href=\"https://www.cyberscoop.com/dhs-cyber-alert-subpoena-us/\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://www.cyberscoop.com\" track-metadata-module=\"post\"\u003egovernments\u003c/a\u003e. President Biden even issued an \u003ca href=\"https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://www.whitehouse.gov\" track-metadata-module=\"post\"\u003eexecutive order\u003c/a\u003e to strengthen software supply-chain security standards.\u003c/p\u003e\u003cp\u003eSolving the software supply chain problem requires players across industries to work together. This is why we co-founded the\u003ca href=\"https://openssf.org/\" target=\"_blank\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://openssf.org\" track-metadata-module=\"post\"\u003e Open Source Security Foundation\u003c/a\u003e (Open SSF). We also proposed \u003ca href=\"https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://security.googleblog.com\" track-metadata-module=\"post\"\u003eSLSA\u003c/a\u003e, an industry-wide framework for maintaining the integrity of software artifacts throughout the software supply chain. \u003c/p\u003e\u003cp\u003eOpen source, with its complex dependency trees, continues to remain a prime target for exploitation. In fact, an estimated \u003ca href=\"https://venturebeat.com/2021/04/13/synopsys-84-of-codebases-contain-an-open-source-vulnerability/\" target=\"_blank\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://venturebeat.com\" track-metadata-module=\"post\"\u003e84% of commercial code bases\u003c/a\u003e have at least one open source vulnerability. Today, developers can use our tools such as \u003ca href=\"https://github.com/ossf/allstar\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eAllstar GitHub App\u003c/a\u003e, \u003ca href=\"https://opensource.googleblog.com/2020/11/security-scorecards-for-open-source.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://opensource.googleblog.com\" track-metadata-module=\"post\"\u003eopen source security score cards\u003c/a\u003e and \u003ca href=\"https://deps.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://deps.dev\" track-metadata-module=\"post\"\u003eOpen Source Insights\u003c/a\u003e to implement security best practices, determine a risk score for open source projects, and visualize a project\u0026#39;s deep dependencies. And several of these same  kinds of open-source innovations are available out of the box to Google Cloud customers. Here are a few examples: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDetailed \u003ca href=\"https://cloud.google.com/blog/products/identity-security/recommendations-for-apache-log4j2-vulnerability\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/identity-security/recommendations-for-apache-log4j2-vulnerability\" track-metadata-module=\"post\"\u003erecommendations\u003c/a\u003e to help mitigate the Apache Log4j vulnerability. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/container-analysis/docs/java-overview\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/container-analysis/docs/java-overview\" track-metadata-module=\"post\"\u003eJava scanning feature\u003c/a\u003e of Google Cloud \u003ca href=\"https://cloud.google.com/container-analysis/docs/container-scanning-overview\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/container-analysis/docs/container-scanning-overview\" track-metadata-module=\"post\"\u003eOn-Demand Scanning\u003c/a\u003e, which can be quite handy for developers to identify Linux-based container images that use an impacted version of Log4j. On-Demand Scanning can be used with no charge until December 31, 2021. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Build, our serverless CI/CD service, offers \u003ca href=\"https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework\" track-metadata-module=\"post\"\u003eSLSA Level 1 compliance\u003c/a\u003e by default. This verifiable build provenance lets you trace a binary to the source code to prevent tampering and prove that the code you’re running is the code you think you’re running. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Build’s new \u003ca href=\"https://cloud.google.com/build/docs/securing-builds/use-provenance-and-binary-authorization\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/build/docs/securing-builds/use-provenance-and-binary-authorization\" track-metadata-module=\"post\"\u003ebuild integrity feature\u003c/a\u003e improves on this by automatically generating digital signatures, which can be validated before deployment by \u003ca href=\"https://cloud.google.com/binary-authorization\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/binary-authorization\" track-metadata-module=\"post\"\u003eBinary Authorization\u003c/a\u003e. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eSimplifying running cloud-native applications\u003c/h3\u003e\u003cp\u003eInnovation is rarely a straight road, there are many wrong turns along the way. Developers need a cost effective runtime, a way to run experiments and fail forward fast. That\u0026#39;s why \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\" track-metadata-module=\"post\"\u003eGKE Autopilot\u003c/a\u003e takes GKE, the \u003ca href=\"https://cloud.google.com/resources/gartner-solution-scorecard-for-kubernetes-analyst-report\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/resources/gartner-solution-scorecard-for-kubernetes-analyst-report\" track-metadata-module=\"post\"\u003emost\u003c/a\u003e mature Kubernetes service on the market and further simplifies Kubernetes operations by providing a managed control and data plane, an optimized configuration out-of-the-box, automated scalability, health checks and repairs, and pay-for-use pricing. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“With GKE Autopilot, we can do more with our business. We can continue developing and upgrading our products, rather than focusing on fine-tuning infrastructure.”—\u003cb\u003eJun Sakata, Software Engineer, Site Reliability, \u003ca href=\"https://cloud.google.com/customers/ubie\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/customers/ubie\" track-metadata-module=\"post\"\u003eUbie\u003c/a\u003e \u003c/b\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003eSimpler still is no cluster all. Cloud Run provides developers the freedom to run services from code or container images with no cluster or VM to manage. At the same time, it provides a hypervisor grade secure sandbox environment and several built in DevOps capabilities such as, \u003ca href=\"https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration\" track-metadata-module=\"post\"\u003emulti-versioned deployments, gradual rollouts and rollbacks\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build\" track-metadata-module=\"post\"\u003eGitHub\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/run/docs/building/containers\" track-type=\"inline link\" track-name=\"31\" track-metadata-eventdetail=\"https://cloud.google.com/run/docs/building/containers\" track-metadata-module=\"post\"\u003eCloud Build\u003c/a\u003e integrations. This is ideal for web and mobile application development. In 2021, with \u003ca href=\"https://cloud.google.com/blog/products/serverless/the-next-big-evolution-in-cloud-computing\" track-type=\"inline link\" track-name=\"32\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/the-next-big-evolution-in-cloud-computing\" track-metadata-module=\"post\"\u003eadditions\u003c/a\u003e like \u003ca href=\"https://cloud.google.com/run/docs/about-concurrency\" track-type=\"inline link\" track-name=\"33\" track-metadata-eventdetail=\"https://cloud.google.com/run/docs/about-concurrency\" track-metadata-module=\"post\"\u003ehigher per-instance concurrency\u003c/a\u003e, new \u003ca href=\"https://cloud.google.com/run/docs/configuring/cpu-allocation\" track-type=\"inline link\" track-name=\"34\" track-metadata-eventdetail=\"https://cloud.google.com/run/docs/configuring/cpu-allocation\" track-metadata-module=\"post\"\u003eCPU allocation controls\u003c/a\u003e, and support for \u003ca href=\"https://cloud.google.com/run/docs/deploying\" track-type=\"inline link\" track-name=\"35\" track-metadata-eventdetail=\"https://cloud.google.com/run/docs/deploying\" track-metadata-module=\"post\"\u003estandard Docker images\u003c/a\u003e, the benefits of serverless can now be expanded to a wider range of workloads, including legacy ones. Additionally, with newer cost controls along with billing flexibility like \u003ca href=\"https://cloud.google.com/run/cud\" track-type=\"inline link\" track-name=\"36\" track-metadata-eventdetail=\"https://cloud.google.com/run/cud\" track-metadata-module=\"post\"\u003ecommitted use contracts\u003c/a\u003e and features like \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\" track-type=\"inline link\" track-name=\"37\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\" track-metadata-module=\"post\"\u003ealways-on CPU\u003c/a\u003e, it’s possible to run more steady-state pattern workloads cost effectively in a serverless environment.  Best of all, thanks to improvements like these, organizations using Cloud Run have reported reduction in \u003ca href=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\" track-type=\"inline link\" track-name=\"38\" track-metadata-eventdetail=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\" track-metadata-module=\"post\"\u003edeveloper recruiting costs by 40%\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eCloud Run is also the first platform to provide developers the option to optimize their carbon footprint.  With the news self-service \u003ca href=\"https://cloud.google.com/blog/topics/sustainability/google-cloud-region-picker-helps-you-make-the-green-choice\" track-type=\"inline link\" track-name=\"39\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/sustainability/google-cloud-region-picker-helps-you-make-the-green-choice\" track-metadata-module=\"post\"\u003eRegion Picker\u003c/a\u003e you can choose the data center region with the lowest gross carbon cost on which to run your Cloud Run workloads. Further, with just one click, \u003ca href=\"https://cloud.google.com/carbon-footprint\" track-type=\"inline link\" track-name=\"40\" track-metadata-eventdetail=\"https://cloud.google.com/carbon-footprint\" track-metadata-module=\"post\"\u003eGoogle Cloud Carbon Footprint \u003c/a\u003egives you access to the energy-related emissions data for external carbon disclosures. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“With Cloud Run, we only need half the people to manage our systems as compared to before” \u003cb\u003eGoogle Cloud Platform Architect, \u003ca href=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\" track-type=\"inline link\" track-name=\"41\" track-metadata-eventdetail=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\" track-metadata-module=\"post\"\u003eCosmetics\u003c/a\u003e \u003c/b\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003e“Cloud Run is one of the easiest services on Google Cloud Platform you can deploy to. It’s just super simple.” \u003cb\u003eCTO, \u003c/b\u003e\u003ca href=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\" track-type=\"inline link\" track-name=\"42\" track-metadata-eventdetail=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\" track-metadata-module=\"post\"\u003e\u003cb\u003eHealthcare SaaS\u003c/b\u003e\u003c/a\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003eIf you want to give Cloud Run and associated Cloud Functions a try, check out the \u003ca href=\"https://cloud.google.com/blog/products/serverless/serverless-hackathon\" track-type=\"inline link\" track-name=\"43\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/serverless-hackathon\" track-metadata-module=\"post\"\u003eEasy as Pie Serverless Hackathon\u003c/a\u003e, which offers  over $20,000 USD in cash prizes.\u003c/p\u003e\u003ch3\u003e2022: More to come  \u003c/h3\u003e\u003cp\u003e2021 brought simplification and greater attention to developer productivity. It is essential that developers continue to operate at even higher levels of the stack, without worrying about infrastructure, security, compliance and integrations. This is the Northstar for 2022. In 2022, look for Google Cloud to co-innovate with our ISV partners, developers, and SecOps team to bring you the 10X innovation you need from the cloud that is built for developers.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e2021 was a seminal year for software developers. Every company accelerated their digital and online efforts, while simultaneously moving to remote development. Innovation by driving developer productivity was top of mind for nearly every IT executive we spoke to. Many asked us about Alphabet's long track record of innovation. From Google search to Waymo’s driverless cars,  is there a secret to developing the next big thing? \u003c/p\u003e\u003cp\u003eThe answer is simple: 10X thinking. Look for solutions that help customers drive 10X improvements, through a series of smaller increments that compound to a large impact over time. At Google Cloud, we follow a similar philosophy to help our customers become innovative technology companies. In recent times, we’ve worked closely with partners, customers, and developers on services that help unlock 10X improvements in developer productivity. \u003c/p\u003e\u003cp\u003eSix years ago, we introduced a managed Kubernetes service, Google Kubernetes Engine (GKE). This year, we added \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\"\u003eGKE Autopilot\u003c/a\u003e, which revolutionized Kubernetes management by eliminating all node management operations. Likewise, our \u003ca href=\"https://cloud.google.com/run\"\u003eCloud Run\u003c/a\u003e serverless platform was the first service of its kind, allowing developers to go beyond running small bits of code and run full applications in a serverless environment. From September 2020 to September 2021, Cloud Run deployments more than quadrupled. More recently, we co-founded the \u003ca href=\"https://openssf.org/\" target=\"_blank\"\u003eOpen Source Security Foundation\u003c/a\u003e and began working on secure continuous Integration and delivery (CI/CD) services a year or so ahead of the cybersecurity threats that made it to headlines. \u003c/p\u003e\u003cp\u003eHere are the top developer challenges that customers asked us to solve in 2021: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDriving distributed developer productivity\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSecuring the software supply chain\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSimplifying running of cloud-native applications \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eRead on for more insights. \u003c/p\u003e\u003ch3\u003eDriving distributed developer productivity\u003c/h3\u003e\u003cp\u003eA critical prerequisite for innovation is time. Investments in developer productivity free developers to work on the important things. Traditionally, developers have spent hours downloading and installing tools to their local environments, updating them with the latest versions, or dependencies. \u003ca href=\"https://cloud.google.com/shell/docs/editor-overview\"\u003eCloud Shell Editor\u003c/a\u003e is a full remote development environment with a growing set of built in security capabilities. It comes with developer tools pre-installed, including MySql, Kubernetes, Docker, minikube, Skaffold, etc. Developers just needed a web browser and internet connection to be productive. Developers now have access to \u003ca href=\"https://cloud.google.com/shell/docs/cloud-shell-tutorials/tutorials\"\u003etutorials\u003c/a\u003e right from Cloud Shell Editor, and can try code samples directly in \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/run-code-samples-directly-google-cloud-documentation\"\u003eour documentation\u003c/a\u003e. Additionally, with support for \u003ca href=\"https://cloud.google.com/run/docs/building/containers\"\u003ebuildpacks\u003c/a\u003e, developers can create container images \u003ca href=\"https://cloud.google.com/blog/products/serverless/build-and-deploy-an-app-to-cloud-run-with-a-single-command\"\u003edirectly from source code\u003c/a\u003e, without knowing anything about docker or containers. \u003c/p\u003e\u003ch3\u003eSecuring the software supply chain\u003c/h3\u003e\u003cp\u003eSoftware supply chain vulnerabilities had far reaching consequences in 2021, with events such as \u003ca href=\"https://blogs.microsoft.com/on-the-issues/2020/12/17/cyberattacks-cybersecurity-solarwinds-fireeye/\" target=\"_blank\"\u003eSolarWinds\u003c/a\u003e, \u003ca href=\"https://www.mimecast.com/blog/important-update-from-mimecast/\" target=\"_blank\"\u003eMimecast/Microsoft Exchange\u003c/a\u003e, and \u003ca href=\"https://logging.apache.org/log4j/2.x/\" target=\"_blank\"\u003eLog4j\u003c/a\u003eaffecting businesses, daily life, and entire \u003ca href=\"https://www.cyberscoop.com/dhs-cyber-alert-subpoena-us/\" target=\"_blank\"\u003egovernments\u003c/a\u003e. President Biden even issued an \u003ca href=\"https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-the-nations-cybersecurity/\" target=\"_blank\"\u003eexecutive order\u003c/a\u003e to strengthen software supply-chain security standards.\u003c/p\u003e\u003cp\u003eSolving the software supply chain problem requires players across industries to work together. This is why we co-founded the\u003ca href=\"https://openssf.org/\" target=\"_blank\"\u003eOpen Source Security Foundation\u003c/a\u003e (Open SSF). We also proposed \u003ca href=\"https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html\" target=\"_blank\"\u003eSLSA\u003c/a\u003e, an industry-wide framework for maintaining the integrity of software artifacts throughout the software supply chain. \u003c/p\u003e\u003cp\u003eOpen source, with its complex dependency trees, continues to remain a prime target for exploitation. In fact, an estimated \u003ca href=\"https://venturebeat.com/2021/04/13/synopsys-84-of-codebases-contain-an-open-source-vulnerability/\" target=\"_blank\"\u003e84% of commercial code bases\u003c/a\u003e have at least one open source vulnerability. Today, developers can use our tools such as \u003ca href=\"https://github.com/ossf/allstar\" target=\"_blank\"\u003eAllstar GitHub App\u003c/a\u003e, \u003ca href=\"https://opensource.googleblog.com/2020/11/security-scorecards-for-open-source.html\" target=\"_blank\"\u003eopen source security score cards\u003c/a\u003e and \u003ca href=\"https://deps.dev/\" target=\"_blank\"\u003eOpen Source Insights\u003c/a\u003e to implement security best practices, determine a risk score for open source projects, and visualize a project's deep dependencies. And several of these same  kinds of open-source innovations are available out of the box to Google Cloud customers. Here are a few examples: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDetailed \u003ca href=\"https://cloud.google.com/blog/products/identity-security/recommendations-for-apache-log4j2-vulnerability\"\u003erecommendations\u003c/a\u003e to help mitigate the Apache Log4j vulnerability. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/container-analysis/docs/java-overview\"\u003eJava scanning feature\u003c/a\u003e of Google Cloud \u003ca href=\"https://cloud.google.com/container-analysis/docs/container-scanning-overview\"\u003eOn-Demand Scanning\u003c/a\u003e, which can be quite handy for developers to identify Linux-based container images that use an impacted version of Log4j. On-Demand Scanning can be used with no charge until December 31, 2021. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Build, our serverless CI/CD service, offers \u003ca href=\"https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework\"\u003eSLSA Level 1 compliance\u003c/a\u003e by default. This verifiable build provenance lets you trace a binary to the source code to prevent tampering and prove that the code you’re running is the code you think you’re running. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Build’s new \u003ca href=\"https://cloud.google.com/build/docs/securing-builds/use-provenance-and-binary-authorization\"\u003ebuild integrity feature\u003c/a\u003e improves on this by automatically generating digital signatures, which can be validated before deployment by \u003ca href=\"https://cloud.google.com/binary-authorization\"\u003eBinary Authorization\u003c/a\u003e. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eSimplifying running cloud-native applications\u003c/h3\u003e\u003cp\u003eInnovation is rarely a straight road, there are many wrong turns along the way. Developers need a cost effective runtime, a way to run experiments and fail forward fast. That's why \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\"\u003eGKE Autopilot\u003c/a\u003e takes GKE, the \u003ca href=\"https://cloud.google.com/resources/gartner-solution-scorecard-for-kubernetes-analyst-report\"\u003emost\u003c/a\u003e mature Kubernetes service on the market and further simplifies Kubernetes operations by providing a managed control and data plane, an optimized configuration out-of-the-box, automated scalability, health checks and repairs, and pay-for-use pricing. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“With GKE Autopilot, we can do more with our business. We can continue developing and upgrading our products, rather than focusing on fine-tuning infrastructure.”—\u003cb\u003eJun Sakata, Software Engineer, Site Reliability, \u003ca href=\"https://cloud.google.com/customers/ubie\"\u003eUbie\u003c/a\u003e \u003c/b\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003eSimpler still is no cluster all. Cloud Run provides developers the freedom to run services from code or container images with no cluster or VM to manage. At the same time, it provides a hypervisor grade secure sandbox environment and several built in DevOps capabilities such as, \u003ca href=\"https://cloud.google.com/run/docs/rollouts-rollbacks-traffic-migration\"\u003emulti-versioned deployments, gradual rollouts and rollbacks\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/run/docs/continuous-deployment-with-cloud-build\"\u003eGitHub\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/run/docs/building/containers\"\u003eCloud Build\u003c/a\u003e integrations. This is ideal for web and mobile application development. In 2021, with \u003ca href=\"https://cloud.google.com/blog/products/serverless/the-next-big-evolution-in-cloud-computing\"\u003eadditions\u003c/a\u003e like \u003ca href=\"https://cloud.google.com/run/docs/about-concurrency\"\u003ehigher per-instance concurrency\u003c/a\u003e, new \u003ca href=\"https://cloud.google.com/run/docs/configuring/cpu-allocation\"\u003eCPU allocation controls\u003c/a\u003e, and support for \u003ca href=\"https://cloud.google.com/run/docs/deploying\"\u003estandard Docker images\u003c/a\u003e, the benefits of serverless can now be expanded to a wider range of workloads, including legacy ones. Additionally, with newer cost controls along with billing flexibility like \u003ca href=\"https://cloud.google.com/run/cud\"\u003ecommitted use contracts\u003c/a\u003e and features like \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\"\u003ealways-on CPU\u003c/a\u003e, it’s possible to run more steady-state pattern workloads cost effectively in a serverless environment.  Best of all, thanks to improvements like these, organizations using Cloud Run have reported reduction in \u003ca href=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\"\u003edeveloper recruiting costs by 40%\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eCloud Run is also the first platform to provide developers the option to optimize their carbon footprint.  With the news self-service \u003ca href=\"https://cloud.google.com/blog/topics/sustainability/google-cloud-region-picker-helps-you-make-the-green-choice\"\u003eRegion Picker\u003c/a\u003e you can choose the data center region with the lowest gross carbon cost on which to run your Cloud Run workloads. Further, with just one click, \u003ca href=\"https://cloud.google.com/carbon-footprint\"\u003eGoogle Cloud Carbon Footprint\u003c/a\u003egives you access to the energy-related emissions data for external carbon disclosures. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“With Cloud Run, we only need half the people to manage our systems as compared to before” \u003cb\u003eGoogle Cloud Platform Architect, \u003ca href=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\"\u003eCosmetics\u003c/a\u003e \u003c/b\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003e“Cloud Run is one of the easiest services on Google Cloud Platform you can deploy to. It’s just super simple.” \u003cb\u003eCTO,\u003c/b\u003e\u003ca href=\"https://cloud.google.com/resources/forrester-cloudrun-benefits-report\"\u003e\u003cb\u003eHealthcare SaaS\u003c/b\u003e\u003c/a\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003eIf you want to give Cloud Run and associated Cloud Functions a try, check out the \u003ca href=\"https://cloud.google.com/blog/products/serverless/serverless-hackathon\"\u003eEasy as Pie Serverless Hackathon\u003c/a\u003e, which offers  over $20,000 USD in cash prizes.\u003c/p\u003e\u003ch3\u003e2022: More to come  \u003c/h3\u003e\u003cp\u003e2021 brought simplification and greater attention to developer productivity. It is essential that developers continue to operate at even higher levels of the stack, without worrying about infrastructure, security, compliance and integrations. This is the Northstar for 2022. In 2022, look for Google Cloud to co-innovate with our ISV partners, developers, and SecOps team to bring you the 10X innovation you need from the cloud that is built for developers.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_GCP_HLl2OQm.max-2200x2200.png",
      "date_published": "2021-12-23T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eUrs Hölzle\u003c/name\u003e\u003ctitle\u003eSenior Vice President, Technical Infrastructure\u003c/title\u003e\u003cdepartment\u003eGoogle Cloud\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/loon-sre-use-postmortems-to-launch-and-iterate/",
      "title": "Postmortems at Loon: a guiding force for rapid development",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c67=\"\"\u003e\u003cdiv _ngcontent-c67=\"\" innerhtml=\"\u0026lt;p\u0026gt;Founded by Google SRE alumni, it is no surprise that Loon\u0026#39;s Production Engineering/SRE team instituted a culture of blameless postmortems that became a key feature of Loon\u0026#39;s approach to incident response. Blameless postmortems originated as an aerospace practice in the mid-20th century, so it was particularly fitting that they came full circle to be used at a company that melded cutting edge aerospace work with the development of a communications platform and the world\u0026#39;s first stratospheric temporospatial software defined network. The use of postmortems became a standardizing factor across Loon\u0026#39;s teams\u0026amp;#8212; from avionics and manufacturing, to flight operations, to software platforms and network service. This blog post discusses how Loon moved from a heterogeneous approach to postmortems to eventually standardize and share this practice across the organization\u0026amp;#8212; a shift that helped the company move from R\u0026amp;amp;D to commercial service in 2020.\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Background\u0026lt;/h2\u0026gt;\u0026lt;h3\u0026gt;Postmortems\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Many industries have adopted the use of postmortems\u0026amp;#8212; they are fairly common in high-risk fields where mistakes can be fatal or extremely expensive. Postmortems are also widespread in industries and projects where bad processes or assumptions can incur expensive project development costs and avoiding repeat mistakes is a priority. Individual industries and organizations often develop their own postmortem standards or templates so that postmortems are easier to create and digest across teams.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Blameless postmortems likely originated in the healthcare and aerospace industries in the mid-20th century. Because of the high cost of failure, these industries needed to create a culture of transparency and continuous improvement that could only come from openly discussing failure. As the original SRE book states, blameless postmortems are key to \u0026amp;#34;an environment where every \u0026#39;mistake\u0026#39; is seen as an opportunity to strengthen the system.\u0026amp;#34;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The goal of a postmortem is to document an incident or event in order to foster learning from it, both among the affected teams and beyond. The postmortem usually includes a timeline of what happened, the solutions implemented, the incident\u0026#39;s impact, the investigation into root causes, and changes or follow-ups to stop it from happening again. To facilitate learning, SRE\u0026#39;s postmortem format includes both what went well\u0026amp;#8212; acknowledging the successes that should be maintained and expanded\u0026amp;#8212; and what went poorly and needs to be changed. In this way, postmortem action items are key to prioritizing work that ensures the same failures don\u0026#39;t happen again.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Loon\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Loon aimed to supply internet access to unserved and underserved populations around the world by providing connectivity via stratospheric balloons. These high altitude \u0026amp;#8220;flying cell towers\u0026amp;#8221; covered a much wider footprint than a terrestrial tower, and could be deployed (and repositioned) into the most remote corners of the earth without expensive overland transportation and installation. As the first company to attempt anything like this, Loon dealt with a number of systems that were complex, challenging, or novel: superpressure balloons designed to stay aloft for hundreds of days, wind-dependant steering, a software defined network consisting of constantly moving nodes, and extremes of temperature and weather at 20km above Earth\u0026#39;s surface.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Prod Team\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The initial high-risk operations of Loon\u0026#39;s mission were avionic: could we launch and steer balloons carrying a networking payload long enough to reach and serve the targeted region? As such, the earliest failure reports within Loon (which weren\u0026#39;t officially called \u0026amp;#34;postmortems\u0026amp;#34; at the time) mostly involved balloon construction or flight, and drew on the experience of team members who had worked in the Avionics, Reliability Engineering, and/or Flight Safety fields. As Loon\u0026#39;s systems evolved and matured, they started to require operational reliability, as well. Just before graduating from a purely R\u0026amp;amp;D project in Google\u0026#39;s \u0026amp;#34;moonshot factory\u0026amp;#34; incubator X to a company with commercial goals, Loon started building a Site Reliability Engineering (SRE) team known internally as Prod Team.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In order to effectively offer internet connectivity to users, Loon had to solve network serving failures with the same rigor as hardware failures. Prod Team took the lead on a number of practices to improve network reliability. The Prod Team had three primary goals:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Ensure that the fleet\u0026#39;s automation, management, and safety-critical systems were built and operated to meet the high safety bar of the aviation industry.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Lead the integration of the communications services (e.g., LTE) end to end.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Own the mission of fielding and providing a reliable commercial service (Loon Library) in the real world.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h2\u0026gt;Postmortems at Loon\u0026lt;/h2\u0026gt;\u0026lt;h3\u0026gt;The Early Days\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Postmortems were one tool for reaching Prod Team\u0026#39;s (SRE\u0026#39;s) goals. Prod Team often interacted with SREs in other infrastructure support teams that the Loon service connected to, such as the team developing the Evolved Packet Core (EPC), our telco partner counterparts, and teams that handle edge network connectivity. Postmortems provided a common tool for sharing incident information across all these teams, and could even span multiple companies when upstream problems impacted customers.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;At Loon, postmortems served the following goals:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Document and transcribe the events, actions, and remedies related to an incident.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Provide a feedback loop to rectify problems.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Indicate where to build better safeguards and alerts.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Break down silos between teams in order to facilitate cross-functional knowledge sharing and accelerate development.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Identify macro themes and blind spots over the longer term.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;The combination of aerospace and high tech brought two strong practices of writing postmortems, but also the challenge of how to own, investigate, or follow up on problems that crossed those boundaries, or when it wasn\u0026#39;t clear where the system fault lay.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Loon\u0026#39;s teams across hardware, software, and operations orgs used postmortems, as was standard practice in their fields for incident response. The Flight Operations Team, which handled the day-to-day operations of steering launched balloons, captured in-flight issues in a tracking system. The tracking system was part of the anomaly resolution system devised to identify and resolve root cause problems. Seeking to complement the anomaly resolution system, the Flight Operations Team incorporated the SRE software team\u0026#39;s postmortem format for incidents that needed further investigation\u0026amp;#8212; for example, failure to avoid a storm system, deviations from the simulated (expected) flight path that led to an incident, and flight operator actions that directly or indirectly caused an incident. Given that most incidents spanned multiple teams (e.g., when automation failed to catch an incorrect command sent by a flight operator, which resulted in a hardware failure), utilizing a consistent postmortem format across teams simplified collaboration.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The Aviation and Systems Safety Team, which focused on safety related to the flight system and flight process, also brought their own tradition and best practices of postmortems. Their motto, \u0026amp;#34;Own our Safety\u0026amp;#34;, brought a commitment to continually improving safety performance and building a positive safety culture across the company. This was one of the strengths of Loon\u0026#39;s culture: all the organizations were aligned not just on our audacious vision to \u0026amp;#34;connect people everywhere\u0026amp;#34;, but also on doing so safely and effectively. However, because industry standards for postmortems and how to handle different types of problems varied across teams, there was some divergence in process. We proactively encouraged teams to share postmortems between teams, between orgs, and across the company so that anyone could provide feedback and insight into an incident. In that way, anyone at Loon could contribute to a postmortem, see how an incident was handled, and learn about the breadth of challenges that Loon was solving.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Challenges\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;While everyone agreed that postmortems were an important practice, in a fast moving start-up culture, it was a struggle to comprehensively follow through on action items. This probably comes as no surprise to developers in similar environments\u0026amp;#8212; when the platform or services that require investment are rapidly changing or being replaced, it\u0026#39;s hard to spend resources on not repeating the same mistakes. Ideally, we would have prioritized postmortems that focused on best practices and learnings that were applicable to multiple generations of the platform, but those weren\u0026#39;t easy to identify at the time of each incident.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Even though the company was not especially large, the novelty of Loon\u0026#39;s platform and interconnectedness of its operations made determining which team was responsible for writing a postmortem and investigating root causes difficult. For example, a 20 minute service disruption on the ground might be caused by a loss of connectivity from the balloon to the backhaul network, a pointing error with the antennae on the payload, insufficient battery levels, or wind that temporarily blew the balloon out of range. Actual causes could be quite nuanced, and often were attributable to interactions between multiple sub-systems. Thus, we had a chicken-and-egg problem: which team should start the postmortem and investigation, and when should they hand off the postmortem to the teams that likely owned the faulty system or process? Not all teams had a culture of postmortems, so the process could stall depending on the system where the root cause originated. For that reason, Loon\u0026#39;s Prod Team/SREs advocated for a company-wide blameless postmortem culture.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Much of how Loon used postmortems, especially in software development and Prod Team, was in line with SRE industry standards. In the early days of Loon, however, there were no service level objectives or agreements (SLO/As). As Loon was an R\u0026amp;amp;D project, we wrote postmortems when a test network failed to boot after launch, or when performance didn\u0026#39;t meet the team\u0026#39;s predictions, rather than for \u0026amp;#34;service outages\u0026amp;#34;. Later on, when Loon supplied commercial service in disaster relief areas in Peru and Kenya, the Prod Team could more clearly identify the types of user-facing incidents that required postmortems due to failure to meet SLAs.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Improving and Standardizing Loon\u0026#39;s Postmortem Processes\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Moving Loon from an R\u0026amp;amp;D model to the model of reliability and safety necessary for a commercial offering required more than simply performing postmortems. Sharing the postmortems openly and widely across Loon was critical to building a culture of continuous improvement and addressing root causes.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To increase cross-team awareness of incidents, in 2019 we instituted a Postmortem Working Group. In addition to reading and discussing recent postmortems from across the company, the goals of the working group were to make it easier to write postmortems, promote the practice of writing postmortems, increase sharing across teams, and discuss the findings of these incidents in order to learn the patterns of failure. Its founding goal was to \u0026amp;#34;\u0026lt;i\u0026gt;Cultivate a postmortem culture in Loon to encourage thoughtful risk taking, to take advantage of mistakes, and to provide structure to support improvement over time.\u0026lt;/i\u0026gt;\u0026amp;#34; While the volume of postmortems could ebb and flow across weeks and months, over multiple years of commercial service we expected to be able to identify macro-trends that needed to be addressed with the cooperation of multiple teams.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In addition to the Postmortem Working Group, we also created a postmortem mailing list and a repository of all postmortems, and presented a \u0026amp;#34;Lunch \u0026amp;amp; Learn\u0026amp;#34; on blameless postmortems (see example slide below). Prod Team and several other teams\u0026#39; meetings had a standing agenda item to review postmortems of interest from across the company, and we sent a semi-annual email celebrating Loon\u0026#39;s \u0026amp;#34;best-of\u0026amp;#34; recent incidents: the most interesting or educational outages.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eFounded by Google SRE alumni, it is no surprise that Loon\u0026#39;s Production Engineering/SRE team instituted a culture of blameless postmortems that became a key feature of Loon\u0026#39;s approach to incident response. Blameless postmortems originated as an aerospace practice in the mid-20th century, so it was particularly fitting that they came full circle to be used at a company that melded cutting edge aerospace work with the development of a communications platform and the world\u0026#39;s first stratospheric temporospatial software defined network. The use of postmortems became a standardizing factor across Loon\u0026#39;s teams— from avionics and manufacturing, to flight operations, to software platforms and network service. This blog post discusses how Loon moved from a heterogeneous approach to postmortems to eventually standardize and share this practice across the organization— a shift that helped the company move from R\u0026amp;D to commercial service in 2020.\u003c/p\u003e\u003ch2\u003eBackground\u003c/h2\u003e\u003ch3\u003ePostmortems\u003c/h3\u003e\u003cp\u003eMany industries have adopted the use of postmortems— they are fairly common in high-risk fields where mistakes can be fatal or extremely expensive. Postmortems are also widespread in industries and projects where bad processes or assumptions can incur expensive project development costs and avoiding repeat mistakes is a priority. Individual industries and organizations often develop their own postmortem standards or templates so that postmortems are easier to create and digest across teams.\u003c/p\u003e\u003cp\u003eBlameless postmortems likely originated in the healthcare and aerospace industries in the mid-20th century. Because of the high cost of failure, these industries needed to create a culture of transparency and continuous improvement that could only come from openly discussing failure. As the original SRE book states, blameless postmortems are key to \u0026#34;an environment where every \u0026#39;mistake\u0026#39; is seen as an opportunity to strengthen the system.\u0026#34; \u003c/p\u003e\u003cp\u003eThe goal of a postmortem is to document an incident or event in order to foster learning from it, both among the affected teams and beyond. The postmortem usually includes a timeline of what happened, the solutions implemented, the incident\u0026#39;s impact, the investigation into root causes, and changes or follow-ups to stop it from happening again. To facilitate learning, SRE\u0026#39;s postmortem format includes both what went well— acknowledging the successes that should be maintained and expanded— and what went poorly and needs to be changed. In this way, postmortem action items are key to prioritizing work that ensures the same failures don\u0026#39;t happen again.\u003c/p\u003e\u003ch3\u003eLoon\u003c/h3\u003e\u003cp\u003eLoon aimed to supply internet access to unserved and underserved populations around the world by providing connectivity via stratospheric balloons. These high altitude “flying cell towers” covered a much wider footprint than a terrestrial tower, and could be deployed (and repositioned) into the most remote corners of the earth without expensive overland transportation and installation. As the first company to attempt anything like this, Loon dealt with a number of systems that were complex, challenging, or novel: superpressure balloons designed to stay aloft for hundreds of days, wind-dependant steering, a software defined network consisting of constantly moving nodes, and extremes of temperature and weather at 20km above Earth\u0026#39;s surface.\u003c/p\u003e\u003ch3\u003eProd Team\u003c/h3\u003e\u003cp\u003eThe initial high-risk operations of Loon\u0026#39;s mission were avionic: could we launch and steer balloons carrying a networking payload long enough to reach and serve the targeted region? As such, the earliest failure reports within Loon (which weren\u0026#39;t officially called \u0026#34;postmortems\u0026#34; at the time) mostly involved balloon construction or flight, and drew on the experience of team members who had worked in the Avionics, Reliability Engineering, and/or Flight Safety fields. As Loon\u0026#39;s systems evolved and matured, they started to require operational reliability, as well. Just before graduating from a purely R\u0026amp;D project in Google\u0026#39;s \u0026#34;moonshot factory\u0026#34; incubator X to a company with commercial goals, Loon started building a Site Reliability Engineering (SRE) team known internally as Prod Team. \u003c/p\u003e\u003cp\u003eIn order to effectively offer internet connectivity to users, Loon had to solve network serving failures with the same rigor as hardware failures. Prod Team took the lead on a number of practices to improve network reliability. The Prod Team had three primary goals: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eEnsure that the fleet\u0026#39;s automation, management, and safety-critical systems were built and operated to meet the high safety bar of the aviation industry.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLead the integration of the communications services (e.g., LTE) end to end.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOwn the mission of fielding and providing a reliable commercial service (Loon Library) in the real world.  \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003ePostmortems at Loon\u003c/h2\u003e\u003ch3\u003eThe Early Days\u003c/h3\u003e\u003cp\u003ePostmortems were one tool for reaching Prod Team\u0026#39;s (SRE\u0026#39;s) goals. Prod Team often interacted with SREs in other infrastructure support teams that the Loon service connected to, such as the team developing the Evolved Packet Core (EPC), our telco partner counterparts, and teams that handle edge network connectivity. Postmortems provided a common tool for sharing incident information across all these teams, and could even span multiple companies when upstream problems impacted customers.\u003c/p\u003e\u003cp\u003eAt Loon, postmortems served the following goals:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDocument and transcribe the events, actions, and remedies related to an incident.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eProvide a feedback loop to rectify problems.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIndicate where to build better safeguards and alerts.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBreak down silos between teams in order to facilitate cross-functional knowledge sharing and accelerate development.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIdentify macro themes and blind spots over the longer term. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe combination of aerospace and high tech brought two strong practices of writing postmortems, but also the challenge of how to own, investigate, or follow up on problems that crossed those boundaries, or when it wasn\u0026#39;t clear where the system fault lay.\u003c/p\u003e\u003cp\u003eLoon\u0026#39;s teams across hardware, software, and operations orgs used postmortems, as was standard practice in their fields for incident response. The Flight Operations Team, which handled the day-to-day operations of steering launched balloons, captured in-flight issues in a tracking system. The tracking system was part of the anomaly resolution system devised to identify and resolve root cause problems. Seeking to complement the anomaly resolution system, the Flight Operations Team incorporated the SRE software team\u0026#39;s postmortem format for incidents that needed further investigation— for example, failure to avoid a storm system, deviations from the simulated (expected) flight path that led to an incident, and flight operator actions that directly or indirectly caused an incident. Given that most incidents spanned multiple teams (e.g., when automation failed to catch an incorrect command sent by a flight operator, which resulted in a hardware failure), utilizing a consistent postmortem format across teams simplified collaboration.\u003c/p\u003e\u003cp\u003eThe Aviation and Systems Safety Team, which focused on safety related to the flight system and flight process, also brought their own tradition and best practices of postmortems. Their motto, \u0026#34;Own our Safety\u0026#34;, brought a commitment to continually improving safety performance and building a positive safety culture across the company. This was one of the strengths of Loon\u0026#39;s culture: all the organizations were aligned not just on our audacious vision to \u0026#34;connect people everywhere\u0026#34;, but also on doing so safely and effectively. However, because industry standards for postmortems and how to handle different types of problems varied across teams, there was some divergence in process. We proactively encouraged teams to share postmortems between teams, between orgs, and across the company so that anyone could provide feedback and insight into an incident. In that way, anyone at Loon could contribute to a postmortem, see how an incident was handled, and learn about the breadth of challenges that Loon was solving. \u003c/p\u003e\u003ch3\u003eChallenges\u003c/h3\u003e\u003cp\u003eWhile everyone agreed that postmortems were an important practice, in a fast moving start-up culture, it was a struggle to comprehensively follow through on action items. This probably comes as no surprise to developers in similar environments— when the platform or services that require investment are rapidly changing or being replaced, it\u0026#39;s hard to spend resources on not repeating the same mistakes. Ideally, we would have prioritized postmortems that focused on best practices and learnings that were applicable to multiple generations of the platform, but those weren\u0026#39;t easy to identify at the time of each incident.\u003c/p\u003e\u003cp\u003eEven though the company was not especially large, the novelty of Loon\u0026#39;s platform and interconnectedness of its operations made determining which team was responsible for writing a postmortem and investigating root causes difficult. For example, a 20 minute service disruption on the ground might be caused by a loss of connectivity from the balloon to the backhaul network, a pointing error with the antennae on the payload, insufficient battery levels, or wind that temporarily blew the balloon out of range. Actual causes could be quite nuanced, and often were attributable to interactions between multiple sub-systems. Thus, we had a chicken-and-egg problem: which team should start the postmortem and investigation, and when should they hand off the postmortem to the teams that likely owned the faulty system or process? Not all teams had a culture of postmortems, so the process could stall depending on the system where the root cause originated. For that reason, Loon\u0026#39;s Prod Team/SREs advocated for a company-wide blameless postmortem culture. \u003c/p\u003e\u003cp\u003eMuch of how Loon used postmortems, especially in software development and Prod Team, was in line with SRE industry standards. In the early days of Loon, however, there were no service level objectives or agreements (SLO/As). As Loon was an R\u0026amp;D project, we wrote postmortems when a test network failed to boot after launch, or when performance didn\u0026#39;t meet the team\u0026#39;s predictions, rather than for \u0026#34;service outages\u0026#34;. Later on, when Loon supplied commercial service in disaster relief areas in Peru and Kenya, the Prod Team could more clearly identify the types of user-facing incidents that required postmortems due to failure to meet SLAs.\u003c/p\u003e\u003ch3\u003eImproving and Standardizing Loon\u0026#39;s Postmortem Processes\u003c/h3\u003e\u003cp\u003eMoving Loon from an R\u0026amp;D model to the model of reliability and safety necessary for a commercial offering required more than simply performing postmortems. Sharing the postmortems openly and widely across Loon was critical to building a culture of continuous improvement and addressing root causes. \u003c/p\u003e\u003cp\u003eTo increase cross-team awareness of incidents, in 2019 we instituted a Postmortem Working Group. In addition to reading and discussing recent postmortems from across the company, the goals of the working group were to make it easier to write postmortems, promote the practice of writing postmortems, increase sharing across teams, and discuss the findings of these incidents in order to learn the patterns of failure. Its founding goal was to \u0026#34;\u003ci\u003eCultivate a postmortem culture in Loon to encourage thoughtful risk taking, to take advantage of mistakes, and to provide structure to support improvement over time.\u003c/i\u003e\u0026#34; While the volume of postmortems could ebb and flow across weeks and months, over multiple years of commercial service we expected to be able to identify macro-trends that needed to be addressed with the cooperation of multiple teams.\u003c/p\u003e\u003cp\u003eIn addition to the Postmortem Working Group, we also created a postmortem mailing list and a repository of all postmortems, and presented a \u0026#34;Lunch \u0026amp; Learn\u0026#34; on blameless postmortems (see example slide below). Prod Team and several other teams\u0026#39; meetings had a standing agenda item to review postmortems of interest from across the company, and we sent a semi-annual email celebrating Loon\u0026#39;s \u0026#34;best-of\u0026#34; recent incidents: the most interesting or educational outages.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eFounded by Google SRE alumni, it is no surprise that Loon's Production Engineering/SRE team instituted a culture of blameless postmortems that became a key feature of Loon's approach to incident response. Blameless postmortems originated as an aerospace practice in the mid-20th century, so it was particularly fitting that they came full circle to be used at a company that melded cutting edge aerospace work with the development of a communications platform and the world's first stratospheric temporospatial software defined network. The use of postmortems became a standardizing factor across Loon's teams— from avionics and manufacturing, to flight operations, to software platforms and network service. This blog post discusses how Loon moved from a heterogeneous approach to postmortems to eventually standardize and share this practice across the organization— a shift that helped the company move from R\u0026amp;D to commercial service in 2020.\u003c/p\u003e\u003ch2\u003eBackground\u003c/h2\u003e\u003ch3\u003ePostmortems\u003c/h3\u003e\u003cp\u003eMany industries have adopted the use of postmortems— they are fairly common in high-risk fields where mistakes can be fatal or extremely expensive. Postmortems are also widespread in industries and projects where bad processes or assumptions can incur expensive project development costs and avoiding repeat mistakes is a priority. Individual industries and organizations often develop their own postmortem standards or templates so that postmortems are easier to create and digest across teams.\u003c/p\u003e\u003cp\u003eBlameless postmortems likely originated in the healthcare and aerospace industries in the mid-20th century. Because of the high cost of failure, these industries needed to create a culture of transparency and continuous improvement that could only come from openly discussing failure. As the original SRE book states, blameless postmortems are key to \"an environment where every 'mistake' is seen as an opportunity to strengthen the system.\" \u003c/p\u003e\u003cp\u003eThe goal of a postmortem is to document an incident or event in order to foster learning from it, both among the affected teams and beyond. The postmortem usually includes a timeline of what happened, the solutions implemented, the incident's impact, the investigation into root causes, and changes or follow-ups to stop it from happening again. To facilitate learning, SRE's postmortem format includes both what went well— acknowledging the successes that should be maintained and expanded— and what went poorly and needs to be changed. In this way, postmortem action items are key to prioritizing work that ensures the same failures don't happen again.\u003c/p\u003e\u003ch3\u003eLoon\u003c/h3\u003e\u003cp\u003eLoon aimed to supply internet access to unserved and underserved populations around the world by providing connectivity via stratospheric balloons. These high altitude “flying cell towers” covered a much wider footprint than a terrestrial tower, and could be deployed (and repositioned) into the most remote corners of the earth without expensive overland transportation and installation. As the first company to attempt anything like this, Loon dealt with a number of systems that were complex, challenging, or novel: superpressure balloons designed to stay aloft for hundreds of days, wind-dependant steering, a software defined network consisting of constantly moving nodes, and extremes of temperature and weather at 20km above Earth's surface.\u003c/p\u003e\u003ch3\u003eProd Team\u003c/h3\u003e\u003cp\u003eThe initial high-risk operations of Loon's mission were avionic: could we launch and steer balloons carrying a networking payload long enough to reach and serve the targeted region? As such, the earliest failure reports within Loon (which weren't officially called \"postmortems\" at the time) mostly involved balloon construction or flight, and drew on the experience of team members who had worked in the Avionics, Reliability Engineering, and/or Flight Safety fields. As Loon's systems evolved and matured, they started to require operational reliability, as well. Just before graduating from a purely R\u0026amp;D project in Google's \"moonshot factory\" incubator X to a company with commercial goals, Loon started building a Site Reliability Engineering (SRE) team known internally as Prod Team. \u003c/p\u003e\u003cp\u003eIn order to effectively offer internet connectivity to users, Loon had to solve network serving failures with the same rigor as hardware failures. Prod Team took the lead on a number of practices to improve network reliability. The Prod Team had three primary goals: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eEnsure that the fleet's automation, management, and safety-critical systems were built and operated to meet the high safety bar of the aviation industry.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLead the integration of the communications services (e.g., LTE) end to end.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOwn the mission of fielding and providing a reliable commercial service (Loon Library) in the real world.  \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003ePostmortems at Loon\u003c/h2\u003e\u003ch3\u003eThe Early Days\u003c/h3\u003e\u003cp\u003ePostmortems were one tool for reaching Prod Team's (SRE's) goals. Prod Team often interacted with SREs in other infrastructure support teams that the Loon service connected to, such as the team developing the Evolved Packet Core (EPC), our telco partner counterparts, and teams that handle edge network connectivity. Postmortems provided a common tool for sharing incident information across all these teams, and could even span multiple companies when upstream problems impacted customers.\u003c/p\u003e\u003cp\u003eAt Loon, postmortems served the following goals:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDocument and transcribe the events, actions, and remedies related to an incident.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eProvide a feedback loop to rectify problems.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIndicate where to build better safeguards and alerts.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBreak down silos between teams in order to facilitate cross-functional knowledge sharing and accelerate development.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIdentify macro themes and blind spots over the longer term. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe combination of aerospace and high tech brought two strong practices of writing postmortems, but also the challenge of how to own, investigate, or follow up on problems that crossed those boundaries, or when it wasn't clear where the system fault lay.\u003c/p\u003e\u003cp\u003eLoon's teams across hardware, software, and operations orgs used postmortems, as was standard practice in their fields for incident response. The Flight Operations Team, which handled the day-to-day operations of steering launched balloons, captured in-flight issues in a tracking system. The tracking system was part of the anomaly resolution system devised to identify and resolve root cause problems. Seeking to complement the anomaly resolution system, the Flight Operations Team incorporated the SRE software team's postmortem format for incidents that needed further investigation— for example, failure to avoid a storm system, deviations from the simulated (expected) flight path that led to an incident, and flight operator actions that directly or indirectly caused an incident. Given that most incidents spanned multiple teams (e.g., when automation failed to catch an incorrect command sent by a flight operator, which resulted in a hardware failure), utilizing a consistent postmortem format across teams simplified collaboration.\u003c/p\u003e\u003cp\u003eThe Aviation and Systems Safety Team, which focused on safety related to the flight system and flight process, also brought their own tradition and best practices of postmortems. Their motto, \"Own our Safety\", brought a commitment to continually improving safety performance and building a positive safety culture across the company. This was one of the strengths of Loon's culture: all the organizations were aligned not just on our audacious vision to \"connect people everywhere\", but also on doing so safely and effectively. However, because industry standards for postmortems and how to handle different types of problems varied across teams, there was some divergence in process. We proactively encouraged teams to share postmortems between teams, between orgs, and across the company so that anyone could provide feedback and insight into an incident. In that way, anyone at Loon could contribute to a postmortem, see how an incident was handled, and learn about the breadth of challenges that Loon was solving. \u003c/p\u003e\u003ch3\u003eChallenges\u003c/h3\u003e\u003cp\u003eWhile everyone agreed that postmortems were an important practice, in a fast moving start-up culture, it was a struggle to comprehensively follow through on action items. This probably comes as no surprise to developers in similar environments— when the platform or services that require investment are rapidly changing or being replaced, it's hard to spend resources on not repeating the same mistakes. Ideally, we would have prioritized postmortems that focused on best practices and learnings that were applicable to multiple generations of the platform, but those weren't easy to identify at the time of each incident.\u003c/p\u003e\u003cp\u003eEven though the company was not especially large, the novelty of Loon's platform and interconnectedness of its operations made determining which team was responsible for writing a postmortem and investigating root causes difficult. For example, a 20 minute service disruption on the ground might be caused by a loss of connectivity from the balloon to the backhaul network, a pointing error with the antennae on the payload, insufficient battery levels, or wind that temporarily blew the balloon out of range. Actual causes could be quite nuanced, and often were attributable to interactions between multiple sub-systems. Thus, we had a chicken-and-egg problem: which team should start the postmortem and investigation, and when should they hand off the postmortem to the teams that likely owned the faulty system or process? Not all teams had a culture of postmortems, so the process could stall depending on the system where the root cause originated. For that reason, Loon's Prod Team/SREs advocated for a company-wide blameless postmortem culture. \u003c/p\u003e\u003cp\u003eMuch of how Loon used postmortems, especially in software development and Prod Team, was in line with SRE industry standards. In the early days of Loon, however, there were no service level objectives or agreements (SLO/As). As Loon was an R\u0026amp;D project, we wrote postmortems when a test network failed to boot after launch, or when performance didn't meet the team's predictions, rather than for \"service outages\". Later on, when Loon supplied commercial service in disaster relief areas in Peru and Kenya, the Prod Team could more clearly identify the types of user-facing incidents that required postmortems due to failure to meet SLAs.\u003c/p\u003e\u003ch3\u003eImproving and Standardizing Loon's Postmortem Processes\u003c/h3\u003e\u003cp\u003eMoving Loon from an R\u0026amp;D model to the model of reliability and safety necessary for a commercial offering required more than simply performing postmortems. Sharing the postmortems openly and widely across Loon was critical to building a culture of continuous improvement and addressing root causes. \u003c/p\u003e\u003cp\u003eTo increase cross-team awareness of incidents, in 2019 we instituted a Postmortem Working Group. In addition to reading and discussing recent postmortems from across the company, the goals of the working group were to make it easier to write postmortems, promote the practice of writing postmortems, increase sharing across teams, and discuss the findings of these incidents in order to learn the patterns of failure. Its founding goal was to \"\u003ci\u003eCultivate a postmortem culture in Loon to encourage thoughtful risk taking, to take advantage of mistakes, and to provide structure to support improvement over time.\u003c/i\u003e\" While the volume of postmortems could ebb and flow across weeks and months, over multiple years of commercial service we expected to be able to identify macro-trends that needed to be addressed with the cooperation of multiple teams.\u003c/p\u003e\u003cp\u003eIn addition to the Postmortem Working Group, we also created a postmortem mailing list and a repository of all postmortems, and presented a \"Lunch \u0026amp; Learn\" on blameless postmortems (see example slide below). Prod Team and several other teams' meetings had a standing agenda item to review postmortems of interest from across the company, and we sent a semi-annual email celebrating Loon's \"best-of\" recent incidents: the most interesting or educational outages.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"loon.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/loon.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOnce we had a standardized postmortem template in place, we could adopt and reuse it to document commercial service field tests. By recording a timeline and incidents, defining a process and space to determine root causes of problems, recording measurements and metrics, and providing the structure for action item tracking, we brought the benefits of postmortem retrospectives to prospective tasks. \u003c/p\u003e\u003cp\u003eWhen Loon began commercial trials in countries like Peru and Kenya, we conducted numerous field tests. These tests required engineers from Loon and/or the telco partner to travel to remote locations to measure the strength of the LTE signal on the ground. Prod Team proactively used the postmortem template to document the field tests. It provided a useful format to record the log of test events, results that did and did not match expectations, and links to further investigations into those failures. As a cutting edge project in a highly variable operating environment, using the postmortem template as our default testing template was an acknowledgement that we were in a state of constant and rapid iteration and improvement. These trials took place in early to mid 2020, under the sudden specter of Covid and the subsequent shift towards working from home. The structured communications at the core of Loon's postmortem structure were particularly helpful as we moved from in-person coordination rooms to WFH.\u003c/p\u003e\u003ch3\u003eWhat Loon Learned from Standardizing Postmortems\u003c/h3\u003e\u003cp\u003ePostmortems are widely used in various industries because they are effective. At Loon, we saw that even fast moving startups and R\u0026amp;D projects should invest early in a transparent and blameless postmortem culture. That culture should include a clear process for writing postmortems, clear guidelines for when to conduct a postmortem, and a staffed commitment to follow up on action items. \u003c/p\u003e\u003cp\u003eMeta-reviews across postmortems and outages revealed several trends. \u003c/p\u003e\u003cp\u003eThe many points of failure we observed across the range of postmortems were indicative of both the complexity of Loon's systems and the complexity of some of its supporting infrastructure. Postmortems are equally adept at finding flaky tests and fragile processes vs. hardware failures or satellite network outages. These are complexities familiar to many startups, where postmortems can help manage the tradeoff between making changes safely vs. moving quickly and trying many new things.\u003c/p\u003e\u003cp\u003eLoon was still operating a superhero culture: across a wide range of issues, a small set of experts were repeatedly called upon to fix the system. This dynamic is common in startups, and not meant as a pejorative, but was markedly different from the system maturity that many of Prod Team/SRE were used to. Once we identified this pattern, our plan for commercial service was to staff a 24x7 oncall rotation, complemented by Program Managers driving intention processes to de-risk production\u003c/p\u003e\u003cp\u003ePostmortems provided a space to ask questions like, \"What other issues could pop up in this realm?\", which prompted us to solve for the broader case of problems rather than specific problems we'd already seen. This practice also stopped people from brushing off problems in the name of development speed, or from dismissing issues because they \"just concerned a prototype\".\u003c/p\u003e\u003ch2\u003eTips and Takeaways\u003c/h2\u003e\u003cp\u003eWhile the specifics of Loon's journey to standardize postmortems tell the story of one company, we have some tips and takeaways that should be applicable at most organizations.\u003c/p\u003e\u003ch3\u003eTip 1: Adopting a blameless postmortem culture requires everyone to participate\u003c/h3\u003e\u003cp\u003eAlthough the initiative of writing postmortems often originates with a software team, if you want every team to adopt the practice, we suggest trying the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eGive a talk about postmortems and how and why they could benefit all.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eForm a postmortem working group.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eInvite people representing different teams to be part of the postmortem working group. They will give insights into what could work better for their respective teams.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDon't make the postmortem working group responsible for writing the postmortems— this approach doesn't scale. Reviewing and consulting on postmortems may be in scope of their duties, especially while new teams are adopting this practice.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eTip 2: Define a lightweight postmortem process\u003c/h3\u003e\u003cp\u003eEspecially during adoption, you want teams to see the benefits of postmortems, not the burden of writing them. Creating a postmortem template with minimum requirements can be helpful.\u003c/p\u003e\u003ch3\u003eTip 3: Define a clear owner for postmortems\u003c/h3\u003e\u003cp\u003eWho should write a postmortem and when? For software teams with an oncall rotation, the answer is clear: the person who was oncall during the incident is the owner, and we write postmortems when a service interruption breached SLOs. But when the service has no SLOs, or when a team doesn't have an oncall rotation, you need defined criteria. Bonus points if the outage involves multiple systems and teams. The following exercises can help in this area:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eReflect on these topics from the point of view of each team, and from the point of view of the interaction between teams.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eFor each team, define what type of incident(s) should trigger a postmortem.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eWithin the team, define who should own writing each postmortem. Avoid putting the entire burden on the same person frequently; consider forming a rotation.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eTip 4: Encourage blameless postmortems and make people proud of them\u003c/h3\u003e\u003cp\u003eConsider some activities that can help foster the blameless postmortem culture:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eWrite a report of the best postmortems over a given period and circulate them broadly.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eConduct training on how to write postmortems.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTrain managers and encourage them to prioritize postmortems on their teams.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eConclusion\u003c/h2\u003e\u003cp\u003eWhen Loon shut down, addressing all these points was still a work in progress. We don't have a teachable moment of “this postmortem process will solve your failures”, because postmortems don't do that. However, we could see where postmortems stopped us from needing to deal with the same failures repeatedly… and where sometimes we did experience repeat incidents because the AIs from the first postmortem weren't prioritized enough. And so this piece of writing— effectively, a postmortem on Loon's postmortems—serves up a familiar lesson: postmortems work, but only as well as they are widely accepted and adhered to.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Genric_GCP_upA1oyz.max-2200x2200.png",
      "date_published": "2021-12-07T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eGiselle Font\u003c/name\u003e\u003ctitle\u003eSite Reliability Engineer, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/terraform-gitops-with-google-cloud-build-and-storage/",
      "title": "Ensuring scale and compliance of your Terraform Deployment with Cloud Build",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://www.terraform.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Terraform\u0026lt;/a\u0026gt; is an open source Infrastructure as Code tool that is popular with platform developers building reusable cloud automation. The \u0026lt;a href=\u0026#34;https://registry.terraform.io/providers/hashicorp/google/latest/docs\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Terraform Provider for Google Cloud Platform\u0026lt;/a\u0026gt; continues to add support for the latest Google Cloud features, such as \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster\u0026#34;\u0026gt;Anthos on GKE\u0026lt;/a\u0026gt;, and our teams continue to expand Terraform integrations including \u0026lt;a href=\u0026#34;https://cloud.google.com/foundation-toolkit\u0026#34;\u0026gt;Cloud Foundation Toolkit\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/terraform-validator#terraform-validator\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Terraform Validator\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;How do teams use Terraform on Google Cloud? While the simplest approach is to run \u0026lt;code\u0026gt;terraform init\u0026lt;/code\u0026gt;, \u0026lt;code\u0026gt;plan\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;apply\u0026lt;/code\u0026gt; directly from your terminal,\u0026amp;#160; it cannot be recommended for automating your production deployments. First, there is a decision on how to store your Terraform \u0026lt;a href=\u0026#34;https://www.terraform.io/docs/language/state/index.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;state\u0026lt;/a\u0026gt; in a way that is secure, compliant and enables team collaboration. Secondly there\u0026amp;#8217;s a question of scale and reliability. Over the course of even the simplest cloud deployment, Terraform can end up making thousands of Create/Read/Update/Delete API calls to the endpoints used by the Terraform providers, some of which will inevitably hit quota issues or need to be retried for other reasons. For platform administrators, who are looking to ensure the best deployment practices for their curated Terraform solutions,\u0026amp;#160; while benefiting from the simplicity of Google Cloud Console, there\u0026amp;#8217;s \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/private-catalog-uses-terraform-to-update-available-solutions\u0026#34;\u0026gt;Terraform Private Catalog integration\u0026lt;/a\u0026gt; that we enabled earlier this year.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Outside of Private Catalog, \u0026lt;a href=\u0026#34;https://cloud.google.com/build\u0026#34;\u0026gt;Cloud Build\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/storage\u0026#34;\u0026gt;Cloud Storage\u0026lt;/a\u0026gt; have been the recommended approach to use Terraform on Google Cloud. Using a remote\u0026lt;a href=\u0026#34;https://www.terraform.io/docs/language/settings/backends/index.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; backend\u0026lt;/a\u0026gt; prevents race conditions and simplifies sharing reusable modules between different configurations. With Cloud Build you can configure a GitOps CI/CD pipeline to automatically \u0026lt;code\u0026gt;plan\u0026lt;/code\u0026gt; and \u0026lt;code\u0026gt;apply\u0026lt;/code\u0026gt; your Terraform configuration when changes are pushed into the repo.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;These are widely popularized benefits explored in \u0026lt;a href=\u0026#34;https://cloud.google.com/architecture/managing-infrastructure-as-code\u0026#34;\u0026gt;Managing infrastructure as code with Terraform, Cloud Build, and GitOps\u0026lt;/a\u0026gt;. In addition, there are lesser known advantages of Cloud Build, particularly for enterprise customers: Cloud Build\u0026amp;#8217;s concurrency capabilities and VPC-SC support, Cloud Storage versioning, security and compliance. Let\u0026amp;#8217;s explore these benefits in more detail.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Cloud Build\u0026amp;#8217;s ability to scale makes it capable to process multiple Terraform deployments across the regions globally and simultaneously. By default, Cloud Build supports 30 concurrent builds, with additional builds queued and processed after the running builds complete. In some cases it may not be enough. Customers who initiate parallel deployments to multiple zones, or, those who provision infrastructure on behalf of multiple tenants, often require running more concurrent deployments to complete all of them within the allotted deployment window. Cloud Build private pool feature allows up to \u0026lt;a href=\u0026#34;https://cloud.google.com/build/quotas\u0026#34;\u0026gt;100 concurrent builds\u0026lt;/a\u0026gt; which may be further adjusted upon request. This is an example of creating a private pool and then using it when submitting a build:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003ca href=\"https://www.terraform.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://www.terraform.io\" track-metadata-module=\"post\"\u003eTerraform\u003c/a\u003e is an open source Infrastructure as Code tool that is popular with platform developers building reusable cloud automation. The \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://registry.terraform.io\" track-metadata-module=\"post\"\u003eTerraform Provider for Google Cloud Platform\u003c/a\u003e continues to add support for the latest Google Cloud features, such as \u003ca href=\"https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster\" track-metadata-module=\"post\"\u003eAnthos on GKE\u003c/a\u003e, and our teams continue to expand Terraform integrations including \u003ca href=\"https://cloud.google.com/foundation-toolkit\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/foundation-toolkit\" track-metadata-module=\"post\"\u003eCloud Foundation Toolkit\u003c/a\u003e and \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-validator#terraform-validator\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eTerraform Validator\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHow do teams use Terraform on Google Cloud? While the simplest approach is to run \u003ccode\u003eterraform init\u003c/code\u003e, \u003ccode\u003eplan\u003c/code\u003e and \u003ccode\u003eapply\u003c/code\u003e directly from your terminal,  it cannot be recommended for automating your production deployments. First, there is a decision on how to store your Terraform \u003ca href=\"https://www.terraform.io/docs/language/state/index.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://www.terraform.io\" track-metadata-module=\"post\"\u003estate\u003c/a\u003e in a way that is secure, compliant and enables team collaboration. Secondly there’s a question of scale and reliability. Over the course of even the simplest cloud deployment, Terraform can end up making thousands of Create/Read/Update/Delete API calls to the endpoints used by the Terraform providers, some of which will inevitably hit quota issues or need to be retried for other reasons. For platform administrators, who are looking to ensure the best deployment practices for their curated Terraform solutions,  while benefiting from the simplicity of Google Cloud Console, there’s \u003ca href=\"https://cloud.google.com/blog/products/management-tools/private-catalog-uses-terraform-to-update-available-solutions\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/private-catalog-uses-terraform-to-update-available-solutions\" track-metadata-module=\"post\"\u003eTerraform Private Catalog integration\u003c/a\u003e that we enabled earlier this year.\u003c/p\u003e\u003cp\u003eOutside of Private Catalog, \u003ca href=\"https://cloud.google.com/build\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/build\" track-metadata-module=\"post\"\u003eCloud Build\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/storage\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/storage\" track-metadata-module=\"post\"\u003eCloud Storage\u003c/a\u003e have been the recommended approach to use Terraform on Google Cloud. Using a remote\u003ca href=\"https://www.terraform.io/docs/language/settings/backends/index.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://www.terraform.io\" track-metadata-module=\"post\"\u003e backend\u003c/a\u003e prevents race conditions and simplifies sharing reusable modules between different configurations. With Cloud Build you can configure a GitOps CI/CD pipeline to automatically \u003ccode\u003eplan\u003c/code\u003e and \u003ccode\u003eapply\u003c/code\u003e your Terraform configuration when changes are pushed into the repo. \u003c/p\u003e\u003cp\u003eThese are widely popularized benefits explored in \u003ca href=\"https://cloud.google.com/architecture/managing-infrastructure-as-code\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/architecture/managing-infrastructure-as-code\" track-metadata-module=\"post\"\u003eManaging infrastructure as code with Terraform, Cloud Build, and GitOps\u003c/a\u003e. In addition, there are lesser known advantages of Cloud Build, particularly for enterprise customers: Cloud Build’s concurrency capabilities and VPC-SC support, Cloud Storage versioning, security and compliance. Let’s explore these benefits in more detail.\u003c/p\u003e\u003cp\u003eCloud Build’s ability to scale makes it capable to process multiple Terraform deployments across the regions globally and simultaneously. By default, Cloud Build supports 30 concurrent builds, with additional builds queued and processed after the running builds complete. In some cases it may not be enough. Customers who initiate parallel deployments to multiple zones, or, those who provision infrastructure on behalf of multiple tenants, often require running more concurrent deployments to complete all of them within the allotted deployment window. Cloud Build private pool feature allows up to \u003ca href=\"https://cloud.google.com/build/quotas\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/build/quotas\" track-metadata-module=\"post\"\u003e100 concurrent builds\u003c/a\u003e which may be further adjusted upon request. This is an example of creating a private pool and then using it when submitting a build:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ca href=\"https://www.terraform.io/\" target=\"_blank\"\u003eTerraform\u003c/a\u003e is an open source Infrastructure as Code tool that is popular with platform developers building reusable cloud automation. The \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs\" target=\"_blank\"\u003eTerraform Provider for Google Cloud Platform\u003c/a\u003e continues to add support for the latest Google Cloud features, such as \u003ca href=\"https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster\"\u003eAnthos on GKE\u003c/a\u003e, and our teams continue to expand Terraform integrations including \u003ca href=\"https://cloud.google.com/foundation-toolkit\"\u003eCloud Foundation Toolkit\u003c/a\u003e and \u003ca href=\"https://github.com/GoogleCloudPlatform/terraform-validator#terraform-validator\" target=\"_blank\"\u003eTerraform Validator\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHow do teams use Terraform on Google Cloud? While the simplest approach is to run \u003ccode\u003eterraform init\u003c/code\u003e, \u003ccode\u003eplan\u003c/code\u003e and \u003ccode\u003eapply\u003c/code\u003e directly from your terminal,  it cannot be recommended for automating your production deployments. First, there is a decision on how to store your Terraform \u003ca href=\"https://www.terraform.io/docs/language/state/index.html\" target=\"_blank\"\u003estate\u003c/a\u003e in a way that is secure, compliant and enables team collaboration. Secondly there’s a question of scale and reliability. Over the course of even the simplest cloud deployment, Terraform can end up making thousands of Create/Read/Update/Delete API calls to the endpoints used by the Terraform providers, some of which will inevitably hit quota issues or need to be retried for other reasons. For platform administrators, who are looking to ensure the best deployment practices for their curated Terraform solutions,  while benefiting from the simplicity of Google Cloud Console, there’s \u003ca href=\"https://cloud.google.com/blog/products/management-tools/private-catalog-uses-terraform-to-update-available-solutions\"\u003eTerraform Private Catalog integration\u003c/a\u003e that we enabled earlier this year.\u003c/p\u003e\u003cp\u003eOutside of Private Catalog, \u003ca href=\"https://cloud.google.com/build\"\u003eCloud Build\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/storage\"\u003eCloud Storage\u003c/a\u003e have been the recommended approach to use Terraform on Google Cloud. Using a remote\u003ca href=\"https://www.terraform.io/docs/language/settings/backends/index.html\" target=\"_blank\"\u003ebackend\u003c/a\u003e prevents race conditions and simplifies sharing reusable modules between different configurations. With Cloud Build you can configure a GitOps CI/CD pipeline to automatically \u003ccode\u003eplan\u003c/code\u003e and \u003ccode\u003eapply\u003c/code\u003e your Terraform configuration when changes are pushed into the repo. \u003c/p\u003e\u003cp\u003eThese are widely popularized benefits explored in \u003ca href=\"https://cloud.google.com/architecture/managing-infrastructure-as-code\"\u003eManaging infrastructure as code with Terraform, Cloud Build, and GitOps\u003c/a\u003e. In addition, there are lesser known advantages of Cloud Build, particularly for enterprise customers: Cloud Build’s concurrency capabilities and VPC-SC support, Cloud Storage versioning, security and compliance. Let’s explore these benefits in more detail.\u003c/p\u003e\u003cp\u003eCloud Build’s ability to scale makes it capable to process multiple Terraform deployments across the regions globally and simultaneously. By default, Cloud Build supports 30 concurrent builds, with additional builds queued and processed after the running builds complete. In some cases it may not be enough. Customers who initiate parallel deployments to multiple zones, or, those who provision infrastructure on behalf of multiple tenants, often require running more concurrent deployments to complete all of them within the allotted deployment window. Cloud Build private pool feature allows up to \u003ca href=\"https://cloud.google.com/build/quotas\"\u003e100 concurrent builds\u003c/a\u003e which may be further adjusted upon request. This is an example of creating a private pool and then using it when submitting a build:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eA full step by step example of creating a private pool and submitting 80+ Terraform deployments with Cloud Build simultaneously is available \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/terraform/examples/infra_at_scale\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eUsing Cloud Build removes the need to build a custom high-scale Terraform provisioning service and provides \u003ca href=\"https://cloud.google.com/build/docs/view-build-results\"\u003eobservability and diagnostics\u003c/a\u003e for each of the build instances launched and their results. \u003c/p\u003e\u003cp\u003eUsing Cloud Build with private pools enables recommended security features, such as \u003ca href=\"https://cloud.google.com/build/docs/private-pools/using-vpc-service-controls\"\u003eVPC Service Controls\u003c/a\u003ethat allows setting secure perimeter to protect against data exfiltration, with additional restrictions to further restrict it to using the specified private pools. This makes it unnecessary to configure a dedicated \u003ca href=\"https://en.wikipedia.org/wiki/Bastion_host\" target=\"_blank\"\u003ebastion host\u003c/a\u003e inside the perimeter, which improves the overall security posture.\u003c/p\u003eBeyond just using Cloud Storage for remote storage, additional reasons to use Cloud Storage include versioning, security and compliance. Enabling versioning protects against state file corruption and allows you to view earlier versions. Versioning can be enabled with \u003ccode\u003egsutil\u003c/code\u003e command:\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn addition to versioning, you can use \u003ca href=\"https://cloud.google.com/security/encryption/customer-supplied-encryption-keys\"\u003eCustomer-Supplied Encryption Keys\u003c/a\u003e to encrypt the Terraform state file. After you generated the key you can specify it as encryption_key parameter of your backend object:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOnce encrypted you can still view the contents of your state by \u003ca href=\"https://cloud.google.com/storage/docs/encryption/customer-supplied-keys#gsutil\"\u003eadding encryption_key option to boto configuration file\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eFinally, Cloud Storage is one of the Google Cloud services covered by \u003ca href=\"https://cloud.google.com/security/compliance/fedramp\"\u003eFedRAMP High\u003c/a\u003e, which is important for enterprises  that are seeking their own FedRAMP on top of Google Cloud (for more details see \u003ca href=\"https://cloud.google.com/security/compliance\"\u003eCompliance resource center\u003c/a\u003e).\u003c/p\u003e\u003cp\u003eTo summarize, using Cloud Build and Cloud Storage for your Terraform deployments enable high scalability, security and compliance with simpler configuration and via familiar \u003ccode\u003egcloud\u003c/code\u003e and Google Cloud console interface. Please check out this \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/terraform/examples/infra_at_scale\" target=\"_blank\"\u003esample\u003c/a\u003e for step by step guidance.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/cloud-build-private-pools-offers-cicd-for-private-networks/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eIntroducing Cloud Build private pools: Secure CI/CD for private networks\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eWith new private pools, you can use Google Cloud’s hosted Cloud Build CI/CD service on resources in your private network or in other clouds.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-2200x2200.jpg",
      "date_published": "2021-12-06T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlex Bulankou\u003c/name\u003e\u003ctitle\u003eEngineering Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/supercharge-your-devops-practice-with-sre-principles/",
      "title": "Want to supercharge your DevOps practice? Research says try SRE",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eReliability matters. When users can’t access your application, if it’s slow to respond, or it behaves unexpectedly, they don’t get the value that you intend to provide. That’s why at Google we like to say that \u003ca href=\"https://sre.google/workbook/reaching-beyond/\" target=\"_blank\"\u003e\u003ci\u003ereliability is the most important feature of any system\u003c/i\u003e\u003c/a\u003e. Its impact can be seen all the way to the bottom line, as downtime comes with steep costs—to revenue, to reputation, and to user loyalty. \u003c/p\u003e\u003cp\u003eFrom the beginning of the \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\"\u003eDevOps Research and Assessment\u003c/a\u003e (DORA) project, we’ve recognized the importance of delivering a consistent experience to users. We measure this with the \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance\"\u003eFour Key\u003c/a\u003e metrics—two metrics that track the velocity of deploying new releases, balanced against two that capture the initial stability of those releases. A team that rates well on all four metrics is not only good at shipping code, they’re shipping code that’s good. \u003c/p\u003e\u003cp\u003eHowever, these four signals, which focus on the path to a deployment and its immediate effects, are less diagnostic of subsequent success throughout the lifespan of a release. In 2018, DORA began to study the ongoing stability of software delivered as a service (as typified by web applications), which we captured in an additional metric for availability, to explore the impact of technical operations on organizational performance. This year, we expanded our inquiry into this area, starting by renaming availability to reliability. Reliability (sometimes abbreviated as r9y) is a more general term that encompasses dimensions including response latency and content validity, as well as availability.\u003c/p\u003e\u003cp\u003eIn the \u003ca href=\"https://cloud.google.com/devops/state-of-devops/\"\u003e2021 State of DevOps Report’s\u003c/a\u003e cluster analysis, teams were segmented into four groups based on the Four Key metrics of software delivery. At first glance, we found that the application of reliability practices is not directly correlated to software delivery performance —  teams that score well on delivery metrics may not be the same as those who consistently practice modern operations. However, in combination, software delivery performance and reliability engineering exert a powerful influence on organizational outcomes: elite software delivery teams that also meet their reliability goals are 1.8 times more likely to report better business outcomes.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"SRE.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SRE.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eHow Google achieves reliability: SRE\u003c/h3\u003e\u003cp\u003eIn Google’s early days, we took a traditional approach to technical operations; the bulk of the work involved manual interventions in reaction to discrete problems. However, as our products began to rapidly acquire users across the globe, we realized that this approach wasn’t sustainable. It couldn’t scale to match the increasing size and complexity of our systems, and even attempting to keep up would require an untenable investment in our operations workforce. So, for the past 15+ years, we’ve been practicing and iterating on an approach called \u003ca href=\"http://sre.google\" target=\"_blank\"\u003eSite Reliability Engineering\u003c/a\u003e (SRE). \u003c/p\u003e\u003cp\u003eSRE provides a framework for measurement, prioritization, and information sharing to help teams balance between the velocity of feature releases and the predictable behavior of deployed services. It emphasizes the use of automation to reduce risk and to free up engineering capacity for strategic work. This may sound a lot like a description of DevOps; indeed, these disciplines have many shared values. That similarity meant that when, in 2016, Google published the \u003ca href=\"http://sre.google/books\" target=\"_blank\"\u003efirst book on Site Reliability Engineering\u003c/a\u003e, it made waves in the DevOps community as practitioners recognized a like-minded movement. It also caused some confusion: some have framed DevOps and SRE as being in conflict or competition with each other.\u003c/p\u003e\u003cp\u003eOur view is that, having arisen from similar challenges and espousing similar objectives, DevOps and SRE can be mutually compatible. We posited that, metaphorically, “\u003ca href=\"https://youtu.be/uTEL8Ff1Zvk\" target=\"_blank\"\u003e\u003ccode\u003eclass SRE implements DevOps\u003c/code\u003e\u003c/a\u003e''—SRE provides a way to realize DevOps objectives. Inspired by these communities’ continued growth and ongoing exchange of ideas, we sought to investigate their relationship further. This year, we expanded the scope of data collection to assess the extent of SRE adoption across the industry, and to learn how such modern operational practices interact with DORA’s model of software delivery performance.\u003c/p\u003e\u003cp\u003eStarting from the \u003ca href=\"http://sre.google/books\" target=\"_blank\"\u003epublished literature on SRE\u003c/a\u003e, we added the key elements of the framework as items in our survey of practitioners. We took care to avoid as much as possible any jargon, instead preferring plain language to describe how modern operations teams go about their work. Respondents reported on such practices as: defining reliability in terms of user-visible behavior; the use of automation to allow engineers to focus on strategic work; and having well-defined, well-practiced protocols for incident response. \u003c/p\u003e\u003cp\u003eAlong the way, we found that using SRE to implement DevOps is much more widely practiced than we thought. SRE, and related disciplines like Facebook’s Production Engineering, have a reputation for being niche disciplines, practiced only by a handful of tech giants. To the contrary, we found that SRE is used in some capacity by a majority of the teams in the DORA survey, with 52% of respondents reporting the use of one or more SRE practices.\u003c/p\u003e\u003ch3\u003eSRE is a force multiplier for software delivery excellence\u003c/h3\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph_with_image\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cfigure class=\"article-image--wrap-medium \"\u003e\u003cimg alt=\"SRE 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SRE_1.1203064715921295.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAnalyzing the results, we found compelling evidence that SRE is an effective approach to modern operations across the spectrum of organizations. In addition to driving better business outcomes, SRE helps focus efforts—teams that achieve their reliability goals report that they are able to spend more time coding, as they’re less consumed by reacting to incidents. These findings are consistent with the observation that \u003ca href=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\"\u003ehaving reliable services can directly impact revenue\u003c/a\u003e, as well as offering engineers greater flexibility to use their time to improve their systems, rather than simply repairing them.\u003c/p\u003e\u003cp\u003eBut while SRE is widely used and has demonstrable benefits, few respondents indicated that their teams have fully implemented every SRE technique we examined. Increased application of SRE has benefits at all levels: within every cluster of software delivery performance, teams that also meet their reliability goals outperform other members of their cluster in regard to business outcomes. \u003c/p\u003e\u003ch3\u003eOn the SRE road to DevOps excellence\u003c/h3\u003e\u003cp\u003eSRE is more than a toolset; it’s also a cultural mindset about the role of operations staff. SRE is a learning discipline, aimed at understanding information and continuously iterating in response. Accordingly, adopting SRE takes time, and success requires starting small, and applying an iterative approach to SRE itself.\u003c/p\u003e\u003cp\u003eHere are some ways to get started:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eFind free books and articles at \u003ca href=\"http://sre.google\"\u003esre.google\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eJoin a conversation with fellow practitioners, at all different stages of SRE implementation, at \u003ca href=\"http://bit.ly/reliability-discuss\"\u003ebit.ly/reliability-discuss\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSpeak to your GCP account manager about our professional service offerings \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003eApply to the \u003ca href=\"https://cloud.google.com/awards/devops/?eligible_for_cloud_free_trial=true\"\u003eDevOps awards\u003c/a\u003e to show how your organization is implementing award winning SRE practices along with the DORA principles!\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/DevOps_BlogHeader_C_Rnd3_n7MW7mI.jpg",
      "date_published": "2021-11-29T18:00:00Z",
      "author": {
        "name": "\u003cname\u003eDave Stanke\u003c/name\u003e\u003ctitle\u003eDeveloper Relations Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/retail/unlocking-the-power-of-modern-devops/",
      "title": "Empowering DevOps to foster customer loyalty in modern retail with MongoDB Atlas on Google Cloud",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c60=\"\"\u003e\u003cdiv _ngcontent-c60=\"\" innerhtml=\"\u0026lt;p\u0026gt;Consumer demands are becoming more complex, driven by high expectations for personalized experiences that strike the right chord at the perfect time. One study from McKinsey found that\u0026lt;a href=\u0026#34;https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; nearly three-quarters of consumers demand personalization\u0026lt;/a\u0026gt; when interacting with retailers.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Retailers old and new of any size must embrace the challenges head on and learn to capture customer loyalty. While each business has a unique journey toward modernizing its business, all of them share something in common: Effective approaches to DevOps and data analytics underpin their success.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Retailers sometimes struggle to change previous retail models into the much more intimate, personalized, and real-time retail experiences that consumers now want, whether shopping in-store or online. At the same time, retailers and many newcomers are jumping all in, and devising exceptional experiences that transform shopper experiences and elevate expectations even further.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;MongoDB and Google Cloud have been helping retailers of all sizes better address quickly changing market opportunities. As retailers continue to need more powerful systems of engagement and data analytics, the combination of MongoDB Atlas and Google Cloud solutions offer retailers such as 1-800-FLOWERS.COM, Inc. a solid mix of proven IT infrastructure and expertise.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Maximizing data value for developers\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;DevOps is increasingly tasked with creating experiences that will bring customers to a retail website, and guide them through the purchasing process. Along the way, they need to build in steps that keep customers fully engaged in the buying process and discourage things like cart abandonment. A successful build depends a lot on how much quality data is available about customer shopping experiences and how easy it is for DevOps teams to derive insights from that information.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud is very much a developer\u0026amp;#8217;s cloud, and we at MongoDB are very much a developer\u0026amp;#8217;s database. We like the breadth of Google Cloud services, which pair well with our products. Our collaboration with Google Cloud feels very natural both in terms of the technology we develop and how we are approaching serving our clients\u0026amp;#8217; needs. Together, we give DevOps teams at retailers a modern toolkit to maximize the value of their work.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The cloud-based environment supported by Google Cloud and MongoDB Atlas increases the speed and success of experimentation and ultimately delivers solutions with the greatest impact. With agile environments like ours, teams experiment much faster, leading to more innovative shopping experiences that differentiate a retailer from their competitors.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Any cloud solution has to be usable for retailers of all sizes so that they can develop services according to their unique needs, expertise, and visions. The goal should be to empower a retailer\u0026amp;#8217;s DevOps team to be as self-sufficient as possible, and not have to rely on a third-party every time changes need to be made.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In an industry facing extremely tight margins\u0026amp;#8212;and where a two percent efficiency gain or 2x acceleration of time to market can make or break the success of a project\u0026amp;#8212;gaining any edge is essential for retailers. Google Cloud and MongoDB provide that edge to retailers, as well as to other companies across industries.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Cultivating a vision at 1-800-FLOWERS.COM, Inc.\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;1-800-FLOWERS.COM, Inc. is an exceptional example of what can be achieved when going all in with modern data and DevOps solutions. Chief Technology Officer Abi Sachdeva, has pursued emerging technologies to support its business teams with the latest technologies to drive value for its customers.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Abi has been laser-focused on delivering new personalized experiences by continually innovating customer-facing services. Driven by a commitment to foster engagement across its industry-leading brands through a centralized customer experience, 1-800-FLOWERS.COM, Inc. built an e-commerce platform that is inclusive of both products and resources aimed at improving how people express themselves.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To best manage all the eCommerce environments associated and ensure outstanding customer service, 1-800-FLOWERS.COM, Inc. with MongoDB and Google Cloud to revolutionize its DevOps.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;With the help of MongoDB and Google Cloud, we transformed people, processes, and our technology. It has been a very stable experience requiring little administrative work,\u0026amp;#8221; says Abi. \u0026amp;#8220;Traditional technologies weighed us down in the past. MongoDB and Google Cloud deliver data models and DevOps solutions that accelerate our development and deployment.\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;MongoDB Atlas , Inc. with aggregation pipelines and a distributed system design that help it to scale quickly, while Google Cloud made its new approach to agile and DevOps a reality.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;he speed and agility that come with cloud services, companies like 1-800-FLOWERS.COM, Inc.\u0026amp;#160; keep up with the constantly changing customer preferences. With proven cloud solutions that at once increase overall IT effectiveness and decrease the burdens on IT teams, 1-800-FLOWERS.COM, Inc.\u0026amp;#160; is better positioned to constantly experiment, innovate, and deliver experiences that delight customers.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;The fully managed MongoDB Atlas database on Google Cloud has unlocked tremendous potential in our IT architecture,\u0026amp;#8221; says Abi. \u0026amp;#8220;From agility in scaling and improved resource management to seamless global clusters and premium monitoring, MongoDB and Google Cloud reduce complexity and allow our teams to stay lean and focused on innovation rather than infrastructure.\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Looking toward the holidays and beyond\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The very same systems that encourage experimentation and innovation can position retailers and other companies to excel in during and long after the holiday season. Companies need elasticity, scalability, and agility to facilitate experimentation and to navigate the turbulent external factors across their marketplace.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Every holiday season is challenging for retailers, but current supply chain concerns combined with massive changes to how people shop as a result of the COVID-19 pandemic will make 2021 a particularly important year for the industry. I believe that companies that have increased their backend elasticity and improved their DevOps culture will fare especially well amid the market upheaval.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As organizations modernize IT, it will be increasingly important to pair the possibilities of software and infrastructure to enable smaller DevOps teams to act independently and quickly. This improves culture across the business as people are more empowered and supported.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In addition, I encourage DevOps professionals to place more importance on understanding customers, business values, and to be people-first in their approaches to work. By combining this level of business experience with great coding skills, smaller teams can bolster a retailer\u0026amp;#8217;s performance this holiday season and beyond.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We are proud to work with Google Cloud to develop and deliver new ways for DevOps in retail and other industries to experiment, innovate, and deploy groundbreaking experiences that transform how people achieve their goals.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To learn more about the future of retail innovation,\u0026lt;a href=\u0026#34;https://www.youtube.com/watch?v=waNVXeHtzOs\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; watch this video\u0026lt;/a\u0026gt; featuring members of the 1-800-FLOWERS.COM, Inc., MongoDB, and Google Cloud teams.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eConsumer demands are becoming more complex, driven by high expectations for personalized experiences that strike the right chord at the perfect time. One study from McKinsey found that\u003ca href=\"https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://www.mckinsey.com\" track-metadata-module=\"post\"\u003e nearly three-quarters of consumers demand personalization\u003c/a\u003e when interacting with retailers.\u003c/p\u003e\u003cp\u003eRetailers old and new of any size must embrace the challenges head on and learn to capture customer loyalty. While each business has a unique journey toward modernizing its business, all of them share something in common: Effective approaches to DevOps and data analytics underpin their success.\u003c/p\u003e\u003cp\u003eRetailers sometimes struggle to change previous retail models into the much more intimate, personalized, and real-time retail experiences that consumers now want, whether shopping in-store or online. At the same time, retailers and many newcomers are jumping all in, and devising exceptional experiences that transform shopper experiences and elevate expectations even further.\u003c/p\u003e\u003cp\u003eMongoDB and Google Cloud have been helping retailers of all sizes better address quickly changing market opportunities. As retailers continue to need more powerful systems of engagement and data analytics, the combination of MongoDB Atlas and Google Cloud solutions offer retailers such as 1-800-FLOWERS.COM, Inc. a solid mix of proven IT infrastructure and expertise.\u003c/p\u003e\u003ch3\u003eMaximizing data value for developers\u003c/h3\u003e\u003cp\u003eDevOps is increasingly tasked with creating experiences that will bring customers to a retail website, and guide them through the purchasing process. Along the way, they need to build in steps that keep customers fully engaged in the buying process and discourage things like cart abandonment. A successful build depends a lot on how much quality data is available about customer shopping experiences and how easy it is for DevOps teams to derive insights from that information.\u003c/p\u003e\u003cp\u003eGoogle Cloud is very much a developer’s cloud, and we at MongoDB are very much a developer’s database. We like the breadth of Google Cloud services, which pair well with our products. Our collaboration with Google Cloud feels very natural both in terms of the technology we develop and how we are approaching serving our clients’ needs. Together, we give DevOps teams at retailers a modern toolkit to maximize the value of their work.\u003c/p\u003e\u003cp\u003eThe cloud-based environment supported by Google Cloud and MongoDB Atlas increases the speed and success of experimentation and ultimately delivers solutions with the greatest impact. With agile environments like ours, teams experiment much faster, leading to more innovative shopping experiences that differentiate a retailer from their competitors.\u003c/p\u003e\u003cp\u003eAny cloud solution has to be usable for retailers of all sizes so that they can develop services according to their unique needs, expertise, and visions. The goal should be to empower a retailer’s DevOps team to be as self-sufficient as possible, and not have to rely on a third-party every time changes need to be made.\u003c/p\u003e\u003cp\u003eIn an industry facing extremely tight margins—and where a two percent efficiency gain or 2x acceleration of time to market can make or break the success of a project—gaining any edge is essential for retailers. Google Cloud and MongoDB provide that edge to retailers, as well as to other companies across industries.\u003c/p\u003e\u003ch3\u003eCultivating a vision at 1-800-FLOWERS.COM, Inc.\u003c/h3\u003e\u003cp\u003e1-800-FLOWERS.COM, Inc. is an exceptional example of what can be achieved when going all in with modern data and DevOps solutions. Chief Technology Officer Abi Sachdeva, has pursued emerging technologies to support its business teams with the latest technologies to drive value for its customers.\u003c/p\u003e\u003cp\u003eAbi has been laser-focused on delivering new personalized experiences by continually innovating customer-facing services. Driven by a commitment to foster engagement across its industry-leading brands through a centralized customer experience, 1-800-FLOWERS.COM, Inc. built an e-commerce platform that is inclusive of both products and resources aimed at improving how people express themselves.\u003c/p\u003e\u003cp\u003eTo best manage all the eCommerce environments associated and ensure outstanding customer service, 1-800-FLOWERS.COM, Inc. with MongoDB and Google Cloud to revolutionize its DevOps.\u003c/p\u003e\u003cp\u003e“With the help of MongoDB and Google Cloud, we transformed people, processes, and our technology. It has been a very stable experience requiring little administrative work,” says Abi. “Traditional technologies weighed us down in the past. MongoDB and Google Cloud deliver data models and DevOps solutions that accelerate our development and deployment.”\u003c/p\u003e\u003cp\u003eMongoDB Atlas , Inc. with aggregation pipelines and a distributed system design that help it to scale quickly, while Google Cloud made its new approach to agile and DevOps a reality.\u003c/p\u003e\u003cp\u003ehe speed and agility that come with cloud services, companies like 1-800-FLOWERS.COM, Inc.  keep up with the constantly changing customer preferences. With proven cloud solutions that at once increase overall IT effectiveness and decrease the burdens on IT teams, 1-800-FLOWERS.COM, Inc.  is better positioned to constantly experiment, innovate, and deliver experiences that delight customers.\u003c/p\u003e\u003cp\u003e“The fully managed MongoDB Atlas database on Google Cloud has unlocked tremendous potential in our IT architecture,” says Abi. “From agility in scaling and improved resource management to seamless global clusters and premium monitoring, MongoDB and Google Cloud reduce complexity and allow our teams to stay lean and focused on innovation rather than infrastructure.”\u003c/p\u003e\u003ch3\u003eLooking toward the holidays and beyond\u003c/h3\u003e\u003cp\u003eThe very same systems that encourage experimentation and innovation can position retailers and other companies to excel in during and long after the holiday season. Companies need elasticity, scalability, and agility to facilitate experimentation and to navigate the turbulent external factors across their marketplace.\u003c/p\u003e\u003cp\u003eEvery holiday season is challenging for retailers, but current supply chain concerns combined with massive changes to how people shop as a result of the COVID-19 pandemic will make 2021 a particularly important year for the industry. I believe that companies that have increased their backend elasticity and improved their DevOps culture will fare especially well amid the market upheaval.\u003c/p\u003e\u003cp\u003eAs organizations modernize IT, it will be increasingly important to pair the possibilities of software and infrastructure to enable smaller DevOps teams to act independently and quickly. This improves culture across the business as people are more empowered and supported.\u003c/p\u003e\u003cp\u003eIn addition, I encourage DevOps professionals to place more importance on understanding customers, business values, and to be people-first in their approaches to work. By combining this level of business experience with great coding skills, smaller teams can bolster a retailer’s performance this holiday season and beyond.\u003c/p\u003e\u003cp\u003eWe are proud to work with Google Cloud to develop and deliver new ways for DevOps in retail and other industries to experiment, innovate, and deploy groundbreaking experiences that transform how people achieve their goals.\u003c/p\u003e\u003cp\u003eTo learn more about the future of retail innovation,\u003ca href=\"https://www.youtube.com/watch?v=waNVXeHtzOs\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://www.youtube.com\" track-metadata-module=\"post\"\u003e watch this video\u003c/a\u003e featuring members of the 1-800-FLOWERS.COM, Inc., MongoDB, and Google Cloud teams.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eConsumer demands are becoming more complex, driven by high expectations for personalized experiences that strike the right chord at the perfect time. One study from McKinsey found that\u003ca href=\"https://www.mckinsey.com/business-functions/marketing-and-sales/our-insights/the-value-of-getting-personalization-right-or-wrong-is-multiplying\" target=\"_blank\"\u003enearly three-quarters of consumers demand personalization\u003c/a\u003e when interacting with retailers.\u003c/p\u003e\u003cp\u003eRetailers old and new of any size must embrace the challenges head on and learn to capture customer loyalty. While each business has a unique journey toward modernizing its business, all of them share something in common: Effective approaches to DevOps and data analytics underpin their success.\u003c/p\u003e\u003cp\u003eRetailers sometimes struggle to change previous retail models into the much more intimate, personalized, and real-time retail experiences that consumers now want, whether shopping in-store or online. At the same time, retailers and many newcomers are jumping all in, and devising exceptional experiences that transform shopper experiences and elevate expectations even further.\u003c/p\u003e\u003cp\u003eMongoDB and Google Cloud have been helping retailers of all sizes better address quickly changing market opportunities. As retailers continue to need more powerful systems of engagement and data analytics, the combination of MongoDB Atlas and Google Cloud solutions offer retailers such as 1-800-FLOWERS.COM, Inc. a solid mix of proven IT infrastructure and expertise.\u003c/p\u003e\u003ch3\u003eMaximizing data value for developers\u003c/h3\u003e\u003cp\u003eDevOps is increasingly tasked with creating experiences that will bring customers to a retail website, and guide them through the purchasing process. Along the way, they need to build in steps that keep customers fully engaged in the buying process and discourage things like cart abandonment. A successful build depends a lot on how much quality data is available about customer shopping experiences and how easy it is for DevOps teams to derive insights from that information.\u003c/p\u003e\u003cp\u003eGoogle Cloud is very much a developer’s cloud, and we at MongoDB are very much a developer’s database. We like the breadth of Google Cloud services, which pair well with our products. Our collaboration with Google Cloud feels very natural both in terms of the technology we develop and how we are approaching serving our clients’ needs. Together, we give DevOps teams at retailers a modern toolkit to maximize the value of their work.\u003c/p\u003e\u003cp\u003eThe cloud-based environment supported by Google Cloud and MongoDB Atlas increases the speed and success of experimentation and ultimately delivers solutions with the greatest impact. With agile environments like ours, teams experiment much faster, leading to more innovative shopping experiences that differentiate a retailer from their competitors.\u003c/p\u003e\u003cp\u003eAny cloud solution has to be usable for retailers of all sizes so that they can develop services according to their unique needs, expertise, and visions. The goal should be to empower a retailer’s DevOps team to be as self-sufficient as possible, and not have to rely on a third-party every time changes need to be made.\u003c/p\u003e\u003cp\u003eIn an industry facing extremely tight margins—and where a two percent efficiency gain or 2x acceleration of time to market can make or break the success of a project—gaining any edge is essential for retailers. Google Cloud and MongoDB provide that edge to retailers, as well as to other companies across industries.\u003c/p\u003e\u003ch3\u003eCultivating a vision at 1-800-FLOWERS.COM, Inc.\u003c/h3\u003e\u003cp\u003e1-800-FLOWERS.COM, Inc. is an exceptional example of what can be achieved when going all in with modern data and DevOps solutions. Chief Technology Officer Abi Sachdeva, has pursued emerging technologies to support its business teams with the latest technologies to drive value for its customers.\u003c/p\u003e\u003cp\u003eAbi has been laser-focused on delivering new personalized experiences by continually innovating customer-facing services. Driven by a commitment to foster engagement across its industry-leading brands through a centralized customer experience, 1-800-FLOWERS.COM, Inc. built an e-commerce platform that is inclusive of both products and resources aimed at improving how people express themselves.\u003c/p\u003e\u003cp\u003eTo best manage all the eCommerce environments associated and ensure outstanding customer service, 1-800-FLOWERS.COM, Inc. with MongoDB and Google Cloud to revolutionize its DevOps.\u003c/p\u003e\u003cp\u003e“With the help of MongoDB and Google Cloud, we transformed people, processes, and our technology. It has been a very stable experience requiring little administrative work,” says Abi. “Traditional technologies weighed us down in the past. MongoDB and Google Cloud deliver data models and DevOps solutions that accelerate our development and deployment.”\u003c/p\u003e\u003cp\u003eMongoDB Atlas , Inc. with aggregation pipelines and a distributed system design that help it to scale quickly, while Google Cloud made its new approach to agile and DevOps a reality.\u003c/p\u003e\u003cp\u003ehe speed and agility that come with cloud services, companies like 1-800-FLOWERS.COM, Inc.  keep up with the constantly changing customer preferences. With proven cloud solutions that at once increase overall IT effectiveness and decrease the burdens on IT teams, 1-800-FLOWERS.COM, Inc.  is better positioned to constantly experiment, innovate, and deliver experiences that delight customers.\u003c/p\u003e\u003cp\u003e“The fully managed MongoDB Atlas database on Google Cloud has unlocked tremendous potential in our IT architecture,” says Abi. “From agility in scaling and improved resource management to seamless global clusters and premium monitoring, MongoDB and Google Cloud reduce complexity and allow our teams to stay lean and focused on innovation rather than infrastructure.”\u003c/p\u003e\u003ch3\u003eLooking toward the holidays and beyond\u003c/h3\u003e\u003cp\u003eThe very same systems that encourage experimentation and innovation can position retailers and other companies to excel in during and long after the holiday season. Companies need elasticity, scalability, and agility to facilitate experimentation and to navigate the turbulent external factors across their marketplace.\u003c/p\u003e\u003cp\u003eEvery holiday season is challenging for retailers, but current supply chain concerns combined with massive changes to how people shop as a result of the COVID-19 pandemic will make 2021 a particularly important year for the industry. I believe that companies that have increased their backend elasticity and improved their DevOps culture will fare especially well amid the market upheaval.\u003c/p\u003e\u003cp\u003eAs organizations modernize IT, it will be increasingly important to pair the possibilities of software and infrastructure to enable smaller DevOps teams to act independently and quickly. This improves culture across the business as people are more empowered and supported.\u003c/p\u003e\u003cp\u003eIn addition, I encourage DevOps professionals to place more importance on understanding customers, business values, and to be people-first in their approaches to work. By combining this level of business experience with great coding skills, smaller teams can bolster a retailer’s performance this holiday season and beyond.\u003c/p\u003e\u003cp\u003eWe are proud to work with Google Cloud to develop and deliver new ways for DevOps in retail and other industries to experiment, innovate, and deploy groundbreaking experiences that transform how people achieve their goals.\u003c/p\u003e\u003cp\u003eTo learn more about the future of retail innovation,\u003ca href=\"https://www.youtube.com/watch?v=waNVXeHtzOs\" target=\"_blank\"\u003ewatch this video\u003c/a\u003e featuring members of the 1-800-FLOWERS.COM, Inc., MongoDB, and Google Cloud teams.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/topics/retail/how-one-retailer-migrated-its-ecommerce-platform-to-google-cloud/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/google_flowers.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eDelivering smiles and sparking innovation at 1-800-FLOWERS.COM, Inc.\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eSee how gift retailer 1-800-FLOWERS.COM, Inc., migrated its customer touchpoints to cloud, including GKE and BigQuery, to build a microse...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/retail_GV8DIe0.max-2200x2200.jpg",
      "date_published": "2021-11-23T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eMark Porter\u003c/name\u003e\u003ctitle\u003eCTO, MongoDB\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/sabre-leverages-google-cloud-and-site-reliability-engineering/",
      "title": "How Sabre is using SRE to lead a successful digital transformation",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c69=\"\"\u003e\u003cdiv _ngcontent-c69=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026lt;b\u0026gt;Editor\u0026amp;#8217;s note\u0026lt;/b\u0026gt;: Today we hear from Kenny Kon, an SRE Director at Sabre. Kenny shares about how they have been able to successfully adopt Google\u0026amp;#8217;s SRE framework by leveraging their partnership with Google Cloud.\u0026amp;#160;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a leader in the travel industry, Sabre Corporation is driving innovation in the global travel industry and developing solutions that help airlines, hotels, and travel agencies transform the traveler experience and satisfy the ever-evolving needs of its customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In order to build these solutions, we joined forces with Google Cloud as our preferred cloud provider to accelerate our digital transformation. We chose Google because they understand the industry we are in as they also manage travel products such as Google Travel. Google also created \u0026lt;a href=\u0026#34;http://cloud.google.com/sre\u0026#34;\u0026gt;SRE (Site Reliability Engineering)\u0026lt;/a\u0026gt;, and operates with SRE principles at the Google scale, which is what intrigued us the most.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Initially we started with a multi-cloud model, but that didn\u0026amp;#8217;t help us move faster so we consolidated to just Google Cloud. To speed our transformation along, we have adopted Google SRE (Site Reliability Engineering) practices which enables us to balance reliability and speed. We have been able to make this transformation with the direct help of Google Cloud\u0026amp;#8217;s \u0026lt;a href=\u0026#34;https://cloud.google.com/consulting\u0026#34;\u0026gt;Professional Services Organization (PSO)\u0026lt;/a\u0026gt; along with Google Cloud\u0026amp;#8217;s tooling, like \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring\u0026#34;\u0026gt;Cloud Monitoring\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/logging\u0026#34;\u0026gt;Cloud Logging\u0026lt;/a\u0026gt;, and operating on \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine (GKE)\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/spanner\u0026#34;\u0026gt;Cloud Spanner\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In adopting SRE at Sabre, we\u0026amp;#8217;d like to highlight three key takeaways from the journey:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;1. Find colleagues who are also passionate about shifting culture and adopting SRE\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Create a community within your organization who is dedicated to the SRE journey and motivated to make things happen. As we adopted SRE at Sabre I saw more and more people rallying and coming together to support the culture change. With some momentum built it was great to bring shared experiences to the team as we all spoke in the same language talking about SLOs, SLIs, and about how we measure things.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Some of the ways in which we built our community was by hosting monthly brown bag sessions. This is an informal gathering where teams come in and share their experiences and challenges, or teach on specific SRE topics such as SLOs or toil. We also created a \u0026lt;a href=\u0026#34;https://gdg.community.dev/gdg-cloud-southlake/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;public Google Developer Group (GDG)\u0026lt;/a\u0026gt; and have hosted several Google SRE subject matter experts to speak on SRE principles and best practices.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;2. Get your mid level leadership stakeholders on board\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We know how important \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\u0026#34;\u0026gt;getting leadership buy in\u0026lt;/a\u0026gt; is to creating a successful SRE movement within an organization. That top-level buy-in is highly important to get resources and drive transformation across the organization, but what is sometimes missed is making it a priority to get mid-level leadership on board as well. It\u0026amp;#8217;s difficult to enact change from the ground up starting with practitioners at the bottom, and it\u0026amp;#8217;s also difficult to just have leadership buy in, as once it gets down to the middle, things may fall apart. It is imperative to have mid-level leaders on board as well, as they directly affect the culture and decisions of their teams. To avoid resistance, it is also important that the mid-level leadership (product, operations and engineering managers), i.e. people managers, will understand the motivations behind change so they will be onboard. Without that understanding, it will hinder mid-level leadership\u0026amp;#8217;s ability to communicate changes to the practitioners level and can impact the teams\u0026#39; goal and allocated bandwidth.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;3. Don\u0026amp;#8217;t be afraid to get help from professionals\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Adopting SRE at a large organization is no simple feat. Partnering with \u0026lt;a href=\u0026#34;https://services.google.com/fh/files/misc/pso_sre_google_cloud.pdf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Google\u0026amp;#8217;s SRE consulting experts\u0026lt;/a\u0026gt; has brought about a huge shift at Sabre. The value PSO brings is not just training, it\u0026#39;s also listening. We\u0026amp;#8217;ve had experienced Googlers who understand our problems and have been at our stage in the SRE journey listen, analyze and tailor the approach specific to our team\u0026#39;s goals. PSO helped us by shifting our engineering teams to be more customer centric, and aligning our product, operations, and development teams. But most importantly, they\u0026amp;#8217;ve helped to make our current teams happier, because they\u0026#39;re not spinning their wheels, waiting around on blocked requests.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When we partnered with PSO we were aware of who the key stakeholders in our organization are: the mid-level leadership and people managers. We made sure to bring them into our PSO discussions and decision making sessions and as a result, helped us to get more traction and solve the gap we had, enabling the middle-level and bringing them on board.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Some of the actions we have taken with help from our PSO SRE partners include adding a \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started\u0026#34;\u0026gt;tiers of service\u0026lt;/a\u0026gt; approach, improving incident management through \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents\u0026#34;\u0026gt;wheels of misfortune (WoM)\u0026lt;/a\u0026gt;, defining \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos\u0026#34;\u0026gt;critical user journeys (CUJs)\u0026lt;/a\u0026gt;, and implementing \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows\u0026#34;\u0026gt;error budgets\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Since putting these SRE practices into place, our business is more aligned to customer experience. We now invest org resources according to the needs of our customers and with that have reduced silos across our teams. Our Ops team is much happier since they can move faster and not have to block requests. SRE has taught us a common language, a common framework. Moreover, it gives this whole discipline a culture and meaning.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s note\u003c/b\u003e: Today we hear from Kenny Kon, an SRE Director at Sabre. Kenny shares about how they have been able to successfully adopt Google’s SRE framework by leveraging their partnership with Google Cloud. \u003c/i\u003e\u003c/p\u003e\u003cp\u003eAs a leader in the travel industry, Sabre Corporation is driving innovation in the global travel industry and developing solutions that help airlines, hotels, and travel agencies transform the traveler experience and satisfy the ever-evolving needs of its customers. \u003c/p\u003e\u003cp\u003eIn order to build these solutions, we joined forces with Google Cloud as our preferred cloud provider to accelerate our digital transformation. We chose Google because they understand the industry we are in as they also manage travel products such as Google Travel. Google also created \u003ca href=\"http://cloud.google.com/sre\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"http://cloud.google.com/sre\" track-metadata-module=\"post\"\u003eSRE (Site Reliability Engineering)\u003c/a\u003e, and operates with SRE principles at the Google scale, which is what intrigued us the most.\u003c/p\u003e\u003cp\u003eInitially we started with a multi-cloud model, but that didn’t help us move faster so we consolidated to just Google Cloud. To speed our transformation along, we have adopted Google SRE (Site Reliability Engineering) practices which enables us to balance reliability and speed. We have been able to make this transformation with the direct help of Google Cloud’s \u003ca href=\"https://cloud.google.com/consulting\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/consulting\" track-metadata-module=\"post\"\u003eProfessional Services Organization (PSO)\u003c/a\u003e along with Google Cloud’s tooling, like \u003ca href=\"https://cloud.google.com/monitoring\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring\" track-metadata-module=\"post\"\u003eCloud Monitoring\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/logging\" track-metadata-module=\"post\"\u003eCloud Logging\u003c/a\u003e, and operating on \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine (GKE)\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/spanner\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/spanner\" track-metadata-module=\"post\"\u003eCloud Spanner\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eIn adopting SRE at Sabre, we’d like to highlight three key takeaways from the journey: \u003c/p\u003e\u003ch3\u003e1. Find colleagues who are also passionate about shifting culture and adopting SRE\u003c/h3\u003e\u003cp\u003eCreate a community within your organization who is dedicated to the SRE journey and motivated to make things happen. As we adopted SRE at Sabre I saw more and more people rallying and coming together to support the culture change. With some momentum built it was great to bring shared experiences to the team as we all spoke in the same language talking about SLOs, SLIs, and about how we measure things. \u003c/p\u003e\u003cp\u003eSome of the ways in which we built our community was by hosting monthly brown bag sessions. This is an informal gathering where teams come in and share their experiences and challenges, or teach on specific SRE topics such as SLOs or toil. We also created a \u003ca href=\"https://gdg.community.dev/gdg-cloud-southlake/\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://gdg.community.dev\" track-metadata-module=\"post\"\u003epublic Google Developer Group (GDG)\u003c/a\u003e and have hosted several Google SRE subject matter experts to speak on SRE principles and best practices. \u003c/p\u003e\u003ch3\u003e2. Get your mid level leadership stakeholders on board\u003c/h3\u003e\u003cp\u003eWe know how important \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\" track-metadata-module=\"post\"\u003egetting leadership buy in\u003c/a\u003e is to creating a successful SRE movement within an organization. That top-level buy-in is highly important to get resources and drive transformation across the organization, but what is sometimes missed is making it a priority to get mid-level leadership on board as well. It’s difficult to enact change from the ground up starting with practitioners at the bottom, and it’s also difficult to just have leadership buy in, as once it gets down to the middle, things may fall apart. It is imperative to have mid-level leaders on board as well, as they directly affect the culture and decisions of their teams. To avoid resistance, it is also important that the mid-level leadership (product, operations and engineering managers), i.e. people managers, will understand the motivations behind change so they will be onboard. Without that understanding, it will hinder mid-level leadership’s ability to communicate changes to the practitioners level and can impact the teams\u0026#39; goal and allocated bandwidth.\u003c/p\u003e\u003ch3\u003e3. Don’t be afraid to get help from professionals\u003c/h3\u003e\u003cp\u003eAdopting SRE at a large organization is no simple feat. Partnering with \u003ca href=\"https://services.google.com/fh/files/misc/pso_sre_google_cloud.pdf\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://services.google.com\" track-metadata-module=\"post\"\u003eGoogle’s SRE consulting experts\u003c/a\u003e has brought about a huge shift at Sabre. The value PSO brings is not just training, it\u0026#39;s also listening. We’ve had experienced Googlers who understand our problems and have been at our stage in the SRE journey listen, analyze and tailor the approach specific to our team\u0026#39;s goals. PSO helped us by shifting our engineering teams to be more customer centric, and aligning our product, operations, and development teams. But most importantly, they’ve helped to make our current teams happier, because they\u0026#39;re not spinning their wheels, waiting around on blocked requests.\u003c/p\u003e\u003cp\u003eWhen we partnered with PSO we were aware of who the key stakeholders in our organization are: the mid-level leadership and people managers. We made sure to bring them into our PSO discussions and decision making sessions and as a result, helped us to get more traction and solve the gap we had, enabling the middle-level and bringing them on board.\u003c/p\u003e\u003cp\u003eSome of the actions we have taken with help from our PSO SRE partners include adding a \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started\" track-metadata-module=\"post\"\u003etiers of service\u003c/a\u003e approach, improving incident management through \u003ca href=\"https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents\" track-metadata-module=\"post\"\u003ewheels of misfortune (WoM)\u003c/a\u003e, defining \u003ca href=\"https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos\" track-metadata-module=\"post\"\u003ecritical user journeys (CUJs)\u003c/a\u003e, and implementing \u003ca href=\"https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows\" track-metadata-module=\"post\"\u003eerror budgets\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eSince putting these SRE practices into place, our business is more aligned to customer experience. We now invest org resources according to the needs of our customers and with that have reduced silos across our teams. Our Ops team is much happier since they can move faster and not have to block requests. SRE has taught us a common language, a common framework. Moreover, it gives this whole discipline a culture and meaning.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s note\u003c/b\u003e: Today we hear from Kenny Kon, an SRE Director at Sabre. Kenny shares about how they have been able to successfully adopt Google’s SRE framework by leveraging their partnership with Google Cloud. \u003c/i\u003e\u003c/p\u003e\u003cp\u003eAs a leader in the travel industry, Sabre Corporation is driving innovation in the global travel industry and developing solutions that help airlines, hotels, and travel agencies transform the traveler experience and satisfy the ever-evolving needs of its customers. \u003c/p\u003e\u003cp\u003eIn order to build these solutions, we joined forces with Google Cloud as our preferred cloud provider to accelerate our digital transformation. We chose Google because they understand the industry we are in as they also manage travel products such as Google Travel. Google also created \u003ca href=\"http://cloud.google.com/sre\"\u003eSRE (Site Reliability Engineering)\u003c/a\u003e, and operates with SRE principles at the Google scale, which is what intrigued us the most.\u003c/p\u003e\u003cp\u003eInitially we started with a multi-cloud model, but that didn’t help us move faster so we consolidated to just Google Cloud. To speed our transformation along, we have adopted Google SRE (Site Reliability Engineering) practices which enables us to balance reliability and speed. We have been able to make this transformation with the direct help of Google Cloud’s \u003ca href=\"https://cloud.google.com/consulting\"\u003eProfessional Services Organization (PSO)\u003c/a\u003e along with Google Cloud’s tooling, like \u003ca href=\"https://cloud.google.com/monitoring\"\u003eCloud Monitoring\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/logging\"\u003eCloud Logging\u003c/a\u003e, and operating on \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine (GKE)\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/spanner\"\u003eCloud Spanner\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eIn adopting SRE at Sabre, we’d like to highlight three key takeaways from the journey: \u003c/p\u003e\u003ch3\u003e1. Find colleagues who are also passionate about shifting culture and adopting SRE\u003c/h3\u003e\u003cp\u003eCreate a community within your organization who is dedicated to the SRE journey and motivated to make things happen. As we adopted SRE at Sabre I saw more and more people rallying and coming together to support the culture change. With some momentum built it was great to bring shared experiences to the team as we all spoke in the same language talking about SLOs, SLIs, and about how we measure things. \u003c/p\u003e\u003cp\u003eSome of the ways in which we built our community was by hosting monthly brown bag sessions. This is an informal gathering where teams come in and share their experiences and challenges, or teach on specific SRE topics such as SLOs or toil. We also created a \u003ca href=\"https://gdg.community.dev/gdg-cloud-southlake/\" target=\"_blank\"\u003epublic Google Developer Group (GDG)\u003c/a\u003e and have hosted several Google SRE subject matter experts to speak on SRE principles and best practices. \u003c/p\u003e\u003ch3\u003e2. Get your mid level leadership stakeholders on board\u003c/h3\u003e\u003cp\u003eWe know how important \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\"\u003egetting leadership buy in\u003c/a\u003e is to creating a successful SRE movement within an organization. That top-level buy-in is highly important to get resources and drive transformation across the organization, but what is sometimes missed is making it a priority to get mid-level leadership on board as well. It’s difficult to enact change from the ground up starting with practitioners at the bottom, and it’s also difficult to just have leadership buy in, as once it gets down to the middle, things may fall apart. It is imperative to have mid-level leaders on board as well, as they directly affect the culture and decisions of their teams. To avoid resistance, it is also important that the mid-level leadership (product, operations and engineering managers), i.e. people managers, will understand the motivations behind change so they will be onboard. Without that understanding, it will hinder mid-level leadership’s ability to communicate changes to the practitioners level and can impact the teams' goal and allocated bandwidth.\u003c/p\u003e\u003ch3\u003e3. Don’t be afraid to get help from professionals\u003c/h3\u003e\u003cp\u003eAdopting SRE at a large organization is no simple feat. Partnering with \u003ca href=\"https://services.google.com/fh/files/misc/pso_sre_google_cloud.pdf\" target=\"_blank\"\u003eGoogle’s SRE consulting experts\u003c/a\u003e has brought about a huge shift at Sabre. The value PSO brings is not just training, it's also listening. We’ve had experienced Googlers who understand our problems and have been at our stage in the SRE journey listen, analyze and tailor the approach specific to our team's goals. PSO helped us by shifting our engineering teams to be more customer centric, and aligning our product, operations, and development teams. But most importantly, they’ve helped to make our current teams happier, because they're not spinning their wheels, waiting around on blocked requests.\u003c/p\u003e\u003cp\u003eWhen we partnered with PSO we were aware of who the key stakeholders in our organization are: the mid-level leadership and people managers. We made sure to bring them into our PSO discussions and decision making sessions and as a result, helped us to get more traction and solve the gap we had, enabling the middle-level and bringing them on board.\u003c/p\u003e\u003cp\u003eSome of the actions we have taken with help from our PSO SRE partners include adding a \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started\"\u003etiers of service\u003c/a\u003e approach, improving incident management through \u003ca href=\"https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents\"\u003ewheels of misfortune (WoM)\u003c/a\u003e, defining \u003ca href=\"https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos\"\u003ecritical user journeys (CUJs)\u003c/a\u003e, and implementing \u003ca href=\"https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows\"\u003eerror budgets\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eSince putting these SRE practices into place, our business is more aligned to customer experience. We now invest org resources according to the needs of our customers and with that have reduced silos across our teams. Our Ops team is much happier since they can move faster and not have to block requests. SRE has taught us a common language, a common framework. Moreover, it gives this whole discipline a culture and meaning.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eFour steps to jumpstarting your SRE practice\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eOnce you have leadership buy-in, there are some things you can do to get the SRE ball rolling, fast.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-2200x2200.jpg",
      "date_published": "2021-11-22T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eKenny Kon\u003c/name\u003e\u003ctitle\u003eSRE Director at Sabre\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/networking/hosting-a-website-on-google-cloud-from-start-to-finish/",
      "title": "Foundations of a scalable website on GCP",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eStarting a website can be hard, we get it. There are many vendors you have to work with and steps to tie together. What DNS records do I need to add? How do I enable DNSSEC? Is my website secure and safe from cyber attacks? These types of questions plague millions of website operators globally. We are excited to share that it is possible to manage all of these steps in one location using Google Cloud.\u003c/p\u003e\u003cp\u003eGoogle Cloud offers you the ability to manage the entire lifecycle of a website from start to finish. You no longer have to worry about managing different subscriptions and understanding the integration between vendors. Leveraging the Google Cloud offering will allow for you to have a scalable, reliable, and safe deployment. Additionally, there are extra benefits that you can take advantage of, like getting Google Managed SSL certificates for free and taking advantage of best in class DDoS protection with our Cloud Armor solution.\u003c/p\u003e\u003ch3\u003eArchitecture diagram\u003c/h3\u003e\u003cp\u003eThe following architecture diagram illustrates all of the components of the solution.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"architecture diagram.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/architecture_diagram.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eKey components of the solution:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eCloud Domains\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud DNS\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCompute and Storage\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eGlobal HTTPs Load Balancer\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Armor\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud CDN\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eBuying a Domain on Google Cloud\u003c/h3\u003e\u003cp\u003ePurchasing and verifying a domain can be a tricky process with many steps. Cloud Domains makes this easy and straightforward to manage. Cloud Domains integrates seamlessly with Cloud DNS making the management even easier. There is full API support which allows for programmatic management if you are managing a larger portfolio. \u003c/p\u003e\u003ch3\u003eManaging DNS with Google Cloud\u003c/h3\u003e\u003cp\u003eOur Cloud DNS solution is a managed DNS infrastructure which is scalable and highly available. Easy management of private and public DNS zones makes this a one stop shop for DNS management. Public DNS records are anycasted globally using Google’s distributed network. It is easy and straightforward to enable DNSSEC which will help protect your end users from malicious actors.  \u003c/p\u003e\u003ch3\u003eInitializing Compute and setting up static object storage\u003c/h3\u003e\u003cp\u003eRunning your backends on Google Cloud compute has numerous advantages. You can use a managed instance group to run your websites. Managed instance groups allow for a highly scalable and efficient deployment. When demand goes up the number of instances will scale seamlessly, and likewise if demand falls the active compute can scale down. This allows for you to only be running what you need at a given moment. You can easily create multi-zone deployments which increases reliability and performance. With full API support, automation and management is easy and fast. Using a managed instance group allows for you to automatically and safely deploy updates with a variety of customizations available.\u003c/p\u003e\u003cp\u003eFor static objects you can store them in our Cloud Storage solution. This is perfect for content like images and videos which are not constantly changing. You can store large quantities of data which is available worldwide. It is easy to transfer content into Cloud Storage with multiple tools available.\u003c/p\u003e\u003ch3\u003eSetting up an external https load balancer\u003c/h3\u003e\u003cp\u003eThe external https load balancer is a global proxy-based layer 7 solution that serves as the entry point for all of your traffic onto Google’s network. Our advanced load balancing solution allows for integrated traffic management and is highly customizable to fit your needs. You can leverage a Google managed SSL certificate for easy deployment and ongoing management.\u003c/p\u003e\u003ch3\u003eSecuring your traffic with Cloud Armor\u003c/h3\u003e\u003cp\u003eCloud Armor is Google’s best in class DDoS defense solution and Web Application Firewall (WAF). You can rest easier knowing that Google’s network has your back. We have a long history of mitigating some of the most complicated and largest DDoS attacks on record ( blog link). With Cloud Armor you can additionally take advantage of preconfigured WAF rules (Mod Security Rule Set 3.02), adaptive protection, and recently rate limiting. All of this ensures that your website stays online and is protected from attacks.\u003c/p\u003e\u003ch3\u003eCaching static content with Cloud CDN\u003c/h3\u003e\u003cp\u003eFor content that is cacheable like images or short videos, you can use Cloud CDN to enable fast and cost efficient delivery. Google has Cloud CDN pops all over the world which will help ensure that users from the regions that matter to you have a seamless and fast experience. Cloud CDN is easy to enable and get started with. \u003c/p\u003e\u003ch3\u003eYoutube video\u003c/h3\u003e\u003cp\u003eIf you would like to see a further overview of the architecture and components of this solution as well as a detailed configuration walkthrough please check out this \u003ca href=\"https://www.youtube.com/watch?v=I5qLiG0vIGE\" target=\"_blank\"\u003evideo\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFor more information on any of these solutions please check out their respective documentation hubs:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/domains/docs/overview\"\u003eCloud Domains\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/dns/docs/overview/\"\u003eCloud DNS\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/compute/docs/instance-groups\"\u003eManaged Instance Group\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/storage/docs\"\u003eCloud Storage\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/load-balancing/docs/https\"\u003eExternal HTTPs Load Balancer\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/armor/docs\"\u003eCloud Armor\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/cdn/docs/overview\"\u003eCloud CDN\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/networking/cloud-domains-is-generally-available/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/google_domain.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eCloud Domains, now GA, makes it easy to register and manage custom domains\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eCloud Domains, now generally available, makes performing domain-related tasks in Google Cloud simple.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/GCP_Networking_7oH4Ie3.jpg",
      "date_published": "2021-11-18T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eArman Rye\u003c/name\u003e\u003ctitle\u003eCustomer Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/hybrid-cloud/introducing-anthos-for-vms-and-other-app-modernization-tools/",
      "title": "Introducing Anthos for VMs and tools to simplify the developer experience",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv _ngcontent-c20=\"\" innerhtml=\"\u0026lt;p\u0026gt;When it comes to software development using Google Cloud, we have three guiding principles. First, developing on Google Cloud needs to be open\u0026amp;#8212;we rely heavily on open-source technologies so that it\u0026#39;s easier to move apps between environments, recruit skilled developers, and access the latest innovations sooner. Second, developing for Google Cloud should also be easy\u0026amp;#8212;we strive to offer intuitive, integrated tools that run well wherever you build your code, while minimizing your operational overhead. Finally, running on Google Cloud should be transformative\u0026amp;#8212;we offer services that help unleash your imagination, along with best practices and professional services to help you bring your ideas to life.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Today, at \u0026lt;a href=\u0026#34;https://cloud.withgoogle.com/next\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Google Cloud Next \u0026amp;#8216;21\u0026lt;/a\u0026gt;, we announced a variety of new tools and capabilities to deliver on those principles.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Opening Anthos to virtual machines\u0026amp;#160;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Since announcing Anthos, our open-source-based platform for hybrid and mutlicloud deployments in 2018, we have continued to receive strong reception from customers and partners. In fact, in Q2 2021, Anthos compute under management grew more than 500% year-over-year. Anthos unifies the management of infrastructure and applications across on-premises, edge, and multiple public clouds, as well as ensuring consistent operation at scale. Based on Google Kubernetes Engine (GKE), Anthos was originally designed to run applications in containers. To help you make that transition, we automated the process to migrate and modernize existing apps using \u0026lt;a href=\u0026#34;https://cloud.google.com/migrate/anthos\u0026#34;\u0026gt;Migrate for Anthos and GKE\u0026lt;/a\u0026gt; from various virtual machine environments to containers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;While we have seen many customers make the leap to containerization, some are not quite ready to move completely off of virtual machines (VMs). They want a unified development platform where developers can build, modify, and deploy applications residing in both containers and VMs in a common, shared environment. Today, we are announcing \u0026lt;a href=\u0026#34;http://cloud.google.com/anthos\u0026#34;\u0026gt;Anthos for Virtual Machines\u0026lt;/a\u0026gt; in preview, allowing you to standardize on Kubernetes while continuing to run some workloads that cannot be easily containerized in virtual machines. Anthos for VMs will help platform developers standardize on an operation model, process and tooling; enable incremental modernization efforts; and support traditional workloads like Virtual Network Functions (VNFs) or stateful monolithic workloads.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;You can take advantage of Anthos for VMs in two ways \u0026amp;#8211; either by attaching your vSphere VMs, or shifting your VMs as-is. For customers with active VMware environments, the Anthos control plane can now connect to your vSphere environment and attach your vSphere VMs, allowing you to apply consistent security and policies across clusters, gain visibility into the health and performance of your services, and manage traffic for both VMs and containers. Alternately, Anthos for VMs allows you to shift VMs as-is onto Anthos with \u0026lt;a href=\u0026#34;https://kubevirt.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;KubeVirt\u0026lt;/a\u0026gt;, an open-source virtualization API for Kubernetes. Now you can build, modify, and deploy applications residing in both application containers as well as VMs on a common, shared Anthos environment. This is a great option for organizations that prefer to use open-source virtualization, as those same organizations often prefer to run \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/clusters/docs/bare-metal/1.6/concepts/about-bare-metal\u0026#34;\u0026gt;Anthos on bare metal\u0026lt;/a\u0026gt;. To help get started, we provide you with a \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos\u0026#34;\u0026gt;fit assessment tool\u0026lt;/a\u0026gt; to identify which approach to take.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Taking your Anthos experience further\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We\u0026amp;#8217;re also making it easier for you to manage containerized workloads already running in other clouds through Anthos. While you can already run containers in AWS and Azure from Anthos, we\u0026amp;#8217;re taking this a step further with the new \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/clusters/docs/multi-cloud\u0026#34;\u0026gt;Anthos Multi-Cloud API\u0026lt;/a\u0026gt;. Generally available in Q4 \u0026amp;#8216;21, this new API lets you provision and manage GKE clusters running on AWS or Azure infrastructure directly from the command line interface or the Google Cloud Console, all while being managed by a central control plane. This gives you a single API to manage all your container deployments regardless of which major public cloud you\u0026#39;re using, thus minimizing the time you spend jumping between user interfaces to accomplish day-to-day management tasks like creating, managing, and updating clusters.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Over the past year, we\u0026amp;#8217;ve brought some of the innovations originally developed for hybrid and multicloud use cases in Anthos back to GKE running in Google Cloud. Specifically, \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/config-management\u0026#34;\u0026gt;Anthos Config Management\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/service-mesh\u0026#34;\u0026gt;Anthos Service Mesh\u0026lt;/a\u0026gt; are now generally available for GKE as standalone services with pay-as-you-go pricing. GKE customers can now use Anthos Config Management to take advantage of config and policy automation at a low incremental per-cluster cost, and use Anthos Service Mesh to enable next-level security and networking on container-based microservices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Last but not least, we are excited to announce that starting today, Anthos Service Mesh is generally available to support a hybrid mesh. This gives you the flexibility to have a common mesh that spans both your Google Cloud and on-prem deployments.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Customers like \u0026lt;a href=\u0026#34;http://www.westerndigital.com\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Western Digital\u0026lt;/a\u0026gt; have already experienced many benefits from adopting Anthos as their application modernization platform:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#34;As a global storage leader with sophisticated manufacturing facilities around the world, Western Digital sees cloud technology as an enabler of our key business priorities: reducing time to deliver products and services, rationalizing our entire application footprint, and meeting customer demand for IoT and edge applications,\u0026amp;#8221;\u0026lt;/i\u0026gt; said Jahidul Khandaker, senior vice president and CIO, Western Digital. \u0026lt;i\u0026gt;\u0026amp;#8220;Anthos is our unified management platform of choice\u0026amp;#8212;it gives us insights across our Google Cloud and on-premises environments, while keeping the doors open for a multi-cloud future. Anthos has delivered several advantages for our developers: a richer user experience, greater security, and enhanced flexibility to manage factory applications\u0026amp;#8212;no matter where they reside\u0026amp;#8212;on-prem, in the cloud or a mix of both.\u0026amp;#34;\u0026lt;/i\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Easy does it\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;In addition to being an open platform, we strive to make Google Cloud easy to use for operators as well as developers. For example, earlier this year\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\u0026#34;\u0026gt;we introduced GKE Autopilot\u0026lt;/a\u0026gt;, a mode of operations in GKE that empowers you to simplify operations by offloading the management of infrastructure, control plane, and nodes. With GKE Autopilot, customers like\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/customers/ubie\u0026#34;\u0026gt;Ubie\u0026lt;/a\u0026gt;, a Japanese-based healthcare technology company, have eliminated the need to configure and maintain infrastructure, which helped their development teams focus on making healthcare more accessible.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;With Cloud Run, our serverless compute platform, you can abstract away infrastructure management entirely. This year, our focus has been on bringing the simplicity of Cloud Run to more workloads, like traditional applications written in Java Spring Boot, ASP.NET, and Django, among others. Along with a new second generation execution environment for enhanced network and CPU performance, we\u0026amp;#8217;ve added\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run\u0026#34;\u0026gt;committed-use discounts\u0026lt;/a\u0026gt;\u0026amp;#160;and introduced\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\u0026#34;\u0026gt;new CPU allocation controls and price\u0026lt;/a\u0026gt;, allowing you to save up to 17% and 25%, respectively, on your compute bill. New\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/developers-practitioners/introducing-new-connectors-workflows\u0026#34;\u0026gt;connectors\u0026lt;/a\u0026gt;\u0026amp;#160;for Workflows,\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/cloud-functions-integrates-with-google-secret-manager\u0026#34;\u0026gt;integration\u0026lt;/a\u0026gt;\u0026amp;#160;between Cloud Functions and Secret Manager, and support for\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/serverless/cloud-functions-supports-min-instances\u0026#34;\u0026gt;min instances\u0026lt;/a\u0026gt;\u0026amp;#160;are just a few of the other ways we\u0026amp;#8217;ve made it easier to build modern, serverless apps.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Easy for developers\u0026amp;#160;\u0026lt;/h3\u0026gt;Developers spend a lot of time inside their integrated development environments (IDEs), writing code. Last year we announced\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/shell\u0026#34;\u0026gt;Cloud Shell Editor\u0026lt;/a\u0026gt;, which makes the process of writing code as seamless as possible. It comes with your favorite developer tools (e.g., docker, minikube, skaffold, and many more) preinstalled, and this year, we added ~100 live tutorials to it\u0026amp;#8212;no more switching between the documentation, the terminal, and your code!\u0026amp;#160;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Once that code is ready, you want building it and deploying it to be as seamless as possible. Today we are announcing\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/build/docs/hybrid/overview\u0026#34;\u0026gt;Cloud Build Hybrid\u0026lt;/a\u0026gt;, which lets you build, test, and deploy across clouds and on-prem systems, so developers get consistent CI/CD tooling across their environments, and platform engineers don\u0026#39;t have to worry about maintaining and scaling their systems. Cloud Build is also integrated with\u0026amp;#160;\u0026lt;a href=\u0026#34;https://docs.google.com/document/d/1Z4vYdjF66UTOJgIpaDlDnXOuDCV6YulMhvNDjgoFiYo/edit?resourcekey=0-Vxxrly14dbfaBHg21HnOfg#\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Artifact Registry\u0026lt;/a\u0026gt;, which now allows you to store not only in containers, but also language-specific artifacts in one place. Finally, with the recently launched\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/overview\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, which is a managed, continuous delivery service initially for GKE, we\u0026amp;#8217;re making it easy to scale your pipelines across your organization.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Easy for operators\u0026lt;/h3\u0026gt;When your applications are up and running, you need to observe and analyze them for better operations and business insights. While we already offer a fully managed metrics and alerting service with\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring\u0026#34;\u0026gt;Cloud Monitoring\u0026lt;/a\u0026gt;, some Kubernetes users want to continue using open-source Prometheus without the scaling and management headaches. This is precisely why today we are announcing the preview of\u0026amp;#160;\u0026lt;a href=\u0026#34;http://cloud.google.com/monitoring\u0026#34;\u0026gt;Managed Service for Prometheus\u0026lt;/a\u0026gt;, helping you avoid vendor lock-in and delivering compatibility with your existing Prometheus alerts, workflows, and Grafana dashboards. Now you have all of the benefits of Prometheus, minus the management hassle.\u0026amp;#160;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To give you easy diagnostics and deeper insights from across your business and systems, today we also combined\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/logging\u0026#34;\u0026gt;Cloud Logging\u0026lt;/a\u0026gt;\u0026amp;#160;with the performance and power of BigQuery to introduce\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/log-analytics\u0026#34;\u0026gt;Log Analytics\u0026lt;/a\u0026gt;. Currently in preview, Log Analytics allows you to rapidly store, manage, and analyze log data. This enables you to effectively move your operations from a reactive to proactive model.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Zero-trust simplified for application developers\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We also make it easy for developers to build secure applications from the get-go, whether they\u0026amp;#8217;re writing code, running it through the CI/CD pipeline, or in production. This zero-trust software supply chain is made possible by fully managed services that provide you with a consistent way to define and enforce policy, establish provenance, and prevent modification or tampering.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;And we\u0026amp;#8217;re continuing to enhance our\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/identity-security/applying-zero-trust-to-user-access-and-production-services\u0026#34;\u0026gt;zero-trust software supply chain capabilities\u0026lt;/a\u0026gt;\u0026amp;#160;with new features. For example, developers can now scan\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd\u0026#34;\u0026gt;containers for vulnerabilities\u0026lt;/a\u0026gt;\u0026amp;#160;using the simple \u0026amp;#8220;gcloud artifacts docker images scan\u0026amp;#8221; command. Now generally available, we\u0026amp;#8217;re also announcing that you can pair Cloud Run with Binary Authorization and, in a few clicks, ensure that only trusted container images make it to production. In addition, Binary Authorization now integrates with Cloud Build to automatically generate digital signatures and make it easy to set up deploy-time constraints that ensure only images signed by Cloud Build are sanctioned. Learn more about how we are making security easier\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/identity-security/next21-how-google-cloud-secures-the-world\u0026#34;\u0026gt;here\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Transform your cloud with Google\u0026lt;/h3\u0026gt;No matter where you are along the journey to transform your applications, we are here to partner with you. Whether its with the new product functionality we described today at Next, research and best practices such as the\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/devops/state-of-devops/\u0026#34;\u0026gt;2021 Accelerate State of DevOps report\u0026lt;/a\u0026gt;\u0026amp;#160;from Cloud\u0026amp;#8217;s DevOps Research and Assessment (DORA) team, or professional services such as the\u0026amp;#160;\u0026lt;a href=\u0026#34;https://cloud.google.com/camp\u0026#34;\u0026gt;Google Cloud Application Modernization Program (CAMP)\u0026lt;/a\u0026gt;, we\u0026amp;#8217;re here to help.\u0026lt;br\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\" _nghost-c20=\"\"\u003e\u003cp\u003eWhen it comes to software development using Google Cloud, we have three guiding principles. First, developing on Google Cloud needs to be open—we rely heavily on open-source technologies so that it\u0026#39;s easier to move apps between environments, recruit skilled developers, and access the latest innovations sooner. Second, developing for Google Cloud should also be easy—we strive to offer intuitive, integrated tools that run well wherever you build your code, while minimizing your operational overhead. Finally, running on Google Cloud should be transformative—we offer services that help unleash your imagination, along with best practices and professional services to help you bring your ideas to life. \u003c/p\u003e\u003cp\u003eToday, at \u003ca href=\"https://cloud.withgoogle.com/next\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.withgoogle.com\" track-metadata-module=\"post\"\u003eGoogle Cloud Next ‘21\u003c/a\u003e, we announced a variety of new tools and capabilities to deliver on those principles. \u003c/p\u003e\u003ch3\u003eOpening Anthos to virtual machines \u003c/h3\u003e\u003cp\u003eSince announcing Anthos, our open-source-based platform for hybrid and mutlicloud deployments in 2018, we have continued to receive strong reception from customers and partners. In fact, in Q2 2021, Anthos compute under management grew more than 500% year-over-year. Anthos unifies the management of infrastructure and applications across on-premises, edge, and multiple public clouds, as well as ensuring consistent operation at scale. Based on Google Kubernetes Engine (GKE), Anthos was originally designed to run applications in containers. To help you make that transition, we automated the process to migrate and modernize existing apps using \u003ca href=\"https://cloud.google.com/migrate/anthos\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/migrate/anthos\" track-metadata-module=\"post\"\u003eMigrate for Anthos and GKE\u003c/a\u003e from various virtual machine environments to containers. \u003c/p\u003e\u003cp\u003eWhile we have seen many customers make the leap to containerization, some are not quite ready to move completely off of virtual machines (VMs). They want a unified development platform where developers can build, modify, and deploy applications residing in both containers and VMs in a common, shared environment. Today, we are announcing \u003ca href=\"http://cloud.google.com/anthos\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"http://cloud.google.com/anthos\" track-metadata-module=\"post\"\u003eAnthos for Virtual Machines\u003c/a\u003e in preview, allowing you to standardize on Kubernetes while continuing to run some workloads that cannot be easily containerized in virtual machines. Anthos for VMs will help platform developers standardize on an operation model, process and tooling; enable incremental modernization efforts; and support traditional workloads like Virtual Network Functions (VNFs) or stateful monolithic workloads. \u003c/p\u003e\u003cp\u003eYou can take advantage of Anthos for VMs in two ways – either by attaching your vSphere VMs, or shifting your VMs as-is. For customers with active VMware environments, the Anthos control plane can now connect to your vSphere environment and attach your vSphere VMs, allowing you to apply consistent security and policies across clusters, gain visibility into the health and performance of your services, and manage traffic for both VMs and containers. Alternately, Anthos for VMs allows you to shift VMs as-is onto Anthos with \u003ca href=\"https://kubevirt.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://kubevirt.io\" track-metadata-module=\"post\"\u003eKubeVirt\u003c/a\u003e, an open-source virtualization API for Kubernetes. Now you can build, modify, and deploy applications residing in both application containers as well as VMs on a common, shared Anthos environment. This is a great option for organizations that prefer to use open-source virtualization, as those same organizations often prefer to run \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/1.6/concepts/about-bare-metal\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/1.6/concepts/about-bare-metal\" track-metadata-module=\"post\"\u003eAnthos on bare metal\u003c/a\u003e. To help get started, we provide you with a \u003ca href=\"https://cloud.google.com/anthos\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/anthos\" track-metadata-module=\"post\"\u003efit assessment tool\u003c/a\u003e to identify which approach to take. \u003c/p\u003e\u003ch3\u003eTaking your Anthos experience further\u003c/h3\u003e\u003cp\u003eWe’re also making it easier for you to manage containerized workloads already running in other clouds through Anthos. While you can already run containers in AWS and Azure from Anthos, we’re taking this a step further with the new \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/multi-cloud\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/clusters/docs/multi-cloud\" track-metadata-module=\"post\"\u003eAnthos Multi-Cloud API\u003c/a\u003e. Generally available in Q4 ‘21, this new API lets you provision and manage GKE clusters running on AWS or Azure infrastructure directly from the command line interface or the Google Cloud Console, all while being managed by a central control plane. This gives you a single API to manage all your container deployments regardless of which major public cloud you\u0026#39;re using, thus minimizing the time you spend jumping between user interfaces to accomplish day-to-day management tasks like creating, managing, and updating clusters. \u003c/p\u003e\u003cp\u003eOver the past year, we’ve brought some of the innovations originally developed for hybrid and multicloud use cases in Anthos back to GKE running in Google Cloud. Specifically, \u003ca href=\"https://cloud.google.com/anthos/config-management\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/config-management\" track-metadata-module=\"post\"\u003eAnthos Config Management\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/service-mesh\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/service-mesh\" track-metadata-module=\"post\"\u003eAnthos Service Mesh\u003c/a\u003e are now generally available for GKE as standalone services with pay-as-you-go pricing. GKE customers can now use Anthos Config Management to take advantage of config and policy automation at a low incremental per-cluster cost, and use Anthos Service Mesh to enable next-level security and networking on container-based microservices.\u003c/p\u003e\u003cp\u003eLast but not least, we are excited to announce that starting today, Anthos Service Mesh is generally available to support a hybrid mesh. This gives you the flexibility to have a common mesh that spans both your Google Cloud and on-prem deployments. \u003c/p\u003e\u003cp\u003eCustomers like \u003ca href=\"http://www.westerndigital.com\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"http://www.westerndigital.com\" track-metadata-module=\"post\"\u003eWestern Digital\u003c/a\u003e have already experienced many benefits from adopting Anthos as their application modernization platform:\u003c/p\u003e\u003cp\u003e\u003ci\u003e\u0026#34;As a global storage leader with sophisticated manufacturing facilities around the world, Western Digital sees cloud technology as an enabler of our key business priorities: reducing time to deliver products and services, rationalizing our entire application footprint, and meeting customer demand for IoT and edge applications,”\u003c/i\u003e said Jahidul Khandaker, senior vice president and CIO, Western Digital. \u003ci\u003e“Anthos is our unified management platform of choice—it gives us insights across our Google Cloud and on-premises environments, while keeping the doors open for a multi-cloud future. Anthos has delivered several advantages for our developers: a richer user experience, greater security, and enhanced flexibility to manage factory applications—no matter where they reside—on-prem, in the cloud or a mix of both.\u0026#34;\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eEasy does it\u003c/h3\u003e\u003cp\u003eIn addition to being an open platform, we strive to make Google Cloud easy to use for operators as well as developers. For example, earlier this year \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\" track-metadata-module=\"post\"\u003ewe introduced GKE Autopilot\u003c/a\u003e, a mode of operations in GKE that empowers you to simplify operations by offloading the management of infrastructure, control plane, and nodes. With GKE Autopilot, customers like \u003ca href=\"https://cloud.google.com/customers/ubie\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/customers/ubie\" track-metadata-module=\"post\"\u003eUbie\u003c/a\u003e, a Japanese-based healthcare technology company, have eliminated the need to configure and maintain infrastructure, which helped their development teams focus on making healthcare more accessible.\u003c/p\u003e\u003cp\u003eWith Cloud Run, our serverless compute platform, you can abstract away infrastructure management entirely. This year, our focus has been on bringing the simplicity of Cloud Run to more workloads, like traditional applications written in Java Spring Boot, ASP.NET, and Django, among others. Along with a new second generation execution environment for enhanced network and CPU performance, we’ve added \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run\" track-metadata-module=\"post\"\u003ecommitted-use discounts\u003c/a\u003e and introduced \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\" track-metadata-module=\"post\"\u003enew CPU allocation controls and price\u003c/a\u003e, allowing you to save up to 17% and 25%, respectively, on your compute bill. New \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/introducing-new-connectors-workflows\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/developers-practitioners/introducing-new-connectors-workflows\" track-metadata-module=\"post\"\u003econnectors\u003c/a\u003e for Workflows, \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-functions-integrates-with-google-secret-manager\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/cloud-functions-integrates-with-google-secret-manager\" track-metadata-module=\"post\"\u003eintegration\u003c/a\u003e between Cloud Functions and Secret Manager, and support for \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-functions-supports-min-instances\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/serverless/cloud-functions-supports-min-instances\" track-metadata-module=\"post\"\u003emin instances\u003c/a\u003e are just a few of the other ways we’ve made it easier to build modern, serverless apps. \u003c/p\u003e\u003ch3\u003eEasy for developers \u003c/h3\u003e\u003cp\u003eDevelopers spend a lot of time inside their integrated development environments (IDEs), writing code. Last year we announced \u003ca href=\"https://cloud.google.com/shell\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/shell\" track-metadata-module=\"post\"\u003eCloud Shell Editor\u003c/a\u003e, which makes the process of writing code as seamless as possible. It comes with your favorite developer tools (e.g., docker, minikube, skaffold, and many more) preinstalled, and this year, we added ~100 live tutorials to it—no more switching between the documentation, the terminal, and your code! \u003c/p\u003e\u003cp\u003eOnce that code is ready, you want building it and deploying it to be as seamless as possible. Today we are announcing \u003ca href=\"https://cloud.google.com/build/docs/hybrid/overview\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/build/docs/hybrid/overview\" track-metadata-module=\"post\"\u003eCloud Build Hybrid\u003c/a\u003e, which lets you build, test, and deploy across clouds and on-prem systems, so developers get consistent CI/CD tooling across their environments, and platform engineers don\u0026#39;t have to worry about maintaining and scaling their systems. Cloud Build is also integrated with \u003ca href=\"https://docs.google.com/document/d/1Z4vYdjF66UTOJgIpaDlDnXOuDCV6YulMhvNDjgoFiYo/edit?resourcekey=0-Vxxrly14dbfaBHg21HnOfg#\" target=\"_blank\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://docs.google.com\" track-metadata-module=\"post\"\u003eArtifact Registry\u003c/a\u003e, which now allows you to store not only in containers, but also language-specific artifacts in one place. Finally, with the recently launched \u003ca href=\"https://cloud.google.com/deploy/docs/overview\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/overview\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, which is a managed, continuous delivery service initially for GKE, we’re making it easy to scale your pipelines across your organization.\u003c/p\u003e\u003ch3\u003eEasy for operators\u003c/h3\u003e\u003cp\u003eWhen your applications are up and running, you need to observe and analyze them for better operations and business insights. While we already offer a fully managed metrics and alerting service with \u003ca href=\"https://cloud.google.com/monitoring\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring\" track-metadata-module=\"post\"\u003eCloud Monitoring\u003c/a\u003e, some Kubernetes users want to continue using open-source Prometheus without the scaling and management headaches. This is precisely why today we are announcing the preview of \u003ca href=\"http://cloud.google.com/monitoring\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"http://cloud.google.com/monitoring\" track-metadata-module=\"post\"\u003eManaged Service for Prometheus\u003c/a\u003e, helping you avoid vendor lock-in and delivering compatibility with your existing Prometheus alerts, workflows, and Grafana dashboards. Now you have all of the benefits of Prometheus, minus the management hassle. \u003c/p\u003e\u003cp\u003eTo give you easy diagnostics and deeper insights from across your business and systems, today we also combined \u003ca href=\"https://cloud.google.com/logging\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/logging\" track-metadata-module=\"post\"\u003eCloud Logging\u003c/a\u003e with the performance and power of BigQuery to introduce \u003ca href=\"https://cloud.google.com/logging/docs/log-analytics\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/log-analytics\" track-metadata-module=\"post\"\u003eLog Analytics\u003c/a\u003e. Currently in preview, Log Analytics allows you to rapidly store, manage, and analyze log data. This enables you to effectively move your operations from a reactive to proactive model. \u003c/p\u003e\u003ch3\u003eZero-trust simplified for application developers\u003c/h3\u003e\u003cp\u003eWe also make it easy for developers to build secure applications from the get-go, whether they’re writing code, running it through the CI/CD pipeline, or in production. This zero-trust software supply chain is made possible by fully managed services that provide you with a consistent way to define and enforce policy, establish provenance, and prevent modification or tampering. \u003c/p\u003e\u003cp\u003eAnd we’re continuing to enhance our \u003ca href=\"https://cloud.google.com/blog/products/identity-security/applying-zero-trust-to-user-access-and-production-services\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/identity-security/applying-zero-trust-to-user-access-and-production-services\" track-metadata-module=\"post\"\u003ezero-trust software supply chain capabilities\u003c/a\u003e with new features. For example, developers can now scan \u003ca href=\"https://cloud.google.com/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd\" track-metadata-module=\"post\"\u003econtainers for vulnerabilities\u003c/a\u003e using the simple “gcloud artifacts docker images scan” command. Now generally available, we’re also announcing that you can pair Cloud Run with Binary Authorization and, in a few clicks, ensure that only trusted container images make it to production. In addition, Binary Authorization now integrates with Cloud Build to automatically generate digital signatures and make it easy to set up deploy-time constraints that ensure only images signed by Cloud Build are sanctioned. Learn more about how we are making security easier \u003ca href=\"https://cloud.google.com/blog/products/identity-security/next21-how-google-cloud-secures-the-world\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/identity-security/next21-how-google-cloud-secures-the-world\" track-metadata-module=\"post\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eTransform your cloud with Google\u003c/h3\u003e\u003cp\u003eNo matter where you are along the journey to transform your applications, we are here to partner with you. Whether its with the new product functionality we described today at Next, research and best practices such as the \u003ca href=\"https://cloud.google.com/devops/state-of-devops/\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/devops/state-of-devops/\" track-metadata-module=\"post\"\u003e2021 Accelerate State of DevOps report\u003c/a\u003e from Cloud’s DevOps Research and Assessment (DORA) team, or professional services such as the \u003ca href=\"https://cloud.google.com/camp\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://cloud.google.com/camp\" track-metadata-module=\"post\"\u003eGoogle Cloud Application Modernization Program (CAMP)\u003c/a\u003e, we’re here to help.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen it comes to software development using Google Cloud, we have three guiding principles. First, developing on Google Cloud needs to be open—we rely heavily on open-source technologies so that it's easier to move apps between environments, recruit skilled developers, and access the latest innovations sooner. Second, developing for Google Cloud should also be easy—we strive to offer intuitive, integrated tools that run well wherever you build your code, while minimizing your operational overhead. Finally, running on Google Cloud should be transformative—we offer services that help unleash your imagination, along with best practices and professional services to help you bring your ideas to life. \u003c/p\u003e\u003cp\u003eToday, at \u003ca href=\"https://cloud.withgoogle.com/next\" target=\"_blank\"\u003eGoogle Cloud Next ‘21\u003c/a\u003e, we announced a variety of new tools and capabilities to deliver on those principles. \u003c/p\u003e\u003ch3\u003eOpening Anthos to virtual machines \u003c/h3\u003e\u003cp\u003eSince announcing Anthos, our open-source-based platform for hybrid and mutlicloud deployments in 2018, we have continued to receive strong reception from customers and partners. In fact, in Q2 2021, Anthos compute under management grew more than 500% year-over-year. Anthos unifies the management of infrastructure and applications across on-premises, edge, and multiple public clouds, as well as ensuring consistent operation at scale. Based on Google Kubernetes Engine (GKE), Anthos was originally designed to run applications in containers. To help you make that transition, we automated the process to migrate and modernize existing apps using \u003ca href=\"https://cloud.google.com/migrate/anthos\"\u003eMigrate for Anthos and GKE\u003c/a\u003e from various virtual machine environments to containers. \u003c/p\u003e\u003cp\u003eWhile we have seen many customers make the leap to containerization, some are not quite ready to move completely off of virtual machines (VMs). They want a unified development platform where developers can build, modify, and deploy applications residing in both containers and VMs in a common, shared environment. Today, we are announcing \u003ca href=\"http://cloud.google.com/anthos\"\u003eAnthos for Virtual Machines\u003c/a\u003e in preview, allowing you to standardize on Kubernetes while continuing to run some workloads that cannot be easily containerized in virtual machines. Anthos for VMs will help platform developers standardize on an operation model, process and tooling; enable incremental modernization efforts; and support traditional workloads like Virtual Network Functions (VNFs) or stateful monolithic workloads. \u003c/p\u003e\u003cp\u003eYou can take advantage of Anthos for VMs in two ways – either by attaching your vSphere VMs, or shifting your VMs as-is. For customers with active VMware environments, the Anthos control plane can now connect to your vSphere environment and attach your vSphere VMs, allowing you to apply consistent security and policies across clusters, gain visibility into the health and performance of your services, and manage traffic for both VMs and containers. Alternately, Anthos for VMs allows you to shift VMs as-is onto Anthos with \u003ca href=\"https://kubevirt.io/\" target=\"_blank\"\u003eKubeVirt\u003c/a\u003e, an open-source virtualization API for Kubernetes. Now you can build, modify, and deploy applications residing in both application containers as well as VMs on a common, shared Anthos environment. This is a great option for organizations that prefer to use open-source virtualization, as those same organizations often prefer to run \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/bare-metal/1.6/concepts/about-bare-metal\"\u003eAnthos on bare metal\u003c/a\u003e. To help get started, we provide you with a \u003ca href=\"https://cloud.google.com/anthos\"\u003efit assessment tool\u003c/a\u003e to identify which approach to take. \u003c/p\u003e\u003ch3\u003eTaking your Anthos experience further\u003c/h3\u003e\u003cp\u003eWe’re also making it easier for you to manage containerized workloads already running in other clouds through Anthos. While you can already run containers in AWS and Azure from Anthos, we’re taking this a step further with the new \u003ca href=\"https://cloud.google.com/anthos/clusters/docs/multi-cloud\"\u003eAnthos Multi-Cloud API\u003c/a\u003e. Generally available in Q4 ‘21, this new API lets you provision and manage GKE clusters running on AWS or Azure infrastructure directly from the command line interface or the Google Cloud Console, all while being managed by a central control plane. This gives you a single API to manage all your container deployments regardless of which major public cloud you're using, thus minimizing the time you spend jumping between user interfaces to accomplish day-to-day management tasks like creating, managing, and updating clusters. \u003c/p\u003e\u003cp\u003eOver the past year, we’ve brought some of the innovations originally developed for hybrid and multicloud use cases in Anthos back to GKE running in Google Cloud. Specifically, \u003ca href=\"https://cloud.google.com/anthos/config-management\"\u003eAnthos Config Management\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/anthos/service-mesh\"\u003eAnthos Service Mesh\u003c/a\u003e are now generally available for GKE as standalone services with pay-as-you-go pricing. GKE customers can now use Anthos Config Management to take advantage of config and policy automation at a low incremental per-cluster cost, and use Anthos Service Mesh to enable next-level security and networking on container-based microservices.\u003c/p\u003e\u003cp\u003eLast but not least, we are excited to announce that starting today, Anthos Service Mesh is generally available to support a hybrid mesh. This gives you the flexibility to have a common mesh that spans both your Google Cloud and on-prem deployments. \u003c/p\u003e\u003cp\u003eCustomers like \u003ca href=\"http://www.westerndigital.com\" target=\"_blank\"\u003eWestern Digital\u003c/a\u003e have already experienced many benefits from adopting Anthos as their application modernization platform:\u003c/p\u003e\u003cp\u003e\u003ci\u003e\"As a global storage leader with sophisticated manufacturing facilities around the world, Western Digital sees cloud technology as an enabler of our key business priorities: reducing time to deliver products and services, rationalizing our entire application footprint, and meeting customer demand for IoT and edge applications,”\u003c/i\u003e said Jahidul Khandaker, senior vice president and CIO, Western Digital. \u003ci\u003e“Anthos is our unified management platform of choice—it gives us insights across our Google Cloud and on-premises environments, while keeping the doors open for a multi-cloud future. Anthos has delivered several advantages for our developers: a richer user experience, greater security, and enhanced flexibility to manage factory applications—no matter where they reside—on-prem, in the cloud or a mix of both.\"\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003ch3\u003eEasy does it\u003c/h3\u003e\u003cp\u003eIn addition to being an open platform, we strive to make Google Cloud easy to use for operators as well as developers. For example, earlier this year \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/introducing-gke-autopilot\"\u003ewe introduced GKE Autopilot\u003c/a\u003e, a mode of operations in GKE that empowers you to simplify operations by offloading the management of infrastructure, control plane, and nodes. With GKE Autopilot, customers like \u003ca href=\"https://cloud.google.com/customers/ubie\"\u003eUbie\u003c/a\u003e, a Japanese-based healthcare technology company, have eliminated the need to configure and maintain infrastructure, which helped their development teams focus on making healthcare more accessible.\u003c/p\u003e\u003cp\u003eWith Cloud Run, our serverless compute platform, you can abstract away infrastructure management entirely. This year, our focus has been on bringing the simplicity of Cloud Run to more workloads, like traditional applications written in Java Spring Boot, ASP.NET, and Django, among others. Along with a new second generation execution environment for enhanced network and CPU performance, we’ve added \u003ca href=\"https://cloud.google.com/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run\"\u003ecommitted-use discounts\u003c/a\u003e and introduced \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-run-gets-always-on-cpu-allocation\"\u003enew CPU allocation controls and price\u003c/a\u003e, allowing you to save up to 17% and 25%, respectively, on your compute bill. New \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/introducing-new-connectors-workflows\"\u003econnectors\u003c/a\u003e for Workflows, \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-functions-integrates-with-google-secret-manager\"\u003eintegration\u003c/a\u003e between Cloud Functions and Secret Manager, and support for \u003ca href=\"https://cloud.google.com/blog/products/serverless/cloud-functions-supports-min-instances\"\u003emin instances\u003c/a\u003e are just a few of the other ways we’ve made it easier to build modern, serverless apps. \u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003ch3\u003eEasy for developers \u003c/h3\u003eDevelopers spend a lot of time inside their integrated development environments (IDEs), writing code. Last year we announced \u003ca href=\"https://cloud.google.com/shell\"\u003eCloud Shell Editor\u003c/a\u003e, which makes the process of writing code as seamless as possible. It comes with your favorite developer tools (e.g., docker, minikube, skaffold, and many more) preinstalled, and this year, we added ~100 live tutorials to it—no more switching between the documentation, the terminal, and your code! \u003cp\u003e\u003c/p\u003e\u003cp\u003eOnce that code is ready, you want building it and deploying it to be as seamless as possible. Today we are announcing \u003ca href=\"https://cloud.google.com/build/docs/hybrid/overview\"\u003eCloud Build Hybrid\u003c/a\u003e, which lets you build, test, and deploy across clouds and on-prem systems, so developers get consistent CI/CD tooling across their environments, and platform engineers don't have to worry about maintaining and scaling their systems. Cloud Build is also integrated with \u003ca href=\"https://docs.google.com/document/d/1Z4vYdjF66UTOJgIpaDlDnXOuDCV6YulMhvNDjgoFiYo/edit?resourcekey=0-Vxxrly14dbfaBHg21HnOfg#\" target=\"_blank\"\u003eArtifact Registry\u003c/a\u003e, which now allows you to store not only in containers, but also language-specific artifacts in one place. Finally, with the recently launched \u003ca href=\"https://cloud.google.com/deploy/docs/overview\"\u003eGoogle Cloud Deploy\u003c/a\u003e, which is a managed, continuous delivery service initially for GKE, we’re making it easy to scale your pipelines across your organization.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003ch3\u003eEasy for operators\u003c/h3\u003eWhen your applications are up and running, you need to observe and analyze them for better operations and business insights. While we already offer a fully managed metrics and alerting service with \u003ca href=\"https://cloud.google.com/monitoring\"\u003eCloud Monitoring\u003c/a\u003e, some Kubernetes users want to continue using open-source Prometheus without the scaling and management headaches. This is precisely why today we are announcing the preview of \u003ca href=\"http://cloud.google.com/monitoring\"\u003eManaged Service for Prometheus\u003c/a\u003e, helping you avoid vendor lock-in and delivering compatibility with your existing Prometheus alerts, workflows, and Grafana dashboards. Now you have all of the benefits of Prometheus, minus the management hassle. \u003cp\u003e\u003c/p\u003e\u003cp\u003eTo give you easy diagnostics and deeper insights from across your business and systems, today we also combined \u003ca href=\"https://cloud.google.com/logging\"\u003eCloud Logging\u003c/a\u003e with the performance and power of BigQuery to introduce \u003ca href=\"https://cloud.google.com/logging/docs/log-analytics\"\u003eLog Analytics\u003c/a\u003e. Currently in preview, Log Analytics allows you to rapidly store, manage, and analyze log data. This enables you to effectively move your operations from a reactive to proactive model. \u003c/p\u003e\u003ch3\u003eZero-trust simplified for application developers\u003c/h3\u003e\u003cp\u003eWe also make it easy for developers to build secure applications from the get-go, whether they’re writing code, running it through the CI/CD pipeline, or in production. This zero-trust software supply chain is made possible by fully managed services that provide you with a consistent way to define and enforce policy, establish provenance, and prevent modification or tampering. \u003c/p\u003e\u003cp\u003eAnd we’re continuing to enhance our \u003ca href=\"https://cloud.google.com/blog/products/identity-security/applying-zero-trust-to-user-access-and-production-services\"\u003ezero-trust software supply chain capabilities\u003c/a\u003e with new features. For example, developers can now scan \u003ca href=\"https://cloud.google.com/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd\"\u003econtainers for vulnerabilities\u003c/a\u003e using the simple “gcloud artifacts docker images scan” command. Now generally available, we’re also announcing that you can pair Cloud Run with Binary Authorization and, in a few clicks, ensure that only trusted container images make it to production. In addition, Binary Authorization now integrates with Cloud Build to automatically generate digital signatures and make it easy to set up deploy-time constraints that ensure only images signed by Cloud Build are sanctioned. Learn more about how we are making security easier \u003ca href=\"https://cloud.google.com/blog/products/identity-security/next21-how-google-cloud-secures-the-world\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003eTransform your cloud with Google\u003c/h3\u003eNo matter where you are along the journey to transform your applications, we are here to partner with you. Whether its with the new product functionality we described today at Next, research and best practices such as the \u003ca href=\"https://cloud.google.com/devops/state-of-devops/\"\u003e2021 Accelerate State of DevOps report\u003c/a\u003e from Cloud’s DevOps Research and Assessment (DORA) team, or professional services such as the \u003ca href=\"https://cloud.google.com/camp\"\u003eGoogle Cloud Application Modernization Program (CAMP)\u003c/a\u003e, we’re here to help.\u003cbr/\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/CloudNext21_11.max-2200x2200.jpg",
      "date_published": "2021-10-13T12:00:00Z",
      "author": {
        "name": "\u003cname\u003eChen Goldberg\u003c/name\u003e\u003ctitle\u003eVP of Engineering, Application Modernization Platform\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/application-development/node-python-and-javarepos-are-generally-available/",
      "title": "Artifact Registry for language packages now generally available",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c48=\"\"\u003e\u003cdiv _ngcontent-c48=\"\" innerhtml=\"\u0026lt;p\u0026gt;Using a centralized, private repository to host your internal code as a package not only enables code reuse, but also simplifies and secures your existing software delivery pipeline. By using the same formats and tools as you would in the open-source ecosystem, you can leverage the same advantages, simplify your build, and keep your business logic and applications secure.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Language repository formats, now generally available\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;As of today, support for language repositories in \u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/\u0026#34;\u0026gt;Artifact Registry is now generally available\u0026lt;/a\u0026gt;, allowing you to store all your language-specific artifacts in one place. Supported package types include:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/java\u0026#34;\u0026gt;Java\u0026lt;/a\u0026gt; packages\u0026amp;#160; (using the Maven repository format)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/nodejs\u0026#34;\u0026gt;Node.js\u0026lt;/a\u0026gt; packages (using the npm repository format)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/python\u0026#34;\u0026gt;Python\u0026lt;/a\u0026gt; packages (using the PyPI repository format)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;OS repository formats in preview\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Additionally, support for new repository formats for Linux distributions is in public preview, allowing developers to create private internal-only packages and securely use them across multiple applications deployed to Linux environments. New supported artifact formats include:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/os-packages/debian\u0026#34;\u0026gt;Debian\u0026lt;/a\u0026gt; packages (using the Apt repository format)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/os-packages/rpm\u0026#34;\u0026gt;RPM\u0026lt;/a\u0026gt; packages (using the Yum repository format)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;This is in addition to existing container images and Helm charts (using the Docker repository format).\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Your own secure supply chain\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Storing your packages in Artifact Registry not only enables code reuse, but also simplifies and secures your existing build pipeline. In addition to bringing your internal packages to a managed repository, using Artifact Registry also allows you to take additional steps to improve the security of your software delivery pipeline:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Use \u0026lt;a href=\u0026#34;https://cloud.google.com/container-analysis/docs/container-scanning-overview\u0026#34;\u0026gt;Container Analysis\u0026lt;/a\u0026gt; to scan containers that use your private packages for vulnerabilities\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Include your repositories in a \u0026lt;a href=\u0026#34;https://cloud.google.com/vpc\u0026#34;\u0026gt;Virtual Private Cloud\u0026lt;/a\u0026gt; to control access\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Monitor repository usage with \u0026lt;a href=\u0026#34;https://cloud.google.com/logging/docs/audit\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Use the \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/binauthz-attestation\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;binauthz-attestation\u0026lt;/a\u0026gt; builder with \u0026lt;a href=\u0026#34;https://cloud.google.com/build\u0026#34;\u0026gt;Cloud Build\u0026lt;/a\u0026gt; to create attestations that \u0026lt;a href=\u0026#34;https://cloud.google.com/binary-authorization\u0026#34;\u0026gt;Binary Authorization\u0026lt;/a\u0026gt; verifies before allowing container deployment\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Use\u0026amp;#160; Cloud Identity and Access Management (IAM) for \u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/access-control\u0026#34;\u0026gt;repository access control\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Seamless authentication\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;With credential helpers to authenticate access for installers based on \u0026lt;a href=\u0026#34;https://cloud.google.com/iam\u0026#34;\u0026gt;Cloud Identity and Access Management (IAM)\u0026lt;/a\u0026gt; permissions, using Artifact Registry to host your packages makes authentication to private repositories easy. By managing IAM groups, administrators can control access to repositories via the same tools used across Google Cloud.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Regional repositories lower cost and enable data compliance\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Artifact Registry provides \u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/repo-locations\u0026#34;\u0026gt;regional support\u0026lt;/a\u0026gt;, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Get started today\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;These repository formats are now generally available to all Artifact Registry customers. Pricing for language repositories is the same as container pricing; see the \u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/pricing\u0026#34;\u0026gt;pricing documentation\u0026lt;/a\u0026gt; for details. To get started using language and OS repositories, try the quickstarts in the Artifact Registry documentation.\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/nodejs\u0026#34;\u0026gt;Node.js\u0026lt;/a\u0026gt; Quickstart Guide\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/python\u0026#34;\u0026gt;Python\u0026lt;/a\u0026gt; Quickstart Guide\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/java\u0026#34;\u0026gt;Java\u0026lt;/a\u0026gt; Quickstart Guide\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/os-packages/debian/apt-quickstart\u0026#34;\u0026gt;Apt\u0026lt;/a\u0026gt; Quickstart Guide\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/os-packages/rpm/yum-quickstart\u0026#34;\u0026gt;RPM\u0026lt;/a\u0026gt; Quickstart Guide\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\"\u003e\u003cp\u003eUsing a centralized, private repository to host your internal code as a package not only enables code reuse, but also simplifies and secures your existing software delivery pipeline. By using the same formats and tools as you would in the open-source ecosystem, you can leverage the same advantages, simplify your build, and keep your business logic and applications secure.\u003c/p\u003e\u003ch3\u003eLanguage repository formats, now generally available\u003c/h3\u003e\u003cp\u003eAs of today, support for language repositories in \u003ca href=\"https://cloud.google.com/artifact-registry/\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/\" track-metadata-module=\"post\"\u003eArtifact Registry is now generally available\u003c/a\u003e, allowing you to store all your language-specific artifacts in one place. Supported package types include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/java\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/java\" track-metadata-module=\"post\"\u003eJava\u003c/a\u003e packages  (using the Maven repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/nodejs\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/nodejs\" track-metadata-module=\"post\"\u003eNode.js\u003c/a\u003e packages (using the npm repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/python\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/python\" track-metadata-module=\"post\"\u003ePython\u003c/a\u003e packages (using the PyPI repository format)\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eOS repository formats in preview\u003c/h3\u003e\u003cp\u003eAdditionally, support for new repository formats for Linux distributions is in public preview, allowing developers to create private internal-only packages and securely use them across multiple applications deployed to Linux environments. New supported artifact formats include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/os-packages/debian\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/os-packages/debian\" track-metadata-module=\"post\"\u003eDebian\u003c/a\u003e packages (using the Apt repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/os-packages/rpm\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/os-packages/rpm\" track-metadata-module=\"post\"\u003eRPM\u003c/a\u003e packages (using the Yum repository format)\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis is in addition to existing container images and Helm charts (using the Docker repository format). \u003c/p\u003e\u003cp\u003eYour own secure supply chain\u003c/p\u003e\u003cp\u003eStoring your packages in Artifact Registry not only enables code reuse, but also simplifies and secures your existing build pipeline. In addition to bringing your internal packages to a managed repository, using Artifact Registry also allows you to take additional steps to improve the security of your software delivery pipeline:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eUse \u003ca href=\"https://cloud.google.com/container-analysis/docs/container-scanning-overview\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/container-analysis/docs/container-scanning-overview\" track-metadata-module=\"post\"\u003eContainer Analysis\u003c/a\u003e to scan containers that use your private packages for vulnerabilities\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eInclude your repositories in a \u003ca href=\"https://cloud.google.com/vpc\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/vpc\" track-metadata-module=\"post\"\u003eVirtual Private Cloud\u003c/a\u003e to control access\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eMonitor repository usage with \u003ca href=\"https://cloud.google.com/logging/docs/audit\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/audit\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse the \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/binauthz-attestation\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ebinauthz-attestation\u003c/a\u003e builder with \u003ca href=\"https://cloud.google.com/build\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/build\" track-metadata-module=\"post\"\u003eCloud Build\u003c/a\u003e to create attestations that \u003ca href=\"https://cloud.google.com/binary-authorization\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/binary-authorization\" track-metadata-module=\"post\"\u003eBinary Authorization\u003c/a\u003e verifies before allowing container deployment\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse  Cloud Identity and Access Management (IAM) for \u003ca href=\"https://cloud.google.com/artifact-registry/docs/access-control\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/access-control\" track-metadata-module=\"post\"\u003erepository access control\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eSeamless authentication\u003c/h3\u003e\u003cp\u003eWith credential helpers to authenticate access for installers based on \u003ca href=\"https://cloud.google.com/iam\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/iam\" track-metadata-module=\"post\"\u003eCloud Identity and Access Management (IAM)\u003c/a\u003e permissions, using Artifact Registry to host your packages makes authentication to private repositories easy. By managing IAM groups, administrators can control access to repositories via the same tools used across Google Cloud.\u003c/p\u003e\u003ch3\u003eRegional repositories lower cost and enable data compliance\u003c/h3\u003e\u003cp\u003eArtifact Registry provides \u003ca href=\"https://cloud.google.com/artifact-registry/docs/repo-locations\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/repo-locations\" track-metadata-module=\"post\"\u003eregional support\u003c/a\u003e, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.\u003c/p\u003e\u003ch3\u003eGet started today\u003c/h3\u003e\u003cp\u003eThese repository formats are now generally available to all Artifact Registry customers. Pricing for language repositories is the same as container pricing; see the \u003ca href=\"https://cloud.google.com/artifact-registry/pricing\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/pricing\" track-metadata-module=\"post\"\u003epricing documentation\u003c/a\u003e for details. To get started using language and OS repositories, try the quickstarts in the Artifact Registry documentation.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/nodejs\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/nodejs\" track-metadata-module=\"post\"\u003eNode.js\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/python\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/python\" track-metadata-module=\"post\"\u003ePython\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/java\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/java\" track-metadata-module=\"post\"\u003eJava\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/os-packages/debian/apt-quickstart\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/os-packages/debian/apt-quickstart\" track-metadata-module=\"post\"\u003eApt\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/os-packages/rpm/yum-quickstart\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/os-packages/rpm/yum-quickstart\" track-metadata-module=\"post\"\u003eRPM\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eUsing a centralized, private repository to host your internal code as a package not only enables code reuse, but also simplifies and secures your existing software delivery pipeline. By using the same formats and tools as you would in the open-source ecosystem, you can leverage the same advantages, simplify your build, and keep your business logic and applications secure.\u003c/p\u003e\u003ch3\u003eLanguage repository formats, now generally available\u003c/h3\u003e\u003cp\u003eAs of today, support for language repositories in \u003ca href=\"https://cloud.google.com/artifact-registry/\"\u003eArtifact Registry is now generally available\u003c/a\u003e, allowing you to store all your language-specific artifacts in one place. Supported package types include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/java\"\u003eJava\u003c/a\u003e packages  (using the Maven repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/nodejs\"\u003eNode.js\u003c/a\u003e packages (using the npm repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/python\"\u003ePython\u003c/a\u003e packages (using the PyPI repository format)\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eOS repository formats in preview\u003c/h3\u003e\u003cp\u003eAdditionally, support for new repository formats for Linux distributions is in public preview, allowing developers to create private internal-only packages and securely use them across multiple applications deployed to Linux environments. New supported artifact formats include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/os-packages/debian\"\u003eDebian\u003c/a\u003e packages (using the Apt repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/os-packages/rpm\"\u003eRPM\u003c/a\u003e packages (using the Yum repository format)\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis is in addition to existing container images and Helm charts (using the Docker repository format). \u003c/p\u003e\u003cp\u003eYour own secure supply chain\u003c/p\u003e\u003cp\u003eStoring your packages in Artifact Registry not only enables code reuse, but also simplifies and secures your existing build pipeline. In addition to bringing your internal packages to a managed repository, using Artifact Registry also allows you to take additional steps to improve the security of your software delivery pipeline:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eUse \u003ca href=\"https://cloud.google.com/container-analysis/docs/container-scanning-overview\"\u003eContainer Analysis\u003c/a\u003e to scan containers that use your private packages for vulnerabilities\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eInclude your repositories in a \u003ca href=\"https://cloud.google.com/vpc\"\u003eVirtual Private Cloud\u003c/a\u003e to control access\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eMonitor repository usage with \u003ca href=\"https://cloud.google.com/logging/docs/audit\"\u003eCloud Audit Logs\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse the \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-builders-community/tree/master/binauthz-attestation\" target=\"_blank\"\u003ebinauthz-attestation\u003c/a\u003e builder with \u003ca href=\"https://cloud.google.com/build\"\u003eCloud Build\u003c/a\u003e to create attestations that \u003ca href=\"https://cloud.google.com/binary-authorization\"\u003eBinary Authorization\u003c/a\u003e verifies before allowing container deployment\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse  Cloud Identity and Access Management (IAM) for \u003ca href=\"https://cloud.google.com/artifact-registry/docs/access-control\"\u003erepository access control\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eSeamless authentication\u003c/h3\u003e\u003cp\u003eWith credential helpers to authenticate access for installers based on \u003ca href=\"https://cloud.google.com/iam\"\u003eCloud Identity and Access Management (IAM)\u003c/a\u003e permissions, using Artifact Registry to host your packages makes authentication to private repositories easy. By managing IAM groups, administrators can control access to repositories via the same tools used across Google Cloud.\u003c/p\u003e\u003ch3\u003eRegional repositories lower cost and enable data compliance\u003c/h3\u003e\u003cp\u003eArtifact Registry provides \u003ca href=\"https://cloud.google.com/artifact-registry/docs/repo-locations\"\u003eregional support\u003c/a\u003e, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.\u003c/p\u003e\u003ch3\u003eGet started today\u003c/h3\u003e\u003cp\u003eThese repository formats are now generally available to all Artifact Registry customers. Pricing for language repositories is the same as container pricing; see the \u003ca href=\"https://cloud.google.com/artifact-registry/pricing\"\u003epricing documentation\u003c/a\u003e for details. To get started using language and OS repositories, try the quickstarts in the Artifact Registry documentation.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/nodejs\"\u003eNode.js\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/python\"\u003ePython\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/java\"\u003eJava\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/os-packages/debian/apt-quickstart\"\u003eApt\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/os-packages/rpm/yum-quickstart\"\u003eRPM\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/application-development/artifact-registry-adds-node-python-and-java-repositories/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_7fdTm09.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eNode, Python and Java repositories now available in Artifact Registry\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eExpanded language support lets you store Java, Node and Python artifacts in Artifact Registry, for a more secure software supply chain.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/appdev.max-2200x2200.jpg",
      "date_published": "2021-10-07T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eDustin Ingram\u003c/name\u003e\u003ctitle\u003eSenior Developer Advocate\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-connector-on-a-gke-cluster/",
      "title": "Deploy Anthos on GKE with Terraform Part 3: Enabling Cloud Resources Provisioning",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn the previous two parts of the series (\u003ca href=\"https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster\"\u003e1\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-policy-controller-on-a-gke-cluster\"\u003e2\u003c/a\u003e) we discussed how new features in \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs\" target=\"_blank\"\u003eTerraform Provider for GCP\u003c/a\u003e make it easier for platform administrators to extend their Terraform automation to add \u003ca href=\"https://cloud.google.com/anthos/config-management\"\u003eAnthos Config Management (ACM)\u003c/a\u003e features to their GKE clusters. Using familiar Terraform resource syntax, you can add \u003ccode\u003egoogle_gke_hub_feature\u003c/code\u003e and \u003ccode\u003egoogle_gke_hub_feature_membership\u003c/code\u003e resource with \u003ccode\u003econfigmanagement\u003c/code\u003e section to enable Config Sync for GitOps integration and \u003ccode\u003epolicy_controller\u003c/code\u003e section for policy validation.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eSo far the cluster in our example was only used to host the configuration consisting of Kubernetes native resources - containerized Wordpress application powered by in-cluster MySQL database. We are getting all the advantages of Kubernetes: continuous \u003ca href=\"https://cloud.google.com/config-connector/docs/concepts/reconciliation\"\u003ereconciliation\u003c/a\u003e and drift correction, \u003ca href=\"https://en.wikipedia.org/wiki/Eventual_consistency\" target=\"_blank\"\u003eeventual consistency,\u003c/a\u003e order independence and \u003ca href=\"https://en.wikipedia.org/wiki/Idempotence\" target=\"_blank\"\u003eidempotence\u003c/a\u003e. We are also deriving benefits from GitOps approach using the repo as the source of truth, enabling reviewable and version-controlled workflow.\u003c/p\u003e\u003cp\u003eNow let’s take it even further. \u003cb\u003eWe’ll demonstrate how\u003c/b\u003e \u003ca href=\"https://cloud.google.com/blog/topics/developers-practitioners/build-platform-krm-part-1-whats-platform\"\u003e\u003cb\u003ethe same model\u003c/b\u003e\u003c/a\u003e \u003cb\u003ecan be expanded to create and manage not just native Kubernetes resources (Kubernetes service accounts, pods, deployments) but also GCP cloud resources - Cloud databases, storage buckets, VM instances and\u003c/b\u003e \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/overview\"\u003emany other GCP resources\u003c/a\u003e. Since \u003ca href=\"https://cloud.google.com/config-connector/docs/overview\"\u003eConfig Connector\u003c/a\u003e was \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/config-connector-bridges-kubernetes-gcp-resources\"\u003elaunched\u003c/a\u003e in 2020, many Kuberentes shops have embraced its convenient way of managing GCP resources. Now that we have enabled Terraform support for Anthos features, combined with a Terraform configuration \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster#config_connector_config\" target=\"_blank\"\u003eoption\u003c/a\u003e to install Config Connector on the cluster,  the full GitOps workflow and Kubernetes lifecycle spanning native and cloud resources can be enabled during cluster creation.\u003c/p\u003e\u003cp\u003eIn our \u003ca href=\"https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/master/examples/acm-terraform-blog-part3/terraform/gke.tf\" target=\"_blank\"\u003eexample\u003c/a\u003e, we are enabling Config Connector using the \u003ccode\u003econfig_connector\u003c/code\u003e setting in the \u003ccode\u003egke\u003c/code\u003e Terraform module. We also use the \u003ca href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity\"\u003e\u003ccode\u003eworkload-identity\u003c/code\u003e\u003c/a\u003e module to create a GCP service account that will be used to make the changes to K8s resources and bind it to Kubernetes Service Account (\u003ccode\u003ecnrm-controller-manager\u003c/code\u003e in \u003ccode\u003ecnrm-system namespace\u003c/code\u003e). You can choose the appropriate permissions to GCP service account -  in our examples we are giving it the \u003ccode\u003eowner\u003c/code\u003e role for simplicity.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eLet’s review what changed in this part in the repo that is synchronized with our cluster via Config Sync. In  \u003ca href=\"https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/tree/master/examples/acm-terraform-blog-part1/config-root\" target=\"_blank\"\u003ethe first part\u003c/a\u003e, we added a collection of configs, all native Kubernetes objects. These configs provisioned an in-cluster Wordpress application with an in-cluster MySQL database. In the second part, we added a set of rules used by PolicyController to audit our cluster. In this part, we start by  adding a \u003ca href=\"https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/master/examples/acm-terraform-blog-part3/config-root/configconnector.yaml\" target=\"_blank\"\u003econfig\u003c/a\u003e representing an instance of the Config Connector addon. While the addon is enabled on the cluster, this config instance is required to activate it. It specifies the settings, such as \u003ccode\u003emode\u003c/code\u003e (cluster or namespace) and GCP service account, linking it to the cnrmsa account that we created above using the \u003ccode\u003eworkload-identity\u003c/code\u003e module.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAnd now we can create GCP resources in addition to in-cluster native resources directly in our K8s configuration. We are configuring an \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/resource-docs/sql/sqldatabase\"\u003eSQLDatabase\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/resource-docs/sql/sqlinstance\"\u003eSQLInstance\u003c/a\u003e resources:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eas well as \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/resource-docs/sql/sqluser\"\u003eSQLUser\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/resource-docs/iam/iampolicy\"\u003eIAMPolicy\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/resource-docs/iam/iampolicymember\"\u003eIAMPolicyMember\u003c/a\u003e (see complete example \u003ca href=\"https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/master/examples/acm-terraform-blog-part3/config-root/wordpress-bundle.yaml\" target=\"_blank\"\u003ehere\u003c/a\u003e). Overall, \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/overview\"\u003emore than 130 resources are now supported with Config Connector\u003c/a\u003e covering many of the most popular GCP configuration patterns.\u003c/p\u003e\u003cp\u003eYou will notice that this configuration is expanded and parameterized for our specific project. How did we specify the parameter values? While many tools can be used, including \u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e and \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e and some of them used together, we recommend \u003ca href=\"https://cloud.google.com/architecture/managing-cloud-infrastructure-using-kpt\"\u003eKpt\u003c/a\u003e that fully embraces the principles of \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes\"\u003econfiguration-as-data\u003c/a\u003e. In this example we used set-project-id kpt function to specify project id on the config-root directory before submitting the change to Git.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis \u003ca href=\"https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/tree/master/examples/acm-terraform-blog-part3\" target=\"_blank\"\u003erepo\u003c/a\u003e provides a complete example of provisioning a cluster that is synchronized with a repo that contains a WordPress configuration powered, this time, by GCP MySQL database.\u003c/p\u003e\u003cp\u003eThis was the third and the final article of the three part series that showcased Terraform support for ACM features and how it simplifies cluster provisioning for platform administrators.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Anthos.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eDeploy Anthos on GKE with Terraform part 1: GitOps with Config Sync\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eIt is now simple to use Terraform to configure Anthos features on your GKE clusters. This is the first part of the 3 part series that des...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Google_Cloud_Anthos_A.jpg",
      "date_published": "2021-10-06T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eSteven Linde\u003c/name\u003e\u003ctitle\u003eEngineering Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/",
      "title": "Introducing Google Cloud Deploy: Managed continuous delivery to GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#gcp\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e S. Bogdan \u003c/p\u003e\u003cp\u003e Product Manager \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e September 22, 2021 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c17=\"\"\u003e\u003cdiv _ngcontent-c17=\"\"\u003e\u003ch4 _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eNext ’21 registration is open\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eJoin us October 12–14, 2021, for our digital flagship event\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c17=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"Next21 registration\" track-metadata-eventdetail=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\" href=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eRegister now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Continuous delivery is frequently top-of-mind for organizations adopting \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE). However, continuous delivery \u0026amp;#8212;deploying container image artifacts into your various environments\u0026amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It doesn\u0026amp;#8217;t have to be this way.\u0026amp;#160;\u0026lt;/p\u0026gt;Today, we are pleased to announce \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003e\u003cp\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Solving for continuous delivery challenges\u0026lt;br\u0026gt;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cost of ownership\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current\u0026amp;#8212;to say nothing of maintenance\u0026amp;#8212;is resource-intensive and takes time away from the core business.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;We can\u0026amp;#8217;t afford to be innovating in continuous delivery,\u0026amp;#8221; one customer told us. \u0026amp;#8220;We want an opinionated product that supports best practices out of the box.\u0026amp;#8221;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy addresses cost of ownership head-on.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy also provides structure. \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\u0026#34;\u0026gt;Delivery pipelines\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology\u0026#34;\u0026gt;targets\u0026lt;/a\u0026gt; are defined \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/config-files\u0026#34;\u0026gt;declaratively\u0026lt;/a\u0026gt; and are \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/pipeline-instances\u0026#34;\u0026gt;stored alongside each release\u0026lt;/a\u0026gt;. That means if your delivery pipeline changes, the release\u0026amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-metadata-module=\"post\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology\" track-metadata-module=\"post\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/config-files\" track-metadata-module=\"post\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-metadata-module=\"post\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application\" track-metadata-module=\"post\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Security and audit\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn\u0026amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/iam-roles-permissions\u0026#34;\u0026gt;discrete resource access control\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment\u0026#34;\u0026gt;execution-level security\u0026lt;/a\u0026gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\u0026#34;\u0026gt;approvals\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u0026lt;a href=\u0026#34;https://cloud.google.com/audit-logs\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/audit-logs\u0026#34;\u0026gt;audits\u0026lt;/a\u0026gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Integration\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;embraces the GKE delivery tooling ecosystems\u0026lt;/a\u0026gt; in three ways: connectivity to CI systems, support for leading configuration (\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#render\u0026#34;\u0026gt;rendering\u0026lt;/a\u0026gt;) tooling, and \u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Pub/Sub\u0026lt;/a\u0026gt; notifications to enable third-party integrations.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\u0026#34;\u0026gt;Connecting Google Cloud Deploy\u0026lt;/a\u0026gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple\u0026amp;#160;\u0026lt;i\u0026gt;`\u0026lt;/i\u0026gt;\u0026lt;i\u0026gt;gcloud beta deploy releases create`.\u0026lt;/i\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-metadata-module=\"post\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment\" track-metadata-module=\"post\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-metadata-module=\"post\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/audit-logs\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/audit-logs\" track-metadata-module=\"post\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#render\" track-metadata-module=\"post\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-metadata-module=\"post\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy\u0026amp;#160; leverages \u0026lt;a href=\u0026#34;https://skaffold.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Skaffold\u0026lt;/a\u0026gt;, allowing you to \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/skaffold\u0026#34;\u0026gt;standardize your configuration\u0026lt;/a\u0026gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u0026lt;a href=\u0026#34;https://helm.sh/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Helm\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kustomize.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Kustomize\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kpt.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;kpt\u0026lt;/a\u0026gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, to facilitate other integrations, such as a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\u0026#34;\u0026gt;post-deployment test execution\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\u0026#34;\u0026gt;third party approval workflows\u0026lt;/a\u0026gt;, Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\u0026#34;\u0026gt;emits Pub/Sub messages\u0026lt;/a\u0026gt; throughout a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\u0026#34;\u0026gt;release\u0026amp;#8217;s lifecycle\u0026lt;/a\u0026gt;.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The future\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it\u0026amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we\u0026amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the meantime, to get started with the Preview, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;product page\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/quickstart-basic\u0026#34;\u0026gt;quickstart\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt;. Finally, If you have feedback on Google Cloud Deploy, you can \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;join the conversation\u0026lt;/a\u0026gt;. We look forward to hearing from you!\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://skaffold.dev\" track-metadata-module=\"post\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/skaffold\" track-metadata-module=\"post\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://helm.sh\" track-metadata-module=\"post\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://kustomize.io\" track-metadata-module=\"post\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://kpt.dev\" track-metadata-module=\"post\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-metadata-module=\"post\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-metadata-module=\"post\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-metadata-module=\"post\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-metadata-module=\"post\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-metadata-module=\"post\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-video-block _nghost-c16=\"\"\u003e\u003cp _ngcontent-c16=\"\"\u003e\u003ciframe _ngcontent-c16=\"\" allow=\"encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" position=\"absolute\" width=\"100%\" src=\"https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1\u0026amp;\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/article-video-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c18=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy GIF\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eContextualized deployment approvals\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-Il8FlhR9jKM-\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\"\u003e\u003cimg alt=\"Introducing Cloud Deploy\" src=\"//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-Il8FlhR9jKM-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"Il8FlhR9jKM\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/",
      "title": "Introducing Google Cloud Deploy: Managed continuous delivery to GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#gcp\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e S. Bogdan \u003c/p\u003e\u003cp\u003e Product Manager \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e September 22, 2021 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c17=\"\"\u003e\u003cdiv _ngcontent-c17=\"\"\u003e\u003ch4 _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eNext ’21 registration is open\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eJoin us October 12–14, 2021, for our digital flagship event\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c17=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"Next21 registration\" track-metadata-eventdetail=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\" href=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eRegister now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Continuous delivery is frequently top-of-mind for organizations adopting \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE). However, continuous delivery \u0026amp;#8212;deploying container image artifacts into your various environments\u0026amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It doesn\u0026amp;#8217;t have to be this way.\u0026amp;#160;\u0026lt;/p\u0026gt;Today, we are pleased to announce \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003e\u003cp\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Solving for continuous delivery challenges\u0026lt;br\u0026gt;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cost of ownership\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current\u0026amp;#8212;to say nothing of maintenance\u0026amp;#8212;is resource-intensive and takes time away from the core business.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;We can\u0026amp;#8217;t afford to be innovating in continuous delivery,\u0026amp;#8221; one customer told us. \u0026amp;#8220;We want an opinionated product that supports best practices out of the box.\u0026amp;#8221;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy addresses cost of ownership head-on.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy also provides structure. \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\u0026#34;\u0026gt;Delivery pipelines\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology\u0026#34;\u0026gt;targets\u0026lt;/a\u0026gt; are defined \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/config-files\u0026#34;\u0026gt;declaratively\u0026lt;/a\u0026gt; and are \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/pipeline-instances\u0026#34;\u0026gt;stored alongside each release\u0026lt;/a\u0026gt;. That means if your delivery pipeline changes, the release\u0026amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-metadata-module=\"post\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology\" track-metadata-module=\"post\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/config-files\" track-metadata-module=\"post\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-metadata-module=\"post\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application\" track-metadata-module=\"post\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Security and audit\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn\u0026amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/iam-roles-permissions\u0026#34;\u0026gt;discrete resource access control\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment\u0026#34;\u0026gt;execution-level security\u0026lt;/a\u0026gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\u0026#34;\u0026gt;approvals\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u0026lt;a href=\u0026#34;https://cloud.google.com/audit-logs\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/audit-logs\u0026#34;\u0026gt;audits\u0026lt;/a\u0026gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Integration\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;embraces the GKE delivery tooling ecosystems\u0026lt;/a\u0026gt; in three ways: connectivity to CI systems, support for leading configuration (\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#render\u0026#34;\u0026gt;rendering\u0026lt;/a\u0026gt;) tooling, and \u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Pub/Sub\u0026lt;/a\u0026gt; notifications to enable third-party integrations.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\u0026#34;\u0026gt;Connecting Google Cloud Deploy\u0026lt;/a\u0026gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple\u0026amp;#160;\u0026lt;i\u0026gt;`\u0026lt;/i\u0026gt;\u0026lt;i\u0026gt;gcloud beta deploy releases create`.\u0026lt;/i\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-metadata-module=\"post\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment\" track-metadata-module=\"post\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-metadata-module=\"post\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/audit-logs\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/audit-logs\" track-metadata-module=\"post\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#render\" track-metadata-module=\"post\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-metadata-module=\"post\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy\u0026amp;#160; leverages \u0026lt;a href=\u0026#34;https://skaffold.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Skaffold\u0026lt;/a\u0026gt;, allowing you to \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/skaffold\u0026#34;\u0026gt;standardize your configuration\u0026lt;/a\u0026gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u0026lt;a href=\u0026#34;https://helm.sh/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Helm\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kustomize.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Kustomize\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kpt.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;kpt\u0026lt;/a\u0026gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, to facilitate other integrations, such as a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\u0026#34;\u0026gt;post-deployment test execution\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\u0026#34;\u0026gt;third party approval workflows\u0026lt;/a\u0026gt;, Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\u0026#34;\u0026gt;emits Pub/Sub messages\u0026lt;/a\u0026gt; throughout a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\u0026#34;\u0026gt;release\u0026amp;#8217;s lifecycle\u0026lt;/a\u0026gt;.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The future\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it\u0026amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we\u0026amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the meantime, to get started with the Preview, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;product page\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/quickstart-basic\u0026#34;\u0026gt;quickstart\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt;. Finally, If you have feedback on Google Cloud Deploy, you can \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;join the conversation\u0026lt;/a\u0026gt;. We look forward to hearing from you!\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://skaffold.dev\" track-metadata-module=\"post\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/skaffold\" track-metadata-module=\"post\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://helm.sh\" track-metadata-module=\"post\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://kustomize.io\" track-metadata-module=\"post\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://kpt.dev\" track-metadata-module=\"post\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-metadata-module=\"post\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-metadata-module=\"post\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-metadata-module=\"post\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-metadata-module=\"post\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-metadata-module=\"post\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-video-block _nghost-c16=\"\"\u003e\u003cp _ngcontent-c16=\"\"\u003e\u003ciframe _ngcontent-c16=\"\" allow=\"encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" position=\"absolute\" width=\"100%\" src=\"https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1\u0026amp;\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/article-video-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c18=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy GIF\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eContextualized deployment approvals\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-Il8FlhR9jKM-\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\"\u003e\u003cimg alt=\"Introducing Cloud Deploy\" src=\"//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-Il8FlhR9jKM-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"Il8FlhR9jKM\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/",
      "title": "Introducing Google Cloud Deploy: Managed continuous delivery to GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#gcp\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e S. Bogdan \u003c/p\u003e\u003cp\u003e Product Manager \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e September 22, 2021 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c38=\"\"\u003e\u003cdiv _ngcontent-c38=\"\"\u003e\u003ch4 _ngcontent-c38=\"\"\u003e\u003cspan _ngcontent-c38=\"\"\u003eNext ’21 registration is open\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c38=\"\"\u003e\u003cspan _ngcontent-c38=\"\"\u003eJoin us October 12–14, 2021, for our digital flagship event\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c38=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"Next21 registration\" track-metadata-eventdetail=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\" href=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\"\u003e\u003cspan _ngcontent-c38=\"\"\u003eRegister now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c40=\"\"\u003e\u003cdiv _ngcontent-c40=\"\" innerhtml=\"\u0026lt;p\u0026gt;Continuous delivery is frequently top-of-mind for organizations adopting \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE). However, continuous delivery \u0026amp;#8212;deploying container image artifacts into your various environments\u0026amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It doesn\u0026amp;#8217;t have to be this way.\u0026amp;#160;\u0026lt;/p\u0026gt;Today, we are pleased to announce \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003e\u003cp\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c40=\"\"\u003e\u003cdiv _ngcontent-c40=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Solving for continuous delivery challenges\u0026lt;br\u0026gt;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cost of ownership\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current\u0026amp;#8212;to say nothing of maintenance\u0026amp;#8212;is resource-intensive and takes time away from the core business.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;We can\u0026amp;#8217;t afford to be innovating in continuous delivery,\u0026amp;#8221; one customer told us. \u0026amp;#8220;We want an opinionated product that supports best practices out of the box.\u0026amp;#8221;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy addresses cost of ownership head-on.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy also provides structure. \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\u0026#34;\u0026gt;Delivery pipelines\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology\u0026#34;\u0026gt;targets\u0026lt;/a\u0026gt; are defined \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/config-files\u0026#34;\u0026gt;declaratively\u0026lt;/a\u0026gt; and are \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/pipeline-instances\u0026#34;\u0026gt;stored alongside each release\u0026lt;/a\u0026gt;. That means if your delivery pipeline changes, the release\u0026amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-metadata-module=\"post\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology\" track-metadata-module=\"post\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/config-files\" track-metadata-module=\"post\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-metadata-module=\"post\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c40=\"\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application\" track-metadata-module=\"post\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c40=\"\"\u003e\u003cdiv _ngcontent-c40=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Security and audit\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn\u0026amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/iam-roles-permissions\u0026#34;\u0026gt;discrete resource access control\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment\u0026#34;\u0026gt;execution-level security\u0026lt;/a\u0026gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\u0026#34;\u0026gt;approvals\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u0026lt;a href=\u0026#34;https://cloud.google.com/audit-logs\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/audit-logs\u0026#34;\u0026gt;audits\u0026lt;/a\u0026gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Integration\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;embraces the GKE delivery tooling ecosystems\u0026lt;/a\u0026gt; in three ways: connectivity to CI systems, support for leading configuration (\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#render\u0026#34;\u0026gt;rendering\u0026lt;/a\u0026gt;) tooling, and \u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Pub/Sub\u0026lt;/a\u0026gt; notifications to enable third-party integrations.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\u0026#34;\u0026gt;Connecting Google Cloud Deploy\u0026lt;/a\u0026gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple\u0026amp;#160;\u0026lt;i\u0026gt;`\u0026lt;/i\u0026gt;\u0026lt;i\u0026gt;gcloud beta deploy releases create`.\u0026lt;/i\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-metadata-module=\"post\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment\" track-metadata-module=\"post\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-metadata-module=\"post\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/audit-logs\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/audit-logs\" track-metadata-module=\"post\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#render\" track-metadata-module=\"post\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-metadata-module=\"post\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c40=\"\"\u003e\u003cdiv _ngcontent-c40=\"\" innerhtml=\"\u0026lt;p\u0026gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy\u0026amp;#160; leverages \u0026lt;a href=\u0026#34;https://skaffold.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Skaffold\u0026lt;/a\u0026gt;, allowing you to \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/skaffold\u0026#34;\u0026gt;standardize your configuration\u0026lt;/a\u0026gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u0026lt;a href=\u0026#34;https://helm.sh/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Helm\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kustomize.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Kustomize\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kpt.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;kpt\u0026lt;/a\u0026gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, to facilitate other integrations, such as a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\u0026#34;\u0026gt;post-deployment test execution\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\u0026#34;\u0026gt;third party approval workflows\u0026lt;/a\u0026gt;, Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\u0026#34;\u0026gt;emits Pub/Sub messages\u0026lt;/a\u0026gt; throughout a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\u0026#34;\u0026gt;release\u0026amp;#8217;s lifecycle\u0026lt;/a\u0026gt;.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The future\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it\u0026amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we\u0026amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the meantime, to get started with the Preview, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;product page\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/quickstart-basic\u0026#34;\u0026gt;quickstart\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt;. Finally, If you have feedback on Google Cloud Deploy, you can \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;join the conversation\u0026lt;/a\u0026gt;. We look forward to hearing from you!\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://skaffold.dev\" track-metadata-module=\"post\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/skaffold\" track-metadata-module=\"post\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://helm.sh\" track-metadata-module=\"post\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://kustomize.io\" track-metadata-module=\"post\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://kpt.dev\" track-metadata-module=\"post\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-metadata-module=\"post\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-metadata-module=\"post\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-metadata-module=\"post\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-metadata-module=\"post\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-metadata-module=\"post\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-video-block _nghost-c37=\"\"\u003e\u003cp _ngcontent-c37=\"\"\u003e\u003ciframe _ngcontent-c37=\"\" allow=\"encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" position=\"absolute\" width=\"100%\" src=\"https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1\u0026amp;\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/article-video-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c39=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy GIF\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eContextualized deployment approvals\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-Il8FlhR9jKM-\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\"\u003e\u003cimg alt=\"Introducing Cloud Deploy\" src=\"//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-Il8FlhR9jKM-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"Il8FlhR9jKM\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/",
      "title": "Introducing Google Cloud Deploy: Managed continuous delivery to GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#gcp\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e S. Bogdan \u003c/p\u003e\u003cp\u003e Product Manager \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e September 22, 2021 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c17=\"\"\u003e\u003cdiv _ngcontent-c17=\"\"\u003e\u003ch4 _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eNext ’21 registration is open\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eJoin us October 12–14, 2021, for our digital flagship event\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c17=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"Next21 registration\" track-metadata-eventdetail=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\" href=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eRegister now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Continuous delivery is frequently top-of-mind for organizations adopting \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE). However, continuous delivery \u0026amp;#8212;deploying container image artifacts into your various environments\u0026amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It doesn\u0026amp;#8217;t have to be this way.\u0026amp;#160;\u0026lt;/p\u0026gt;Today, we are pleased to announce \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003e\u003cp\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Solving for continuous delivery challenges\u0026lt;br\u0026gt;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cost of ownership\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current\u0026amp;#8212;to say nothing of maintenance\u0026amp;#8212;is resource-intensive and takes time away from the core business.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;We can\u0026amp;#8217;t afford to be innovating in continuous delivery,\u0026amp;#8221; one customer told us. \u0026amp;#8220;We want an opinionated product that supports best practices out of the box.\u0026amp;#8221;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy addresses cost of ownership head-on.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy also provides structure. \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\u0026#34;\u0026gt;Delivery pipelines\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology\u0026#34;\u0026gt;targets\u0026lt;/a\u0026gt; are defined \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/config-files\u0026#34;\u0026gt;declaratively\u0026lt;/a\u0026gt; and are \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/pipeline-instances\u0026#34;\u0026gt;stored alongside each release\u0026lt;/a\u0026gt;. That means if your delivery pipeline changes, the release\u0026amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-metadata-module=\"post\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology\" track-metadata-module=\"post\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/config-files\" track-metadata-module=\"post\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-metadata-module=\"post\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application\" track-metadata-module=\"post\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Security and audit\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn\u0026amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/iam-roles-permissions\u0026#34;\u0026gt;discrete resource access control\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment\u0026#34;\u0026gt;execution-level security\u0026lt;/a\u0026gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\u0026#34;\u0026gt;approvals\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u0026lt;a href=\u0026#34;https://cloud.google.com/audit-logs\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/audit-logs\u0026#34;\u0026gt;audits\u0026lt;/a\u0026gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Integration\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;embraces the GKE delivery tooling ecosystems\u0026lt;/a\u0026gt; in three ways: connectivity to CI systems, support for leading configuration (\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#render\u0026#34;\u0026gt;rendering\u0026lt;/a\u0026gt;) tooling, and \u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Pub/Sub\u0026lt;/a\u0026gt; notifications to enable third-party integrations.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\u0026#34;\u0026gt;Connecting Google Cloud Deploy\u0026lt;/a\u0026gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple\u0026amp;#160;\u0026lt;i\u0026gt;`\u0026lt;/i\u0026gt;\u0026lt;i\u0026gt;gcloud beta deploy releases create`.\u0026lt;/i\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-metadata-module=\"post\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment\" track-metadata-module=\"post\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-metadata-module=\"post\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/audit-logs\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/audit-logs\" track-metadata-module=\"post\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#render\" track-metadata-module=\"post\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-metadata-module=\"post\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy\u0026amp;#160; leverages \u0026lt;a href=\u0026#34;https://skaffold.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Skaffold\u0026lt;/a\u0026gt;, allowing you to \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/skaffold\u0026#34;\u0026gt;standardize your configuration\u0026lt;/a\u0026gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u0026lt;a href=\u0026#34;https://helm.sh/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Helm\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kustomize.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Kustomize\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kpt.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;kpt\u0026lt;/a\u0026gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, to facilitate other integrations, such as a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\u0026#34;\u0026gt;post-deployment test execution\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\u0026#34;\u0026gt;third party approval workflows\u0026lt;/a\u0026gt;, Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\u0026#34;\u0026gt;emits Pub/Sub messages\u0026lt;/a\u0026gt; throughout a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\u0026#34;\u0026gt;release\u0026amp;#8217;s lifecycle\u0026lt;/a\u0026gt;.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The future\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it\u0026amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we\u0026amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the meantime, to get started with the Preview, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;product page\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/quickstart-basic\u0026#34;\u0026gt;quickstart\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt;. Finally, If you have feedback on Google Cloud Deploy, you can \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;join the conversation\u0026lt;/a\u0026gt;. We look forward to hearing from you!\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://skaffold.dev\" track-metadata-module=\"post\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/skaffold\" track-metadata-module=\"post\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://helm.sh\" track-metadata-module=\"post\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://kustomize.io\" track-metadata-module=\"post\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://kpt.dev\" track-metadata-module=\"post\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-metadata-module=\"post\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-metadata-module=\"post\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-metadata-module=\"post\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-metadata-module=\"post\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-metadata-module=\"post\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-video-block _nghost-c16=\"\"\u003e\u003cp _ngcontent-c16=\"\"\u003e\u003ciframe _ngcontent-c16=\"\" allow=\"encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" position=\"absolute\" width=\"100%\" src=\"https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1\u0026amp;\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/article-video-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c18=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy GIF\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eContextualized deployment approvals\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-Il8FlhR9jKM-\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\"\u003e\u003cimg alt=\"Introducing Cloud Deploy\" src=\"//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-Il8FlhR9jKM-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"Il8FlhR9jKM\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/",
      "title": "Introducing Google Cloud Deploy: Managed continuous delivery to GKE",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy GIF\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eContextualized deployment approvals\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-Il8FlhR9jKM-\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\"\u003e\u003cimg alt=\"Introducing Cloud Deploy\" src=\"//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-Il8FlhR9jKM-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"Il8FlhR9jKM\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/gcp_ZPje3k8.jpg",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/",
      "title": "Introducing Google Cloud Deploy: Managed continuous delivery to GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#gcp\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e S. Bogdan \u003c/p\u003e\u003cp\u003e Product Manager \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e September 22, 2021 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c46=\"\"\u003e\u003cdiv _ngcontent-c46=\"\"\u003e\u003ch4 _ngcontent-c46=\"\"\u003e\u003cspan _ngcontent-c46=\"\"\u003eNext ’21 registration is open\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c46=\"\"\u003e\u003cspan _ngcontent-c46=\"\"\u003eJoin us October 12–14, 2021, for our digital flagship event\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c46=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"Next21 registration\" track-metadata-eventdetail=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\" href=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\"\u003e\u003cspan _ngcontent-c46=\"\"\u003eRegister now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c48=\"\"\u003e\u003cdiv _ngcontent-c48=\"\" innerhtml=\"\u0026lt;p\u0026gt;Continuous delivery is frequently top-of-mind for organizations adopting \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE). However, continuous delivery \u0026amp;#8212;deploying container image artifacts into your various environments\u0026amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It doesn\u0026amp;#8217;t have to be this way.\u0026amp;#160;\u0026lt;/p\u0026gt;Today, we are pleased to announce \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003e\u003cp\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c48=\"\"\u003e\u003cdiv _ngcontent-c48=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Solving for continuous delivery challenges\u0026lt;br\u0026gt;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cost of ownership\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current\u0026amp;#8212;to say nothing of maintenance\u0026amp;#8212;is resource-intensive and takes time away from the core business.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;We can\u0026amp;#8217;t afford to be innovating in continuous delivery,\u0026amp;#8221; one customer told us. \u0026amp;#8220;We want an opinionated product that supports best practices out of the box.\u0026amp;#8221;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy addresses cost of ownership head-on.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy also provides structure. \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\u0026#34;\u0026gt;Delivery pipelines\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology\u0026#34;\u0026gt;targets\u0026lt;/a\u0026gt; are defined \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/config-files\u0026#34;\u0026gt;declaratively\u0026lt;/a\u0026gt; and are \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/pipeline-instances\u0026#34;\u0026gt;stored alongside each release\u0026lt;/a\u0026gt;. That means if your delivery pipeline changes, the release\u0026amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-metadata-module=\"post\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology\" track-metadata-module=\"post\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/config-files\" track-metadata-module=\"post\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-metadata-module=\"post\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c48=\"\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application\" track-metadata-module=\"post\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c48=\"\"\u003e\u003cdiv _ngcontent-c48=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Security and audit\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn\u0026amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/iam-roles-permissions\u0026#34;\u0026gt;discrete resource access control\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment\u0026#34;\u0026gt;execution-level security\u0026lt;/a\u0026gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\u0026#34;\u0026gt;approvals\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u0026lt;a href=\u0026#34;https://cloud.google.com/audit-logs\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/audit-logs\u0026#34;\u0026gt;audits\u0026lt;/a\u0026gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Integration\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;embraces the GKE delivery tooling ecosystems\u0026lt;/a\u0026gt; in three ways: connectivity to CI systems, support for leading configuration (\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#render\u0026#34;\u0026gt;rendering\u0026lt;/a\u0026gt;) tooling, and \u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Pub/Sub\u0026lt;/a\u0026gt; notifications to enable third-party integrations.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\u0026#34;\u0026gt;Connecting Google Cloud Deploy\u0026lt;/a\u0026gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple\u0026amp;#160;\u0026lt;i\u0026gt;`\u0026lt;/i\u0026gt;\u0026lt;i\u0026gt;gcloud beta deploy releases create`.\u0026lt;/i\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-metadata-module=\"post\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment\" track-metadata-module=\"post\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-metadata-module=\"post\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/audit-logs\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/audit-logs\" track-metadata-module=\"post\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#render\" track-metadata-module=\"post\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-metadata-module=\"post\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c48=\"\"\u003e\u003cdiv _ngcontent-c48=\"\" innerhtml=\"\u0026lt;p\u0026gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy\u0026amp;#160; leverages \u0026lt;a href=\u0026#34;https://skaffold.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Skaffold\u0026lt;/a\u0026gt;, allowing you to \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/skaffold\u0026#34;\u0026gt;standardize your configuration\u0026lt;/a\u0026gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u0026lt;a href=\u0026#34;https://helm.sh/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Helm\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kustomize.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Kustomize\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kpt.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;kpt\u0026lt;/a\u0026gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, to facilitate other integrations, such as a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\u0026#34;\u0026gt;post-deployment test execution\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\u0026#34;\u0026gt;third party approval workflows\u0026lt;/a\u0026gt;, Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\u0026#34;\u0026gt;emits Pub/Sub messages\u0026lt;/a\u0026gt; throughout a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\u0026#34;\u0026gt;release\u0026amp;#8217;s lifecycle\u0026lt;/a\u0026gt;.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The future\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it\u0026amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we\u0026amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the meantime, to get started with the Preview, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;product page\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/quickstart-basic\u0026#34;\u0026gt;quickstart\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt;. Finally, If you have feedback on Google Cloud Deploy, you can \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;join the conversation\u0026lt;/a\u0026gt;. We look forward to hearing from you!\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://skaffold.dev\" track-metadata-module=\"post\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/skaffold\" track-metadata-module=\"post\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://helm.sh\" track-metadata-module=\"post\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://kustomize.io\" track-metadata-module=\"post\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://kpt.dev\" track-metadata-module=\"post\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-metadata-module=\"post\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-metadata-module=\"post\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-metadata-module=\"post\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-metadata-module=\"post\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-metadata-module=\"post\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-video-block _nghost-c45=\"\"\u003e\u003cp _ngcontent-c45=\"\"\u003e\u003ciframe _ngcontent-c45=\"\" allow=\"encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" position=\"absolute\" width=\"100%\" src=\"https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1\u0026amp;\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/article-video-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c47=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy GIF\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eContextualized deployment approvals\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-Il8FlhR9jKM-\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\"\u003e\u003cimg alt=\"Introducing Cloud Deploy\" src=\"//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-Il8FlhR9jKM-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"Il8FlhR9jKM\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/",
      "title": "Introducing Google Cloud Deploy: Managed continuous delivery to GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#gcp\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e S. Bogdan \u003c/p\u003e\u003cp\u003e Product Manager \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e September 22, 2021 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c50=\"\"\u003e\u003cdiv _ngcontent-c50=\"\"\u003e\u003ch4 _ngcontent-c50=\"\"\u003e\u003cspan _ngcontent-c50=\"\"\u003eNext ’21 registration is open\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c50=\"\"\u003e\u003cspan _ngcontent-c50=\"\"\u003eJoin us October 12–14, 2021, for our digital flagship event\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c50=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"Next21 registration\" track-metadata-eventdetail=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\" href=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\"\u003e\u003cspan _ngcontent-c50=\"\"\u003eRegister now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c52=\"\"\u003e\u003cdiv _ngcontent-c52=\"\" innerhtml=\"\u0026lt;p\u0026gt;Continuous delivery is frequently top-of-mind for organizations adopting \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE). However, continuous delivery \u0026amp;#8212;deploying container image artifacts into your various environments\u0026amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It doesn\u0026amp;#8217;t have to be this way.\u0026amp;#160;\u0026lt;/p\u0026gt;Today, we are pleased to announce \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003e\u003cp\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c52=\"\"\u003e\u003cdiv _ngcontent-c52=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Solving for continuous delivery challenges\u0026lt;br\u0026gt;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cost of ownership\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current\u0026amp;#8212;to say nothing of maintenance\u0026amp;#8212;is resource-intensive and takes time away from the core business.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;We can\u0026amp;#8217;t afford to be innovating in continuous delivery,\u0026amp;#8221; one customer told us. \u0026amp;#8220;We want an opinionated product that supports best practices out of the box.\u0026amp;#8221;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy addresses cost of ownership head-on.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy also provides structure. \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\u0026#34;\u0026gt;Delivery pipelines\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology\u0026#34;\u0026gt;targets\u0026lt;/a\u0026gt; are defined \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/config-files\u0026#34;\u0026gt;declaratively\u0026lt;/a\u0026gt; and are \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/pipeline-instances\u0026#34;\u0026gt;stored alongside each release\u0026lt;/a\u0026gt;. That means if your delivery pipeline changes, the release\u0026amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-metadata-module=\"post\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology\" track-metadata-module=\"post\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/config-files\" track-metadata-module=\"post\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-metadata-module=\"post\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c52=\"\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application\" track-metadata-module=\"post\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c52=\"\"\u003e\u003cdiv _ngcontent-c52=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Security and audit\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn\u0026amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/iam-roles-permissions\u0026#34;\u0026gt;discrete resource access control\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment\u0026#34;\u0026gt;execution-level security\u0026lt;/a\u0026gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\u0026#34;\u0026gt;approvals\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u0026lt;a href=\u0026#34;https://cloud.google.com/audit-logs\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/audit-logs\u0026#34;\u0026gt;audits\u0026lt;/a\u0026gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Integration\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;embraces the GKE delivery tooling ecosystems\u0026lt;/a\u0026gt; in three ways: connectivity to CI systems, support for leading configuration (\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#render\u0026#34;\u0026gt;rendering\u0026lt;/a\u0026gt;) tooling, and \u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Pub/Sub\u0026lt;/a\u0026gt; notifications to enable third-party integrations.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\u0026#34;\u0026gt;Connecting Google Cloud Deploy\u0026lt;/a\u0026gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple\u0026amp;#160;\u0026lt;i\u0026gt;`\u0026lt;/i\u0026gt;\u0026lt;i\u0026gt;gcloud beta deploy releases create`.\u0026lt;/i\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-metadata-module=\"post\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment\" track-metadata-module=\"post\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-metadata-module=\"post\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/audit-logs\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/audit-logs\" track-metadata-module=\"post\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#render\" track-metadata-module=\"post\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-metadata-module=\"post\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c52=\"\"\u003e\u003cdiv _ngcontent-c52=\"\" innerhtml=\"\u0026lt;p\u0026gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy\u0026amp;#160; leverages \u0026lt;a href=\u0026#34;https://skaffold.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Skaffold\u0026lt;/a\u0026gt;, allowing you to \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/skaffold\u0026#34;\u0026gt;standardize your configuration\u0026lt;/a\u0026gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u0026lt;a href=\u0026#34;https://helm.sh/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Helm\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kustomize.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Kustomize\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kpt.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;kpt\u0026lt;/a\u0026gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, to facilitate other integrations, such as a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\u0026#34;\u0026gt;post-deployment test execution\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\u0026#34;\u0026gt;third party approval workflows\u0026lt;/a\u0026gt;, Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\u0026#34;\u0026gt;emits Pub/Sub messages\u0026lt;/a\u0026gt; throughout a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\u0026#34;\u0026gt;release\u0026amp;#8217;s lifecycle\u0026lt;/a\u0026gt;.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The future\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it\u0026amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we\u0026amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the meantime, to get started with the Preview, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;product page\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/quickstart-basic\u0026#34;\u0026gt;quickstart\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt;. Finally, If you have feedback on Google Cloud Deploy, you can \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;join the conversation\u0026lt;/a\u0026gt;. We look forward to hearing from you!\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://skaffold.dev\" track-metadata-module=\"post\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/skaffold\" track-metadata-module=\"post\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://helm.sh\" track-metadata-module=\"post\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://kustomize.io\" track-metadata-module=\"post\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://kpt.dev\" track-metadata-module=\"post\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-metadata-module=\"post\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-metadata-module=\"post\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-metadata-module=\"post\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-metadata-module=\"post\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-metadata-module=\"post\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-video-block _nghost-c49=\"\"\u003e\u003cp _ngcontent-c49=\"\"\u003e\u003ciframe _ngcontent-c49=\"\" allow=\"encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" position=\"absolute\" width=\"100%\" src=\"https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1\u0026amp;\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/article-video-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c51=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy GIF\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eContextualized deployment approvals\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-Il8FlhR9jKM-\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\"\u003e\u003cimg alt=\"Introducing Cloud Deploy\" src=\"//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-Il8FlhR9jKM-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"Il8FlhR9jKM\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/google-cloud-deploy-automates-deploys-to-gke/",
      "title": "Introducing Google Cloud Deploy: Managed continuous delivery to GKE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#gcp\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e S. Bogdan \u003c/p\u003e\u003cp\u003e Product Manager \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e September 22, 2021 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c13=\"\"\u003e\u003cdiv _ngcontent-c13=\"\"\u003e\u003ch4 _ngcontent-c13=\"\"\u003e\u003cspan _ngcontent-c13=\"\"\u003eNext ’21 registration is open\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c13=\"\"\u003e\u003cspan _ngcontent-c13=\"\"\u003eJoin us October 12–14, 2021, for our digital flagship event\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c13=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"Next21 registration\" track-metadata-eventdetail=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\" href=\"https://cloud.withgoogle.com/next/register?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY21-Q4-global-ES903-onlineevent-er-next-2021\u0026amp;utm_content=blog-next-21-registration\"\u003e\u003cspan _ngcontent-c13=\"\"\u003eRegister now\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c15=\"\"\u003e\u003cdiv _ngcontent-c15=\"\" innerhtml=\"\u0026lt;p\u0026gt;Continuous delivery is frequently top-of-mind for organizations adopting \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE). However, continuous delivery \u0026amp;#8212;deploying container image artifacts into your various environments\u0026amp;#8212;remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It doesn\u0026amp;#8217;t have to be this way.\u0026amp;#160;\u0026lt;/p\u0026gt;Today, we are pleased to announce \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;Google Cloud Deploy\u0026lt;/a\u0026gt;, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003e\u003cp\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c15=\"\"\u003e\u003cdiv _ngcontent-c15=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Solving for continuous delivery challenges\u0026lt;br\u0026gt;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Cost of ownership\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Time and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current\u0026amp;#8212;to say nothing of maintenance\u0026amp;#8212;is resource-intensive and takes time away from the core business.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026amp;#8220;We can\u0026amp;#8217;t afford to be innovating in continuous delivery,\u0026amp;#8221; one customer told us. \u0026amp;#8220;We want an opinionated product that supports best practices out of the box.\u0026amp;#8221;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy addresses cost of ownership head-on.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy also provides structure. \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\u0026#34;\u0026gt;Delivery pipelines\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology\u0026#34;\u0026gt;targets\u0026lt;/a\u0026gt; are defined \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/config-files\u0026#34;\u0026gt;declaratively\u0026lt;/a\u0026gt; and are \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/pipeline-instances\u0026#34;\u0026gt;stored alongside each release\u0026lt;/a\u0026gt;. That means if your delivery pipeline changes, the release\u0026amp;#8217;s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u0026lt;b\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\" track-metadata-module=\"post\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology\" track-metadata-module=\"post\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/config-files\" track-metadata-module=\"post\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/pipeline-instances\" track-metadata-module=\"post\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c15=\"\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/deploying-application\" track-metadata-module=\"post\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c15=\"\"\u003e\u003cdiv _ngcontent-c15=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Security and audit\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Lots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn\u0026amp;#8217;t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Throughout, Google Cloud Deploy enables fine-grained restriction, with \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/iam-roles-permissions\u0026#34;\u0026gt;discrete resource access control\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/execution-environment\u0026#34;\u0026gt;execution-level security\u0026lt;/a\u0026gt;. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\u0026#34;\u0026gt;approvals\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Auditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u0026lt;a href=\u0026#34;https://cloud.google.com/audit-logs\u0026#34;\u0026gt;Cloud Audit Logs\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/audit-logs\u0026#34;\u0026gt;audits\u0026lt;/a\u0026gt; user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Integration\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Whether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating\u0026#34;\u0026gt;embraces the GKE delivery tooling ecosystems\u0026lt;/a\u0026gt; in three ways: connectivity to CI systems, support for leading configuration (\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/terminology#render\u0026#34;\u0026gt;rendering\u0026lt;/a\u0026gt;) tooling, and \u0026lt;a href=\u0026#34;https://cloud.google.com/pubsub\u0026#34;\u0026gt;Pub/Sub\u0026lt;/a\u0026gt; notifications to enable third-party integrations.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\u0026#34;\u0026gt;Connecting Google Cloud Deploy\u0026lt;/a\u0026gt; to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple\u0026amp;#160;\u0026lt;i\u0026gt;`\u0026lt;/i\u0026gt;\u0026lt;i\u0026gt;gcloud beta deploy releases create`.\u0026lt;/i\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\" track-metadata-module=\"post\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/execution-environment\" track-metadata-module=\"post\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\" track-metadata-module=\"post\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/audit-logs\" track-metadata-module=\"post\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/audit-logs\" track-metadata-module=\"post\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating\" track-metadata-module=\"post\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/terminology#render\" track-metadata-module=\"post\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/pubsub\" track-metadata-module=\"post\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\" track-metadata-module=\"post\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c15=\"\"\u003e\u003cdiv _ngcontent-c15=\"\" innerhtml=\"\u0026lt;p\u0026gt;Delivering to Kubernetes often changes over time. To help, Google Cloud Deploy\u0026amp;#160; leverages \u0026lt;a href=\u0026#34;https://skaffold.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Skaffold\u0026lt;/a\u0026gt;, allowing you to \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/skaffold\u0026#34;\u0026gt;standardize your configuration\u0026lt;/a\u0026gt; between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u0026lt;a href=\u0026#34;https://helm.sh/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Helm\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kustomize.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Kustomize\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://kpt.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;kpt\u0026lt;/a\u0026gt;). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, to facilitate other integrations, such as a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\u0026#34;\u0026gt;post-deployment test execution\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\u0026#34;\u0026gt;third party approval workflows\u0026lt;/a\u0026gt;, Google Cloud Deploy \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\u0026#34;\u0026gt;emits Pub/Sub messages\u0026lt;/a\u0026gt; throughout a \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\u0026#34;\u0026gt;release\u0026amp;#8217;s lifecycle\u0026lt;/a\u0026gt;.\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;The future\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Comprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it\u0026amp;#8217;s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we\u0026amp;#8217;re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In the meantime, to get started with the Preview, check out the \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy\u0026#34;\u0026gt;product page\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs\u0026#34;\u0026gt;documentation\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/quickstart-basic\u0026#34;\u0026gt;quickstart\u0026lt;/a\u0026gt;, and \u0026lt;a href=\u0026#34;https://cloud.google.com/deploy/docs/tutorials\u0026#34;\u0026gt;tutorials\u0026lt;/a\u0026gt;. Finally, If you have feedback on Google Cloud Deploy, you can \u0026lt;a href=\u0026#34;https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;join the conversation\u0026lt;/a\u0026gt;. We look forward to hearing from you!\u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://skaffold.dev\" track-metadata-module=\"post\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/skaffold\" track-metadata-module=\"post\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://helm.sh\" track-metadata-module=\"post\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://kustomize.io\" track-metadata-module=\"post\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://kpt.dev\" track-metadata-module=\"post\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\" track-metadata-module=\"post\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\" track-metadata-module=\"post\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\" track-metadata-module=\"post\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\" track-metadata-module=\"post\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/deploy\" track-metadata-module=\"post\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs\" track-metadata-module=\"post\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/quickstart-basic\" track-metadata-module=\"post\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/deploy/docs/tutorials\" track-metadata-module=\"post\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://www.googlecloudcommunity.com\" track-metadata-module=\"post\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-video-block _nghost-c12=\"\"\u003e\u003cp _ngcontent-c12=\"\"\u003e\u003ciframe _ngcontent-c12=\"\" allow=\"encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" position=\"absolute\" width=\"100%\" src=\"https://www.youtube.com/embed/Il8FlhR9jKM?enablejsapi=1\u0026amp;\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/article-video-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c14=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eContinuous delivery is frequently top-of-mind for organizations adopting \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE). However, continuous delivery —deploying container image artifacts into your various environments—remains complex, particularly in Kubernetes environments. With little in the way of accepted best practices, building and scaling continuous delivery tooling, pipelines, and repeatable processes is hard work that requires a lot of on-the-job experience.\u003c/p\u003e\u003cp\u003eIt doesn’t have to be this way. \u003c/p\u003eToday, we are pleased to announce \u003ca href=\"https://cloud.google.com/deploy\"\u003eGoogle Cloud Deploy\u003c/a\u003e, a managed, opinionated continuous delivery service that makes continuous delivery to GKE easier, faster, and more reliable.\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eSolving for continuous delivery challenges\u003cbr/\u003e\u003c/h3\u003e\u003cp\u003eGoogle Cloud Deploy is the product of discussions with more than 50 customers to better understand the challenges they face doing continuous delivery to GKE. From cloud-native to more traditional businesses, three themes consistently emerged: cost of ownership, security and audit, and integration.\u003c/p\u003e\u003cp\u003eLet’s take a deeper look at these challenges and how we address them with Google Cloud Deploy.\u003c/p\u003e\u003cp\u003e\u003cb\u003eCost of ownership\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTime and again we heard that the operational cost of Kubernetes continuous delivery is high. Identifying best and repeatable practices, scaling delivery tooling and pipelines, and staying current—to say nothing of maintenance—is resource-intensive and takes time away from the core business. \u003c/p\u003e\u003cp\u003e\u003ci\u003e“We can’t afford to be innovating in continuous delivery,” one customer told us. “We want an opinionated product that supports best practices out of the box.”\u003c/i\u003e\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy addresses cost of ownership head-on.\u003c/p\u003e\u003cp\u003eAs a managed service, Google Cloud Deploy eliminates the scaling and maintenance responsibilities that typically come with self-managed continuous delivery solutions. Now you can reclaim the time spent maintaining your continuous delivery tooling and spend it delivering value to your customers. \u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy also provides structure. \u003ca href=\"https://cloud.google.com/deploy/docs/terminology#delivery_pipeline\"\u003eDelivery pipelines\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/terminology\"\u003etargets\u003c/a\u003e are defined \u003ca href=\"https://cloud.google.com/deploy/docs/config-files\"\u003edeclaratively\u003c/a\u003e and are \u003ca href=\"https://cloud.google.com/deploy/docs/pipeline-instances\"\u003estored alongside each release\u003c/a\u003e. That means if your delivery pipeline changes, the release’s path to production remains durable. No more time lost troubleshooting issues on in-flight releases caused by changes made to the delivery pipeline.\u003cb\u003e\u003cbr/\u003e\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy GIF\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/original_images/cloud-deploy-pp-blog-post-3.gif\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe have found that a variety of GKE roles and personas interact with continuous delivery processes. A DevOps engineer may be focused on release \u003ca href=\"https://cloud.google.com/deploy/docs/deploying-application\"\u003epromotion and rollback\u003c/a\u003e decisions, while a business decision maker thinks about delivery pipeline health and velocity. Google Cloud Deploy’s user experience keeps these multiple perspectives in mind, making it easier for various personas to perform contextualized reviews and make decisions, improving efficiency and reducing cost of ownership.\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deploy_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003eContextualized deployment approvals\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eSecurity and audit\u003c/b\u003e\u003c/p\u003e\u003cp\u003eLots of different users interact with a continuous delivery system, making a variety of decisions. Not all users and decisions carry the same authority, however. Being able to define a delivery pipeline and make updates doesn’t always mean you can create releases, for example, nor does being able to promote a release to staging mean you can approve it to production. Modern continuous delivery is full of security and audit considerations. Restricting who can access what, where, and how is necessary to maintain release integrity and safety.\u003c/p\u003e\u003cp\u003eThroughout, Google Cloud Deploy enables fine-grained restriction, with \u003ca href=\"https://cloud.google.com/deploy/docs/iam-roles-permissions\"\u003ediscrete resource access control\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/deploy/docs/execution-environment\"\u003eexecution-level security\u003c/a\u003e. For additional safeguards against unwanted approvals, you can also take advantage of flow management features such as release promotion, rollback, and \u003ca href=\"https://cloud.google.com/deploy/docs/managing-delivery-pipeline#requiring_approval\"\u003eapprovals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAuditing with Google Cloud Deploy works just like it does for other Google Cloud services. \u003ca href=\"https://cloud.google.com/audit-logs\"\u003eCloud Audit Logs\u003c/a\u003e \u003ca href=\"https://cloud.google.com/deploy/docs/audit-logs\"\u003eaudits\u003c/a\u003e user-invoked Google Cloud Deploy activities, providing centralized awareness into who promoted a specific release or made an update to a delivery pipeline.\u003c/p\u003e\u003cp\u003e\u003cb\u003eIntegration\u003c/b\u003e\u003c/p\u003e\u003cp\u003eWhether or not you already have continuous delivery capabilities, you likely already have continuous integration (CI), approval and/or operation workflows, and other systems that intersect with your software delivery practices.\u003c/p\u003e\u003cp\u003eGoogle Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/integrating\"\u003eembraces the GKE delivery tooling ecosystems\u003c/a\u003e in three ways: connectivity to CI systems, support for leading configuration (\u003ca href=\"https://cloud.google.com/deploy/docs/terminology#render\"\u003erendering\u003c/a\u003e) tooling, and \u003ca href=\"https://cloud.google.com/pubsub\"\u003ePub/Sub\u003c/a\u003e notifications to enable third-party integrations.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_your_ci_system\"\u003eConnecting Google Cloud Deploy\u003c/a\u003e to existing CI tools is straightforward. After you build your containers, Google Cloud Deploy creates a delivery pipeline release that initiates the Kubernetes manifest configuration (render) and deployment process to the first environment in a progression sequence. Whether you are using Jenkins, Cloud Build, or another CI tool, this is usually a simple \u003ci\u003e`\u003c/i\u003e\u003ci\u003egcloud beta deploy releases create`.\u003c/i\u003e\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Deploy 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Deployt_3_efyUGIq.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDelivering to Kubernetes often changes over time. To help, Google Cloud Deploy  leverages \u003ca href=\"https://skaffold.dev/\" target=\"_blank\"\u003eSkaffold\u003c/a\u003e, allowing you to \u003ca href=\"https://cloud.google.com/deploy/docs/skaffold\"\u003estandardize your configuration\u003c/a\u003e between development and production environments. Organizations new to Kubernetes typically deploy using raw manifests, but as they become more sophisticated, may want to use more advanced tooling (\u003ca href=\"https://helm.sh/\" target=\"_blank\"\u003eHelm\u003c/a\u003e, \u003ca href=\"https://kustomize.io/\" target=\"_blank\"\u003eKustomize\u003c/a\u003e, \u003ca href=\"https://kpt.dev/\" target=\"_blank\"\u003ekpt\u003c/a\u003e). The combination of Google Cloud Deploy and Skaffold lets you transition to these tools without impacting your delivery pipelines.\u003c/p\u003e\u003cp\u003eFinally, to facilitate other integrations, such as a \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_automated_testing\"\u003epost-deployment test execution\u003c/a\u003e or \u003ca href=\"https://cloud.google.com/deploy/docs/integrating#integrating_with_third-party_workflow_management\"\u003ethird party approval workflows\u003c/a\u003e, Google Cloud Deploy \u003ca href=\"https://cloud.google.com/deploy/docs/subscribe-deploy-notifications\"\u003eemits Pub/Sub messages\u003c/a\u003e throughout a \u003ca href=\"https://cloud.google.com/deploy/docs/architecture#how_they_fit_together_to_deliver_your_release\"\u003erelease’s lifecycle\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eThe future\u003c/h3\u003e\u003cp\u003eComprehensive, easy-to-use, and cost-effective DevOps tools are key to building an efficient software development team, and it’s our hope that Google Cloud Deploy will help you complete your CI/CD pipelines. And we’re just getting started! Stay tuned as we continue to introduce exciting new capabilities and features to Google Cloud Deploy in the months and quarters to come.\u003c/p\u003e\u003cp\u003eIn the meantime, to get started with the Preview, check out the \u003ca href=\"https://cloud.google.com/deploy\"\u003eproduct page\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs\"\u003edocumentation\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/deploy/docs/quickstart-basic\"\u003equickstart\u003c/a\u003e, and \u003ca href=\"https://cloud.google.com/deploy/docs/tutorials\"\u003etutorials\u003c/a\u003e. Finally, If you have feedback on Google Cloud Deploy, you can \u003ca href=\"https://www.googlecloudcommunity.com/gc/forums/filteredbylabelpage/board-id/cloud-developer-tools/label-name/google%20cloud%20deploy\" target=\"_blank\"\u003ejoin the conversation\u003c/a\u003e. We look forward to hearing from you!\u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-Il8FlhR9jKM-\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\"\u003e\u003cimg alt=\"Introducing Cloud Deploy\" src=\"//img.youtube.com/vi/Il8FlhR9jKM/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-Il8FlhR9jKM-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"Il8FlhR9jKM\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=Il8FlhR9jKM\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR2021_1920x1080.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e2021 Accelerate State of DevOps report addresses burnout, team performance\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe SODR is continually one of the most downloaded assets on the GCP website. We are releasing the updated version of the report with new...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_ZPje3k8.max-2200x2200.jpg",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eS. Bogdan\u003c/name\u003e\u003ctitle\u003eProduct Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/the-five-phases-of-organizational-reliability/",
      "title": "What’s your org’s reliability mindset? Insights from Google SREs",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003eEditor’s note: There’s more to ensuring a product’s reliability than following a bunch of prescriptive rules. Today, we hear from some Google SREs—Vartika Agarwal, Senior Technical Program Manager, Development; Tracy Ferrell, Senior SRE Manager; Mahesh Palekar, Director SRE; and Magi Agrama, Senior Technical Program Manager, SRE—about how to evaluate your team’s current reliability mindset, and what you want it to be.\u003c/i\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003eHaving a reliable software product can improve users’ trust in your organization, the effectiveness of your development processes, and the quality of your products overall. More than ever, product reliability is front and center, as outages negatively impact customers and their businesses. But in an effort to develop new features, many organizations limit their reliability efforts to what happens after an outage, and tactically solve for the immediate problems that sparked it. They often fail to realize that they can move quickly while still improving their product’s reliability.\u003c/p\u003e\u003cp\u003eAt Google, we’ve given a lot of thought to product reliability—and several of its aspects are well understood, for example product or system design. What people think about less is the culture and the mindset of the organization that creates a reliable product in the first place. We believe that the reliability of a product is a property of the architecture of its system, processes, culture, as well as the mindset of the product team or organization that built it. In other words, reliability should be woven into the fabric of an organization, not just the result of a strong design ethos. \u003c/p\u003e\u003cp\u003eIn this blog post, we discuss the lessons we’ve learned relevant to organizational or product leads who have the ability to influence the culture of the entire product team, from (but not limited to) engineering, product management, marketing, reliability engineering, and support organizations.\u003c/p\u003e\u003ch3\u003eGoals\u003c/h3\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eReliability should be woven into the fabric of how an organization executes. At Google, we’ve developed a terminology to categorize and describe your organization’s reliability mindset, to help you understand how intentional your organization is in this respect. Our ultimate goal is to help you improve and adopt product reliability practices that will permeate the ethos of the organization.\u003c/p\u003e\u003cp\u003eBy identifying these reliability phases, we do not mean to offer a prescriptive list of things to do that will improve your product’s reliability. Nor should they be read as a set of mandated principles that everyone should apply, or be used to publicly label a team, spurring competition between teams. Rather, leaders should consider these phases as a way to help them develop their team’s culture, on the road to sustainably building reliable products.  \u003c/p\u003e\u003ch3\u003eThe organizational reliability continuum\u003c/h3\u003e\u003cp\u003eBased on our observations here at Google, there are five basic stages of organizational reliability, and they are based on the classic organizational model of absent, reactive, proactive, strategic and visionary. These phases describe the mindset of an organization at a point in time, and each one of them is characterized by a series of attributes, and is appropriate for different classes of workloads.\u003c/p\u003e\u003cp\u003eAbsent: Reliability is a secondary consideration for the organization. \u003cbr/\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eA feature launch is the key organizational metric and is the focus for incentives\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe majority of issues are found by users or testers. This organization is not aware of their long-term reliability risks. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDeveloper velocity is rarely exchanged for reliability.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eThis reliability phase maybe appropriate for products and projects that are still under development.\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eReactive\u003c/b\u003e\u003cb\u003e:\u003c/b\u003eResponses to reliability issues/risks are tied to recent outages with sporadic follow-through and rarely are there longer-term investments in fixing system issues.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eTeams have some reliability metrics defined and react when required.\u003c/li\u003e\u003cli\u003eThey write postmortems for outages and create action items for tactical fixes.\u003c/li\u003e\u003cli\u003eReasonable availability is maintained through heroic efforts by a few individuals or teams \u003c/li\u003e\u003cli\u003eDeveloper productivity is throttled due to a temporary shift in priority on reliability work due to outages. Feature development may be frozen for a short period of time.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eThis level is appropriate for products/projects in pre-launch or in a stable long-term maintenance phase.\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eProactive:\u003c/b\u003e\u003cb\u003e\u003c/b\u003ePotential reliability risks are identified and addressed through regular organizational processes.\u003cbr/\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eRisks are regularly reviewed and prioritized.\u003c/li\u003e\u003cli\u003eTeams proactively manage dependencies and review their reliability metrics (SLOs)\u003c/li\u003e\u003cli\u003eNew designs are assessed for known risks and failure modes early on. Graceful degradation is a basic requirement.\u003c/li\u003e\u003cli\u003eThe business understands the need to continuously invest in reliability and maintain its balance with developer velocity. \u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eMost services/products should be at this level; particularly if they have a large blast radius or are critical to the business.\u003c/i\u003e\u003c/p\u003e\u003cb\u003eStrategic:\u003c/b\u003eOrganizations at this level manage classes of risk via systemic changes to  architectures, products and processes.\u003cbr/\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eReliability is inherent and ingrained in how the organization designs, operates and develops software. Reliability is systemic.\u003c/li\u003e\u003cli\u003eComplexity is addressed holistically through product architecture. Dependencies are constantly reduced or improved.\u003c/li\u003e\u003cli\u003eThe cross-functional organization can sustain reliability and developer velocity simultaneously.\u003c/li\u003e\u003cli\u003eOrganizations widely celebrate quality and stability milestones.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eThis level is appropriate for services and products that need very high availability to meet business-critical needs.\u003c/i\u003e\u003c/p\u003e\u003cb\u003eVisionary:\u003c/b\u003eThe organization has reached the highest order of reliability and is able to drive broader reliability efforts within and outside the company (e.g., writing papers, sharing knowledge), based on their best practices and experiences. \u003cbr/\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003eReliability knowledge exists broadly across all engineers and teams at a fairly advanced level and is carried forward as they move across organizations.\u003c/li\u003e\u003cli\u003eSystems are self-healing.\u003c/li\u003e\u003cli\u003eArchitectural improvements for reliability positively impact productivity (release velocity) due to reduction of maintenance work/toil.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eVery few services or products are at this level, and when they are, are industry leading.\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003e\u003c/i\u003e\u003c/p\u003e\u003ch3\u003e\u003ci\u003e\u003ci\u003e\u003ci\u003e\u003ci\u003e\u003ci\u003e\u003ci\u003e\u003ci\u003e\u003ci\u003e\u003ci\u003e\u003ci\u003eWhere should you be on the reliability spectrum?\u003c/i\u003e\u003c/i\u003e\u003c/i\u003e\u003c/i\u003e\u003c/i\u003e\u003c/i\u003e\u003c/i\u003e\u003c/i\u003e\u003c/i\u003e\u003c/i\u003e\u003c/h3\u003e\u003cp\u003e\u003ci\u003e\u003ci\u003eIt is very important to understand your organization does not necessarily need to be at the strategic or visionary phase. There is a significant cost associated with moving from one phase to another and a cost to remain very high on this curve. In our experience, being proactive is a healthy level to target and is ideal for most products. \u003c/i\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003e\u003ci\u003eTo illustrate this point, here is a simple graph of where various Google product teams are on the organizational reliability spectrum; as you can see, it produces a standard bell-curve distribution. While many Google’s product teams have a reactive or proactive reliability culture, most can be described as proactive. You, as an organizational leader, must consciously decide to be at a level based on the product requirements and client expectations.\u003c/i\u003e\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Google’s Reliability culture.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eFurther, it’s common to have attributes across several phases, for example, an organization may be largely reactive with a few proactive attributes. Team culture will wax and wane between phases, as it takes effort to maintain a strategic reliability culture. However, as more of the organization embraces and celebrates reliability as a key feature, the cost of maintenance decreases. \u003c/p\u003e\u003cp\u003eThe key to success is making an honest assessment of what phase you’re in, and then doing concerted work to move to the phase that makes sense for your product. If your organization is in the absent or reactive phase, remember that many products in nascent stages of their life cycle may be comfortable there (in both the startup and long term maintenance of a stable product).\u003c/p\u003e\u003ch3\u003eReliability phases in action\u003c/h3\u003e\u003cp\u003eTo illustrate the reliability phases in practice, it is interesting to look at examples of organizations and how they have progressed or regressed through them.  \u003c/p\u003e\u003cp\u003eIt should be noted that all companies and teams are different and the progress through these phases can take varying amounts of time. It is not uncommon to take two to three years to move into a truly proactive state. In a proactive state all parts of the organization contribute to reliability without worrying that it will negatively impact feature velocity. Staying in the proactive phase also takes time and effort.\u003c/p\u003e\u003cp\u003e\u003cb\u003eNobody can be a hero forever\u003c/b\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eOne infrastructure services team started small with a few well understood APIs. One key member of the team, a product architect, understood the system well and ensured that things ran smoothly by ensuring design decisions were sound and being at each major incident to rapidly mitigate the issue. This was the one person who understood the entire system and was able to predict what can and cannot impact its stability. But when they left the team, the system complexity grew by leaps and bounds. Suddenly there were many critical user-facing and internal outages. \u003c/p\u003e\u003cp\u003eOrganizational leaders initiated both short and long-term reliability programs to restore stability. They focused on reducing the blast radius and the impact of global outages. Leadership recognized that to sustain this trajectory, they recognized that they had to go beyond engineering solutions and implement cultural changes such as recognizing reliability as their number-one feature. This led to broad training around reliability best practices, incorporating reliability in architectural/design reviews and recognizing and rewarding reliability beyond hero moments. \u003c/p\u003e\u003cp\u003eAs a result, the organization evolved from a reactive to a strategic reliability mindset, aided by setting reliability as their number-one feature, recognizing and rewarding long-term reliability improvements, and adopting the systemic belief that reliability is everyone’s responsibility—not just that of a few heroes.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_4.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Google’s Reliability culture 4.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_4.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eIf you think you are done, think again\u003c/h3\u003e\u003cp\u003eEnd users are highly dependent on the reliability of this product and it ties directly to user trust. For this reason, reliability was top of mind for one Google organization for years, and the product was held as the gold standard of reliability by other Google teams. The org was deemed visionary in its reliability processes and work. \u003c/p\u003e\u003cp\u003eHowever, over the years, new products were added to the base service. The high level of reliability did not come as freely and easily as it did with the simpler product. Reliability was impacted at the cost of developer velocity and the organization moved to a more reactive reliability mindset.\u003c/p\u003e\u003cp\u003eTo turn the ship around, the organization’s leaders had to be intentional about their reliability posture and overall practices, for example, how much they thought about and prioritized reliability. It took several years to move the team back to a strategic mindset. \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_3.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Google’s Reliability culture 3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_3.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eEmbrace reliability principles from the start\u003c/h3\u003e\u003cp\u003eAnother team with a new user-facing product was focused on adding features and growing their user base. Before they knew it, the product took off and saw exponential growth.\u003c/p\u003e\u003cp\u003eUnfortunately, their laser-focus on managing user requirements and growing user adoption led to high technical debt and reliability issues. Since the service didn’t start off with reliability as a primary focus, it was very hard to incorporate it after the fact. \u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eMuch of the code had to be re-written and re-architected to reach a sustainable state. The team’s leaders incentivized attention to reliability throughout the organization, from product management through to development and UX domains, constantly reminding the organization about the importance of reliability to the long-term success of the product. This mindshift took years to set in.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"Google’s Reliability culture 2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Googles_Reliability_culture_2.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eConclusion\u003c/h3\u003e\u003cp\u003eIt is important that cross-functional organizations be honest about their reliability journeys and determine what is appropriate for their business and product. It is not uncommon for organizations to move from one level to another and then back again as the product matures, stabilizes and then is sunset for the next generation. Getting to a strategic level can be 4+ years in the making and require very high levels of investment from all aspects of the business.  Leaders should ensure their product requires this level of continued investment.\u003c/p\u003e\u003cp\u003eWe encourage you to study your culture of reliability, assess what phase you are in, determine where you should be on the continuum and carefully and thoughtfully move there. Changing culture is hard and can not be done by edicts or penalties. Most of all, remember that this is a journey and the business is ever-evolving; you cannot set reliability on the shelf and expect it to maintain itself in perpetuity.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eAre we there yet? Thoughts on assessing an SRE team’s maturity\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eExamining the key indicators that signal a mature SRE team.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2021-09-22T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eGoogle Site Reliability Engineering team \u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report/",
      "title": "2021 Accelerate State of DevOps report addresses burnout, team performance",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOver the past seven years, more than 32,000 professionals worldwide have taken part in the \u003ca href=\"https://cloud.google.com/devops\"\u003eAccelerate State of DevOps reports\u003c/a\u003e, making it the largest and longest-running research of its kind. Year over year, the Accelerate State of DevOps reports provide data-driven industry insights that examine the capabilities and practices that drive software delivery as well as operational and organizational performance. That is why Google Cloud’s DevOps Research and Assessment (DORA) team is very excited to announce our \u003ca href=\"https://cloud.google.com/devops/state-of-devops/\"\u003e2021 Accelerate State of DevOps Report\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eOur research continues to illustrate that excellence in software delivery and operational performance drives organizational performance in technology transformations. This year we also investigated the effects of SRE best practices, a secure software supply chain, quality documentation, and multicloud—all while gaining a deeper understanding of how this past year affected team’s culture and burnout.  \u003c/p\u003e\u003cp\u003eRead below to find some of the new findings from this year’s report:\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003ch3\u003eSoftware delivery performance metrics\u003c/h3\u003e\u003cp\u003eBased on key findings from previous Accelerate State of DevOps reports, we again used \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance\"\u003efour metrics\u003c/a\u003e to classify teams as elite, high, medium or low performers based on their software delivery: deployment frequency, lead time for changes, mean-time-to-restore, and change fail rate. This year we saw that elite performers continue to accelerate their pace of software delivery, increasing their lead time for changes from less than one day to less than one hour. Not only that, but elite performers deploy 973x more frequently than low performers, have a 6570x faster lead time to deploy, a 3x lower change failure rate, and an impressive 6570x faster time-to-recover from incidents when failure does happen. You read that right: compared to low performers, elite performers are continually able to empirically demonstrate organizational success with DevOps.\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_1.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"SODR_2021_1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_1.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eThe fifth metric: from availability to reliability\u003c/h3\u003e\u003cp\u003eHistorically we have measured availability rather than reliability, but because availability is a specific focus of reliability engineering, we’ve expanded our measure to reliability so that availability, latency, performance, and scalability are more broadly represented. Specifically, we asked respondents to rate their ability to meet or exceed their reliability targets. We found that teams with varying degrees of delivery performance see better outcomes when they also prioritize operational performance. \u003c/p\u003e\u003ch3\u003e2021 insights: the impact of reliability, COVID and secure software supply chains\u003c/h3\u003e\u003cp\u003eIn addition to measuring the impact of DevOps adoption on software delivery performance, this year’s DORA report also revealed many other new trends. Here’s a sampling. \u003c/p\u003e\u003cp\u003e\u003cb\u003e1) A healthy team culture mitigates burnout during challenging times\u003c/b\u003e\u003c/p\u003e\u003cp\u003eRespondents who worked from home because of the pandemic experienced more burnout than those who stayed in the office (a small portion of our sample). Inclusive teams with a generative culture were half as likely to experience burnout during the COVID-19 pandemic. \u003c/p\u003e\u003cp\u003e\u003cb\u003e2) The highest performers continue to raise the bar\u003c/b\u003e\u003c/p\u003e\u003cp\u003eFor the first time, high and elite performers make up two-thirds of respondents—compared to the \u003ca href=\"https://cloud.google.com/devops\"\u003e2019 report\u003c/a\u003e where low and medium performers made up 56% of respondents. We can confidently say that as the industry continues to accelerate its adoption of DevOps principles teams see meaningful benefits as a result.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_2.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"SODR_2021_2.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_2.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cb\u003e3) SRE and DevOps are complementary philosophies \u003c/b\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eExtending from its core principles, \u003ca href=\"https://sre.google/\" target=\"_blank\"\u003eSite Reliability Engineering (SRE)\u003c/a\u003e provides practical techniques, including the service level indicator/service level objective (SLI/SLO) metrics framework. The SRE framework offers definitions on practices and tooling that can enhance a team’s ability to consistently keep promises to their users. Teams that prioritize both delivery and operational excellence report the highest organizational performance. \u003c/p\u003e\u003cp\u003eTo investigate this, we included \u003ca href=\"https://cloud.google.com/products/operations\"\u003eoperations\u003c/a\u003e questions in the survey for the first time this year. The evidence from the survey indicated teams who excel at modern operational practices are 1.4 times more likely to report greater software delivery and operational (SDO) performance  performance, and 1.8 times more likely to report better business outcomes.\u003c/p\u003e\u003cp\u003e\u003cb\u003e4) Cloud adoption continues to drive performance\u003c/b\u003e\u003c/p\u003e\u003cp\u003eTeams continue to move workloads to the cloud and those that leverage \u003ca href=\"https://cloud.google.com/architecture/devops/devops-tech-cloud-infrastructure\"\u003eall five capabilities\u003c/a\u003e of cloud see increases in SDO performance, as well as in organizational performance. Multicloud adoption is also on the rise so that teams can leverage the unique capabilities of each provider. In fact, respondents who use hybrid or multicloud were 1.6 times more likely to exceed their organizational performance targets. \u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_3.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"SODR_2021_3.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/SODR_2021_3.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cb\u003e5) A secure software supply chain is both essential and drives performance \u003c/b\u003e\u003cp\u003eSecurity can no longer be an afterthought—it must be integrated throughout every stage of the software development lifecycle to build a \u003ca href=\"https://cloud.google.com/blog/products/application-development/best-practices-and-tools-for-software-supply-chain-security\"\u003esecure software supply chain\u003c/a\u003e. Elite performers who met or exceeded their reliability targets were twice as likely to have shifted their security practices left, i.e., implemented security practices earlier on in the software development lifecycle, and deliver reliable software quickly, and safely. \u003c/p\u003e\u003cp\u003e\u003cb\u003e6) Good documentation is foundational for successfully implementing DevOps capabilities\u003c/b\u003e\u003c/p\u003e\u003cp\u003eFor the first time, we measured the quality of internal documentation and its effect on other capabilities and practices. We found documentation is foundational for successfully implementing DevOps capabilities. Teams with high-quality documentation are 3.8x more likely to implement security best practices and 2.5x more likely to fully leverage the cloud to its fullest potential.\u003c/p\u003e\u003ch3\u003eIntroducing the DevOps Awards\u003c/h3\u003e\u003cp\u003eNow that we have shared some of our DevOps best practices with you, we would love to hear about how you are transforming your organization with DevOps. In our first annual DevOps Awards, we’ll recognize Google Cloud customers that have improved their deployment frequency, successfully shifted left on security, or improved their change fail rate percentage, etc. Tell us about the positive impact that DevOps has had on your teams, customers, and organization. Enter your submission \u003ca href=\"https://cloud.google.com/awards/devops\"\u003ehere\u003c/a\u003e today!\u003c/p\u003e\u003cp\u003eThanks to everyone who took our 2021 survey. We hope this Accelerate State of DevOps report helps organizations of all sizes, industries, and regions improve their DevOps capabilities, and we look forward to hearing your thoughts and feedback. To learn more  about the report and implementing DevOps with Google cloud, check out the following resources:\u003cbr/\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://cloud.google.com/devops/state-of-devops/\"\u003eDownload the report\u003c/a\u003e\u003cbr/\u003e\u003c/li\u003e\u003cli\u003eTo find out more about how your organization stacks up against others in your industry, take the\u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\"\u003eDevOps Quick Check\u003c/a\u003e\u003cbr/\u003e\u003c/li\u003e\u003cli\u003eFor customized DevOps solutions for your organization, check out our newly launched\u003ca href=\"http://cloud.google.com/camp\"\u003eCAMP website\u003c/a\u003e\u003cbr/\u003e\u003c/li\u003e\u003cli\u003eLearn more about DevOps capabilities for \u003ca href=\"https://cloud.google.com/devops\"\u003eelite performance\u003c/a\u003e\u003cbr/\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/SODR2021_1920x1080.png",
      "date_published": "2021-09-21T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eDustin Smith\u003c/name\u003e\u003ctitle\u003eDORA Research Lead\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/how-lowes-improved-incident-response-processes-with-sre/",
      "title": "How Lowe’s SRE reduced its mean time to recovery (MTTR) by over 80 percent",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s Note:\u003c/b\u003eIn a \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices\"\u003eprevious blog\u003c/a\u003e, we discussed how home improvement retailer Lowe’s was able to increase the number of releases it supports by \u003ca href=\"https://cloud.google.com/sre\"\u003eadopting Google’s Site Reliability Engineering (SRE) framework on Google Cloud\u003c/a\u003e. Lowe’s went from one release every two weeks to 20+ releases daily, helping meet its customer needs faster and more effectively. Today, the Lowe’s SRE team shares how they used SRE principles to decrease their mean-time-to-recovery (MTTR) by over 80 percent.\u003c/i\u003e\u003c/p\u003e\u003cp\u003eThe stakes of managing Lowes.com have never been higher, and that means spotting, troubleshooting and recovering from incidents as quickly as possible, so that customers can continue to do business on our site. \u003c/p\u003e\u003cp\u003eTo do that, it’s crucial to have solid incident engineering practices in place. Resolving an incident means mitigating the impact and/or restoring the service to its previous condition. The average time it takes to do this is called mean time to recovery (MTTR). Tracking this metric helps us stay on top of the overall reliability of our systems at Lowe’s, while simultaneously improving the speed with which we recover. Our goal is to keep the MTTR metric as low as possible, so that failures don’t negatively impact our business. Here are the four areas we addressed to drive holistic improvement in our MTTR.\u003c/p\u003e\u003ch3\u003eLowe’s incident reporting process\u003c/h3\u003e\u003cp\u003eTo reduce MTTR, we created a seamless incident reporting process following SRE principles. Our incident reporting process is a workflow that starts at the time an incident occurs, and ends with an SRE captain who closes the action items after a postmortem report. With this approach, we are able to limit the number of critical incidents. The reporting process involves three core components: monitoring, alerting, and blameless postmortems.\u003cbr/\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eMonitoring and alerting\u003c/b\u003e\u003c/p\u003e\u003cp\u003eHaving proper monitoring and alerting in place is crucial when it comes to incident management. Monitoring and alerting tools let you detect issues as soon as they occur, and notify the right person in the shortest possible time to take action. From a measurement standpoint, we track this as our mean time to acknowledge (MTTA). This is the average time it takes from when an alert is triggered, to when work on the issue begins.\u003cbr/\u003e\u003c/p\u003e\u003cp\u003eAt the time of an incident, our \u003ca href=\"https://cloud.google.com/monitoring\"\u003emonitoring and alerting tools\u003c/a\u003e notify the on-call SRE first responder via \u003ca href=\"https://cloud.google.com/monitoring/support/notification-options\"\u003ePagerDuty\u003c/a\u003e in the form of a phone call, text message and email. Our SRE software engineering team has done a lot of automation to enable various \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-sli-vs-slo-vs-sla\"\u003eService Level Indicator (SLI) alerts and Service Level Agreement (SLA)\u003c/a\u003e notifications. The on-call SRE then initiates a triage call with our service/domain stakeholders to resolve the incident. As a result, we reduced our MTTA from 30 minutes in 2019, to one minute – a 97 percent decrease. \u003c/p\u003e\u003cp\u003e\u003cb\u003eBlameless postmortems: learning from incidents\u003c/b\u003e\u003c/p\u003e\u003cp\u003eA postmortem is a written record of an incident, its impact, the actions taken to resolve it, the root cause and the follow-up actions to prevent the incident from recurring (\u003ca href=\"https://sre.google/sre-book/example-postmortem/\" target=\"_blank\"\u003esee example here\u003c/a\u003e). A blameless postmortem builds on that and is a core part of an SRE culture, and our culture at Lowe’s. We ensure that individuals are not singled out, and the outcome for all postmortems are directed toward learnings and process improvement.\u003c/p\u003e\u003cp\u003eFor us, the postmortem process is the biggest part of our incident workflow. When an SRE creates a new postmortem report, the first step is to conduct a \u003ca href=\"https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons\"\u003epostmortem session\u003c/a\u003e with domain stakeholders to review the report. The postmortem then goes into the review stage and gets reviewed by more stakeholders in our weekly postmortem meeting. In the final stage of this process, the SRE captain will close the report once everyone in the weekly meeting agrees that the report is complete.\u003c/p\u003e\u003cp\u003eTo conduct a successful postmortem, it is critical to keep the focus on identifying gaps and issues with the system and operations processes, rather than an individual, and generate concrete actions to address the problems we’ve identified. To ensure this, we follow a couple of best practices:\u003c/p\u003e\u003col\u003e\u003cli\u003eWe start by gathering the facts from the person who identified the problem, and each SLI owner has to identify a gap or the next SLI upstream owner who created the impact for them.\u003c/li\u003e\u003cli\u003eEvery SLI owner is provided full opportunity to present their case, and identifying the issue is done as a community exercise. \u003c/li\u003e\u003cli\u003eOnce action items and process changes are identified, an owner is nominated to complete the actions, or they will volunteer. \u003c/li\u003e\u003cli\u003eFor easy reference, we publish and store postmortems in our incident knowledge base. This process helps SREs continuously improve as future incidents arise. \u003c/li\u003e\u003c/ol\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003cb\u003eContinuous Improvement \u003c/b\u003e\u003c/p\u003e\u003cp\u003eEncouraging a culture of honest, transparent and direct feedback that you need for blameless postmortems is often an iterative process that needs sponsorship from executives, empowering incident captains to lead the entirety of the discussion and outcomes. Running successful postmortems, and completing action items from them, needs to be recognized and accounted for in SRE performance objective assessment. As shared in \u003ca href=\"https://sre.google/sre-book/postmortem-culture/\" target=\"_blank\"\u003eGoogle’s SRE book\u003c/a\u003e, the best practice is to ensure that writing effective postmortems is a rewarded and celebrated practice, with leadership’s acknowledgement and participation. This is possibly the hardest part to accomplish in an effective postmortem during a cultural transformation unless you have full buy-in from leadership.\u003c/p\u003e\u003cp\u003eHowever, it’s all well worth it. This process is a key part of how we were able to improve our MTTR over time—from two hours in 2019 to just 17 minutes! \u003c/p\u003e\u003cp\u003eOur SRE incident reporting process has also transformed how our company solves issues. By streamlining this workflow from alerting, to solving an issue, to blameless postmortems, we have reduced our MTTR by 82 percent and our MTTA by 97 percent. Most importantly, our team is learning from every incident and becoming better engineers as a result. Visit the \u003ca href=\"https://cloud.google.com/sre\"\u003eSRE Google Cloud website\u003c/a\u003e to learn more about implementing SRE best practices in the cloud.\u003c/p\u003e\u003chr/\u003e\u003cp\u003e\u003ci\u003eAcknowledgement\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eSpecial thanks to Rahul Mohan Kola Kandy, Vivek Balivada, and the Digital SRE team at Lowe’s for contributing to this blog post.\u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/how-lowes-leverages-google-sre-practices/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eHow Lowe’s meets customer demand with Google SRE practices\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eLowe’s has adopted Google SRE practices to help developer and operations teams keep up with ecommerce demand.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/gcp_ZPje3k8.jpg",
      "date_published": "2021-09-07T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eNishanth Prasad\u003c/name\u003e\u003ctitle\u003eLead Software Engineer, Digital SRE, Lowe’s Companies, Inc.\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/application-development/understanding-artifact-registry-vs-container-registry/",
      "title": "Artifact Registry: the next generation of Container Registry",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eEnterprise application teams need to manage more than just containers in their software supply chain. That’s why we created \u003ca href=\"https://cloud.google.com/artifact-registry\"\u003eArtifact Registry\u003c/a\u003e, a fully-managed service with support for both container images and non-container artifacts.\u003c/p\u003e\u003cp\u003eArtifact Registry improves and extends upon the existing capabilities of \u003ca href=\"https://cloud.google.com/container-registry\"\u003eContainer Registry\u003c/a\u003e, such as customer-managed encryption keys, VPC-SC support, Pub/Sub notifications, and more, providing a foundation for major upgrades in security, scalability and control. While Container Registry is still available and will continue to be supported as a \u003ca href=\"https://cloud.google.com/blog/topics/inside-google-cloud/new-api-stability-tenets-govern-google-enterprise-apis\"\u003eGoogle Enterprise API\u003c/a\u003e, going forward new features will only be available in Artifact Registry, and Container Registry will only receive critical security fixes.\u003c/p\u003e\u003cp\u003eBelow, we’ll highlight the key improvements Artifact Registry provides over Container Registry, as well as the steps to start using it today.\u003c/p\u003e\u003ch3\u003eA unified control plane for container, OS and language repositories\u003c/h3\u003e\u003cp\u003eArtifact Registry includes more than just container images: as a developer, you can store multiple artifact formats, including OS packages for Debian and RPM, as well as language packages for popular languages like Python, Java, and Node. In addition, you can manage them all from a single, unified interface. \u003c/p\u003e\u003ch3\u003eA more granular permission model with Cloud IAM\u003c/h3\u003e\u003cp\u003eArtifact Registry comes with fine-grained access control via \u003ca href=\"https://cloud.google.com/iam\"\u003eCloud IAM\u003c/a\u003e. Unlike Container Registry, this allows you to control access on a per-repository basis, rather than all images stored in a project. This enables you to scope permissions as granularly as possible, for example to specific regions or environments as necessary.\u003c/p\u003e\u003ch3\u003eRepositories in the region of your choice\u003c/h3\u003e\u003cp\u003eArtifact Registry supports the creation of regional repositories, which allows you to put your artifacts and data directly in the location that they'll be used, allowing for higher availability and speed. In Container Registry, you’re limited to “multi-regions”: for example, the closest multi-region for Australia is Asia. However, with Artifact Registry’s regional support, you can create a repository directly in the Sydney data center.\u003c/p\u003e\u003ch3\u003eA pricing model that respects your region\u003c/h3\u003e\u003cp\u003eWhile Artifact Registry’s pricing is still based on a combination of network egress and storage usage, support for regional repositories means that you can choose in what region to host your container repositories. Although per unit storage costs are higher for Artifact Registry, optimizing the locations of your repositories to be hosted in the same region where they are used can result in cost savings, because any network traffic within the same region is not considered egress and is thus free.\u003c/p\u003e\u003ch3\u003ePart of a secure supply chain\u003c/h3\u003e\u003cp\u003eArtifact Registry was designed from the ground up to integrate into our suite of secure supply chain products. This means that it can optionally use \u003ca href=\"https://cloud.google.com/container-analysis/\"\u003eContainer Analysis\u003c/a\u003e to scan your container images for vulnerabilities as they’re uploaded to Artifact Registry, and works directly with \u003ca href=\"https://cloud.google.com/binary-authorization\"\u003eBinary Authorization\u003c/a\u003e to secure your deployments.\u003c/p\u003e\u003ch3\u003eWe’re here to help you migrate\u003c/h3\u003e\u003cp\u003eIf you already use Container Registry, you can take advantage of all the current and upcoming features of container image storage with Artifact Registry by migrating to it. To help, we’ve prepared the following guides:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr\"\u003eTransitioning from Container Registry\u003c/a\u003e provides an overview of how to use Artifact Registry instead of Container Registry in a backwards-compatible way\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/docker/copy-from-gcr\"\u003eCopying images from Container Registry\u003c/a\u003e guide you to move container images from an existing repository to an Artifact Registry repository\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIf you’re currently hosting your container images with a third party, you can begin using Artifact Registry directly, by following the instructions in our guide, \u003ca href=\"https://cloud.google.com/artifact-registry/docs/docker/migrate-external-containers\"\u003eMigrating containers from a third-party registry\u003c/a\u003e, which shows you how to avoid rate limits on image pulls or third-party outages which can disrupt your builds and deployments.\u003c/p\u003e\u003cp\u003eAnd if you're just getting started storing container images, you can begin using Artifact Registry as your image repository right away. To learn how, check out \u003ca href=\"https://cloud.google.com/artifact-registry/docs/docker/quickstart\"\u003eArtifact Registry quickstart for Docker\u003c/a\u003e, a guide to using Artifact Registry as a single location for managing private packages and Docker container images.\u003c/p\u003e\u003ch3\u003eJoin our community \u003c/h3\u003e\u003cp\u003eOur Artifact Registry communities are also great resources to help answer your questions and for guidance on best practices: \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eAsk questions on Stack Overflow using the \u003ca href=\"https://stackoverflow.com/questions/tagged/google-artifact-registry\" target=\"_blank\"\u003egoogle-artifact-registry\u003c/a\u003e tag\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eVisit the \u003ca href=\"https://googlecloud-community.slack.com/\" target=\"_blank\"\u003eGoogle Cloud Slack community\u003c/a\u003e and ask a question in the #artifact-registry channel. If you haven't already joined the Slack community, use \u003ca href=\"https://join.slack.com/t/googlecloud-community/shared_invite/zt-m973j990-IMij2Xh8qKPu7SaHfOcCFg\" target=\"_blank\"\u003ethis form\u003c/a\u003e to sign up.\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/application-development/artifact-registry-adds-node-python-and-java-repositories/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/gcp_Artifact_Registry.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eNode, Python and Java repositories now available in Artifact Registry\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eExpanded language support lets you store Java, Node and Python artifacts in Artifact Registry, for a more secure software supply chain.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/gcp_Artifact_Registry.jpg",
      "date_published": "2021-08-19T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eDustin Ingram\u003c/name\u003e\u003ctitle\u003eSenior Developer Advocate\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/anthos/using-terraform-to-enable-config-sync-on-a-gke-cluster/",
      "title": "Deploy Anthos on GKE with Terraform part 1: GitOps with Config Sync",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/anthos/config-management\"\u003eAnthos Config Management (ACM)\u003c/a\u003eoffers cloud platform administrators a variety of techniques to streamline cluster configuration. One ACM feature, \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/config-sync-overview\"\u003eConfig Sync\u003c/a\u003e, allows them to use a Git repository to create common configurations that are automatically applied on Kubernetes clusters in their fleet, bringing a familiar code review collaboration process to config management. Another ACM feature, \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller\"\u003ePolicy Controller\u003c/a\u003e, enforces security guardrails in compliance with their organization’s requirements. This blog series explores these offerings and how to get started using them with Terraform.\u003c/p\u003e\u003cp\u003eMany platform administrators prefer \u003ca href=\"https://cloud.google.com/solutions/infrastructure-as-code\"\u003eInfrastructure as Code\u003c/a\u003e to achieve repeatable and predictable deployments. This also applies to configuring ACM features on Kubernetes clusters. \u003c/p\u003e\u003cp\u003eIn the past, platform administrators who used Terraform lacked a smooth transition from HCL to modeling cluster configuration. They had to resort to manual processes that required additional temporary permissions granted to operators to complete provisioning.\u003c/p\u003e\u003cp\u003eThe new \u003ca href=\"https://cloud.google.com/anthos/multicluster-management/reference/rest/v1beta/projects.locations.features\"\u003eGKEHub API\u003c/a\u003e and new resources enabled in \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs\" target=\"_blank\"\u003eTerraform Provider for Google Cloud Platform\u003c/a\u003e —\u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/gke_hub_feature\" target=\"_blank\"\u003egoogle_gke_hub_feature\u003c/a\u003e, \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/gke_hub_feature_membership\" target=\"_blank\"\u003egoogle_hub_feature_membership\u003c/a\u003e and \u003ca href=\"https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/gke_hub_membership\" target=\"_blank\"\u003egoogle_gke_hub_membership—\u003c/a\u003emake it possible to automate last mile cluster configuration, including pointing it to a Git repository and turning on the Policy Controller.\u003c/p\u003e\u003cp\u003eFor platform administrators, this solves previous challenges of modeling cluster configuration such as namespaces, services accounts, RBAC, in a Kubernetes idiomatic way, i.e. without the awkward Terraform HCL counterparts. Better still this natural, IaC approach improves auditability and transparency and reduces risk of misconfigurations or security gaps.\u003c/p\u003e\u003cp\u003eIn this 3 part blog series, we’ll show how you can enable Anthos features on GKE. We’ll start with \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/config-sync-overview\"\u003eConfig Sync\u003c/a\u003e to reconcile the cluster state with the specified Git repository. \u003c/p\u003e\u003cp\u003eBased on a GKE cluster resource in your Terraform configuration:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou can then enable GKE Hub membership, and the \u003cb\u003econfigmanagement\u003c/b\u003e feature:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAdditional settings can then be configured for each of the features - \u003cb\u003esync_repo\u003c/b\u003e to point at the repo storing your cluster configurations, \u003cb\u003epoliy_dir\u003c/b\u003e to point at the root of the repo to reconcile, and the specific \u003cb\u003esync_branch\u003c/b\u003e in the repo.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eApplying this configuration with Terraform will enable Config Sync and will automatically synchronize the state of the cluster with the repo, immediately creating the Kubernetes config objects on the cluster. Your pods, deployments, services and other native K8s objects will automatically be created. See this \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/concepts/configs\"\u003earticle\u003c/a\u003e for more details on how to organize configs in a repo.\u003c/p\u003e\u003cp\u003eThe cluster now is fully provisioned and requires no “last mile” configuration steps.\u003c/p\u003e\u003cp\u003eThis \u003ca href=\"https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/tree/master/examples/acm-terraform-blog-part1\" target=\"_blank\"\u003erepo\u003c/a\u003e provides a complete example of provisioning a cluster that is synchronized with a repo that contains a popular WordPress configuration. \u003c/p\u003e\u003cp\u003eIn the next part of the series we’ll show you how you can use Terraform to configure another ACM feature - Policy Controller.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/anthos-config-management-config-controller-available-on-gke/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/GCP_Kubernetes_A.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eGet in sync: Consistent Kubernetes with new Anthos Config Management features\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eAnthos Config Management and Config Controller bring Kubernetes-style declarative policy and config management to GKE environments.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/original_images/Anthos.png",
      "date_published": "2021-08-16T16:30:00Z",
      "author": {
        "name": "\u003cname\u003eAlex Bulankou\u003c/name\u003e\u003ctitle\u003eEngineering Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/containers-kubernetes/anthos-config-management-config-controller-available-on-gke/",
      "title": "Get in sync: Consistent Kubernetes with new Anthos Config Management features",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;h3\u0026gt;Describe your intent with a single resource model\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The Kubernetes API server includes controllers that make sure your container infrastructure state always matches the state you declare in YAML. For example, Kubernetes can ensure that a load balancer and service proxy are always created, connected to the right pods, and configured properly. But KRM can manage more than just container infrastructure. You can use KRM to deploy and manage resources such as cloud databases, storage, and networks. It can also manage your custom-developed apps and services using \u0026lt;a href=\u0026#34;https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;custom resource definitions\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Create what you need from a single source of truth\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;With Anthos Config Management, you declare and set configurations once and forget them. You don\u0026amp;#8217;t have to be an expert in KRM or GitOps-style configuration because the hosted Config Controller service takes care of it. Config Controller provisions infrastructure, apps, and cloud services; configures them to meet your desired intent; monitors them for configuration drift; and applies changes every time you push a new resource declaration to your Git repository. Config changes are as easy as a git push\u0026amp;#8212;and easily integrate with your development workflows.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Anthos Config Management uses \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos-config-management/docs/config-sync-overview\u0026#34;\u0026gt;Config Sync\u0026lt;/a\u0026gt; to continuously reconcile the state of your registered clusters and resources\u0026amp;#8212;that means any GKE, Anthos, or \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos/multicluster-management/connect/registering-a-cluster\u0026#34;\u0026gt;other registered\u0026lt;/a\u0026gt; cluster\u0026amp;#8212;and makes sure unvetted changes are never pushed to live clusters. Anthos Config Management reduces the risk of dev or ops teams making any changes outside the Git source of truth by requiring code reviews and rolling back any breaking changes to a good working state. In short, using Anthos Config Management both encourages and enforces best practices.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Repair what breaks for automated compliance\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Anthos Config Management\u0026amp;#8217;s \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller\u0026#34;\u0026gt;Policy Controller\u0026lt;/a\u0026gt; makes it easier to create and enforce fully programmable policies across all connected clusters. Policies act as guardrails to prevent any changes to configuration from violating your custom security, operational, or compliance controls. For example, you can set policies to actively block any non-compliant API requests, require every namespace to have a label, prevent pods from running privileged containers, restrict the types of storage volumes a container can mount, and more.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Policy Controller is based on the open source \u0026lt;a href=\u0026#34;https://open-policy-agent.github.io/gatekeeper/website/docs/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Open Policy Agent Gatekeeper\u0026lt;/a\u0026gt; project, augmented by Google Cloud with a ready-to-use \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos-config-management/docs/reference/constraint-template-library\u0026#34;\u0026gt;library of pre-built policies\u0026lt;/a\u0026gt; for the most common security and compliance controls. Customers can establish a secure baseline easily without deep expertise and ACM applies policies to a single cluster (e.g. GKE) or to a distributed set of Anthos clusters on-prem or in other cloud platforms. You can audit and add your own custom policies by allowing your security-savvy experts to \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos-config-management/docs/how-to/write-a-constraint-template\u0026#34;\u0026gt;create constraint templates\u0026lt;/a\u0026gt; which anyone \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints\u0026#34;\u0026gt;can invoke\u0026lt;/a\u0026gt; in different dev or production environments without learning how to write or manage policy code. The \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos-config-management/docs/how-to/auditing-constraints\u0026#34;\u0026gt;audit functionality\u0026lt;/a\u0026gt; included allows platform admins to audit all violations, simplifying compliance reviews.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Configure and control every cluster consistently\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The hosted service, \u0026lt;a href=\u0026#34;https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview\u0026#34;\u0026gt;Config Controller\u0026lt;/a\u0026gt;, which runs Config Connector, Config Sync, and Policy Controller for you, is available in Preview. Config Controller leverages \u0026lt;a href=\u0026#34;https://cloud.google.com/config-connector/docs/overview\u0026#34;\u0026gt;Config Connector,\u0026lt;/a\u0026gt; which lets you manage \u0026lt;a href=\u0026#34;https://cloud.google.com/config-connector/docs/reference/overview\u0026#34;\u0026gt;Google Cloud resources\u0026lt;/a\u0026gt; the same way you manage other Kubernetes resources, with continuous monitoring and self-healing. For example, you can ask Config Connector to create a Cloud SQL instance and a database. Config Connector can manage more than 60 Google Cloud resources, including Bigtable, BigQuery, Pub/Sub, Spanner, Cloud Storage, and Cloud Load Balancer.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003eDescribe your intent with a single resource model\u003c/h3\u003e\u003cp\u003eThe Kubernetes API server includes controllers that make sure your container infrastructure state always matches the state you declare in YAML. For example, Kubernetes can ensure that a load balancer and service proxy are always created, connected to the right pods, and configured properly. But KRM can manage more than just container infrastructure. You can use KRM to deploy and manage resources such as cloud databases, storage, and networks. It can also manage your custom-developed apps and services using \u003ca href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://kubernetes.io\" track-metadata-module=\"post\"\u003ecustom resource definitions\u003c/a\u003e. \u003c/p\u003e\u003ch3\u003eCreate what you need from a single source of truth\u003c/h3\u003e\u003cp\u003eWith Anthos Config Management, you declare and set configurations once and forget them. You don’t have to be an expert in KRM or GitOps-style configuration because the hosted Config Controller service takes care of it. Config Controller provisions infrastructure, apps, and cloud services; configures them to meet your desired intent; monitors them for configuration drift; and applies changes every time you push a new resource declaration to your Git repository. Config changes are as easy as a git push—and easily integrate with your development workflows. \u003c/p\u003e\u003cp\u003eAnthos Config Management uses \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/config-sync-overview\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/anthos-config-management/docs/config-sync-overview\" track-metadata-module=\"post\"\u003eConfig Sync\u003c/a\u003e to continuously reconcile the state of your registered clusters and resources—that means any GKE, Anthos, or \u003ca href=\"https://cloud.google.com/anthos/multicluster-management/connect/registering-a-cluster\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/anthos/multicluster-management/connect/registering-a-cluster\" track-metadata-module=\"post\"\u003eother registered\u003c/a\u003e cluster—and makes sure unvetted changes are never pushed to live clusters. Anthos Config Management reduces the risk of dev or ops teams making any changes outside the Git source of truth by requiring code reviews and rolling back any breaking changes to a good working state. In short, using Anthos Config Management both encourages and enforces best practices.\u003c/p\u003e\u003ch3\u003eRepair what breaks for automated compliance\u003c/h3\u003e\u003cp\u003eAnthos Config Management’s \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller\" track-metadata-module=\"post\"\u003ePolicy Controller\u003c/a\u003e makes it easier to create and enforce fully programmable policies across all connected clusters. Policies act as guardrails to prevent any changes to configuration from violating your custom security, operational, or compliance controls. For example, you can set policies to actively block any non-compliant API requests, require every namespace to have a label, prevent pods from running privileged containers, restrict the types of storage volumes a container can mount, and more.\u003c/p\u003e\u003cp\u003ePolicy Controller is based on the open source \u003ca href=\"https://open-policy-agent.github.io/gatekeeper/website/docs/\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://open-policy-agent.github.io\" track-metadata-module=\"post\"\u003eOpen Policy Agent Gatekeeper\u003c/a\u003e project, augmented by Google Cloud with a ready-to-use \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/reference/constraint-template-library\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/anthos-config-management/docs/reference/constraint-template-library\" track-metadata-module=\"post\"\u003elibrary of pre-built policies\u003c/a\u003e for the most common security and compliance controls. Customers can establish a secure baseline easily without deep expertise and ACM applies policies to a single cluster (e.g. GKE) or to a distributed set of Anthos clusters on-prem or in other cloud platforms. You can audit and add your own custom policies by allowing your security-savvy experts to \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/how-to/write-a-constraint-template\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/anthos-config-management/docs/how-to/write-a-constraint-template\" track-metadata-module=\"post\"\u003ecreate constraint templates\u003c/a\u003e which anyone \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints\" track-metadata-module=\"post\"\u003ecan invoke\u003c/a\u003e in different dev or production environments without learning how to write or manage policy code. The \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/how-to/auditing-constraints\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/anthos-config-management/docs/how-to/auditing-constraints\" track-metadata-module=\"post\"\u003eaudit functionality\u003c/a\u003e included allows platform admins to audit all violations, simplifying compliance reviews.\u003c/p\u003e\u003ch3\u003eConfigure and control every cluster consistently\u003c/h3\u003e\u003cp\u003eThe hosted service, \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview\" track-metadata-module=\"post\"\u003eConfig Controller\u003c/a\u003e, which runs Config Connector, Config Sync, and Policy Controller for you, is available in Preview. Config Controller leverages \u003ca href=\"https://cloud.google.com/config-connector/docs/overview\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/config-connector/docs/overview\" track-metadata-module=\"post\"\u003eConfig Connector,\u003c/a\u003e which lets you manage \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/overview\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/config-connector/docs/reference/overview\" track-metadata-module=\"post\"\u003eGoogle Cloud resources\u003c/a\u003e the same way you manage other Kubernetes resources, with continuous monitoring and self-healing. For example, you can ask Config Connector to create a Cloud SQL instance and a database. Config Connector can manage more than 60 Google Cloud resources, including Bigtable, BigQuery, Pub/Sub, Spanner, Cloud Storage, and Cloud Load Balancer.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eFrom large digital-native powerhouses to midsized manufacturing firms, every company today is creating and deploying \u003cb\u003emore software to more places more often\u003c/b\u003e. \u003ca href=\"https://cloud.google.com/anthos/config-management\"\u003eAnthos Config Management\u003c/a\u003e lets you set and enforce consistent configurations and policies for your Kubernetes resources—wherever you build and run them—and manage Google Cloud services the same way. \u003c/p\u003e\u003cp\u003eToday, as a part of Anthos Config Management, we are introducing \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview\"\u003eConfig Controller\u003c/a\u003e, a hosted service to provision and orchestrate Google Cloud resources. This service offers an API endpoint that can provision, actuate, and orchestrate Google Cloud resources the same way it manages Kubernetes resources. You don’t have to install or manage the components—or be an expert in Kubernetes resource management or GitOps—because Google Cloud will manage them for you. \u003c/p\u003e\u003cp\u003eToday, we’re also announcing that, in addition to using it for hybrid and multicloud use cases, Anthos Config Management is now available for Google Kubernetes Engine (GKE) as a standalone service. GKE customers can now take advantage of config and policy automation in Google Cloud at a low incremental per-cluster cost.\u003c/p\u003e\u003cp\u003eThese announcements deliver a whole new approach to config and policy management—one that’s descriptive or \u003ci\u003edeclarative\u003c/i\u003e, rather than procedural or \u003ci\u003eimperative\u003c/i\u003e. Let’s take a closer look.  \u003c/p\u003e\u003ch3\u003eLet Kubernetes automate your configs and policies \u003c/h3\u003e\u003cp\u003eDevelopment teams need stable and secure environments to build apps quickly and deploy them easily. Today, platform teams often scramble to provision and configure the necessary infrastructure components, apps, and cloud services the same way—in many different places—and keep them all up-to-date, patched, and secure. \u003c/p\u003e\u003cp\u003eThe struggle is real, and it’s not new. Platform administrators have been hand-crafting and partially automating configuration with new infrastructure-as-code languages and tools for years. We can spin up new containerized dev environments in minutes in the cloud and on-prem. We can push code to production hundreds of times a day with automated CI/CD processes. So why do configurations drift and fall out of sync with production? \u003c/p\u003e\u003cp\u003eBecause it takes time and toil to develop a \u003cb\u003econsistent and automated way to \u003ci\u003edescribe\u003c/i\u003e what we want, \u003ci\u003ecreate\u003c/i\u003e what we need, and repair what we break.\u003c/b\u003e The declarative \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes\"\u003eKubernetes Resource Model (KRM)\u003c/a\u003e reduces this toil with a consistent way to define and update resources: describe what you want and Kubernetes makes it happen. ACM makes it even easier by adding \u003cb\u003epre-built, opinionated config and policy automations\u003c/b\u003e, such as creating a \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/tutorials/landing-zone\"\u003esecure landing zone\u003c/a\u003e and provisioning a \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/tutorials/gke-cluster-blueprint\"\u003eGKE cluster from a blueprint\u003c/a\u003e. Blueprints help platform teams configure both Kubernetes and Google Cloud services the same way every time.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/GKE_cluster_from_a_blueprint.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"GKE cluster from a blueprint.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/GKE_cluster_from_a_blueprint.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eDescribe your intent with a single resource model\u003c/h3\u003e\u003cp\u003eThe Kubernetes API server includes controllers that make sure your container infrastructure state always matches the state you declare in YAML. For example, Kubernetes can ensure that a load balancer and service proxy are always created, connected to the right pods, and configured properly. But KRM can manage more than just container infrastructure. You can use KRM to deploy and manage resources such as cloud databases, storage, and networks. It can also manage your custom-developed apps and services using \u003ca href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\" target=\"_blank\"\u003ecustom resource definitions\u003c/a\u003e. \u003c/p\u003e\u003ch3\u003eCreate what you need from a single source of truth\u003c/h3\u003e\u003cp\u003eWith Anthos Config Management, you declare and set configurations once and forget them. You don’t have to be an expert in KRM or GitOps-style configuration because the hosted Config Controller service takes care of it. Config Controller provisions infrastructure, apps, and cloud services; configures them to meet your desired intent; monitors them for configuration drift; and applies changes every time you push a new resource declaration to your Git repository. Config changes are as easy as a git push—and easily integrate with your development workflows. \u003c/p\u003e\u003cp\u003eAnthos Config Management uses \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/config-sync-overview\"\u003eConfig Sync\u003c/a\u003e to continuously reconcile the state of your registered clusters and resources—that means any GKE, Anthos, or \u003ca href=\"https://cloud.google.com/anthos/multicluster-management/connect/registering-a-cluster\"\u003eother registered\u003c/a\u003e cluster—and makes sure unvetted changes are never pushed to live clusters. Anthos Config Management reduces the risk of dev or ops teams making any changes outside the Git source of truth by requiring code reviews and rolling back any breaking changes to a good working state. In short, using Anthos Config Management both encourages and enforces best practices.\u003c/p\u003e\u003ch3\u003eRepair what breaks for automated compliance\u003c/h3\u003e\u003cp\u003eAnthos Config Management’s \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/concepts/policy-controller\"\u003ePolicy Controller\u003c/a\u003e makes it easier to create and enforce fully programmable policies across all connected clusters. Policies act as guardrails to prevent any changes to configuration from violating your custom security, operational, or compliance controls. For example, you can set policies to actively block any non-compliant API requests, require every namespace to have a label, prevent pods from running privileged containers, restrict the types of storage volumes a container can mount, and more.\u003c/p\u003e\u003cp\u003ePolicy Controller is based on the open source \u003ca href=\"https://open-policy-agent.github.io/gatekeeper/website/docs/\" target=\"_blank\"\u003eOpen Policy Agent Gatekeeper\u003c/a\u003e project, augmented by Google Cloud with a ready-to-use \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/reference/constraint-template-library\"\u003elibrary of pre-built policies\u003c/a\u003e for the most common security and compliance controls. Customers can establish a secure baseline easily without deep expertise and ACM applies policies to a single cluster (e.g. GKE) or to a distributed set of Anthos clusters on-prem or in other cloud platforms. You can audit and add your own custom policies by allowing your security-savvy experts to \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/how-to/write-a-constraint-template\"\u003ecreate constraint templates\u003c/a\u003e which anyone \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/how-to/creating-constraints\"\u003ecan invoke\u003c/a\u003e in different dev or production environments without learning how to write or manage policy code. The \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/how-to/auditing-constraints\"\u003eaudit functionality\u003c/a\u003e included allows platform admins to audit all violations, simplifying compliance reviews.\u003c/p\u003e\u003ch3\u003eConfigure and control every cluster consistently\u003c/h3\u003e\u003cp\u003eThe hosted service, \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/concepts/config-controller-overview\"\u003eConfig Controller\u003c/a\u003e, which runs Config Connector, Config Sync, and Policy Controller for you, is available in Preview. Config Controller leverages \u003ca href=\"https://cloud.google.com/config-connector/docs/overview\"\u003eConfig Connector,\u003c/a\u003e which lets you manage \u003ca href=\"https://cloud.google.com/config-connector/docs/reference/overview\"\u003eGoogle Cloud resources\u003c/a\u003e the same way you manage other Kubernetes resources, with continuous monitoring and self-healing. For example, you can ask Config Connector to create a Cloud SQL instance and a database. Config Connector can manage more than 60 Google Cloud resources, including Bigtable, BigQuery, Pub/Sub, Spanner, Cloud Storage, and Cloud Load Balancer.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/anthos_config_manager.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"anthos config manager.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/anthos_config_manager.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOnce you’ve embraced a consistent resource model, using ACM to enforce configuration and policy automatically for individual resources, take the next step with blueprints. A \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/concepts/blueprints\"\u003eblueprint\u003c/a\u003e is a package of config and policy that documents an opinionated solution to deploy and manage \u003cb\u003emultiple\u003c/b\u003e resources at once. Blueprints capture best practices and policy guardrails, package them together, and let you deploy them as a complete solution to any Kubernetes clusters using Config Controller. Use Blueprints to manage multiple resources at once, or to create customized \u003ca href=\"https://cloud.google.com/anthos-config-management/docs/tutorials/landing-zone\"\u003elanding zones\u003c/a\u003e—compliant, properly configured, and easily duplicated environments that meet your own best practice guidelines and that are properly networked and secured. \u003c/p\u003e\u003cp\u003eThe Vienna Insurance Group uses Anthos Config Management in its Viesure Innovation Center, which it credits with improving its compliance posture.\u003c/p\u003e\u003cp\u003e\u003ci\u003e\"Google's Landing Zones and Config Controller equipped us with an extensive set of tools to set up our Google Cloud infrastructure quickly and securely. Their policy controllers are a powerful instrument for ensuring compliance for all our Google Cloud resources.\"\u003c/i\u003e —Rene Schakmann, Head of Technology at viesure innovation center GmbH\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eGet started today\u003c/h3\u003e\u003cp\u003eAnthos Config Management on GKE is generally available today. If you’re a GKE customer, you can also now use Anthos Config Management at a low incremental cost. By making it available to GKE customers, and offering it as a hosted, managed service for everyone, we’re making it easier than ever for you to leverage “KRM as a service” to simplify and secure Kubernetes resource management from the data center to the cloud.\u003c/p\u003e\u003cp\u003eTo learn more about the technical details behind ACM, check out \u003ca href=\"https://kubernetespodcast.com/episode/154-gatekeeper-and-policy-controller/\" target=\"_blank\"\u003ethis recent episode\u003c/a\u003e of the \u003ca href=\"https://kubernetespodcast.com/\" target=\"_blank\"\u003eKubernetes Podcast from Google\u003c/a\u003e with the TL for Policy Controller, Max Smythe.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eI do declare! Infrastructure automation with Configuration as Data\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eConfiguration as Data enables operational consistency, security, and velocity on Google Cloud with products like Config Connector.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/GCP_Kubernetes_A.max-2200x2200.jpg",
      "date_published": "2021-08-03T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eJeff Reed\u003c/name\u003e\u003ctitle\u003eVP of Product, GKE and Anthos\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/application-development/google-introduces-slsa-framework/",
      "title": "Securing the software development lifecycle with Cloud Build and SLSA",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Each level of SLSA represents an incremental step towards a more secure software supply chain, adding additional security guidelines to address the most common threats to source and build integrity. Nor are these guidelines Google-specific: they are developed by the security extended community and established by consensus to be adopted amongst the wider industry.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Although SLSA is a new framework, many of the security guidelines it advocates for are already available for and adopted by consumers today.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Understanding SLSA\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The SLSA framework introduces a number of new tools and concepts for securing the software development lifecycle:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Artifact - any file produced as the result of a build pipeline, such as container images, language packages, compiled binaries, etc.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Provenance - metadata about how an artifact was built, including the build process, top-level source, and dependencies\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Digest - the result of a cryptographic hash function which produces a fixed-size value uniquely identifying an artifact, such as a SHA-256 hash of a container image\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Attestation - a cryptographically signed file recording the provenance of the build pipeline at the time a specific artifact or set of artifacts was produced\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Attestor - any system or process that produces an attestation, often included as part of a build pipeline after artifact creation and prior to deployment\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Immutable references - an identifier, such as a URL, that is guaranteed to always point to the same, immutable artifact, such as a specific container image or language package\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Build integrity - the verification of the output of a build pipeline via attestations\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;When used in combination, these represent a build pipeline that adheres to the standards set by the SLSA framework.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Cloud Build supports SLSA 1\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;If you use \u0026lt;a href=\u0026#34;https://cloud.google.com/build\u0026#34;\u0026gt;Cloud Build\u0026lt;/a\u0026gt;, Google Cloud\u0026amp;#8217;s hosted CI/CD platform, you are likely already operating a software development lifecycle at SLSA 1, the first step in securing your software delivery pipeline. That\u0026amp;#8217;s because by default, Cloud Build allows you to create an automated build pipeline, and because any Cloud Build pipeline automatically generates provenance. While provenance for Cloud Build has been available for quite some time, it hasn\u0026amp;#8217;t been widely used to verify build pipelines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Having a software supply chain at SLSA 1 does not entirely protect against tampering, but it does offer a basic level of code source identification and may aid in vulnerability management, protecting against software delivery that is not a product of the CI/CD system. At the same time, Cloud Build represents a foundation for a hosted software build system upon which you can reach higher SLSA levels, using techniques like verifiable source control, automatically verified provenance, and tools like \u0026lt;a href=\u0026#34;https://cloud.google.com/binary-authorization\u0026#34;\u0026gt;Binary Authorization\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;You can start now\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;By creating a build process that\u0026amp;#8217;s fully automated, mandating the use of a build system for production workflows, and by building your software pipeline with Cloud Build, you can have a SLSA 1 supply chain right from the start.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To get started today, you can follow the Cloud Build quickstart for \u0026lt;a href=\u0026#34;https://cloud.google.com/build/docs/quickstart-build\u0026#34;\u0026gt;building a Docker image and pushing the image to Artifact Registry\u0026lt;/a\u0026gt;, followed by the quickstart for \u0026lt;a href=\u0026#34;https://cloud.google.com/build/docs/quickstart-deploy\u0026#34;\u0026gt;deploying that containerized application to Cloud Run\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For more details on SLSA, you can read more here:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;https://slsa.dev/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SLSA: Supply-chain Levels for Software Artifacts\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Introducing SLSA, an End-to-End Framework for Supply Chain Integrity\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;Want to learn more about how you as a developer can help improve the security of your software? Today, we\u0026amp;#8217;re hosting \u0026lt;a href=\u0026#34;https://cloudonair.withgoogle.com/events/container-security\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Building trust in your software supply chain\u0026lt;/a\u0026gt;, which explores this topic in depth. Click here to \u0026lt;a href=\u0026#34;https://cloudonair.withgoogle.com/events/container-security\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;register\u0026lt;/a\u0026gt; for the live event or to watch it on demand.\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eEach level of SLSA represents an incremental step towards a more secure software supply chain, adding additional security guidelines to address the most common threats to source and build integrity. Nor are these guidelines Google-specific: they are developed by the security extended community and established by consensus to be adopted amongst the wider industry.\u003c/p\u003e\u003cp\u003eAlthough SLSA is a new framework, many of the security guidelines it advocates for are already available for and adopted by consumers today.\u003c/p\u003e\u003ch3\u003eUnderstanding SLSA\u003c/h3\u003e\u003cp\u003eThe SLSA framework introduces a number of new tools and concepts for securing the software development lifecycle:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eArtifact - any file produced as the result of a build pipeline, such as container images, language packages, compiled binaries, etc.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eProvenance - metadata about how an artifact was built, including the build process, top-level source, and dependencies\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDigest - the result of a cryptographic hash function which produces a fixed-size value uniquely identifying an artifact, such as a SHA-256 hash of a container image\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAttestation - a cryptographically signed file recording the provenance of the build pipeline at the time a specific artifact or set of artifacts was produced\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAttestor - any system or process that produces an attestation, often included as part of a build pipeline after artifact creation and prior to deployment\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eImmutable references - an identifier, such as a URL, that is guaranteed to always point to the same, immutable artifact, such as a specific container image or language package\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBuild integrity - the verification of the output of a build pipeline via attestations\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhen used in combination, these represent a build pipeline that adheres to the standards set by the SLSA framework.\u003c/p\u003e\u003ch3\u003eCloud Build supports SLSA 1\u003c/h3\u003e\u003cp\u003eIf you use \u003ca href=\"https://cloud.google.com/build\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/build\" track-metadata-module=\"post\"\u003eCloud Build\u003c/a\u003e, Google Cloud’s hosted CI/CD platform, you are likely already operating a software development lifecycle at SLSA 1, the first step in securing your software delivery pipeline. That’s because by default, Cloud Build allows you to create an automated build pipeline, and because any Cloud Build pipeline automatically generates provenance. While provenance for Cloud Build has been available for quite some time, it hasn’t been widely used to verify build pipelines.\u003c/p\u003e\u003cp\u003eHaving a software supply chain at SLSA 1 does not entirely protect against tampering, but it does offer a basic level of code source identification and may aid in vulnerability management, protecting against software delivery that is not a product of the CI/CD system. At the same time, Cloud Build represents a foundation for a hosted software build system upon which you can reach higher SLSA levels, using techniques like verifiable source control, automatically verified provenance, and tools like \u003ca href=\"https://cloud.google.com/binary-authorization\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/binary-authorization\" track-metadata-module=\"post\"\u003eBinary Authorization\u003c/a\u003e.   \u003c/p\u003e\u003ch3\u003eYou can start now\u003c/h3\u003e\u003cp\u003eBy creating a build process that’s fully automated, mandating the use of a build system for production workflows, and by building your software pipeline with Cloud Build, you can have a SLSA 1 supply chain right from the start. \u003c/p\u003e\u003cp\u003eTo get started today, you can follow the Cloud Build quickstart for \u003ca href=\"https://cloud.google.com/build/docs/quickstart-build\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/build/docs/quickstart-build\" track-metadata-module=\"post\"\u003ebuilding a Docker image and pushing the image to Artifact Registry\u003c/a\u003e, followed by the quickstart for \u003ca href=\"https://cloud.google.com/build/docs/quickstart-deploy\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/build/docs/quickstart-deploy\" track-metadata-module=\"post\"\u003edeploying that containerized application to Cloud Run\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFor more details on SLSA, you can read more here:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://slsa.dev/\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://slsa.dev\" track-metadata-module=\"post\"\u003eSLSA: Supply-chain Levels for Software Artifacts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://security.googleblog.com\" track-metadata-module=\"post\"\u003eIntroducing SLSA, an End-to-End Framework for Supply Chain Integrity\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003ci\u003eWant to learn more about how you as a developer can help improve the security of your software? Today, we’re hosting \u003ca href=\"https://cloudonair.withgoogle.com/events/container-security\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloudonair.withgoogle.com\" track-metadata-module=\"post\"\u003eBuilding trust in your software supply chain\u003c/a\u003e, which explores this topic in depth. Click here to \u003ca href=\"https://cloudonair.withgoogle.com/events/container-security\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloudonair.withgoogle.com\" track-metadata-module=\"post\"\u003eregister\u003c/a\u003e for the live event or to watch it on demand.\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOne of the biggest challenges for software developers is the need to make informed choices about the external software and products they use in their own software systems. Evaluating whether a given system is appropriately secured can be challenging, especially if it’s external or owned by a third party.\u003c/p\u003e\u003cp\u003eThis so-called software supply chain has been under increasing scrutiny in recent years, with attacks on software systems being responsible for damages to both public and private interests. In collaboration with the \u003ca href=\"https://openssf.org/\" target=\"_blank\"\u003eOpenSSF\u003c/a\u003e, Google has proposed Supply-chain Levels for Software Artifacts (SLSA). The new \u003ca href=\"https://slsa.dev/\" target=\"_blank\"\u003eSLSA\u003c/a\u003e framework formalizes criteria around software supply chain integrity, to help the industry and open-source ecosystem secure the software development lifecycle.\u003c/p\u003e\u003ch3\u003eSecure your own software development lifecycle\u003c/h3\u003e\u003cp\u003eSLSA is not just for the public software supply chain. You can also apply these same levels, originally inspired by Google’s internal framework for secure software delivery, to your own software development life cycle.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"software development lifecycle.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/software_development_life.0480027109600294.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eEach level of SLSA represents an incremental step towards a more secure software supply chain, adding additional security guidelines to address the most common threats to source and build integrity. Nor are these guidelines Google-specific: they are developed by the security extended community and established by consensus to be adopted amongst the wider industry.\u003c/p\u003e\u003cp\u003eAlthough SLSA is a new framework, many of the security guidelines it advocates for are already available for and adopted by consumers today.\u003c/p\u003e\u003ch3\u003eUnderstanding SLSA\u003c/h3\u003e\u003cp\u003eThe SLSA framework introduces a number of new tools and concepts for securing the software development lifecycle:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eArtifact - any file produced as the result of a build pipeline, such as container images, language packages, compiled binaries, etc.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eProvenance - metadata about how an artifact was built, including the build process, top-level source, and dependencies\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDigest - the result of a cryptographic hash function which produces a fixed-size value uniquely identifying an artifact, such as a SHA-256 hash of a container image\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAttestation - a cryptographically signed file recording the provenance of the build pipeline at the time a specific artifact or set of artifacts was produced\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAttestor - any system or process that produces an attestation, often included as part of a build pipeline after artifact creation and prior to deployment\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eImmutable references - an identifier, such as a URL, that is guaranteed to always point to the same, immutable artifact, such as a specific container image or language package\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBuild integrity - the verification of the output of a build pipeline via attestations\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWhen used in combination, these represent a build pipeline that adheres to the standards set by the SLSA framework.\u003c/p\u003e\u003ch3\u003eCloud Build supports SLSA 1\u003c/h3\u003e\u003cp\u003eIf you use \u003ca href=\"https://cloud.google.com/build\"\u003eCloud Build\u003c/a\u003e, Google Cloud’s hosted CI/CD platform, you are likely already operating a software development lifecycle at SLSA 1, the first step in securing your software delivery pipeline. That’s because by default, Cloud Build allows you to create an automated build pipeline, and because any Cloud Build pipeline automatically generates provenance. While provenance for Cloud Build has been available for quite some time, it hasn’t been widely used to verify build pipelines.\u003c/p\u003e\u003cp\u003eHaving a software supply chain at SLSA 1 does not entirely protect against tampering, but it does offer a basic level of code source identification and may aid in vulnerability management, protecting against software delivery that is not a product of the CI/CD system. At the same time, Cloud Build represents a foundation for a hosted software build system upon which you can reach higher SLSA levels, using techniques like verifiable source control, automatically verified provenance, and tools like \u003ca href=\"https://cloud.google.com/binary-authorization\"\u003eBinary Authorization\u003c/a\u003e.   \u003c/p\u003e\u003ch3\u003eYou can start now\u003c/h3\u003e\u003cp\u003eBy creating a build process that’s fully automated, mandating the use of a build system for production workflows, and by building your software pipeline with Cloud Build, you can have a SLSA 1 supply chain right from the start. \u003c/p\u003e\u003cp\u003eTo get started today, you can follow the Cloud Build quickstart for \u003ca href=\"https://cloud.google.com/build/docs/quickstart-build\"\u003ebuilding a Docker image and pushing the image to Artifact Registry\u003c/a\u003e, followed by the quickstart for \u003ca href=\"https://cloud.google.com/build/docs/quickstart-deploy\"\u003edeploying that containerized application to Cloud Run\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFor more details on SLSA, you can read more here:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://slsa.dev/\" target=\"_blank\"\u003eSLSA: Supply-chain Levels for Software Artifacts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://security.googleblog.com/2021/06/introducing-slsa-end-to-end-framework.html\" target=\"_blank\"\u003eIntroducing SLSA, an End-to-End Framework for Supply Chain Integrity\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003ci\u003eWant to learn more about how you as a developer can help improve the security of your software? Today, we’re hosting \u003ca href=\"https://cloudonair.withgoogle.com/events/container-security\" target=\"_blank\"\u003eBuilding trust in your software supply chain\u003c/a\u003e, which explores this topic in depth. Click here to \u003ca href=\"https://cloudonair.withgoogle.com/events/container-security\" target=\"_blank\"\u003eregister\u003c/a\u003e for the live event or to watch it on demand.\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/identity-security/cloud-ciso-perspectives-june-2021/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Datastorage_8NMQKRy.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eCloud CISO Perspectives: June 2021\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eGoogle Cloud CISO Phil Venables shares his thoughts on ransomware, software supply chains, and RSA retrospectives.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Security_BlogHeader_B_epmyJP1.max-2200x2200.jpg",
      "date_published": "2021-07-29T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eDustin Ingram\u003c/name\u003e\u003ctitle\u003eSenior Developer Advocate\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/cloud-build-private-pools-offers-cicd-for-private-networks/",
      "title": "Introducing Cloud Build private pools: Secure CI/CD for private networks",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;A recent \u0026lt;a href=\u0026#34;https://devops.com/survey-shows-mounting-devops-frustration-and-costs/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;survey\u0026lt;/a\u0026gt; found that developers spend 39% of their time managing the DevOps infrastructure that powers their continuous integration (CI) and continuous delivery (CD) pipelines. Unreliable availability, manual provisioning, limited scaling, breaking upgrades, long queue times, and high fixed costs all slow down development and take valuable time and focus away from DevOps teams. And while cloud-based CI/CD solutions can solve many of these friction points, they largely only work with cloud-hosted resources.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;That\u0026amp;#8217;s why we\u0026amp;#8217;re excited to announce that starting today, you can take advantage of serverless build environments within your own private network, with new \u0026lt;b\u0026gt;Cloud Build private pools\u0026lt;/b\u0026gt;. Launched in 2018, \u0026lt;a href=\u0026#34;https://cloud.google.com/build\u0026#34;\u0026gt;Cloud Build\u0026lt;/a\u0026gt;\u0026amp;#160;has helped thousands of customers modernize their CI/CD workloads to run on fully managed, secure, pay-as-you-go \u0026amp;#8216;workers\u0026amp;#8217; with no infrastructure to manage.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Cloud Build offers on-demand auto-scaling capabilities, active build minute billing, all with no infrastructure to manage. The new private pools feature augments Cloud Build with secure, fully managed CI/CD and DevOps workflow automation that uses network peering to connect into your private networks. Private pools also unlocks a host of new customization options such as new machine types, higher maximum concurrency, regional builds, and network configuration options.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;With Cloud Build private pools, you get the benefits of a cloud-hosted, fully managed CI/CD product while meeting enterprise security and compliance requirements\u0026amp;#8212;even for highly regulated industries like finance, healthcare, retail, and others. For instance, you can trigger fully managed DevOps workflows from source-code repositories hosted in private networks, including Github Enterprise.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;With private pools, Cloud Build now supports:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;VPC Peering\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;VPC-SC\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Static IP ranges\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;No public IPs\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Org policy enforcement\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cross-project builds\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Build from private source repositories with first class integrations, including Github Enterprise\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Regionalization in 15 regions across the US, EU, Asia, Australia, and South America\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Hundreds of concurrent builds per pool\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;15 machine types\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;And while designed primarily for private networking use cases, private pools work just as well with resources in Google Cloud, if you\u0026amp;#8217;re interested in trying out new features like higher concurrency or additional machine types.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Same Cloud Build, new build environment\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Private pools introduces a new build environment for executing your builds with Cloud Build while maintaining a consistent product and API experience. All the same great features of Cloud Build are available with private pools, including fully managed workers, pay-as-you-go pricing, Cloud Console UI, source repo integrations, IAM permissions, Secret Manager and Pub/Sub integrations, and native support for Google Cloud runtimes like Google Kubernetes Engine (GKE), Cloud Run, Cloud Functions, App Engine, and Firebase.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Running builds on a private pool is as easy as creating the pool and setting it as your \u0026lt;a href=\u0026#34;https://cloud.google.com/build/docs/private-pools/run-builds-in-private-pool\u0026#34;\u0026gt;build environment in your cloudbuild.yaml config file\u0026lt;/a\u0026gt;. Private networking is optionally configured via Service Networking by \u0026lt;a href=\u0026#34;https://cloud.google.com/build/docs/private-pools/set-up-private-pool-environment#setup-private-connection\u0026#34;\u0026gt;peering your private pool to your customer-managed VPC\u0026lt;/a\u0026gt; and supports both peered and shared VPCs.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Running your first build is easy:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eA recent \u003ca href=\"https://devops.com/survey-shows-mounting-devops-frustration-and-costs/\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://devops.com\" track-metadata-module=\"post\"\u003esurvey\u003c/a\u003e found that developers spend 39% of their time managing the DevOps infrastructure that powers their continuous integration (CI) and continuous delivery (CD) pipelines. Unreliable availability, manual provisioning, limited scaling, breaking upgrades, long queue times, and high fixed costs all slow down development and take valuable time and focus away from DevOps teams. And while cloud-based CI/CD solutions can solve many of these friction points, they largely only work with cloud-hosted resources. \u003c/p\u003e\u003cp\u003eThat’s why we’re excited to announce that starting today, you can take advantage of serverless build environments within your own private network, with new \u003cb\u003eCloud Build private pools\u003c/b\u003e. Launched in 2018, \u003ca href=\"https://cloud.google.com/build\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/build\" track-metadata-module=\"post\"\u003eCloud Build\u003c/a\u003e has helped thousands of customers modernize their CI/CD workloads to run on fully managed, secure, pay-as-you-go ‘workers’ with no infrastructure to manage. \u003c/p\u003e\u003cp\u003eCloud Build offers on-demand auto-scaling capabilities, active build minute billing, all with no infrastructure to manage. The new private pools feature augments Cloud Build with secure, fully managed CI/CD and DevOps workflow automation that uses network peering to connect into your private networks. Private pools also unlocks a host of new customization options such as new machine types, higher maximum concurrency, regional builds, and network configuration options.\u003c/p\u003e\u003cp\u003eWith Cloud Build private pools, you get the benefits of a cloud-hosted, fully managed CI/CD product while meeting enterprise security and compliance requirements—even for highly regulated industries like finance, healthcare, retail, and others. For instance, you can trigger fully managed DevOps workflows from source-code repositories hosted in private networks, including Github Enterprise.\u003c/p\u003e\u003cp\u003eWith private pools, Cloud Build now supports:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eVPC Peering\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eVPC-SC\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eStatic IP ranges\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNo public IPs\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOrg policy enforcement\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCross-project builds\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBuild from private source repositories with first class integrations, including Github Enterprise\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRegionalization in 15 regions across the US, EU, Asia, Australia, and South America\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eHundreds of concurrent builds per pool\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e15 machine types\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAnd while designed primarily for private networking use cases, private pools work just as well with resources in Google Cloud, if you’re interested in trying out new features like higher concurrency or additional machine types.\u003c/p\u003e\u003ch3\u003eSame Cloud Build, new build environment\u003c/h3\u003e\u003cp\u003ePrivate pools introduces a new build environment for executing your builds with Cloud Build while maintaining a consistent product and API experience. All the same great features of Cloud Build are available with private pools, including fully managed workers, pay-as-you-go pricing, Cloud Console UI, source repo integrations, IAM permissions, Secret Manager and Pub/Sub integrations, and native support for Google Cloud runtimes like Google Kubernetes Engine (GKE), Cloud Run, Cloud Functions, App Engine, and Firebase.\u003c/p\u003e\u003cp\u003eRunning builds on a private pool is as easy as creating the pool and setting it as your \u003ca href=\"https://cloud.google.com/build/docs/private-pools/run-builds-in-private-pool\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/build/docs/private-pools/run-builds-in-private-pool\" track-metadata-module=\"post\"\u003ebuild environment in your cloudbuild.yaml config file\u003c/a\u003e. Private networking is optionally configured via Service Networking by \u003ca href=\"https://cloud.google.com/build/docs/private-pools/set-up-private-pool-environment#setup-private-connection\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/build/docs/private-pools/set-up-private-pool-environment#setup-private-connection\" track-metadata-module=\"post\"\u003epeering your private pool to your customer-managed VPC\u003c/a\u003e and supports both peered and shared VPCs.\u003c/p\u003e\u003cp\u003eRunning your first build is easy:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eA recent \u003ca href=\"https://devops.com/survey-shows-mounting-devops-frustration-and-costs/\" target=\"_blank\"\u003esurvey\u003c/a\u003e found that developers spend 39% of their time managing the DevOps infrastructure that powers their continuous integration (CI) and continuous delivery (CD) pipelines. Unreliable availability, manual provisioning, limited scaling, breaking upgrades, long queue times, and high fixed costs all slow down development and take valuable time and focus away from DevOps teams. And while cloud-based CI/CD solutions can solve many of these friction points, they largely only work with cloud-hosted resources. \u003c/p\u003e\u003cp\u003eThat’s why we’re excited to announce that starting today, you can take advantage of serverless build environments within your own private network, with new \u003cb\u003eCloud Build private pools\u003c/b\u003e. Launched in 2018, \u003ca href=\"https://cloud.google.com/build\"\u003eCloud Build\u003c/a\u003e has helped thousands of customers modernize their CI/CD workloads to run on fully managed, secure, pay-as-you-go ‘workers’ with no infrastructure to manage. \u003c/p\u003e\u003cp\u003eCloud Build offers on-demand auto-scaling capabilities, active build minute billing, all with no infrastructure to manage. The new private pools feature augments Cloud Build with secure, fully managed CI/CD and DevOps workflow automation that uses network peering to connect into your private networks. Private pools also unlocks a host of new customization options such as new machine types, higher maximum concurrency, regional builds, and network configuration options.\u003c/p\u003e\u003cp\u003eWith Cloud Build private pools, you get the benefits of a cloud-hosted, fully managed CI/CD product while meeting enterprise security and compliance requirements—even for highly regulated industries like finance, healthcare, retail, and others. For instance, you can trigger fully managed DevOps workflows from source-code repositories hosted in private networks, including Github Enterprise.\u003c/p\u003e\u003cp\u003eWith private pools, Cloud Build now supports:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eVPC Peering\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eVPC-SC\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eStatic IP ranges\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNo public IPs\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eOrg policy enforcement\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCross-project builds\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBuild from private source repositories with first class integrations, including Github Enterprise\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRegionalization in 15 regions across the US, EU, Asia, Australia, and South America\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eHundreds of concurrent builds per pool\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e15 machine types\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAnd while designed primarily for private networking use cases, private pools work just as well with resources in Google Cloud, if you’re interested in trying out new features like higher concurrency or additional machine types.\u003c/p\u003e\u003ch3\u003eSame Cloud Build, new build environment\u003c/h3\u003e\u003cp\u003ePrivate pools introduces a new build environment for executing your builds with Cloud Build while maintaining a consistent product and API experience. All the same great features of Cloud Build are available with private pools, including fully managed workers, pay-as-you-go pricing, Cloud Console UI, source repo integrations, IAM permissions, Secret Manager and Pub/Sub integrations, and native support for Google Cloud runtimes like Google Kubernetes Engine (GKE), Cloud Run, Cloud Functions, App Engine, and Firebase.\u003c/p\u003e\u003cp\u003eRunning builds on a private pool is as easy as creating the pool and setting it as your \u003ca href=\"https://cloud.google.com/build/docs/private-pools/run-builds-in-private-pool\"\u003ebuild environment in your cloudbuild.yaml config file\u003c/a\u003e. Private networking is optionally configured via Service Networking by \u003ca href=\"https://cloud.google.com/build/docs/private-pools/set-up-private-pool-environment#setup-private-connection\"\u003epeering your private pool to your customer-managed VPC\u003c/a\u003e and supports both peered and shared VPCs.\u003c/p\u003e\u003cp\u003eRunning your first build is easy:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe’re excited to share private pools with you, so you can enjoy the secure, fully managed Cloud Build developer automation platform from your private network. The private pools feature is generally available today, and we look forward to introducing per-trigger service accounts and approval gates soon. To get started, try the \u003ca href=\"https://cloud.google.com/build/docs/private-pools/quickstart-private-pools\"\u003equickstart\u003c/a\u003e or read the \u003ca href=\"https://cloud.google.com/build/docs/private-pools/private-pools-overview\"\u003eoverview documentation\u003c/a\u003e for more details.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003eWant to learn more about Cloud Build, and how to use it to improve the security of your software supply chain? On July 29 event \u003ca href=\"https://cloudonair.withgoogle.com/events/container-security\" target=\"_blank\"\u003eBuilding trust in your software supply chain\u003c/a\u003e explores this topic in depth. Click here to \u003ca href=\"https://cloudonair.withgoogle.com/events/container-security\" target=\"_blank\"\u003eregister\u003c/a\u003e for the live event or to watch it on demand.\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/application-development/forgerock-developers-stay-productive-with-google-cloud/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/appdev.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eDevOps on Google Cloud: tools to speed up software development velocity\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eGoogle Cloud’s application development and continuous integration/continuous delivery (CI/CD) tools help ForgeRock developers stay produc...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-2200x2200.jpg",
      "date_published": "2021-07-29T15:00:00Z",
      "author": {
        "name": "\u003cname\u003eChristopher Sanson\u003c/name\u003e\u003ctitle\u003eProduct Manager, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/announcing-2021-state-devops-report-sponsors/",
      "title": "Announcing the 2021 State of DevOps Report Sponsors",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003esodr\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003carticle-cta _nghost-c17=\"\"\u003e\u003cdiv _ngcontent-c17=\"\"\u003e\u003ch4 _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eAccelerate State of DevOps Report\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eGet a comprehensive view of the DevOps industry, providing actionable guidance for organizations of all sizes.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c17=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"DORA_2019\" track-metadata-eventdetail=\"https://cloud.google.com/devops/state-of-devops?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY19-Q3-global-demandgen-website-wd-gcp_gtm_stateofdevops\" href=\"https://cloud.google.com/devops/state-of-devops?utm_source=google\u0026amp;utm_medium=blog\u0026amp;utm_campaign=FY19-Q3-global-demandgen-website-wd-gcp_gtm_stateofdevops\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eDownload\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Google Cloud and the \u0026lt;a href=\u0026#34;https://www.devops-research.com/research.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DORA\u0026lt;/a\u0026gt; research team are excited to announce our eight sponsors for the 2021 State of DevOps report. We recently launched the \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;2021 State of DevOps survey\u0026lt;/a\u0026gt;, a 25-min survey for the DevOps community to share how they are using DevOps to improve software delivery performance. So if you haven\u0026amp;#8217;t taken the survey yet, this is your chance!\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For those unfamiliar with the State Of DevOps report, it is the largest and longest running research of its kind. It provides an independent view into how teams and companies can drive powerful business outcomes, no matter what stage of the DevOps journey.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;With the \u0026lt;a href=\u0026#34;https://cloud.google.com/devops\u0026#34;\u0026gt;State of DevOps\u0026lt;/a\u0026gt; reports we aim to capture how teams and companies are driving excellence in technology delivery through the implementation of DevOps practices, no matter what stage of the DevOps journey your team is in. If you\u0026amp;#8217;re wondering how your team measures up in your industry take our \u0026lt;a href=\u0026#34;https://www.devops-research.com/quickcheck.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DevOps Quick Check\u0026lt;/a\u0026gt; and discover which capabilities you should focus on to improve your performance.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To capture a diverse array of information on how those in the DevOps community are performing, our eight sponsors of the 2021 State of DevOps survey are focussed on helping organizations of all sizes and industries to develop and deliver software faster across the whole DevOps lifecycle. Captured below, you\u0026amp;#8217;ll find more information on our sponsors for this year and why they chose to sponsor the State Of the DevOps 2021 Report.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://www.armory.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Armory\u0026lt;/a\u0026gt; \u0026lt;br\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eGoogle Cloud and the \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDORA\u003c/a\u003e research team are excited to announce our eight sponsors for the 2021 State of DevOps report. We recently launched the \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003e2021 State of DevOps survey\u003c/a\u003e, a 25-min survey for the DevOps community to share how they are using DevOps to improve software delivery performance. So if you haven’t taken the survey yet, this is your chance! \u003c/p\u003e\u003cp\u003eFor those unfamiliar with the State Of DevOps report, it is the largest and longest running research of its kind. It provides an independent view into how teams and companies can drive powerful business outcomes, no matter what stage of the DevOps journey.\u003c/p\u003e\u003cp\u003eWith the \u003ca href=\"https://cloud.google.com/devops\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/devops\" track-metadata-module=\"post\"\u003eState of DevOps\u003c/a\u003e reports we aim to capture how teams and companies are driving excellence in technology delivery through the implementation of DevOps practices, no matter what stage of the DevOps journey your team is in. If you’re wondering how your team measures up in your industry take our \u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDevOps Quick Check\u003c/a\u003e and discover which capabilities you should focus on to improve your performance.\u003c/p\u003e\u003cp\u003eTo capture a diverse array of information on how those in the DevOps community are performing, our eight sponsors of the 2021 State of DevOps survey are focussed on helping organizations of all sizes and industries to develop and deliver software faster across the whole DevOps lifecycle. Captured below, you’ll find more information on our sponsors for this year and why they chose to sponsor the State Of the DevOps 2021 Report.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.armory.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://www.armory.io\" track-metadata-module=\"post\"\u003eArmory\u003c/a\u003e \u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Armory enables enterprise companies to ship better software, faster through trusted, reliable, safe, and secure deployments -- at its core, Armory is powered by Spinnaker OSS.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;Armory exists to unlock innovation through software. To achieve this, we help enterprises reliably deploy software at scale, naturally aligning with DevOps practices to improve software delivery performance. Given this, we applaud efforts like the State of DevOps Report that provides an independent view into the practices and capabilities that organizations can employ to drive better performance.\u0026amp;#34;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Carl Timm, Senior Director of Product Marketing at Armory\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://circleci.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;CircleCI\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eArmory enables enterprise companies to ship better software, faster through trusted, reliable, safe, and secure deployments -- at its core, Armory is powered by Spinnaker OSS.\u003c/p\u003e\u003cp\u003e“Armory exists to unlock innovation through software. To achieve this, we help enterprises reliably deploy software at scale, naturally aligning with DevOps practices to improve software delivery performance. Given this, we applaud efforts like the State of DevOps Report that provides an independent view into the practices and capabilities that organizations can employ to drive better performance.\u0026#34; \u003c/p\u003e\u003cp\u003eCarl Timm, Senior Director of Product Marketing at Armory\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://circleci.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://circleci.com\" track-metadata-module=\"post\"\u003eCircleCI\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;The world\u0026amp;#8217;s best software teams deliver quality code, confidently, with CircleCI. The world\u0026amp;#8217;s best software teams use CircleCI to go from next-up to feature shipped, at the speed ambitious businesses require.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;Though the DevOps space is only over a decade old, it moves incredibly quickly. Google\u0026amp;#8217;s State of DevOps report is both a reflection and projection of the industry, capturing how DevOps culture drives business results and where DevOps practitioners can look to improve. At CircleCI, we rely heavily on this survey data to glean valuable insights into our market and our customers overall. We also build upon these insights to guide our own research into how engineering teams move code through pipelines in our annual \u0026lt;a href=\u0026#34;https://circleci.com/resources/2020-state-of-software-delivery/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;State of Software Delivery report\u0026lt;/a\u0026gt;. Taken together, this research highlights teams\u0026amp;#8217; reported and actual behavior and paints a vibrant picture of how technology-driven organizations drive for success.\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Emma Webb, VP, Corporate Communications, CircleCI\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cd.foundation/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Continuous Delivery Foundation\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThe world’s best software teams deliver quality code, confidently, with CircleCI. The world’s best software teams use CircleCI to go from next-up to feature shipped, at the speed ambitious businesses require. \u003c/p\u003e\u003cp\u003e“Though the DevOps space is only over a decade old, it moves incredibly quickly. Google’s State of DevOps report is both a reflection and projection of the industry, capturing how DevOps culture drives business results and where DevOps practitioners can look to improve. At CircleCI, we rely heavily on this survey data to glean valuable insights into our market and our customers overall. We also build upon these insights to guide our own research into how engineering teams move code through pipelines in our annual \u003ca href=\"https://circleci.com/resources/2020-state-of-software-delivery/\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://circleci.com\" track-metadata-module=\"post\"\u003eState of Software Delivery report\u003c/a\u003e. Taken together, this research highlights teams’ reported and actual behavior and paints a vibrant picture of how technology-driven organizations drive for success.”\u003c/p\u003e\u003cp\u003eEmma Webb, VP, Corporate Communications, CircleCI \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cd.foundation/\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cd.foundation\" track-metadata-module=\"post\"\u003eContinuous Delivery Foundation\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;CD Foundation is an open-source community improving the world\u0026#39;s ability to deliver software with security and speed.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#34;Good decision-making is based on good data. Open source is a critical piece of the technology roadmap for DevOps, and the 2021 State of DevOps Report will provide actionable information for high performing teams in organizations of all types and sizes. The report will show where open source and DevOps intersect, and in a space that is changing so quickly, new relevant data is critical,\u0026amp;#34; said Tracy Miranda, Continuous Delivery Foundation Executive Director. \u0026amp;#34;CD Foundation members are deeply involved with producing this annual report, with over 10 years of historic research. Thank you to Google and our other members who have worked so hard on it. This report is a tangible result of working together.\u0026amp;#34;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Tracy Miranda, Continuous Delivery Foundation Executive Director\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://www2.deloitte.com/us/en.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Deloitte\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eCD Foundation is an open-source community improving the world\u0026#39;s ability to deliver software with security and speed.\u003c/p\u003e\u003cp\u003e\u0026#34;Good decision-making is based on good data. Open source is a critical piece of the technology roadmap for DevOps, and the 2021 State of DevOps Report will provide actionable information for high performing teams in organizations of all types and sizes. The report will show where open source and DevOps intersect, and in a space that is changing so quickly, new relevant data is critical,\u0026#34; said Tracy Miranda, Continuous Delivery Foundation Executive Director. \u0026#34;CD Foundation members are deeply involved with producing this annual report, with over 10 years of historic research. Thank you to Google and our other members who have worked so hard on it. This report is a tangible result of working together.\u0026#34;\u003c/p\u003e\u003cp\u003eTracy Miranda, Continuous Delivery Foundation Executive Director\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www2.deloitte.com/us/en.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://www2.deloitte.com\" track-metadata-module=\"post\"\u003eDeloitte\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Deloitte provides audit and assurance, tax, consulting, and risk and financial advisory services to a broad cross-section of the largest corporations and governmental agencies. At Deloitte, they are continuously evolving how they work and how they look at marketplace challenges so they can continually deliver measurable, sustainable results for their clients and communities.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;Software delivery is evolving rapidly and we know our customers need unique and compelling insights to make the right decisions. State of DevOps is a widely used report and considered as an Industry standard to understand the drivers for excellence in Software Development and Delivery. Deloitte is excited to help the team at DORA and Google Cloud in publishing this report through our sponsoring since we believe the insights from this report will help make software delivery better.\u0026amp;#8221;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Manoj Mishra, Consulting Managing Director, Deloitte Consulting LLP\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://about.gitlab.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;GitLab\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDeloitte provides audit and assurance, tax, consulting, and risk and financial advisory services to a broad cross-section of the largest corporations and governmental agencies. At Deloitte, they are continuously evolving how they work and how they look at marketplace challenges so they can continually deliver measurable, sustainable results for their clients and communities.\u003c/p\u003e\u003cp\u003e“Software delivery is evolving rapidly and we know our customers need unique and compelling insights to make the right decisions. State of DevOps is a widely used report and considered as an Industry standard to understand the drivers for excellence in Software Development and Delivery. Deloitte is excited to help the team at DORA and Google Cloud in publishing this report through our sponsoring since we believe the insights from this report will help make software delivery better.” \u003c/p\u003e\u003cp\u003eManoj Mishra, Consulting Managing Director, Deloitte Consulting LLP\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://about.gitlab.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://about.gitlab.com\" track-metadata-module=\"post\"\u003eGitLab\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;GitLab is the open DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;We\u0026#39;re happy to sponsor the DORA Report and the vital work behind it. The more we understand the DevOps journey, the better we and our GitLab community can contribute to its future. We\u0026#39;re particularly interested to see this year\u0026#39;s results and the impact COVID-19 and remote work have had on DevOps. We appreciate the chance to be part of this exciting, informative process.\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Brendon O\u0026amp;#8217;Leary, Senior Developer Evangelist\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://www.liquibase.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Liquibase\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eGitLab is the open DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project.\u003c/p\u003e\u003cp\u003e“We\u0026#39;re happy to sponsor the DORA Report and the vital work behind it. The more we understand the DevOps journey, the better we and our GitLab community can contribute to its future. We\u0026#39;re particularly interested to see this year\u0026#39;s results and the impact COVID-19 and remote work have had on DevOps. We appreciate the chance to be part of this exciting, informative process.”\u003c/p\u003e\u003cp\u003eBrendon O’Leary, Senior Developer Evangelist\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.liquibase.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://www.liquibase.com\" track-metadata-module=\"post\"\u003eLiquibase\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Liquibase helps millions of developers easily manage database schema changes by enabling teams to track, version, and deploy database changes by delivering on the promise of CI/CD for the database.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;With their rigorous methodology, the DORA research team delivers actionable information with the simple goal of helping organizations of any size accelerate the development and delivery of software. Liquibase is honored to sponsor the 2021 State of DevOps Report and its role in helping so many organizations build value for their customers and shareholders\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Matt Geise, VP of Marketing at Liquibase\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://www.pagerduty.com/platform/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;PagerDuty\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eLiquibase helps millions of developers easily manage database schema changes by enabling teams to track, version, and deploy database changes by delivering on the promise of CI/CD for the database.\u003c/p\u003e\u003cp\u003e“With their rigorous methodology, the DORA research team delivers actionable information with the simple goal of helping organizations of any size accelerate the development and delivery of software. Liquibase is honored to sponsor the 2021 State of DevOps Report and its role in helping so many organizations build value for their customers and shareholders”\u003c/p\u003e\u003cp\u003eMatt Geise, VP of Marketing at Liquibase\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.pagerduty.com/platform/\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://www.pagerduty.com\" track-metadata-module=\"post\"\u003ePagerDuty\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;PagerDuty is a digital operations management platform that empowers the right action, when seconds matter.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;As a leader in digital operations management, PagerDuty is proud to sponsor this year\u0026amp;#8217;s report as it aligns with our dedication to helping DevOps professionals make better decisions. This report will inform tech and business leaders about the trends/challenges developers are facing and the opportunities there are to accelerate their own DevOps transformation.\u0026amp;#8221;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Carolyn Guss, VP of Corporate Marketing\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://sysdig.com/resources/whitepapers/5-keys-to-a-secure-devops-workflow/?utm_source=google\u0026amp;amp;utm_medium=cpc\u0026amp;amp;utm_campaign=10874493567\u0026amp;amp;adgroupid=106662582683\u0026amp;amp;utm_content=471144145325\u0026amp;amp;utm_term=sysdig\u0026amp;amp;utm_position=\u0026amp;amp;utm_device=c\u0026amp;amp;utm_type=e\u0026amp;amp;utm_geo=9033320\u0026amp;amp;gclid=CjwKCAjw_JuGBhBkEiwA1xmbRSyNOvpZ_6_BMU7Cd_NqwtoumsXkkVHoHcEHLNTQmh2sAWTCOP3WiRoC8ycQAvD_BwE\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SysDig\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003ePagerDuty is a digital operations management platform that empowers the right action, when seconds matter.\u003c/p\u003e\u003cp\u003e“As a leader in digital operations management, PagerDuty is proud to sponsor this year’s report as it aligns with our dedication to helping DevOps professionals make better decisions. This report will inform tech and business leaders about the trends/challenges developers are facing and the opportunities there are to accelerate their own DevOps transformation.” \u003c/p\u003e\u003cp\u003eCarolyn Guss, VP of Corporate Marketing\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://sysdig.com/resources/whitepapers/5-keys-to-a-secure-devops-workflow/?utm_source=google\u0026amp;utm_medium=cpc\u0026amp;utm_campaign=10874493567\u0026amp;adgroupid=106662582683\u0026amp;utm_content=471144145325\u0026amp;utm_term=sysdig\u0026amp;utm_position=\u0026amp;utm_device=c\u0026amp;utm_type=e\u0026amp;utm_geo=9033320\u0026amp;gclid=CjwKCAjw_JuGBhBkEiwA1xmbRSyNOvpZ_6_BMU7Cd_NqwtoumsXkkVHoHcEHLNTQmh2sAWTCOP3WiRoC8ycQAvD_BwE\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://sysdig.com\" track-metadata-module=\"post\"\u003eSysDig\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Sysdig is driving the secure DevOps movement, empowering organizations to confidently secure containers, Kubernetes and cloud.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#8220;There is a learning curve with all new technology, cloud is no exception. Learning from mistakes and sharing best practices is how we will ultimately ship secure applications, faster. For seven years, DORA and Google have partnered to understand the State of DevOps to help vendors and cloud practitioners to learn from each other and implement standards for best practices,\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Loris Degioanni, CTO and founder of Sysdig\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud, the DORA team, and our sponsors are very excited about this year\u0026amp;#8217;s report. We look forward to hearing from you, your colleagues, and networks about how DevOps is integrated into your workflow and ways we can help to further improve your performance. Please share your experience with software delivery by \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;completing our survey\u0026lt;/a\u0026gt; that will be used to foster the next generation of DevOps best practices. To provide ample time for the DevOps community to contribute to this industry wide report we have extended the deadline for the survey until July 2nd.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Thank you to everyone who has participated so far, and the Google Cloud DORA team looks forward to hearing from even more of you soon!\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eSysdig is driving the secure DevOps movement, empowering organizations to confidently secure containers, Kubernetes and cloud.\u003c/p\u003e\u003cp\u003e“There is a learning curve with all new technology, cloud is no exception. Learning from mistakes and sharing best practices is how we will ultimately ship secure applications, faster. For seven years, DORA and Google have partnered to understand the State of DevOps to help vendors and cloud practitioners to learn from each other and implement standards for best practices,”\u003c/p\u003e\u003cp\u003eLoris Degioanni, CTO and founder of Sysdig\u003c/p\u003e\u003cp\u003eGoogle Cloud, the DORA team, and our sponsors are very excited about this year’s report. We look forward to hearing from you, your colleagues, and networks about how DevOps is integrated into your workflow and ways we can help to further improve your performance. Please share your experience with software delivery by \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003ecompleting our survey\u003c/a\u003e that will be used to foster the next generation of DevOps best practices. To provide ample time for the DevOps community to contribute to this industry wide report we have extended the deadline for the survey until July 2nd.\u003c/p\u003e\u003cp\u003eThank you to everyone who has participated so far, and the Google Cloud DORA team looks forward to hearing from even more of you soon!\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c18=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eGoogle Cloud and the \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\"\u003eDORA\u003c/a\u003e research team are excited to announce our eight sponsors for the 2021 State of DevOps report. We recently launched the \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\"\u003e2021 State of DevOps survey\u003c/a\u003e, a 25-min survey for the DevOps community to share how they are using DevOps to improve software delivery performance. So if you haven’t taken the survey yet, this is your chance! \u003c/p\u003e\u003cp\u003eFor those unfamiliar with the State Of DevOps report, it is the largest and longest running research of its kind. It provides an independent view into how teams and companies can drive powerful business outcomes, no matter what stage of the DevOps journey.\u003c/p\u003e\u003cp\u003eWith the \u003ca href=\"https://cloud.google.com/devops\"\u003eState of DevOps\u003c/a\u003e reports we aim to capture how teams and companies are driving excellence in technology delivery through the implementation of DevOps practices, no matter what stage of the DevOps journey your team is in. If you’re wondering how your team measures up in your industry take our \u003ca href=\"https://www.devops-research.com/quickcheck.html\" target=\"_blank\"\u003eDevOps Quick Check\u003c/a\u003e and discover which capabilities you should focus on to improve your performance.\u003c/p\u003e\u003cp\u003eTo capture a diverse array of information on how those in the DevOps community are performing, our eight sponsors of the 2021 State of DevOps survey are focussed on helping organizations of all sizes and industries to develop and deliver software faster across the whole DevOps lifecycle. Captured below, you’ll find more information on our sponsors for this year and why they chose to sponsor the State Of the DevOps 2021 Report.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.armory.io/\" target=\"_blank\"\u003eArmory\u003c/a\u003e \u003cbr/\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"armory\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/armory.max-1000x1000.png\"/\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cbr/\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eArmory enables enterprise companies to ship better software, faster through trusted, reliable, safe, and secure deployments -- at its core, Armory is powered by Spinnaker OSS.\u003c/p\u003e\u003cp\u003e“Armory exists to unlock innovation through software. To achieve this, we help enterprises reliably deploy software at scale, naturally aligning with DevOps practices to improve software delivery performance. Given this, we applaud efforts like the State of DevOps Report that provides an independent view into the practices and capabilities that organizations can employ to drive better performance.\" \u003c/p\u003e\u003cp\u003eCarl Timm, Senior Director of Product Marketing at Armory\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://circleci.com/\" target=\"_blank\"\u003eCircleCI\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"circleci\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/circelci.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe world’s best software teams deliver quality code, confidently, with CircleCI. The world’s best software teams use CircleCI to go from next-up to feature shipped, at the speed ambitious businesses require. \u003c/p\u003e\u003cp\u003e“Though the DevOps space is only over a decade old, it moves incredibly quickly. Google’s State of DevOps report is both a reflection and projection of the industry, capturing how DevOps culture drives business results and where DevOps practitioners can look to improve. At CircleCI, we rely heavily on this survey data to glean valuable insights into our market and our customers overall. We also build upon these insights to guide our own research into how engineering teams move code through pipelines in our annual \u003ca href=\"https://circleci.com/resources/2020-state-of-software-delivery/\" target=\"_blank\"\u003eState of Software Delivery report\u003c/a\u003e. Taken together, this research highlights teams’ reported and actual behavior and paints a vibrant picture of how technology-driven organizations drive for success.”\u003c/p\u003e\u003cp\u003eEmma Webb, VP, Corporate Communications, CircleCI \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cd.foundation/\" target=\"_blank\"\u003eContinuous Delivery Foundation\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"cdf\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/cdf.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eCD Foundation is an open-source community improving the world's ability to deliver software with security and speed.\u003c/p\u003e\u003cp\u003e\"Good decision-making is based on good data. Open source is a critical piece of the technology roadmap for DevOps, and the 2021 State of DevOps Report will provide actionable information for high performing teams in organizations of all types and sizes. The report will show where open source and DevOps intersect, and in a space that is changing so quickly, new relevant data is critical,\" said Tracy Miranda, Continuous Delivery Foundation Executive Director. \"CD Foundation members are deeply involved with producing this annual report, with over 10 years of historic research. Thank you to Google and our other members who have worked so hard on it. This report is a tangible result of working together.\"\u003c/p\u003e\u003cp\u003eTracy Miranda, Continuous Delivery Foundation Executive Director\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www2.deloitte.com/us/en.html\" target=\"_blank\"\u003eDeloitte\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"deloitte\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/deloitte.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDeloitte provides audit and assurance, tax, consulting, and risk and financial advisory services to a broad cross-section of the largest corporations and governmental agencies. At Deloitte, they are continuously evolving how they work and how they look at marketplace challenges so they can continually deliver measurable, sustainable results for their clients and communities.\u003c/p\u003e\u003cp\u003e“Software delivery is evolving rapidly and we know our customers need unique and compelling insights to make the right decisions. State of DevOps is a widely used report and considered as an Industry standard to understand the drivers for excellence in Software Development and Delivery. Deloitte is excited to help the team at DORA and Google Cloud in publishing this report through our sponsoring since we believe the insights from this report will help make software delivery better.” \u003c/p\u003e\u003cp\u003eManoj Mishra, Consulting Managing Director, Deloitte Consulting LLP\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://about.gitlab.com/\" target=\"_blank\"\u003eGitLab\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"gitlab\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/gitlab.max-1000x1000.jpeg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eGitLab is the open DevOps platform built from the ground up as a single application for all stages of the DevOps lifecycle enabling Product, Development, QA, Security, and Operations teams to work concurrently on the same project.\u003c/p\u003e\u003cp\u003e“We're happy to sponsor the DORA Report and the vital work behind it. The more we understand the DevOps journey, the better we and our GitLab community can contribute to its future. We're particularly interested to see this year's results and the impact COVID-19 and remote work have had on DevOps. We appreciate the chance to be part of this exciting, informative process.”\u003c/p\u003e\u003cp\u003eBrendon O’Leary, Senior Developer Evangelist\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.liquibase.com/\" target=\"_blank\"\u003eLiquibase\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"liquibase\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/liquibase.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eLiquibase helps millions of developers easily manage database schema changes by enabling teams to track, version, and deploy database changes by delivering on the promise of CI/CD for the database.\u003c/p\u003e\u003cp\u003e“With their rigorous methodology, the DORA research team delivers actionable information with the simple goal of helping organizations of any size accelerate the development and delivery of software. Liquibase is honored to sponsor the 2021 State of DevOps Report and its role in helping so many organizations build value for their customers and shareholders”\u003c/p\u003e\u003cp\u003eMatt Geise, VP of Marketing at Liquibase\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.pagerduty.com/platform/\" target=\"_blank\"\u003ePagerDuty\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"pagerduty\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/pagerduty.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003ePagerDuty is a digital operations management platform that empowers the right action, when seconds matter.\u003c/p\u003e\u003cp\u003e“As a leader in digital operations management, PagerDuty is proud to sponsor this year’s report as it aligns with our dedication to helping DevOps professionals make better decisions. This report will inform tech and business leaders about the trends/challenges developers are facing and the opportunities there are to accelerate their own DevOps transformation.” \u003c/p\u003e\u003cp\u003eCarolyn Guss, VP of Corporate Marketing\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://sysdig.com/resources/whitepapers/5-keys-to-a-secure-devops-workflow/?utm_source=google\u0026amp;utm_medium=cpc\u0026amp;utm_campaign=10874493567\u0026amp;adgroupid=106662582683\u0026amp;utm_content=471144145325\u0026amp;utm_term=sysdig\u0026amp;utm_position=\u0026amp;utm_device=c\u0026amp;utm_type=e\u0026amp;utm_geo=9033320\u0026amp;gclid=CjwKCAjw_JuGBhBkEiwA1xmbRSyNOvpZ_6_BMU7Cd_NqwtoumsXkkVHoHcEHLNTQmh2sAWTCOP3WiRoC8ycQAvD_BwE\" target=\"_blank\"\u003eSysDig\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--medium h-c-grid__col h-c-grid__col--4 h-c-grid__col--offset-4 \"\u003e\u003cimg alt=\"sysdig\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/sysdig.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eSysdig is driving the secure DevOps movement, empowering organizations to confidently secure containers, Kubernetes and cloud.\u003c/p\u003e\u003cp\u003e“There is a learning curve with all new technology, cloud is no exception. Learning from mistakes and sharing best practices is how we will ultimately ship secure applications, faster. For seven years, DORA and Google have partnered to understand the State of DevOps to help vendors and cloud practitioners to learn from each other and implement standards for best practices,”\u003c/p\u003e\u003cp\u003eLoris Degioanni, CTO and founder of Sysdig\u003c/p\u003e\u003cp\u003eGoogle Cloud, the DORA team, and our sponsors are very excited about this year’s report. We look forward to hearing from you, your colleagues, and networks about how DevOps is integrated into your workflow and ways we can help to further improve your performance. Please share your experience with software delivery by \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\"\u003ecompleting our survey\u003c/a\u003e that will be used to foster the next generation of DevOps best practices. To provide ample time for the DevOps community to contribute to this industry wide report we have extended the deadline for the survey until July 2nd.\u003c/p\u003e\u003cp\u003eThank you to everyone who has participated so far, and the Google Cloud DORA team looks forward to hearing from even more of you soon!\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/sodr.max-1600x1600.png",
      "date_published": "2021-06-30T11:28:00Z",
      "author": {
        "name": "\u003cname\u003eBrenna Washington\u003c/name\u003e\u003ctitle\u003eProduct Marketing Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/topics/developers-practitioners/blueprint-secure-infrastructure-google-cloud/",
      "title": "A blueprint for secure infrastructure on Google Cloud",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When it comes to infrastructure security, every stakeholder has the same goal: maintain the confidentiality and integrity of their company\u0026amp;#8217;s data and systems. Period.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Developing and operating in the Cloud provides the opportunity to achieve these goals by being more secure and having greater visibility and governance over your resources and data. This is due to the relatively uniform environment of cloud infrastructure (as compared with on-prem) and inherent service-centric architecture. In addition, cloud providers take on some of the key responsibilities for security doing their part in a shared responsibility model.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;However, translating this shared goal into reality can be a complex endeavor for a few reasons. Firstly, administering security in public clouds is unlike what you may be used to as the infrastructure primitives (the building blocks available to you) and control abstractions (how you administer security policy) differ from on premise environments. Additionally, ensuring you make the right policy decisions in an area as high-stakes and ever-evolving as security means that you\u0026amp;#8217;ll likely spend hours researching and reading through documentation, perhaps even hiring experts.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud\u0026#34;\u0026gt;partner with you\u0026lt;/a\u0026gt; and help address these challenges, Google Cloud built the security foundations blueprint to identify core security decisions and guide you with opinionated best practices for deploying a secured GCP environment.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;What is the security foundations blueprint?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The security foundations blueprint is made up of two resources: the \u0026lt;a href=\u0026#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;security foundations guide\u0026lt;/a\u0026gt;, and the \u0026lt;a href=\u0026#34;https://github.com/terraform-google-modules/terraform-example-foundation\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Terraform automation repository\u0026lt;/a\u0026gt;. For each security decision, the security foundations guide provides opinionated best practices in order to help you build a secure starting point for your Google Cloud deployment, and can be read and used as a reference guide. The recommended policies and architecture outlined in the document can then be deployed through automation using the \u0026lt;a href=\u0026#34;https://github.com/terraform-google-modules/terraform-example-foundation\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Terraform repository\u0026lt;/a\u0026gt; available on GitHub.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Who is it for?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The security foundations blueprint was designed with the enterprise in mind, including those with the strongest security requirements. However, the best practices are applicable to any size cloud customer, and can be adapted or adopted in pieces as needed for your organization. As far as who in an organization is going to find it most useful, it is beneficial for many roles:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;b\u0026gt;CISOs and compliance officers\u0026lt;/b\u0026gt; will use the \u0026lt;a href=\u0026#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;security foundations guide\u0026lt;/a\u0026gt; as a reference to understand Google\u0026amp;#8217;s key principles for Cloud Security and how they can be applied and implemented to their deployments.\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;b\u0026gt;Security practitioners\u0026lt;/b\u0026gt; and\u0026lt;b\u0026gt; platform teams\u0026lt;/b\u0026gt; will follow the guide\u0026amp;#8217;s detailed instructions and accompanying Terraform templates for applying best practices so that they can actively set-up, configure, deploy, and operate their own security-centric infrastructure.\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;b\u0026gt;Application developers\u0026lt;/b\u0026gt; will deploy their workloads and applications on this foundational infrastructure through an automated application deployment pipeline provided in the blueprint.\u0026amp;#160;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;What topics does it cover?\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;This security foundations blueprint continues to expand the topics it covers, with its most recent release in April 2021 including the following areas:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;Google Cloud organization structure and policy\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Authentication and authorization\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Resource hierarchy and deployment\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Networking (segmentation and security)\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Key and secret management\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Logging\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Detective controls\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Billing setup\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Application security\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Each of the security decisions addressed in these topics come with background and discussion to support your own understanding of the concepts, which in turn enables you to customize the deployment to your own specific use case (if needed).\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The topics are useful separately, which makes it possible to pick-and-choose areas where you need recommendations, but they also work together. For example, by following the best practices for project and resource naming conventions, you will be set up for advanced monitoring capabilities, such as real-time notifications for compliance to custom policies.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;How can I use it?\u0026lt;/h3\u0026gt;While the \u0026lt;a href=\u0026#34;https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;security foundations guide\u0026lt;/a\u0026gt; is incredibly valuable on its own, the real magic for a security practitioner or application developer comes from the ability to adopt, adapt, and deploy the best practices using templates in \u0026lt;a href=\u0026#34;https://www.terraform.io/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Terraform\u0026lt;/a\u0026gt;, a tool for managing Infrastructure as Code (IaC). For anyone new to IaC, simply put, it allows you to automate your infrastructure through writing code that configures and provisions your infrastructure. By using IaC to minimize the amount of manual configuration, you also benefit through limiting the possibility of human error in enforcing these components of your security policy.\"\u003e\u003cp\u003eWhen it comes to infrastructure security, every stakeholder has the same goal: maintain the confidentiality and integrity of their company’s data and systems. Period.\u003c/p\u003e\u003cp\u003eDeveloping and operating in the Cloud provides the opportunity to achieve these goals by being more secure and having greater visibility and governance over your resources and data. This is due to the relatively uniform environment of cloud infrastructure (as compared with on-prem) and inherent service-centric architecture. In addition, cloud providers take on some of the key responsibilities for security doing their part in a shared responsibility model. \u003c/p\u003e\u003cp\u003eHowever, translating this shared goal into reality can be a complex endeavor for a few reasons. Firstly, administering security in public clouds is unlike what you may be used to as the infrastructure primitives (the building blocks available to you) and control abstractions (how you administer security policy) differ from on premise environments. Additionally, ensuring you make the right policy decisions in an area as high-stakes and ever-evolving as security means that you’ll likely spend hours researching and reading through documentation, perhaps even hiring experts. \u003c/p\u003e\u003cp\u003eTo \u003ca href=\"https://cloud.google.com/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud\" track-metadata-module=\"post\"\u003epartner with you\u003c/a\u003e and help address these challenges, Google Cloud built the security foundations blueprint to identify core security decisions and guide you with opinionated best practices for deploying a secured GCP environment. \u003c/p\u003e\u003ch3\u003eWhat is the security foundations blueprint?\u003c/h3\u003e\u003cp\u003eThe security foundations blueprint is made up of two resources: the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://services.google.com\" track-metadata-module=\"post\"\u003esecurity foundations guide\u003c/a\u003e, and the \u003ca href=\"https://github.com/terraform-google-modules/terraform-example-foundation\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eTerraform automation repository\u003c/a\u003e. For each security decision, the security foundations guide provides opinionated best practices in order to help you build a secure starting point for your Google Cloud deployment, and can be read and used as a reference guide. The recommended policies and architecture outlined in the document can then be deployed through automation using the \u003ca href=\"https://github.com/terraform-google-modules/terraform-example-foundation\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eTerraform repository\u003c/a\u003e available on GitHub. \u003c/p\u003e\u003ch3\u003eWho is it for?\u003c/h3\u003e\u003cp\u003eThe security foundations blueprint was designed with the enterprise in mind, including those with the strongest security requirements. However, the best practices are applicable to any size cloud customer, and can be adapted or adopted in pieces as needed for your organization. As far as who in an organization is going to find it most useful, it is beneficial for many roles:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cb\u003eCISOs and compliance officers\u003c/b\u003e will use the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://services.google.com\" track-metadata-module=\"post\"\u003esecurity foundations guide\u003c/a\u003e as a reference to understand Google’s key principles for Cloud Security and how they can be applied and implemented to their deployments.\u003c/li\u003e\u003cli\u003e\u003cb\u003eSecurity practitioners\u003c/b\u003e and\u003cb\u003e platform teams\u003c/b\u003e will follow the guide’s detailed instructions and accompanying Terraform templates for applying best practices so that they can actively set-up, configure, deploy, and operate their own security-centric infrastructure. \u003c/li\u003e\u003cli\u003e\u003cb\u003eApplication developers\u003c/b\u003e will deploy their workloads and applications on this foundational infrastructure through an automated application deployment pipeline provided in the blueprint. \u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eWhat topics does it cover?\u003c/h3\u003e\u003cp\u003eThis security foundations blueprint continues to expand the topics it covers, with its most recent release in April 2021 including the following areas:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGoogle Cloud organization structure and policy\u003c/li\u003e\u003cli\u003eAuthentication and authorization\u003c/li\u003e\u003cli\u003eResource hierarchy and deployment\u003c/li\u003e\u003cli\u003eNetworking (segmentation and security)\u003c/li\u003e\u003cli\u003eKey and secret management\u003c/li\u003e\u003cli\u003eLogging\u003c/li\u003e\u003cli\u003eDetective controls\u003c/li\u003e\u003cli\u003eBilling setup\u003c/li\u003e\u003cli\u003eApplication security\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEach of the security decisions addressed in these topics come with background and discussion to support your own understanding of the concepts, which in turn enables you to customize the deployment to your own specific use case (if needed).\u003c/p\u003e\u003cp\u003eThe topics are useful separately, which makes it possible to pick-and-choose areas where you need recommendations, but they also work together. For example, by following the best practices for project and resource naming conventions, you will be set up for advanced monitoring capabilities, such as real-time notifications for compliance to custom policies.\u003c/p\u003e\u003ch3\u003eHow can I use it?\u003c/h3\u003e\u003cp\u003eWhile the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://services.google.com\" track-metadata-module=\"post\"\u003esecurity foundations guide\u003c/a\u003e is incredibly valuable on its own, the real magic for a security practitioner or application developer comes from the ability to adopt, adapt, and deploy the best practices using templates in \u003ca href=\"https://www.terraform.io/\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://www.terraform.io\" track-metadata-module=\"post\"\u003eTerraform\u003c/a\u003e, a tool for managing Infrastructure as Code (IaC). For anyone new to IaC, simply put, it allows you to automate your infrastructure through writing code that configures and provisions your infrastructure. By using IaC to minimize the amount of manual configuration, you also benefit through limiting the possibility of human error in enforcing these components of your security policy.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eWhen it comes to infrastructure security, every stakeholder has the same goal: maintain the confidentiality and integrity of their company’s data and systems. Period.\u003c/p\u003e\u003cp\u003eDeveloping and operating in the Cloud provides the opportunity to achieve these goals by being more secure and having greater visibility and governance over your resources and data. This is due to the relatively uniform environment of cloud infrastructure (as compared with on-prem) and inherent service-centric architecture. In addition, cloud providers take on some of the key responsibilities for security doing their part in a shared responsibility model. \u003c/p\u003e\u003cp\u003eHowever, translating this shared goal into reality can be a complex endeavor for a few reasons. Firstly, administering security in public clouds is unlike what you may be used to as the infrastructure primitives (the building blocks available to you) and control abstractions (how you administer security policy) differ from on premise environments. Additionally, ensuring you make the right policy decisions in an area as high-stakes and ever-evolving as security means that you’ll likely spend hours researching and reading through documentation, perhaps even hiring experts. \u003c/p\u003e\u003cp\u003eTo \u003ca href=\"https://cloud.google.com/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud\"\u003epartner with you\u003c/a\u003e and help address these challenges, Google Cloud built the security foundations blueprint to identify core security decisions and guide you with opinionated best practices for deploying a secured GCP environment. \u003c/p\u003e\u003ch3\u003eWhat is the security foundations blueprint?\u003c/h3\u003e\u003cp\u003eThe security foundations blueprint is made up of two resources: the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\"\u003esecurity foundations guide\u003c/a\u003e, and the \u003ca href=\"https://github.com/terraform-google-modules/terraform-example-foundation\" target=\"_blank\"\u003eTerraform automation repository\u003c/a\u003e. For each security decision, the security foundations guide provides opinionated best practices in order to help you build a secure starting point for your Google Cloud deployment, and can be read and used as a reference guide. The recommended policies and architecture outlined in the document can then be deployed through automation using the \u003ca href=\"https://github.com/terraform-google-modules/terraform-example-foundation\" target=\"_blank\"\u003eTerraform repository\u003c/a\u003e available on GitHub. \u003c/p\u003e\u003ch3\u003eWho is it for?\u003c/h3\u003e\u003cp\u003eThe security foundations blueprint was designed with the enterprise in mind, including those with the strongest security requirements. However, the best practices are applicable to any size cloud customer, and can be adapted or adopted in pieces as needed for your organization. As far as who in an organization is going to find it most useful, it is beneficial for many roles:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cb\u003eCISOs and compliance officers\u003c/b\u003e will use the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\"\u003esecurity foundations guide\u003c/a\u003e as a reference to understand Google’s key principles for Cloud Security and how they can be applied and implemented to their deployments.\u003c/li\u003e\u003cli\u003e\u003cb\u003eSecurity practitioners\u003c/b\u003e and\u003cb\u003eplatform teams\u003c/b\u003e will follow the guide’s detailed instructions and accompanying Terraform templates for applying best practices so that they can actively set-up, configure, deploy, and operate their own security-centric infrastructure. \u003c/li\u003e\u003cli\u003e\u003cb\u003eApplication developers\u003c/b\u003e will deploy their workloads and applications on this foundational infrastructure through an automated application deployment pipeline provided in the blueprint. \u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eWhat topics does it cover?\u003c/h3\u003e\u003cp\u003eThis security foundations blueprint continues to expand the topics it covers, with its most recent release in April 2021 including the following areas:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGoogle Cloud organization structure and policy\u003c/li\u003e\u003cli\u003eAuthentication and authorization\u003c/li\u003e\u003cli\u003eResource hierarchy and deployment\u003c/li\u003e\u003cli\u003eNetworking (segmentation and security)\u003c/li\u003e\u003cli\u003eKey and secret management\u003c/li\u003e\u003cli\u003eLogging\u003c/li\u003e\u003cli\u003eDetective controls\u003c/li\u003e\u003cli\u003eBilling setup\u003c/li\u003e\u003cli\u003eApplication security\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEach of the security decisions addressed in these topics come with background and discussion to support your own understanding of the concepts, which in turn enables you to customize the deployment to your own specific use case (if needed).\u003c/p\u003e\u003cp\u003eThe topics are useful separately, which makes it possible to pick-and-choose areas where you need recommendations, but they also work together. For example, by following the best practices for project and resource naming conventions, you will be set up for advanced monitoring capabilities, such as real-time notifications for compliance to custom policies.\u003c/p\u003e\u003ch3\u003eHow can I use it?\u003c/h3\u003eWhile the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\"\u003esecurity foundations guide\u003c/a\u003e is incredibly valuable on its own, the real magic for a security practitioner or application developer comes from the ability to adopt, adapt, and deploy the best practices using templates in \u003ca href=\"https://www.terraform.io/\" target=\"_blank\"\u003eTerraform\u003c/a\u003e, a tool for managing Infrastructure as Code (IaC). For anyone new to IaC, simply put, it allows you to automate your infrastructure through writing code that configures and provisions your infrastructure. By using IaC to minimize the amount of manual configuration, you also benefit through limiting the possibility of human error in enforcing these components of your security policy.\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Untitled_2.max-1000x1000.png\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"The Security Foundations Blueprint\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Untitled_2.max-1000x1000.png\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe Security Foundations Blueprint as an automated deployment pipeline\u003c/p\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe \u003ca href=\"https://github.com/terraform-google-modules/terraform-example-foundation\" target=\"_blank\"\u003eTerraform automation repo\u003c/a\u003e includes configuration that defines the environment outlined in the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\"\u003eguide\u003c/a\u003e. You can apply the repo end-to-end to deploy the full security foundations blueprint, or use the included modules individually and modify them so that you can adopt just portions of the blueprint. It’s important to note that there are a few differences between the policies for the sample organization outlined in the guide and what is deployed using the Terraform templates. Luckily, those few differences are outlined in the \u003ca href=\"https://github.com/terraform-google-modules/terraform-example-foundation/blob/master/ERRATA.md\" target=\"_blank\"\u003eerrata pages\u003c/a\u003e that are part of the \u003ca href=\"https://github.com/terraform-google-modules/terraform-example-foundation\" target=\"_blank\"\u003eTerraform automation repo\u003c/a\u003e.\u003cbr/\u003e\u003c/p\u003e\u003ch3\u003eSo what should I do next? \u003c/h3\u003e\u003cp\u003eWe hope you’ll continue following the journey of this blog series where we’ll dive deeper into the best practices provided throughout the topical sections of the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\"\u003eguide\u003c/a\u003e, discuss the different ways in which the blueprints have helped enterprises secure their own Cloud deployment, and take a look inside the \u003ca href=\"https://github.com/terraform-google-modules/terraform-example-foundation\" target=\"_blank\"\u003eTerraform templates\u003c/a\u003e to see how they can be adopted, adapted, and deployed. In the meantime, take a look at \u003ca href=\"https://cloud.google.com/blog/products/identity-security/google-cloud-security-foundations-guide\"\u003ethis recent Cloud blog post\u003c/a\u003e which announces the launch of the blueprint’s latest version and discusses the key security principles that steer the best practices.\u003c/p\u003e\u003cp\u003e If you’re ready to dive straight into the \u003ca href=\"https://services.google.com/fh/files/misc/google-cloud-security-foundations-guide.pdf\" target=\"_blank\"\u003esecurity foundations guide\u003c/a\u003e, you can start at the beginning, or head to a topic in which you’re particularly interested. Reviewing the guide in this way, you will be able to see for yourself the level of detail and discussion, and most importantly, the direct path it provides to move beyond recommendations and into implementation. We don’t expect you to try and apply the blueprint to your security posture right away, but a great first step would be to fork the repo and deploy it in a new folder or organization. Go forth, deploy and stay safe out there.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/identity-security/google-cloud-security-foundations-guide/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_security.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eBuild security into Google Cloud deployments with our updated security foundations blueprint\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eGet step by step guidance for creating a secured environment with Google Cloud with the security foundations guide and Terraform blueprin...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://cloud.google.com/blog/static/assets/GCP_Twitter_Card-2000×1000.png",
      "date_published": "2021-06-24T19:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlicia Williams\u003c/name\u003e\u003ctitle\u003eDeveloper Advocate\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum/",
      "title": "Are we there yet? Thoughts on assessing an SRE team’s maturity",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;One facet of our work as \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\u0026#34;\u0026gt;Customer Reliability Engineers\u0026lt;/a\u0026gt;\u0026amp;#8212;Google Site Reliability Engineers (SREs) tapped to help Google Cloud customers develop that practice in their own organizations\u0026amp;#8212;is advising operations or SRE teams to improve their operational maturity. We\u0026#39;ve noticed a recurring question cropping up across many of these discussions, usually phrased along the lines of \u0026amp;#34;is what we\u0026#39;re currently doing \u0026lt;i\u0026gt;\u0026#39;SRE work\u0026#39;?\u0026amp;#34;\u0026lt;/i\u0026gt; or, with a little more existential dread, \u0026amp;#34;can we call ourselves SREs yet?\u0026amp;#34;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We\u0026#39;ve answered this question before with a \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey\u0026#34;\u0026gt;list of practices\u0026lt;/a\u0026gt; from the \u0026lt;a href=\u0026#34;https://sre.google/workbook/table-of-contents/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;SRE workbook\u0026lt;/a\u0026gt;. But the list is long on the \u0026lt;i\u0026gt;what\u0026lt;/i\u0026gt; and short on the \u0026lt;i\u0026gt;why\u0026lt;/i\u0026gt;, which can make it hard to digest for folks already suffering an identity crisis. Instead, we hope to help answer this question by discussing some principles we consider fundamental to how an SRE team operates. We\u0026#39;ll examine why they\u0026#39;re important and suggest questions that characterize a team\u0026#39;s progress towards embodying them.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Are we there yet?\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;This question is asked in different ways, for a myriad of different reasons, and it can be quite hard to answer due to the wide range of \u0026lt;a href=\u0026#34;https://web.devopstopologies.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;different circumstances\u0026lt;/a\u0026gt; that our customers operate in. Moreover, CRE, and Google in general, is not the final arbiter of what is and isn\u0026#39;t \u0026amp;#34;SRE\u0026amp;#34; for your organization, so we can\u0026#39;t provide an authoritative answer, if one even exists. We can only influence you and the community at large by expressing our opinions and experiences, in person or via our \u0026lt;a href=\u0026#34;https://sre.google/books/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;books\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/cre-life-lessons\u0026#34;\u0026gt;blog posts\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Further, discussions of this topic tend to be complicated by the fact that the term \u0026amp;#34;SRE\u0026amp;#34; is used interchangeably to mean three things:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A \u0026lt;b\u0026gt;job role\u0026lt;/b\u0026gt; primarily focused on maintaining the reliability of a service or product.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A \u0026lt;b\u0026gt;group of people \u0026lt;/b\u0026gt;working within an organization, usually in the above job role.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;A \u0026lt;b\u0026gt;set of principles and practices\u0026lt;/b\u0026gt; that the above people can utilize to improve service reliability.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;When people ask \u0026amp;#34;can we call ourselves SREs yet?\u0026amp;#34; we could interpret it as a desire to link these three definitions together. A clearer restatement of this interpretation might be: \u0026amp;#34;Is our \u0026lt;b\u0026gt;group\u0026lt;/b\u0026gt; sufficiently advanced in our application of the \u0026lt;b\u0026gt;principles and practices\u0026lt;/b\u0026gt; that we can justifiably term our \u0026lt;b\u0026gt;job role\u0026lt;/b\u0026gt; SRE?\u0026amp;#34;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We should stress that we\u0026#39;re not saying that you need a clearly defined \u0026lt;b\u0026gt;job role\u0026lt;/b\u0026gt;\u0026amp;#8212;or even a \u0026lt;b\u0026gt;team\u0026lt;/b\u0026gt;\u0026amp;#8212;before you can start utilizing the \u0026lt;b\u0026gt;principles and practices\u0026lt;/b\u0026gt; to do things that are recognizably SRE-like. Job roles and teams crystallize from a more fluid set of responsibilities as organizations grow larger. But as this process plays out, the people involved may feel less certain of the scope of their responsibilities, precipitating the \u0026amp;#8216;are we there yet?\u0026amp;#8217; question. We suspect that\u0026#39;s where the tone of existential dread comes from...\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Key SRE indicators\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;Within the CRE team here at Google Cloud, the \u0026amp;#8216;are we there yet?\u0026amp;#8217; question surfaced a wide variety of opinions about the core principles that should guide an SRE team. We did manage to reach a rough consensus, with one proviso\u0026amp;#8212;the answer is partially dependent on how a team engages with the services it supports.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We\u0026#39;ve chosen to structure this post around a set of principles that we would broadly expect groups of people working as SREs that \u0026lt;i\u0026gt;directly support services in production\u0026lt;/i\u0026gt; to adhere to. As in a \u0026lt;a href=\u0026#34;https://en.wikipedia.org/wiki/Litmus#Uses\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;litmus test\u0026lt;/a\u0026gt;, this won\u0026#39;t provide pin-point accuracy; but in our collective opinion at least, alignment with most of the principles laid out below is a good signal that a team is practicing something that can recognizably be termed \u0026lt;i\u0026gt;Site Reliability Engineering.\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Directly engaged SRE teams are usually considered \u0026lt;i\u0026gt;Accountable\u0026lt;/i\u0026gt; (in \u0026lt;a href=\u0026#34;https://en.wikipedia.org/wiki/Responsibility_assignment_matrix\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;RACI terms\u0026lt;/a\u0026gt;) for the service\u0026amp;#8217;s reliability, with \u0026lt;i\u0026gt;Responsibility\u0026lt;/i\u0026gt; shared between the SRE and development teams. As a team provides less direct support these indicators may be less applicable. We hope those teams can still adapt the principles to their own circumstances.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;To illustrate how you might do this, for each principle we\u0026#39;ve given a counter-example of a team of SREs operating in an advisory capacity. They\u0026#39;re subject-matter experts who are \u0026lt;i\u0026gt;Consulted\u0026lt;/i\u0026gt; by development teams who are themselves \u0026lt;i\u0026gt;Responsible\u0026lt;/i\u0026gt; and \u0026lt;i\u0026gt;Accountable\u0026lt;/i\u0026gt; for service reliability.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Wherever your engagement model lies on the spectrum, being perceived by the rest of the organization as jointly responsible for a service\u0026#39;s reliability, or as reliability subject-matter experts, is a key indicator of SRE-hood.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Principle #1: SREs mitigate present and \u0026lt;i\u0026gt;future\u0026lt;/i\u0026gt; incidents\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;This principle is the one that usually underlies the perception of \u0026lt;b\u0026gt;responsibility and accountability\u0026lt;/b\u0026gt; for a service\u0026#39;s reliability. All the careful engineering and active regulation in the world can\u0026#39;t guarantee reliability, especially in complex distributed systems\u0026amp;#8212;sometimes, things go wrong unexpectedly and the only thing left to do is react, mitigate, and fix. SREs have both the authority and the technical capability to act fast to restore service in these situations.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;But mitigating the immediate problem isn\u0026#39;t enough. If it can happen again tomorrow, then tomorrow isn\u0026#39;t better than today, so SREs should work to understand the precipitating factors of incidents and propose changes that remediate the entire class of problem in the infrastructure they are responsible for. Don\u0026#39;t have the same outage again next month!\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;How unique are your outages? Ask yourself these questions:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Can you mitigate the majority of the incidents without needing specialist knowledge from the development team?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Do you maintain training materials and practice incident response scenarios?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;After a major outage happens to your service, are you a key participant in blamelessly figuring out what \u0026lt;b\u0026gt;really\u0026lt;/b\u0026gt; went wrong, and how to prevent future outages?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Now, for a counter example. In many organizations, SREs are a scarce resource and may add more value by developing platforms and best practices to uplift large swathes of the company, rather than being primarily focused on incident response. Thus, a consulting SRE team would probably not be directly involved in mitigating most incidents, though they may be called on to coordinate incident response for a widespread outage. Rather than authoring training materials and postmortems, they would be responsible for reviewing those created by the teams they advise.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Principle #2: SREs actively regulate service reliability\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Reliability goals and feedback signals are fundamental for both motivating SRE work and influencing the prioritization of development work.\u0026lt;/b\u0026gt; At Google, we call our reliability goals \u0026lt;a href=\u0026#34;https://sre.google/sre-book/service-level-objectives/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;i\u0026gt;Service Level Objectives\u0026lt;/i\u0026gt;\u0026lt;/a\u0026gt; and our feedback signals \u0026lt;i\u0026gt;Error Budgets\u0026lt;/i\u0026gt;, and you can read more about how we use them in the \u0026lt;a href=\u0026#34;https://sre.google/workbook/implementing-slos/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Site Reliability Workbook\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Do your reliability signals affect your organization\u0026#39;s priorities? Ask yourself these questions:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Do you agree on goals for the reliability of the services you support with your organization, and track performance against those goals in real time?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Do you have an established feedback loop that moderates the behaviour of the organization based on recent service reliability?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Do you have the influence to effect change within the organization in pursuit of the reliability goals?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Do you have the \u0026lt;a href=\u0026#34;https://en.wikipedia.org/wiki/Agency_(sociology)\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;agency\u0026lt;/a\u0026gt; to refuse, or negotiate looser goals, when asked to make changes that may cause a service to miss its current reliability goals?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Each question builds on the last. It is almost impossible to establish a data-driven feedback loop without a well-defined and measured reliability goal. For those goals to be meaningful, SREs must have the capability to defend them. Periods of lower service reliability should result in consequences that temporarily reduce the aggregate risk of future production changes and shift engineering priorities towards reliability.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When it comes down to a choice between service reliability and the rollout of new but unreliable features, SREs need to be able to say \u0026amp;#34;no\u0026amp;#34;. This should be a data driven decision\u0026amp;#8212;when there\u0026#39;s not enough spare error budget, there needs to be a valid business reason for making users unhappy. Sometimes, of course, there will be, and this can be accommodated with new, lower SLO targets that reflect the relaxed reliability requirements.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Consultant SREs, in contrast, help teams draft their reliability goals and may develop shared monitoring infrastructure for measuring them across the organization. They are the de-facto regulators of the reliability feedback loop and maintain the policy documents that underpin it. Their connection to many teams and services gives them broader insights that can spark cross-functional reliability improvements.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Principle #3: SREs engage early and comprehensively\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;As we said earlier, \u0026lt;b\u0026gt;SREs should be empowered to make tomorrow better than today.\u0026lt;/b\u0026gt; Without the ability to change the code and configuration of the services they support, they cannot fix problems as they encounter them. Involving SREs earlier in the design process can head off common reliability anti-patterns that are costly to correct \u0026lt;i\u0026gt;post-facto\u0026lt;/i\u0026gt;. And, with the ability to influence architectural decision making, SREs can drive convergence across an organization so that work to increase the reliability of one service can benefit the entire company.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Is your team actively working to make tomorrow better than today? Ask yourself these questions, which go from fine detail to a broad, high-level scope:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Do you engineer your service \u0026lt;b\u0026gt;now\u0026lt;/b\u0026gt; to improve its reliability, e.g. by viewing and modifying the source code and/or configuration?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Are you involved in analysis and design of \u0026lt;b\u0026gt;future\u0026lt;/b\u0026gt; iterations of your service, providing a lens on reliability/operability/maintainability?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Can you influence your \u0026lt;b\u0026gt;organization\u0026amp;#8217;s\u0026lt;/b\u0026gt; wider architectural decision making?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;Advising other teams naturally shifts priorities away from directly modifying the configuration or code of individual services. But consultant SREs may still maintain frameworks or shared libraries providing core reliability features, like exporting common metrics or graceful service degradation. Their breadth of engagements across many teams makes them naturally suited for providing high-level architectural advice to improve reliability across an entire organization.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Principle #4: SREs automate anything repetitive\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Finally, SREs believe that computers are fundamentally better suited to doing repetitive work than humans are. People often underestimate the \u0026lt;a href=\u0026#34;https://xkcd.com/1205/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;returns on investment\u0026lt;/a\u0026gt; when considering whether to automate a routine task, and that\u0026#39;s before factoring in the exponential growth curve that comes with running a large, successful service. Moreover, computers never become inattentive and make mistakes when doing the same task for the hundredth time, or become demoralized and quit. Hiring or \u0026lt;a href=\u0026#34;https://sre.google/resources/practices-and-processes/training-site-reliability-engineers/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;training SREs\u0026lt;/a\u0026gt; is expensive and time-consuming, so a successful SRE organization depends heavily on making computers do the grunt work.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Are you sufficiently automating your work? Ask yourself these questions:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Do you use\u0026amp;#8212;or create\u0026amp;#8212;automation and other tools to ensure that operational load won\u0026#39;t scale linearly with organic growth or the number of services you support?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Do you try to measure repetitive work on your team, and reduce it over time?\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h2\u0026gt;A call for reflection\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;Most blog posts end with a call for action. We\u0026#39;d rather you took time to reflect, instead of jumping up to make changes straight after reading. \u0026lt;b\u0026gt;There\u0026#39;s a risk, when writing an opinionated piece like this, that the lines drawn in the sand are used to divide, not to grow and improve.\u0026lt;/b\u0026gt; We promise this isn\u0026#39;t a deliberate effort to gatekeep SRE and exclude those who don\u0026#39;t tick the boxes; we see no value in that. But in some ways gatekeeping is what job roles are \u0026lt;i\u0026gt;designed\u0026lt;/i\u0026gt; to do, because specialization and the division of labour is critical to the success of any organization, and this makes it hard to avoid drawing those lines.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For those who aspire to call themselves SREs, or are concerned that others may disagree with their characterization of themselves as SREs, perhaps these opinions can assuage some of that existential dread.\u0026lt;/p\u0026gt;\" _nghost-c19=\"\"\u003e\u003cp\u003eOne facet of our work as \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\" track-metadata-module=\"post\"\u003eCustomer Reliability Engineers\u003c/a\u003e—Google Site Reliability Engineers (SREs) tapped to help Google Cloud customers develop that practice in their own organizations—is advising operations or SRE teams to improve their operational maturity. We\u0026#39;ve noticed a recurring question cropping up across many of these discussions, usually phrased along the lines of \u0026#34;is what we\u0026#39;re currently doing \u003ci\u003e\u0026#39;SRE work\u0026#39;?\u0026#34;\u003c/i\u003e or, with a little more existential dread, \u0026#34;can we call ourselves SREs yet?\u0026#34;\u003c/p\u003e\u003cp\u003eWe\u0026#39;ve answered this question before with a \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey\" track-metadata-module=\"post\"\u003elist of practices\u003c/a\u003e from the \u003ca href=\"https://sre.google/workbook/table-of-contents/\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSRE workbook\u003c/a\u003e. But the list is long on the \u003ci\u003ewhat\u003c/i\u003e and short on the \u003ci\u003ewhy\u003c/i\u003e, which can make it hard to digest for folks already suffering an identity crisis. Instead, we hope to help answer this question by discussing some principles we consider fundamental to how an SRE team operates. We\u0026#39;ll examine why they\u0026#39;re important and suggest questions that characterize a team\u0026#39;s progress towards embodying them. \u003c/p\u003e\u003ch2\u003eAre we there yet?\u003c/h2\u003e\u003cp\u003eThis question is asked in different ways, for a myriad of different reasons, and it can be quite hard to answer due to the wide range of \u003ca href=\"https://web.devopstopologies.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://web.devopstopologies.com\" track-metadata-module=\"post\"\u003edifferent circumstances\u003c/a\u003e that our customers operate in. Moreover, CRE, and Google in general, is not the final arbiter of what is and isn\u0026#39;t \u0026#34;SRE\u0026#34; for your organization, so we can\u0026#39;t provide an authoritative answer, if one even exists. We can only influence you and the community at large by expressing our opinions and experiences, in person or via our \u003ca href=\"https://sre.google/books/\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003ebooks\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/blog/topics/cre-life-lessons\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/cre-life-lessons\" track-metadata-module=\"post\"\u003eblog posts\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eFurther, discussions of this topic tend to be complicated by the fact that the term \u0026#34;SRE\u0026#34; is used interchangeably to mean three things:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eA \u003cb\u003ejob role\u003c/b\u003e primarily focused on maintaining the reliability of a service or product.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA \u003cb\u003egroup of people \u003c/b\u003eworking within an organization, usually in the above job role.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA \u003cb\u003eset of principles and practices\u003c/b\u003e that the above people can utilize to improve service reliability.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eWhen people ask \u0026#34;can we call ourselves SREs yet?\u0026#34; we could interpret it as a desire to link these three definitions together. A clearer restatement of this interpretation might be: \u0026#34;Is our \u003cb\u003egroup\u003c/b\u003e sufficiently advanced in our application of the \u003cb\u003eprinciples and practices\u003c/b\u003e that we can justifiably term our \u003cb\u003ejob role\u003c/b\u003e SRE?\u0026#34; \u003c/p\u003e\u003cp\u003eWe should stress that we\u0026#39;re not saying that you need a clearly defined \u003cb\u003ejob role\u003c/b\u003e—or even a \u003cb\u003eteam\u003c/b\u003e—before you can start utilizing the \u003cb\u003eprinciples and practices\u003c/b\u003e to do things that are recognizably SRE-like. Job roles and teams crystallize from a more fluid set of responsibilities as organizations grow larger. But as this process plays out, the people involved may feel less certain of the scope of their responsibilities, precipitating the ‘are we there yet?’ question. We suspect that\u0026#39;s where the tone of existential dread comes from...\u003c/p\u003e\u003ch2\u003eKey SRE indicators\u003c/h2\u003e\u003cp\u003eWithin the CRE team here at Google Cloud, the ‘are we there yet?’ question surfaced a wide variety of opinions about the core principles that should guide an SRE team. We did manage to reach a rough consensus, with one proviso—the answer is partially dependent on how a team engages with the services it supports.\u003c/p\u003e\u003cp\u003eWe\u0026#39;ve chosen to structure this post around a set of principles that we would broadly expect groups of people working as SREs that \u003ci\u003edirectly support services in production\u003c/i\u003e to adhere to. As in a \u003ca href=\"https://en.wikipedia.org/wiki/Litmus#Uses\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://en.wikipedia.org\" track-metadata-module=\"post\"\u003elitmus test\u003c/a\u003e, this won\u0026#39;t provide pin-point accuracy; but in our collective opinion at least, alignment with most of the principles laid out below is a good signal that a team is practicing something that can recognizably be termed \u003ci\u003eSite Reliability Engineering.\u003c/i\u003e\u003c/p\u003e\u003cp\u003eDirectly engaged SRE teams are usually considered \u003ci\u003eAccountable\u003c/i\u003e (in \u003ca href=\"https://en.wikipedia.org/wiki/Responsibility_assignment_matrix\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://en.wikipedia.org\" track-metadata-module=\"post\"\u003eRACI terms\u003c/a\u003e) for the service’s reliability, with \u003ci\u003eResponsibility\u003c/i\u003e shared between the SRE and development teams. As a team provides less direct support these indicators may be less applicable. We hope those teams can still adapt the principles to their own circumstances. \u003c/p\u003e\u003cp\u003eTo illustrate how you might do this, for each principle we\u0026#39;ve given a counter-example of a team of SREs operating in an advisory capacity. They\u0026#39;re subject-matter experts who are \u003ci\u003eConsulted\u003c/i\u003e by development teams who are themselves \u003ci\u003eResponsible\u003c/i\u003e and \u003ci\u003eAccountable\u003c/i\u003e for service reliability.\u003c/p\u003e\u003cp\u003eWherever your engagement model lies on the spectrum, being perceived by the rest of the organization as jointly responsible for a service\u0026#39;s reliability, or as reliability subject-matter experts, is a key indicator of SRE-hood.\u003c/p\u003e\u003ch3\u003ePrinciple #1: SREs mitigate present and \u003ci\u003efuture\u003c/i\u003e incidents\u003c/h3\u003e\u003cp\u003eThis principle is the one that usually underlies the perception of \u003cb\u003eresponsibility and accountability\u003c/b\u003e for a service\u0026#39;s reliability. All the careful engineering and active regulation in the world can\u0026#39;t guarantee reliability, especially in complex distributed systems—sometimes, things go wrong unexpectedly and the only thing left to do is react, mitigate, and fix. SREs have both the authority and the technical capability to act fast to restore service in these situations.\u003c/p\u003e\u003cp\u003eBut mitigating the immediate problem isn\u0026#39;t enough. If it can happen again tomorrow, then tomorrow isn\u0026#39;t better than today, so SREs should work to understand the precipitating factors of incidents and propose changes that remediate the entire class of problem in the infrastructure they are responsible for. Don\u0026#39;t have the same outage again next month!\u003c/p\u003e\u003cp\u003eHow unique are your outages? Ask yourself these questions:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eCan you mitigate the majority of the incidents without needing specialist knowledge from the development team?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you maintain training materials and practice incident response scenarios?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAfter a major outage happens to your service, are you a key participant in blamelessly figuring out what \u003cb\u003ereally\u003c/b\u003e went wrong, and how to prevent future outages?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, for a counter example. In many organizations, SREs are a scarce resource and may add more value by developing platforms and best practices to uplift large swathes of the company, rather than being primarily focused on incident response. Thus, a consulting SRE team would probably not be directly involved in mitigating most incidents, though they may be called on to coordinate incident response for a widespread outage. Rather than authoring training materials and postmortems, they would be responsible for reviewing those created by the teams they advise.\u003c/p\u003e\u003ch3\u003ePrinciple #2: SREs actively regulate service reliability\u003c/h3\u003e\u003cp\u003e\u003cb\u003eReliability goals and feedback signals are fundamental for both motivating SRE work and influencing the prioritization of development work.\u003c/b\u003e At Google, we call our reliability goals \u003ca href=\"https://sre.google/sre-book/service-level-objectives/\" target=\"_blank\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003e\u003ci\u003eService Level Objectives\u003c/i\u003e\u003c/a\u003e and our feedback signals \u003ci\u003eError Budgets\u003c/i\u003e, and you can read more about how we use them in the \u003ca href=\"https://sre.google/workbook/implementing-slos/\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSite Reliability Workbook\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDo your reliability signals affect your organization\u0026#39;s priorities? Ask yourself these questions:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDo you agree on goals for the reliability of the services you support with your organization, and track performance against those goals in real time?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you have an established feedback loop that moderates the behaviour of the organization based on recent service reliability?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you have the influence to effect change within the organization in pursuit of the reliability goals?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you have the \u003ca href=\"https://en.wikipedia.org/wiki/Agency_(sociology)\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://en.wikipedia.org\" track-metadata-module=\"post\"\u003eagency\u003c/a\u003e to refuse, or negotiate looser goals, when asked to make changes that may cause a service to miss its current reliability goals?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEach question builds on the last. It is almost impossible to establish a data-driven feedback loop without a well-defined and measured reliability goal. For those goals to be meaningful, SREs must have the capability to defend them. Periods of lower service reliability should result in consequences that temporarily reduce the aggregate risk of future production changes and shift engineering priorities towards reliability. \u003c/p\u003e\u003cp\u003eWhen it comes down to a choice between service reliability and the rollout of new but unreliable features, SREs need to be able to say \u0026#34;no\u0026#34;. This should be a data driven decision—when there\u0026#39;s not enough spare error budget, there needs to be a valid business reason for making users unhappy. Sometimes, of course, there will be, and this can be accommodated with new, lower SLO targets that reflect the relaxed reliability requirements.\u003c/p\u003e\u003cp\u003eConsultant SREs, in contrast, help teams draft their reliability goals and may develop shared monitoring infrastructure for measuring them across the organization. They are the de-facto regulators of the reliability feedback loop and maintain the policy documents that underpin it. Their connection to many teams and services gives them broader insights that can spark cross-functional reliability improvements. \u003c/p\u003e\u003ch3\u003ePrinciple #3: SREs engage early and comprehensively\u003c/h3\u003e\u003cp\u003eAs we said earlier, \u003cb\u003eSREs should be empowered to make tomorrow better than today.\u003c/b\u003e Without the ability to change the code and configuration of the services they support, they cannot fix problems as they encounter them. Involving SREs earlier in the design process can head off common reliability anti-patterns that are costly to correct \u003ci\u003epost-facto\u003c/i\u003e. And, with the ability to influence architectural decision making, SREs can drive convergence across an organization so that work to increase the reliability of one service can benefit the entire company.\u003c/p\u003e\u003cp\u003eIs your team actively working to make tomorrow better than today? Ask yourself these questions, which go from fine detail to a broad, high-level scope:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDo you engineer your service \u003cb\u003enow\u003c/b\u003e to improve its reliability, e.g. by viewing and modifying the source code and/or configuration?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAre you involved in analysis and design of \u003cb\u003efuture\u003c/b\u003e iterations of your service, providing a lens on reliability/operability/maintainability?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCan you influence your \u003cb\u003eorganization’s\u003c/b\u003e wider architectural decision making?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAdvising other teams naturally shifts priorities away from directly modifying the configuration or code of individual services. But consultant SREs may still maintain frameworks or shared libraries providing core reliability features, like exporting common metrics or graceful service degradation. Their breadth of engagements across many teams makes them naturally suited for providing high-level architectural advice to improve reliability across an entire organization.\u003c/p\u003e\u003ch3\u003ePrinciple #4: SREs automate anything repetitive\u003c/h3\u003e\u003cp\u003eFinally, SREs believe that computers are fundamentally better suited to doing repetitive work than humans are. People often underestimate the \u003ca href=\"https://xkcd.com/1205/\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://xkcd.com\" track-metadata-module=\"post\"\u003ereturns on investment\u003c/a\u003e when considering whether to automate a routine task, and that\u0026#39;s before factoring in the exponential growth curve that comes with running a large, successful service. Moreover, computers never become inattentive and make mistakes when doing the same task for the hundredth time, or become demoralized and quit. Hiring or \u003ca href=\"https://sre.google/resources/practices-and-processes/training-site-reliability-engineers/\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003etraining SREs\u003c/a\u003e is expensive and time-consuming, so a successful SRE organization depends heavily on making computers do the grunt work.\u003c/p\u003e\u003cp\u003eAre you sufficiently automating your work? Ask yourself these questions:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDo you use—or create—automation and other tools to ensure that operational load won\u0026#39;t scale linearly with organic growth or the number of services you support?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you try to measure repetitive work on your team, and reduce it over time?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eA call for reflection\u003c/h2\u003e\u003cp\u003eMost blog posts end with a call for action. We\u0026#39;d rather you took time to reflect, instead of jumping up to make changes straight after reading. \u003cb\u003eThere\u0026#39;s a risk, when writing an opinionated piece like this, that the lines drawn in the sand are used to divide, not to grow and improve.\u003c/b\u003e We promise this isn\u0026#39;t a deliberate effort to gatekeep SRE and exclude those who don\u0026#39;t tick the boxes; we see no value in that. But in some ways gatekeeping is what job roles are \u003ci\u003edesigned\u003c/i\u003e to do, because specialization and the division of labour is critical to the success of any organization, and this makes it hard to avoid drawing those lines.\u003c/p\u003e\u003cp\u003eFor those who aspire to call themselves SREs, or are concerned that others may disagree with their characterization of themselves as SREs, perhaps these opinions can assuage some of that existential dread.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOne facet of our work as \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\"\u003eCustomer Reliability Engineers\u003c/a\u003e—Google Site Reliability Engineers (SREs) tapped to help Google Cloud customers develop that practice in their own organizations—is advising operations or SRE teams to improve their operational maturity. We've noticed a recurring question cropping up across many of these discussions, usually phrased along the lines of \"is what we're currently doing \u003ci\u003e'SRE work'?\"\u003c/i\u003e or, with a little more existential dread, \"can we call ourselves SREs yet?\"\u003c/p\u003e\u003cp\u003eWe've answered this question before with a \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey\"\u003elist of practices\u003c/a\u003e from the \u003ca href=\"https://sre.google/workbook/table-of-contents/\" target=\"_blank\"\u003eSRE workbook\u003c/a\u003e. But the list is long on the \u003ci\u003ewhat\u003c/i\u003e and short on the \u003ci\u003ewhy\u003c/i\u003e, which can make it hard to digest for folks already suffering an identity crisis. Instead, we hope to help answer this question by discussing some principles we consider fundamental to how an SRE team operates. We'll examine why they're important and suggest questions that characterize a team's progress towards embodying them. \u003c/p\u003e\u003ch2\u003eAre we there yet?\u003c/h2\u003e\u003cp\u003eThis question is asked in different ways, for a myriad of different reasons, and it can be quite hard to answer due to the wide range of \u003ca href=\"https://web.devopstopologies.com/\" target=\"_blank\"\u003edifferent circumstances\u003c/a\u003e that our customers operate in. Moreover, CRE, and Google in general, is not the final arbiter of what is and isn't \"SRE\" for your organization, so we can't provide an authoritative answer, if one even exists. We can only influence you and the community at large by expressing our opinions and experiences, in person or via our \u003ca href=\"https://sre.google/books/\" target=\"_blank\"\u003ebooks\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/blog/topics/cre-life-lessons\"\u003eblog posts\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eFurther, discussions of this topic tend to be complicated by the fact that the term \"SRE\" is used interchangeably to mean three things:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eA \u003cb\u003ejob role\u003c/b\u003e primarily focused on maintaining the reliability of a service or product.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA \u003cb\u003egroup of people\u003c/b\u003eworking within an organization, usually in the above job role.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eA \u003cb\u003eset of principles and practices\u003c/b\u003e that the above people can utilize to improve service reliability.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eWhen people ask \"can we call ourselves SREs yet?\" we could interpret it as a desire to link these three definitions together. A clearer restatement of this interpretation might be: \"Is our \u003cb\u003egroup\u003c/b\u003e sufficiently advanced in our application of the \u003cb\u003eprinciples and practices\u003c/b\u003e that we can justifiably term our \u003cb\u003ejob role\u003c/b\u003e SRE?\" \u003c/p\u003e\u003cp\u003eWe should stress that we're not saying that you need a clearly defined \u003cb\u003ejob role\u003c/b\u003e—or even a \u003cb\u003eteam\u003c/b\u003e—before you can start utilizing the \u003cb\u003eprinciples and practices\u003c/b\u003e to do things that are recognizably SRE-like. Job roles and teams crystallize from a more fluid set of responsibilities as organizations grow larger. But as this process plays out, the people involved may feel less certain of the scope of their responsibilities, precipitating the ‘are we there yet?’ question. We suspect that's where the tone of existential dread comes from...\u003c/p\u003e\u003ch2\u003eKey SRE indicators\u003c/h2\u003e\u003cp\u003eWithin the CRE team here at Google Cloud, the ‘are we there yet?’ question surfaced a wide variety of opinions about the core principles that should guide an SRE team. We did manage to reach a rough consensus, with one proviso—the answer is partially dependent on how a team engages with the services it supports.\u003c/p\u003e\u003cp\u003eWe've chosen to structure this post around a set of principles that we would broadly expect groups of people working as SREs that \u003ci\u003edirectly support services in production\u003c/i\u003e to adhere to. As in a \u003ca href=\"https://en.wikipedia.org/wiki/Litmus#Uses\" target=\"_blank\"\u003elitmus test\u003c/a\u003e, this won't provide pin-point accuracy; but in our collective opinion at least, alignment with most of the principles laid out below is a good signal that a team is practicing something that can recognizably be termed \u003ci\u003eSite Reliability Engineering.\u003c/i\u003e\u003c/p\u003e\u003cp\u003eDirectly engaged SRE teams are usually considered \u003ci\u003eAccountable\u003c/i\u003e (in \u003ca href=\"https://en.wikipedia.org/wiki/Responsibility_assignment_matrix\" target=\"_blank\"\u003eRACI terms\u003c/a\u003e) for the service’s reliability, with \u003ci\u003eResponsibility\u003c/i\u003e shared between the SRE and development teams. As a team provides less direct support these indicators may be less applicable. We hope those teams can still adapt the principles to their own circumstances. \u003c/p\u003e\u003cp\u003eTo illustrate how you might do this, for each principle we've given a counter-example of a team of SREs operating in an advisory capacity. They're subject-matter experts who are \u003ci\u003eConsulted\u003c/i\u003e by development teams who are themselves \u003ci\u003eResponsible\u003c/i\u003e and \u003ci\u003eAccountable\u003c/i\u003e for service reliability.\u003c/p\u003e\u003cp\u003eWherever your engagement model lies on the spectrum, being perceived by the rest of the organization as jointly responsible for a service's reliability, or as reliability subject-matter experts, is a key indicator of SRE-hood.\u003c/p\u003e\u003ch3\u003ePrinciple #1: SREs mitigate present and \u003ci\u003efuture\u003c/i\u003e incidents\u003c/h3\u003e\u003cp\u003eThis principle is the one that usually underlies the perception of \u003cb\u003eresponsibility and accountability\u003c/b\u003e for a service's reliability. All the careful engineering and active regulation in the world can't guarantee reliability, especially in complex distributed systems—sometimes, things go wrong unexpectedly and the only thing left to do is react, mitigate, and fix. SREs have both the authority and the technical capability to act fast to restore service in these situations.\u003c/p\u003e\u003cp\u003eBut mitigating the immediate problem isn't enough. If it can happen again tomorrow, then tomorrow isn't better than today, so SREs should work to understand the precipitating factors of incidents and propose changes that remediate the entire class of problem in the infrastructure they are responsible for. Don't have the same outage again next month!\u003c/p\u003e\u003cp\u003eHow unique are your outages? Ask yourself these questions:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eCan you mitigate the majority of the incidents without needing specialist knowledge from the development team?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you maintain training materials and practice incident response scenarios?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAfter a major outage happens to your service, are you a key participant in blamelessly figuring out what \u003cb\u003ereally\u003c/b\u003e went wrong, and how to prevent future outages?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eNow, for a counter example. In many organizations, SREs are a scarce resource and may add more value by developing platforms and best practices to uplift large swathes of the company, rather than being primarily focused on incident response. Thus, a consulting SRE team would probably not be directly involved in mitigating most incidents, though they may be called on to coordinate incident response for a widespread outage. Rather than authoring training materials and postmortems, they would be responsible for reviewing those created by the teams they advise.\u003c/p\u003e\u003ch3\u003ePrinciple #2: SREs actively regulate service reliability\u003c/h3\u003e\u003cp\u003e\u003cb\u003eReliability goals and feedback signals are fundamental for both motivating SRE work and influencing the prioritization of development work.\u003c/b\u003e At Google, we call our reliability goals \u003ca href=\"https://sre.google/sre-book/service-level-objectives/\" target=\"_blank\"\u003e\u003ci\u003eService Level Objectives\u003c/i\u003e\u003c/a\u003e and our feedback signals \u003ci\u003eError Budgets\u003c/i\u003e, and you can read more about how we use them in the \u003ca href=\"https://sre.google/workbook/implementing-slos/\" target=\"_blank\"\u003eSite Reliability Workbook\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDo your reliability signals affect your organization's priorities? Ask yourself these questions:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDo you agree on goals for the reliability of the services you support with your organization, and track performance against those goals in real time?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you have an established feedback loop that moderates the behaviour of the organization based on recent service reliability?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you have the influence to effect change within the organization in pursuit of the reliability goals?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you have the \u003ca href=\"https://en.wikipedia.org/wiki/Agency_(sociology)\" target=\"_blank\"\u003eagency\u003c/a\u003e to refuse, or negotiate looser goals, when asked to make changes that may cause a service to miss its current reliability goals?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eEach question builds on the last. It is almost impossible to establish a data-driven feedback loop without a well-defined and measured reliability goal. For those goals to be meaningful, SREs must have the capability to defend them. Periods of lower service reliability should result in consequences that temporarily reduce the aggregate risk of future production changes and shift engineering priorities towards reliability. \u003c/p\u003e\u003cp\u003eWhen it comes down to a choice between service reliability and the rollout of new but unreliable features, SREs need to be able to say \"no\". This should be a data driven decision—when there's not enough spare error budget, there needs to be a valid business reason for making users unhappy. Sometimes, of course, there will be, and this can be accommodated with new, lower SLO targets that reflect the relaxed reliability requirements.\u003c/p\u003e\u003cp\u003eConsultant SREs, in contrast, help teams draft their reliability goals and may develop shared monitoring infrastructure for measuring them across the organization. They are the de-facto regulators of the reliability feedback loop and maintain the policy documents that underpin it. Their connection to many teams and services gives them broader insights that can spark cross-functional reliability improvements. \u003c/p\u003e\u003ch3\u003ePrinciple #3: SREs engage early and comprehensively\u003c/h3\u003e\u003cp\u003eAs we said earlier, \u003cb\u003eSREs should be empowered to make tomorrow better than today.\u003c/b\u003e Without the ability to change the code and configuration of the services they support, they cannot fix problems as they encounter them. Involving SREs earlier in the design process can head off common reliability anti-patterns that are costly to correct \u003ci\u003epost-facto\u003c/i\u003e. And, with the ability to influence architectural decision making, SREs can drive convergence across an organization so that work to increase the reliability of one service can benefit the entire company.\u003c/p\u003e\u003cp\u003eIs your team actively working to make tomorrow better than today? Ask yourself these questions, which go from fine detail to a broad, high-level scope:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDo you engineer your service \u003cb\u003enow\u003c/b\u003e to improve its reliability, e.g. by viewing and modifying the source code and/or configuration?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAre you involved in analysis and design of \u003cb\u003efuture\u003c/b\u003e iterations of your service, providing a lens on reliability/operability/maintainability?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCan you influence your \u003cb\u003eorganization’s\u003c/b\u003e wider architectural decision making?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAdvising other teams naturally shifts priorities away from directly modifying the configuration or code of individual services. But consultant SREs may still maintain frameworks or shared libraries providing core reliability features, like exporting common metrics or graceful service degradation. Their breadth of engagements across many teams makes them naturally suited for providing high-level architectural advice to improve reliability across an entire organization.\u003c/p\u003e\u003ch3\u003ePrinciple #4: SREs automate anything repetitive\u003c/h3\u003e\u003cp\u003eFinally, SREs believe that computers are fundamentally better suited to doing repetitive work than humans are. People often underestimate the \u003ca href=\"https://xkcd.com/1205/\" target=\"_blank\"\u003ereturns on investment\u003c/a\u003e when considering whether to automate a routine task, and that's before factoring in the exponential growth curve that comes with running a large, successful service. Moreover, computers never become inattentive and make mistakes when doing the same task for the hundredth time, or become demoralized and quit. Hiring or \u003ca href=\"https://sre.google/resources/practices-and-processes/training-site-reliability-engineers/\" target=\"_blank\"\u003etraining SREs\u003c/a\u003e is expensive and time-consuming, so a successful SRE organization depends heavily on making computers do the grunt work.\u003c/p\u003e\u003cp\u003eAre you sufficiently automating your work? Ask yourself these questions:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDo you use—or create—automation and other tools to ensure that operational load won't scale linearly with organic growth or the number of services you support?\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDo you try to measure repetitive work on your team, and reduce it over time?\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eA call for reflection\u003c/h2\u003e\u003cp\u003eMost blog posts end with a call for action. We'd rather you took time to reflect, instead of jumping up to make changes straight after reading. \u003cb\u003eThere's a risk, when writing an opinionated piece like this, that the lines drawn in the sand are used to divide, not to grow and improve.\u003c/b\u003e We promise this isn't a deliberate effort to gatekeep SRE and exclude those who don't tick the boxes; we see no value in that. But in some ways gatekeeping is what job roles are \u003ci\u003edesigned\u003c/i\u003e to do, because specialization and the division of labour is critical to the success of any organization, and this makes it hard to avoid drawing those lines.\u003c/p\u003e\u003cp\u003eFor those who aspire to call themselves SREs, or are concerned that others may disagree with their characterization of themselves as SREs, perhaps these opinions can assuage some of that existential dread.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps.max-2200x2200.jpg",
      "date_published": "2021-06-18T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eAlex Bramley\u003c/name\u003e\u003ctitle\u003eCustomer Reliability Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/application-development/artifact-registry-adds-node-python-and-java-repositories/",
      "title": "Node, Python and Java repositories now available in Artifact Registry",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;As a developer, you need a secure place to store all your stuff: container images of course, but also language packages that can enable code reuse across multiple applications. Today, we\u0026amp;#8217;re pleased to announce support for Node.js, Python and Java repositories for \u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry\u0026#34;\u0026gt;Artifact Registry\u0026lt;/a\u0026gt; in Preview. With today\u0026amp;#8217;s announcement, you can not only use Artifact Registry to secure and distribute container images, but also manage and secure your other software artifacts.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;At the same time, the Artifact Registry managed service provides advantages over on-premises registries. As a fully serverless platform, it scales based on demand, so you only pay for what you actually use. Enterprise security features such as VPC-SC, CMEK, and granular IAM ensure you get greater control and security features for both container and non-container artifacts. You can also connect to tools you are already using as a part of a CI/CD workflow.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s take a closer look at the features you\u0026amp;#8217;ll find in Artifact Registry, giving you a fully-managed tool to store, manage, and secure all your artifacts.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Expanded repository formats\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;With support for new repository formats, you can streamline and get a consistent view across all your artifacts. Now, supported artifacts include:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/java\u0026#34;\u0026gt;Java\u0026lt;/a\u0026gt; packages\u0026amp;#160; (using the Maven repository format)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/nodejs\u0026#34;\u0026gt;Node.js\u0026lt;/a\u0026gt; packages (using the npm repository format)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/python\u0026#34;\u0026gt;Python\u0026lt;/a\u0026gt; packages (using the PyPI repository format)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;In addition to existing container images and Helm charts (using the Docker repository format).\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Easy integration with your CI/CD toolchain\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;You can also integrate Artifact Registry, including the new repository formats, with Google Cloud\u0026amp;#8217;s build and runtime services or your existing build system. The following are just some of the use cases that are made possible by this integration:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Deployment to Google Kubernetes Engine (GKE), Cloud Run, Compute Engine and other runtime services\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;CI/CD with Cloud Build, with automatic vulnerability scanning for OCI images\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Compatibility with Jenkins, Circle CI, TeamCity and other CI tools\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Native support for \u0026lt;a href=\u0026#34;https://cloud.google.com/binary-authorization\u0026#34;\u0026gt;Binary Authorization\u0026lt;/a\u0026gt; to ensure only approved artifact images are deployed\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Storage and management of artifacts in a variety of formats\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Streamlined authentication and access control across repositories using Google Cloud IAM\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;A more secure software supply chain\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Storing trusted artifacts in private repositories is a key part of a secure software supply chain and helps mitigate the risks associated with using artifacts directly from public repositories. With Artifact Registry, you can:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Scan container images for vulnerabilities\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Protect repositories via a security perimeter (VPC-SC support)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Configure access control at the repository level using Cloud IAM\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Use customer managed encryption keys (CMEK) instead of the default Google-managed encryption\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Use Cloud Audit Logging to track and review repository usage\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Optimize your infrastructure and maintain data compliance\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Artifact Registry provides regional support, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Get started today\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;These new features are available to all Artifact Registry customers. Pricing for language packages is the same as container pricing; see the \u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/pricing\u0026#34;\u0026gt;pricing documentation\u0026lt;/a\u0026gt; for details.To get started using Node.js, Python and Java repositories, try the quickstarts in the Artifact Registry documentation.\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/nodejs\u0026#34;\u0026gt;Node.js\u0026lt;/a\u0026gt; Quickstart Guide\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/python\u0026#34;\u0026gt;Python\u0026lt;/a\u0026gt; Quickstart Guide\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/artifact-registry/docs/java\u0026#34;\u0026gt;Java\u0026lt;/a\u0026gt; Quickstart Guide\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://youtu.be/2-P4cSCk1VM\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Video Overview: using Maven in Artifact Registry\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\"\u003e\u003cp\u003eAs a developer, you need a secure place to store all your stuff: container images of course, but also language packages that can enable code reuse across multiple applications. Today, we’re pleased to announce support for Node.js, Python and Java repositories for \u003ca href=\"https://cloud.google.com/artifact-registry\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry\" track-metadata-module=\"post\"\u003eArtifact Registry\u003c/a\u003e in Preview. With today’s announcement, you can not only use Artifact Registry to secure and distribute container images, but also manage and secure your other software artifacts. \u003c/p\u003e\u003cp\u003eAt the same time, the Artifact Registry managed service provides advantages over on-premises registries. As a fully serverless platform, it scales based on demand, so you only pay for what you actually use. Enterprise security features such as VPC-SC, CMEK, and granular IAM ensure you get greater control and security features for both container and non-container artifacts. You can also connect to tools you are already using as a part of a CI/CD workflow. \u003c/p\u003e\u003cp\u003eLet’s take a closer look at the features you’ll find in Artifact Registry, giving you a fully-managed tool to store, manage, and secure all your artifacts. \u003c/p\u003e\u003ch3\u003eExpanded repository formats\u003c/h3\u003e\u003cp\u003eWith support for new repository formats, you can streamline and get a consistent view across all your artifacts. Now, supported artifacts include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/java\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/java\" track-metadata-module=\"post\"\u003eJava\u003c/a\u003e packages  (using the Maven repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/nodejs\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/nodejs\" track-metadata-module=\"post\"\u003eNode.js\u003c/a\u003e packages (using the npm repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/python\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/python\" track-metadata-module=\"post\"\u003ePython\u003c/a\u003e packages (using the PyPI repository format)\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to existing container images and Helm charts (using the Docker repository format). \u003c/p\u003e\u003ch3\u003eEasy integration with your CI/CD toolchain\u003c/h3\u003e\u003cp\u003eYou can also integrate Artifact Registry, including the new repository formats, with Google Cloud’s build and runtime services or your existing build system. The following are just some of the use cases that are made possible by this integration:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDeployment to Google Kubernetes Engine (GKE), Cloud Run, Compute Engine and other runtime services \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCI/CD with Cloud Build, with automatic vulnerability scanning for OCI images \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCompatibility with Jenkins, Circle CI, TeamCity and other CI tools \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNative support for \u003ca href=\"https://cloud.google.com/binary-authorization\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/binary-authorization\" track-metadata-module=\"post\"\u003eBinary Authorization\u003c/a\u003e to ensure only approved artifact images are deployed\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eStorage and management of artifacts in a variety of formats\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eStreamlined authentication and access control across repositories using Google Cloud IAM\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eA more secure software supply chain\u003c/h3\u003e\u003cp\u003eStoring trusted artifacts in private repositories is a key part of a secure software supply chain and helps mitigate the risks associated with using artifacts directly from public repositories. With Artifact Registry, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eScan container images for vulnerabilities\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eProtect repositories via a security perimeter (VPC-SC support)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eConfigure access control at the repository level using Cloud IAM\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse customer managed encryption keys (CMEK) instead of the default Google-managed encryption\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse Cloud Audit Logging to track and review repository usage\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eOptimize your infrastructure and maintain data compliance\u003c/h3\u003e\u003cp\u003eArtifact Registry provides regional support, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.\u003c/p\u003e\u003ch2\u003eGet started today\u003c/h2\u003e\u003cp\u003eThese new features are available to all Artifact Registry customers. Pricing for language packages is the same as container pricing; see the \u003ca href=\"https://cloud.google.com/artifact-registry/pricing\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/pricing\" track-metadata-module=\"post\"\u003epricing documentation\u003c/a\u003e for details.To get started using Node.js, Python and Java repositories, try the quickstarts in the Artifact Registry documentation.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/nodejs\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/nodejs\" track-metadata-module=\"post\"\u003eNode.js\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/python\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/python\" track-metadata-module=\"post\"\u003ePython\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/java\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/artifact-registry/docs/java\" track-metadata-module=\"post\"\u003eJava\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://youtu.be/2-P4cSCk1VM\" target=\"_blank\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://youtu.be\" track-metadata-module=\"post\"\u003eVideo Overview: using Maven in Artifact Registry\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAs a developer, you need a secure place to store all your stuff: container images of course, but also language packages that can enable code reuse across multiple applications. Today, we’re pleased to announce support for Node.js, Python and Java repositories for \u003ca href=\"https://cloud.google.com/artifact-registry\"\u003eArtifact Registry\u003c/a\u003e in Preview. With today’s announcement, you can not only use Artifact Registry to secure and distribute container images, but also manage and secure your other software artifacts. \u003c/p\u003e\u003cp\u003eAt the same time, the Artifact Registry managed service provides advantages over on-premises registries. As a fully serverless platform, it scales based on demand, so you only pay for what you actually use. Enterprise security features such as VPC-SC, CMEK, and granular IAM ensure you get greater control and security features for both container and non-container artifacts. You can also connect to tools you are already using as a part of a CI/CD workflow. \u003c/p\u003e\u003cp\u003eLet’s take a closer look at the features you’ll find in Artifact Registry, giving you a fully-managed tool to store, manage, and secure all your artifacts. \u003c/p\u003e\u003ch3\u003eExpanded repository formats\u003c/h3\u003e\u003cp\u003eWith support for new repository formats, you can streamline and get a consistent view across all your artifacts. Now, supported artifacts include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/java\"\u003eJava\u003c/a\u003e packages  (using the Maven repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/nodejs\"\u003eNode.js\u003c/a\u003e packages (using the npm repository format)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/python\"\u003ePython\u003c/a\u003e packages (using the PyPI repository format)\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn addition to existing container images and Helm charts (using the Docker repository format). \u003c/p\u003e\u003ch3\u003eEasy integration with your CI/CD toolchain\u003c/h3\u003e\u003cp\u003eYou can also integrate Artifact Registry, including the new repository formats, with Google Cloud’s build and runtime services or your existing build system. The following are just some of the use cases that are made possible by this integration:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eDeployment to Google Kubernetes Engine (GKE), Cloud Run, Compute Engine and other runtime services \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCI/CD with Cloud Build, with automatic vulnerability scanning for OCI images \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCompatibility with Jenkins, Circle CI, TeamCity and other CI tools \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eNative support for \u003ca href=\"https://cloud.google.com/binary-authorization\"\u003eBinary Authorization\u003c/a\u003e to ensure only approved artifact images are deployed\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eStorage and management of artifacts in a variety of formats\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eStreamlined authentication and access control across repositories using Google Cloud IAM\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eA more secure software supply chain\u003c/h3\u003e\u003cp\u003eStoring trusted artifacts in private repositories is a key part of a secure software supply chain and helps mitigate the risks associated with using artifacts directly from public repositories. With Artifact Registry, you can:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eScan container images for vulnerabilities\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eProtect repositories via a security perimeter (VPC-SC support)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eConfigure access control at the repository level using Cloud IAM\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse customer managed encryption keys (CMEK) instead of the default Google-managed encryption\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse Cloud Audit Logging to track and review repository usage\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eOptimize your infrastructure and maintain data compliance\u003c/h3\u003e\u003cp\u003eArtifact Registry provides regional support, enabling you to manage and host artifacts in the regions where your deployments occur, reducing latency and cost. By implementing regional repositories, you can also comply with your local data sovereignty and security requirements.\u003c/p\u003e\u003ch2\u003eGet started today\u003c/h2\u003e\u003cp\u003eThese new features are available to all Artifact Registry customers. Pricing for language packages is the same as container pricing; see the \u003ca href=\"https://cloud.google.com/artifact-registry/pricing\"\u003epricing documentation\u003c/a\u003e for details.To get started using Node.js, Python and Java repositories, try the quickstarts in the Artifact Registry documentation.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/nodejs\"\u003eNode.js\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/python\"\u003ePython\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/artifact-registry/docs/java\"\u003eJava\u003c/a\u003e Quickstart Guide\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://youtu.be/2-P4cSCk1VM\" target=\"_blank\"\u003eVideo Overview: using Maven in Artifact Registry\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/identity-security/how-were-helping-reshape-software-supply-chain-ecosystem-securely/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Security_BlogHeader_B_epmyJP1.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eHow we’re helping to reshape the software supply chain ecosystem securely\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eWe’re sharing some of the security best practices we employ and investments we make in secure software development and supply chain risk ...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Artifact_Registry_ubFsgrO.max-2200x2200.jpg",
      "date_published": "2021-06-07T16:30:00Z",
      "author": {
        "name": "\u003cname\u003ePatrick Faucher\u003c/name\u003e\u003ctitle\u003eSenior Product Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/how-lowes-leverages-google-sre-practices/",
      "title": "How Lowe’s meets customer demand with Google SRE practices",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026lt;b\u0026gt;Editor\u0026amp;#8217;s note:\u0026lt;/b\u0026gt; Today we hear from the Lowe\u0026amp;#8217;s SRE team. They share about how they have been able to increase the number of releases they can support by adopting Google\u0026amp;#8217;s \u0026lt;a href=\u0026#34;https://sre.google/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Site Reliability Engineering\u0026lt;/a\u0026gt; (SRE) framework and leveraging their partnership with Google Cloud.\u0026amp;#160;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;hr\u0026gt;\u0026lt;p\u0026gt;At Lowe\u0026amp;#8217;s, we\u0026amp;#8217;ve made significant progress in our multiyear technology transformation. To modernize our systems and build new capabilities for our customers and associates, we leverage Google\u0026amp;#8217;s SRE framework and Google Cloud, which helps us meet their needs faster and more effectively. With these efforts, we\u0026amp;#8217;ve been able to go from one release every two weeks to 20+ releases daily\u0026amp;#8212;a 300x increase.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Our SRE transformation didn\u0026amp;#8217;t happen overnight, though. Every step along the way brought some challenges. But looking back, we are excited to see how much we have accomplished for our customers as a result.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Back in 2018, before adopting SRE practices, we were more reactive than proactive, following an \u0026amp;#8220;eyes on glass\u0026amp;#8221; approach. On-call structures and incident management efficiency were not at optimal levels with too many repetitive and manual tasks, resulting in operational toil. Production concerns were not surfaced into the product roadmap, which resulted in delays in making fixes.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Bootstrapping SRE at Lowe\u0026amp;#8217;s\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;As we moved from on-prem to \u0026lt;a href=\u0026#34;https://cloud.google.com/\u0026#34;\u0026gt;Google Cloud\u0026lt;/a\u0026gt;, we decided to move from a monolithic- to microservices-based architecture. And to better manage this new architecture, we embarked on an SRE journey.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Then as COVID-19 hit, we really had to accelerate this journey as customers increasingly moved to online ordering and delivery to meet their Total Home Improvement needs. To do so, we followed four key principles that allowed us to meet changing customer needs quickly and release fast and reliably.\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Automate away toil\u0026amp;#160;\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;As we moved from traditional Ops to an SRE ecosystem, our biggest opportunity was \u0026lt;a href=\u0026#34;https://sre.google/sre-book/eliminating-toil/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;reducing toil\u0026lt;/a\u0026gt;, so that engineers can spend time on activities that drive business impact and customer outcomes. We think of toil as work that is manual, repetitive,\u0026amp;#160; tactical, devoid of enduring value\u0026amp;#8212;but automatable. So, to tackle toil, we focused on automating away the need for manual intervention. As an example, we made sure engineers were not the first point of contact for any alert. Any triage or resolution that an engineer can perform, a machine can be trained to do the same. We used supervised and unsupervised learning techniques to automate our toil. With a long-term goal of \u0026amp;#8220;no toil,\u0026amp;#8221; our SREs work on identifying and reducing toil to a manageable level across the organization.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Engineer alignment through roadmaps\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Our goal is to maximize the engineering velocity of developer teams while keeping products reliable. We want an engagement model where product, SRE and development teams are closely aligned. A key way we\u0026amp;#8217;ve been able to create this alignment is by having our SREs embedded into domain and product teams. Each domain has an SRE, who is\u0026amp;#160; involved at the beginning stages of product development to ensure that the domain stakeholders are in alignment with the SRE initiatives. As such, SREs are able to improve the reliability, performance, scalability and launch velocity of the services throughout all phases of the service lifecycle.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Adopt one-touch releases\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Our path to production used to contain many manual steps and validations, slowing the rate at which we released features. Additionally, we used to bulk all our releases together to deploy at once, which increased the risk of failure and created a longer feedback loop from production. To tackle this with an SRE mindset, we created a one-touch release process in which SREs review the product team\u0026amp;#8217;s pull requests. When approved, this triggers a DevSecOps pipeline that deploys the approved changes to production securely. This process created a safe, reliable and sustainable continuous delivery pipeline with quick feedback loops. Striking the right balance between speed, innovation and stability, we were able to increase our releases exponentially for the year, taking less than 30 minutes per release to deliver quality code, including various automated quality checks and processes, all in just one click.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Embrace capacity planning\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;To ensure our services have enough spare capacity to handle any surge in traffic patterns, our SREs emphasize capacity planning, making recommended capacity changes in the continuous delivery (CD) pipeline. They constantly \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring\u0026#34;\u0026gt;monitor performance\u0026lt;/a\u0026gt; to make sure the service is robust, stable and available. And when there\u0026amp;#8217;s a sudden surge beyond the forecasted volume, SREs change the capacity on demand and document changes for the performance and domain teams.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;Capacity planning is especially important for us during peak holiday times such as Black Friday and Cyber Monday (BFCM). We lay out our SRE stability plan three months in advance and surface into the domain team\u0026amp;#8217;s product roadmap. This way development teams are able to allocate sufficient engineering time to reliability. We do performance testing to ensure the environment is able to sustain increased load over long periods of time and also handle sudden surges in traffic. We also do region failover testing at a global scale to validate the automatic failover duration of service level agreements\u0026amp;#160; (SLAs), SRE and domain readiness. Additionally, we conduct Black Friday and Cyber Monday-specific destructive testing to validate customer experience, reliability and more.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Google Cloud\u0026amp;#8217;s Black Friday and Cyber Monday \u0026lt;a href=\u0026#34;https://cloud.google.com/solutions/retail\u0026#34;\u0026gt;white-glove service\u0026lt;/a\u0026gt; played a key role in ensuring our success in both BFCM 2019 and BFCM 2020. This service included on-site visits from Google\u0026amp;#8217;s Customer Reliability Engineering (CRE) team who reviewed Lowe\u0026amp;#8217;s web architecture, capacity planning, operations practices for event risks, and presented workshops on topics such as incident response best practices.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Looking ahead\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;There is always room for improvement, and at Lowe\u0026amp;#8217;s we aim to continuously improve our SRE practices. One thing that has worked well for us, which we plan to continue, has been our road shows, where senior SRE leads present to other SREs and application domain teams on the latest SRE principles and best practices, and to get input in real-time from them.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/products/operations\u0026#34;\u0026gt;Google\u0026amp;#8217;s tools\u0026lt;/a\u0026gt; and methodology have played an instrumental role in helping reshape our SRE practices and better serve our customers. We look forward to building on the momentum and partnership as we continue our SRE journey at Lowe\u0026amp;#8217;s.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;If you want to learn more about how to adopt SRE best practices on Google Cloud, \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring\u0026#34;\u0026gt;check out our documentation\u0026lt;/a\u0026gt;. If you want to learn more about Google SRE, \u0026lt;a href=\u0026#34;https://sre.google/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;visit our website\u0026lt;/a\u0026gt;. Stay tuned for the next blogs with Lowe\u0026amp;#8217;s discussing how they trained their engineering talent to adopt SRE practices and tooling, and how they improved MTTR using SRE principles.\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s note:\u003c/b\u003e Today we hear from the Lowe’s SRE team. They share about how they have been able to increase the number of releases they can support by adopting Google’s \u003ca href=\"https://sre.google/\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSite Reliability Engineering\u003c/a\u003e (SRE) framework and leveraging their partnership with Google Cloud. \u003c/i\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003eAt Lowe’s, we’ve made significant progress in our multiyear technology transformation. To modernize our systems and build new capabilities for our customers and associates, we leverage Google’s SRE framework and Google Cloud, which helps us meet their needs faster and more effectively. With these efforts, we’ve been able to go from one release every two weeks to 20+ releases daily—a 300x increase. \u003c/p\u003e\u003cp\u003eOur SRE transformation didn’t happen overnight, though. Every step along the way brought some challenges. But looking back, we are excited to see how much we have accomplished for our customers as a result. \u003c/p\u003e\u003cp\u003eBack in 2018, before adopting SRE practices, we were more reactive than proactive, following an “eyes on glass” approach. On-call structures and incident management efficiency were not at optimal levels with too many repetitive and manual tasks, resulting in operational toil. Production concerns were not surfaced into the product roadmap, which resulted in delays in making fixes.\u003c/p\u003e\u003ch3\u003eBootstrapping SRE at Lowe’s\u003c/h3\u003e\u003cp\u003eAs we moved from on-prem to \u003ca href=\"https://cloud.google.com/\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/\" track-metadata-module=\"post\"\u003eGoogle Cloud\u003c/a\u003e, we decided to move from a monolithic- to microservices-based architecture. And to better manage this new architecture, we embarked on an SRE journey. \u003c/p\u003e\u003cp\u003eThen as COVID-19 hit, we really had to accelerate this journey as customers increasingly moved to online ordering and delivery to meet their Total Home Improvement needs. To do so, we followed four key principles that allowed us to meet changing customer needs quickly and release fast and reliably.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAutomate away toil \u003cbr/\u003e\u003c/b\u003eAs we moved from traditional Ops to an SRE ecosystem, our biggest opportunity was \u003ca href=\"https://sre.google/sre-book/eliminating-toil/\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003ereducing toil\u003c/a\u003e, so that engineers can spend time on activities that drive business impact and customer outcomes. We think of toil as work that is manual, repetitive,  tactical, devoid of enduring value—but automatable. So, to tackle toil, we focused on automating away the need for manual intervention. As an example, we made sure engineers were not the first point of contact for any alert. Any triage or resolution that an engineer can perform, a machine can be trained to do the same. We used supervised and unsupervised learning techniques to automate our toil. With a long-term goal of “no toil,” our SREs work on identifying and reducing toil to a manageable level across the organization.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eEngineer alignment through roadmaps\u003cbr/\u003e\u003c/b\u003eOur goal is to maximize the engineering velocity of developer teams while keeping products reliable. We want an engagement model where product, SRE and development teams are closely aligned. A key way we’ve been able to create this alignment is by having our SREs embedded into domain and product teams. Each domain has an SRE, who is  involved at the beginning stages of product development to ensure that the domain stakeholders are in alignment with the SRE initiatives. As such, SREs are able to improve the reliability, performance, scalability and launch velocity of the services throughout all phases of the service lifecycle. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAdopt one-touch releases\u003cbr/\u003e\u003c/b\u003eOur path to production used to contain many manual steps and validations, slowing the rate at which we released features. Additionally, we used to bulk all our releases together to deploy at once, which increased the risk of failure and created a longer feedback loop from production. To tackle this with an SRE mindset, we created a one-touch release process in which SREs review the product team’s pull requests. When approved, this triggers a DevSecOps pipeline that deploys the approved changes to production securely. This process created a safe, reliable and sustainable continuous delivery pipeline with quick feedback loops. Striking the right balance between speed, innovation and stability, we were able to increase our releases exponentially for the year, taking less than 30 minutes per release to deliver quality code, including various automated quality checks and processes, all in just one click. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eEmbrace capacity planning\u003cbr/\u003e\u003c/b\u003eTo ensure our services have enough spare capacity to handle any surge in traffic patterns, our SREs emphasize capacity planning, making recommended capacity changes in the continuous delivery (CD) pipeline. They constantly \u003ca href=\"https://cloud.google.com/monitoring\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring\" track-metadata-module=\"post\"\u003emonitor performance\u003c/a\u003e to make sure the service is robust, stable and available. And when there’s a sudden surge beyond the forecasted volume, SREs change the capacity on demand and document changes for the performance and domain teams. \u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eCapacity planning is especially important for us during peak holiday times such as Black Friday and Cyber Monday (BFCM). We lay out our SRE stability plan three months in advance and surface into the domain team’s product roadmap. This way development teams are able to allocate sufficient engineering time to reliability. We do performance testing to ensure the environment is able to sustain increased load over long periods of time and also handle sudden surges in traffic. We also do region failover testing at a global scale to validate the automatic failover duration of service level agreements  (SLAs), SRE and domain readiness. Additionally, we conduct Black Friday and Cyber Monday-specific destructive testing to validate customer experience, reliability and more.\u003c/p\u003e\u003cp\u003eGoogle Cloud’s Black Friday and Cyber Monday \u003ca href=\"https://cloud.google.com/solutions/retail\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/solutions/retail\" track-metadata-module=\"post\"\u003ewhite-glove service\u003c/a\u003e played a key role in ensuring our success in both BFCM 2019 and BFCM 2020. This service included on-site visits from Google’s Customer Reliability Engineering (CRE) team who reviewed Lowe’s web architecture, capacity planning, operations practices for event risks, and presented workshops on topics such as incident response best practices. \u003c/p\u003e\u003ch3\u003eLooking ahead\u003c/h3\u003e\u003cp\u003eThere is always room for improvement, and at Lowe’s we aim to continuously improve our SRE practices. One thing that has worked well for us, which we plan to continue, has been our road shows, where senior SRE leads present to other SREs and application domain teams on the latest SRE principles and best practices, and to get input in real-time from them. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/products/operations\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/products/operations\" track-metadata-module=\"post\"\u003eGoogle’s tools\u003c/a\u003e and methodology have played an instrumental role in helping reshape our SRE practices and better serve our customers. We look forward to building on the momentum and partnership as we continue our SRE journey at Lowe’s. \u003c/p\u003e\u003cp\u003e\u003ci\u003eIf you want to learn more about how to adopt SRE best practices on Google Cloud, \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring\" track-metadata-module=\"post\"\u003echeck out our documentation\u003c/a\u003e. If you want to learn more about Google SRE, \u003ca href=\"https://sre.google/\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003evisit our website\u003c/a\u003e. Stay tuned for the next blogs with Lowe’s discussing how they trained their engineering talent to adopt SRE practices and tooling, and how they improved MTTR using SRE principles.\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s note:\u003c/b\u003e Today we hear from the Lowe’s SRE team. They share about how they have been able to increase the number of releases they can support by adopting Google’s \u003ca href=\"https://sre.google/\" target=\"_blank\"\u003eSite Reliability Engineering\u003c/a\u003e (SRE) framework and leveraging their partnership with Google Cloud. \u003c/i\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003eAt Lowe’s, we’ve made significant progress in our multiyear technology transformation. To modernize our systems and build new capabilities for our customers and associates, we leverage Google’s SRE framework and Google Cloud, which helps us meet their needs faster and more effectively. With these efforts, we’ve been able to go from one release every two weeks to 20+ releases daily—a 300x increase. \u003c/p\u003e\u003cp\u003eOur SRE transformation didn’t happen overnight, though. Every step along the way brought some challenges. But looking back, we are excited to see how much we have accomplished for our customers as a result. \u003c/p\u003e\u003cp\u003eBack in 2018, before adopting SRE practices, we were more reactive than proactive, following an “eyes on glass” approach. On-call structures and incident management efficiency were not at optimal levels with too many repetitive and manual tasks, resulting in operational toil. Production concerns were not surfaced into the product roadmap, which resulted in delays in making fixes.\u003c/p\u003e\u003ch3\u003eBootstrapping SRE at Lowe’s\u003c/h3\u003e\u003cp\u003eAs we moved from on-prem to \u003ca href=\"https://cloud.google.com/\"\u003eGoogle Cloud\u003c/a\u003e, we decided to move from a monolithic- to microservices-based architecture. And to better manage this new architecture, we embarked on an SRE journey. \u003c/p\u003e\u003cp\u003eThen as COVID-19 hit, we really had to accelerate this journey as customers increasingly moved to online ordering and delivery to meet their Total Home Improvement needs. To do so, we followed four key principles that allowed us to meet changing customer needs quickly and release fast and reliably.\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAutomate away toil \u003cbr/\u003e\u003c/b\u003eAs we moved from traditional Ops to an SRE ecosystem, our biggest opportunity was \u003ca href=\"https://sre.google/sre-book/eliminating-toil/\" target=\"_blank\"\u003ereducing toil\u003c/a\u003e, so that engineers can spend time on activities that drive business impact and customer outcomes. We think of toil as work that is manual, repetitive,  tactical, devoid of enduring value—but automatable. So, to tackle toil, we focused on automating away the need for manual intervention. As an example, we made sure engineers were not the first point of contact for any alert. Any triage or resolution that an engineer can perform, a machine can be trained to do the same. We used supervised and unsupervised learning techniques to automate our toil. With a long-term goal of “no toil,” our SREs work on identifying and reducing toil to a manageable level across the organization.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eEngineer alignment through roadmaps\u003cbr/\u003e\u003c/b\u003eOur goal is to maximize the engineering velocity of developer teams while keeping products reliable. We want an engagement model where product, SRE and development teams are closely aligned. A key way we’ve been able to create this alignment is by having our SREs embedded into domain and product teams. Each domain has an SRE, who is  involved at the beginning stages of product development to ensure that the domain stakeholders are in alignment with the SRE initiatives. As such, SREs are able to improve the reliability, performance, scalability and launch velocity of the services throughout all phases of the service lifecycle. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eAdopt one-touch releases\u003cbr/\u003e\u003c/b\u003eOur path to production used to contain many manual steps and validations, slowing the rate at which we released features. Additionally, we used to bulk all our releases together to deploy at once, which increased the risk of failure and created a longer feedback loop from production. To tackle this with an SRE mindset, we created a one-touch release process in which SREs review the product team’s pull requests. When approved, this triggers a DevSecOps pipeline that deploys the approved changes to production securely. This process created a safe, reliable and sustainable continuous delivery pipeline with quick feedback loops. Striking the right balance between speed, innovation and stability, we were able to increase our releases exponentially for the year, taking less than 30 minutes per release to deliver quality code, including various automated quality checks and processes, all in just one click. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eEmbrace capacity planning\u003cbr/\u003e\u003c/b\u003eTo ensure our services have enough spare capacity to handle any surge in traffic patterns, our SREs emphasize capacity planning, making recommended capacity changes in the continuous delivery (CD) pipeline. They constantly \u003ca href=\"https://cloud.google.com/monitoring\"\u003emonitor performance\u003c/a\u003e to make sure the service is robust, stable and available. And when there’s a sudden surge beyond the forecasted volume, SREs change the capacity on demand and document changes for the performance and domain teams. \u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eCapacity planning is especially important for us during peak holiday times such as Black Friday and Cyber Monday (BFCM). We lay out our SRE stability plan three months in advance and surface into the domain team’s product roadmap. This way development teams are able to allocate sufficient engineering time to reliability. We do performance testing to ensure the environment is able to sustain increased load over long periods of time and also handle sudden surges in traffic. We also do region failover testing at a global scale to validate the automatic failover duration of service level agreements  (SLAs), SRE and domain readiness. Additionally, we conduct Black Friday and Cyber Monday-specific destructive testing to validate customer experience, reliability and more.\u003c/p\u003e\u003cp\u003eGoogle Cloud’s Black Friday and Cyber Monday \u003ca href=\"https://cloud.google.com/solutions/retail\"\u003ewhite-glove service\u003c/a\u003e played a key role in ensuring our success in both BFCM 2019 and BFCM 2020. This service included on-site visits from Google’s Customer Reliability Engineering (CRE) team who reviewed Lowe’s web architecture, capacity planning, operations practices for event risks, and presented workshops on topics such as incident response best practices. \u003c/p\u003e\u003ch3\u003eLooking ahead\u003c/h3\u003e\u003cp\u003eThere is always room for improvement, and at Lowe’s we aim to continuously improve our SRE practices. One thing that has worked well for us, which we plan to continue, has been our road shows, where senior SRE leads present to other SREs and application domain teams on the latest SRE principles and best practices, and to get input in real-time from them. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/products/operations\"\u003eGoogle’s tools\u003c/a\u003e and methodology have played an instrumental role in helping reshape our SRE practices and better serve our customers. We look forward to building on the momentum and partnership as we continue our SRE journey at Lowe’s. \u003c/p\u003e\u003cp\u003e\u003ci\u003eIf you want to learn more about how to adopt SRE best practices on Google Cloud, \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring\"\u003echeck out our documentation\u003c/a\u003e. If you want to learn more about Google SRE, \u003ca href=\"https://sre.google/\" target=\"_blank\"\u003evisit our website\u003c/a\u003e. Stay tuned for the next blogs with Lowe’s discussing how they trained their engineering talent to adopt SRE practices and tooling, and how they improved MTTR using SRE principles.\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eFour steps to jumpstarting your SRE practice\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eOnce you have leadership buy-in, there are some things you can do to get the SRE ball rolling, fast.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/devops.max-2200x2200.jpg",
      "date_published": "2021-06-07T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eRahul Mohan Kola Kandy\u003c/name\u003e\u003ctitle\u003eSr. Manager, Digital SRE, Lowe’s Companies, Inc.\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/application-development/forgerock-developers-stay-productive-with-google-cloud/",
      "title": "DevOps on Google Cloud: tools to speed up software development velocity",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;\u0026lt;b\u0026gt;Editor\u0026amp;#8217;s note\u0026lt;/b\u0026gt;: Today we hear from \u0026lt;a href=\u0026#34;https://www.forgerock.com/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;ForgeRock\u0026lt;/a\u0026gt;, a multinational \u0026lt;a href=\u0026#34;https://en.wikipedia.org/wiki/Identity_and_access_management\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;identity and access management\u0026lt;/a\u0026gt; software company with more than 1,100 enterprise customers, including a major public broadcaster. In total, customers use the ForgeRock Identity Platform to authenticate and log in over 45 million users daily, helping them manage identity, governance, and access management across all platforms, including on-premises and multicloud environments.\u0026amp;#160;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;Operating at that kind of scale isn\u0026amp;#8217;t easy. In this blog post, ForgeRock Engineering Director, Warren Strange discusses the three things that help make their developers efficient and productive, and the Google Cloud tools they use along the way.\u0026amp;#160;\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;hr\u0026gt;\u0026lt;p\u0026gt;At ForgeRock, we\u0026amp;#8217;ve been an early adopter of Kubernetes, viewing it as a strategic platform. Running on Kubernetes allows us to drive multicloud support across \u0026lt;a href=\u0026#34;https://cloud.google.com/kubernetes-engine\u0026#34;\u0026gt;Google Kubernetes Engine\u0026lt;/a\u0026gt; (GKE), Amazon (EKS), and Azure (AKS). So no matter which cloud our customers are running on, we are able to seamlessly integrate our products into customers\u0026#39; environments.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Making it easier for ForgeRock\u0026#39;s developers and operators to build, deploy and manage applications has been crucial in our ability to continually provide high quality solutions for our customers. We\u0026amp;#8217;re always looking for tools to improve productivity and keep our developers focused on coding instead of configuration. Google Cloud\u0026amp;#8217;s suite of DevOps tools have streamlined three specific practices to help keep our developers productive:\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;1. Make developers productive within IDEs\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Developer productivity is core to the success of any organization, including ForgeRock. Since developers spend most of their time within their IDE of choice, our goal at ForgeRock has been to make it easier for our developers to write Kubernetes applications within the IDEs they know and love. \u0026lt;a href=\u0026#34;https://cloud.google.com/code\u0026#34;\u0026gt;Cloud Code\u0026lt;/a\u0026gt; helps us precisely with that: it makes the process of building, deploying, scaling, and managing Kubernetes infrastructure and applications a breeze.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In particular, working with the Kubernetes YAML syntax and schema takes time, and a lot of trial and error to master. Thanks to \u0026lt;a href=\u0026#34;https://cloud.google.com/code/docs/vscode/yaml-editing\u0026#34;\u0026gt;YAML authoring support\u0026lt;/a\u0026gt; within Cloud Code, we can easily avoid the complicated and time consuming task of writing YAML files at ForgeRock. With YAML authoring support, developers save time on every bug. Cloud Code\u0026amp;#8217;s inline\u0026amp;#160; snippets, completions, and schema validation, a.k.a. \u0026amp;#8220;linting,\u0026amp;#8221; further streamline working with YAML files.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The benefits of Cloud Code extend to local development as well. Iterating locally on Kubernetes applications often requires multiple manual steps, including building container images, updating Kubernetes manifests, and redeploying applications. Doing these steps over and over again can be a chore. Cloud Code \u0026lt;a href=\u0026#34;https://cloud.google.com/code/docs/intellij/deploying-a-k8-app\u0026#34;\u0026gt;supports Skaffold under the hood,\u0026lt;/a\u0026gt; which tracks changes as they come and automatically rebuilds and redeploys\u0026amp;#8212;reducing repetitive development tasks.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, developing for Kubernetes usually involves jumping between the IDE, documentation, samples etc. Cloud Code reduces this context switching with Kubernetes \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/cloud-code-samples\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;code samples\u0026lt;/a\u0026gt;. With samples, we can get new developers up and running quickly. They spend less time learning about configuration and management of the application\u0026amp;#8212;and spend more time on writing and evolving the code.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;2. Drive end-to-end automation\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;To further improve developer productivity, we\u0026amp;#8217;ve focused on end-to-end automation: from writing code within IDEs, to automatically triggering CI/CD pipelines and running the code in production. In particular, \u0026lt;a href=\u0026#34;https://cloud.google.com/tekton\u0026#34;\u0026gt;Tekton\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/cloud-build\u0026#34;\u0026gt;Cloud Build\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;https://cloud.google.com/container-registry\u0026#34;\u0026gt;Container Registry,\u0026lt;/a\u0026gt; and GKE have been critical to Forgerock as we streamline the flow of code, feedback and remediation through the build and deployment processes. The process looks something like this:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s note\u003c/b\u003e: Today we hear from \u003ca href=\"https://www.forgerock.com/\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://www.forgerock.com\" track-metadata-module=\"post\"\u003eForgeRock\u003c/a\u003e, a multinational \u003ca href=\"https://en.wikipedia.org/wiki/Identity_and_access_management\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://en.wikipedia.org\" track-metadata-module=\"post\"\u003eidentity and access management\u003c/a\u003e software company with more than 1,100 enterprise customers, including a major public broadcaster. In total, customers use the ForgeRock Identity Platform to authenticate and log in over 45 million users daily, helping them manage identity, governance, and access management across all platforms, including on-premises and multicloud environments. \u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eOperating at that kind of scale isn’t easy. In this blog post, ForgeRock Engineering Director, Warren Strange discusses the three things that help make their developers efficient and productive, and the Google Cloud tools they use along the way. \u003c/i\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003eAt ForgeRock, we’ve been an early adopter of Kubernetes, viewing it as a strategic platform. Running on Kubernetes allows us to drive multicloud support across \u003ca href=\"https://cloud.google.com/kubernetes-engine\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/kubernetes-engine\" track-metadata-module=\"post\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE), Amazon (EKS), and Azure (AKS). So no matter which cloud our customers are running on, we are able to seamlessly integrate our products into customers\u0026#39; environments. \u003c/p\u003e\u003cp\u003eMaking it easier for ForgeRock\u0026#39;s developers and operators to build, deploy and manage applications has been crucial in our ability to continually provide high quality solutions for our customers. We’re always looking for tools to improve productivity and keep our developers focused on coding instead of configuration. Google Cloud’s suite of DevOps tools have streamlined three specific practices to help keep our developers productive: \u003c/p\u003e\u003ch3\u003e1. Make developers productive within IDEs\u003c/h3\u003e\u003cp\u003eDeveloper productivity is core to the success of any organization, including ForgeRock. Since developers spend most of their time within their IDE of choice, our goal at ForgeRock has been to make it easier for our developers to write Kubernetes applications within the IDEs they know and love. \u003ca href=\"https://cloud.google.com/code\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/code\" track-metadata-module=\"post\"\u003eCloud Code\u003c/a\u003e helps us precisely with that: it makes the process of building, deploying, scaling, and managing Kubernetes infrastructure and applications a breeze. \u003c/p\u003e\u003cp\u003eIn particular, working with the Kubernetes YAML syntax and schema takes time, and a lot of trial and error to master. Thanks to \u003ca href=\"https://cloud.google.com/code/docs/vscode/yaml-editing\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/code/docs/vscode/yaml-editing\" track-metadata-module=\"post\"\u003eYAML authoring support\u003c/a\u003e within Cloud Code, we can easily avoid the complicated and time consuming task of writing YAML files at ForgeRock. With YAML authoring support, developers save time on every bug. Cloud Code’s inline  snippets, completions, and schema validation, a.k.a. “linting,” further streamline working with YAML files. \u003c/p\u003e\u003cp\u003eThe benefits of Cloud Code extend to local development as well. Iterating locally on Kubernetes applications often requires multiple manual steps, including building container images, updating Kubernetes manifests, and redeploying applications. Doing these steps over and over again can be a chore. Cloud Code \u003ca href=\"https://cloud.google.com/code/docs/intellij/deploying-a-k8-app\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/code/docs/intellij/deploying-a-k8-app\" track-metadata-module=\"post\"\u003esupports Skaffold under the hood,\u003c/a\u003e which tracks changes as they come and automatically rebuilds and redeploys—reducing repetitive development tasks. \u003c/p\u003e\u003cp\u003eFinally, developing for Kubernetes usually involves jumping between the IDE, documentation, samples etc. Cloud Code reduces this context switching with Kubernetes \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-code-samples\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ecode samples\u003c/a\u003e. With samples, we can get new developers up and running quickly. They spend less time learning about configuration and management of the application—and spend more time on writing and evolving the code.\u003c/p\u003e\u003ch3\u003e2. Drive end-to-end automation\u003c/h3\u003e\u003cp\u003eTo further improve developer productivity, we’ve focused on end-to-end automation: from writing code within IDEs, to automatically triggering CI/CD pipelines and running the code in production. In particular, \u003ca href=\"https://cloud.google.com/tekton\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/tekton\" track-metadata-module=\"post\"\u003eTekton\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/cloud-build\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/cloud-build\" track-metadata-module=\"post\"\u003eCloud Build\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/container-registry\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/container-registry\" track-metadata-module=\"post\"\u003eContainer Registry,\u003c/a\u003e and GKE have been critical to Forgerock as we streamline the flow of code, feedback and remediation through the build and deployment processes. The process looks something like this:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003e\u003cb\u003eEditor’s note\u003c/b\u003e: Today we hear from \u003ca href=\"https://www.forgerock.com/\" target=\"_blank\"\u003eForgeRock\u003c/a\u003e, a multinational \u003ca href=\"https://en.wikipedia.org/wiki/Identity_and_access_management\" target=\"_blank\"\u003eidentity and access management\u003c/a\u003e software company with more than 1,100 enterprise customers, including a major public broadcaster. In total, customers use the ForgeRock Identity Platform to authenticate and log in over 45 million users daily, helping them manage identity, governance, and access management across all platforms, including on-premises and multicloud environments. \u003c/i\u003e\u003c/p\u003e\u003cp\u003e\u003ci\u003eOperating at that kind of scale isn’t easy. In this blog post, ForgeRock Engineering Director, Warren Strange discusses the three things that help make their developers efficient and productive, and the Google Cloud tools they use along the way. \u003c/i\u003e\u003c/p\u003e\u003chr/\u003e\u003cp\u003eAt ForgeRock, we’ve been an early adopter of Kubernetes, viewing it as a strategic platform. Running on Kubernetes allows us to drive multicloud support across \u003ca href=\"https://cloud.google.com/kubernetes-engine\"\u003eGoogle Kubernetes Engine\u003c/a\u003e (GKE), Amazon (EKS), and Azure (AKS). So no matter which cloud our customers are running on, we are able to seamlessly integrate our products into customers' environments. \u003c/p\u003e\u003cp\u003eMaking it easier for ForgeRock's developers and operators to build, deploy and manage applications has been crucial in our ability to continually provide high quality solutions for our customers. We’re always looking for tools to improve productivity and keep our developers focused on coding instead of configuration. Google Cloud’s suite of DevOps tools have streamlined three specific practices to help keep our developers productive: \u003c/p\u003e\u003ch3\u003e1. Make developers productive within IDEs\u003c/h3\u003e\u003cp\u003eDeveloper productivity is core to the success of any organization, including ForgeRock. Since developers spend most of their time within their IDE of choice, our goal at ForgeRock has been to make it easier for our developers to write Kubernetes applications within the IDEs they know and love. \u003ca href=\"https://cloud.google.com/code\"\u003eCloud Code\u003c/a\u003e helps us precisely with that: it makes the process of building, deploying, scaling, and managing Kubernetes infrastructure and applications a breeze. \u003c/p\u003e\u003cp\u003eIn particular, working with the Kubernetes YAML syntax and schema takes time, and a lot of trial and error to master. Thanks to \u003ca href=\"https://cloud.google.com/code/docs/vscode/yaml-editing\"\u003eYAML authoring support\u003c/a\u003e within Cloud Code, we can easily avoid the complicated and time consuming task of writing YAML files at ForgeRock. With YAML authoring support, developers save time on every bug. Cloud Code’s inline  snippets, completions, and schema validation, a.k.a. “linting,” further streamline working with YAML files. \u003c/p\u003e\u003cp\u003eThe benefits of Cloud Code extend to local development as well. Iterating locally on Kubernetes applications often requires multiple manual steps, including building container images, updating Kubernetes manifests, and redeploying applications. Doing these steps over and over again can be a chore. Cloud Code \u003ca href=\"https://cloud.google.com/code/docs/intellij/deploying-a-k8-app\"\u003esupports Skaffold under the hood,\u003c/a\u003e which tracks changes as they come and automatically rebuilds and redeploys—reducing repetitive development tasks. \u003c/p\u003e\u003cp\u003eFinally, developing for Kubernetes usually involves jumping between the IDE, documentation, samples etc. Cloud Code reduces this context switching with Kubernetes \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-code-samples\" target=\"_blank\"\u003ecode samples\u003c/a\u003e. With samples, we can get new developers up and running quickly. They spend less time learning about configuration and management of the application—and spend more time on writing and evolving the code.\u003c/p\u003e\u003ch3\u003e2. Drive end-to-end automation\u003c/h3\u003e\u003cp\u003eTo further improve developer productivity, we’ve focused on end-to-end automation: from writing code within IDEs, to automatically triggering CI/CD pipelines and running the code in production. In particular, \u003ca href=\"https://cloud.google.com/tekton\"\u003eTekton\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/cloud-build\"\u003eCloud Build\u003c/a\u003e, \u003ca href=\"https://cloud.google.com/container-registry\"\u003eContainer Registry,\u003c/a\u003e and GKE have been critical to Forgerock as we streamline the flow of code, feedback and remediation through the build and deployment processes. The process looks something like this:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/ForgeRock__Google.0997050516950896.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"ForgeRock + Google.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/ForgeRock__Google.0997050516950896.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWe begin by developing Kubernetes manifests and dockerfiles using Cloud Code. Then we use \u003ca href=\"https://cloud.google.com/blog/products/application-development/kubernetes-development-simplified-skaffold-is-now-ga\"\u003eSkaffold\u003c/a\u003e to build containers locally, while Cloud Build helps with continuous integration (CI). The \u003ca href=\"https://github.com/marketplace/google-cloud-build\" target=\"_blank\"\u003eCloud Build GitHub app\u003c/a\u003e allows us to automate builds and tests as part of our GitHub workflow. Cloud Build is differentiated from other continuous integration tools since it is fully serverless. It scales up and scales down in response to load, with no need for us to pre-provision servers or pay in advance for additional capacity. We pay for the exact resources we use. \u003c/p\u003e\u003cp\u003eOnce the image is built by Cloud Build, it is stored, managed, and secured in Google’s \u003ca href=\"https://cloud.google.com/container-registry\"\u003eContainer Registry\u003c/a\u003e. Just like Cloud Build, Container Registry is serverless, so we only pay for what we  use. Additionally, since Container Registry comes with automatic vulnerability scanning, every time we upload a new image to Container Registry, we can also scan it for vulnerabilities. \u003c/p\u003e\u003cp\u003eNext, a Tekton pipeline is triggered, which deploys the docker images stored in Container Registry and Kubernetes manifests to a running GKE cluster. Along with Cloud Build, Tekton is a critical part of our CI/CD process at ForgeRock. Most importantly, since Tekton comes with standardized Kubernetes-native primitives, we can create continuous delivery workflows very quickly.\u003c/p\u003e\u003cp\u003eAfter deployment, Tekton triggers a functional test suite to ensure that the applications we deploy perform as expected. The test results are posted to our team Slack channel so all developers have instant access and insights about each cluster. From there, we are able to provide our customers with their finished product request.\u003c/p\u003e\u003ch3\u003e3.  Leverage multicloud patterns and practices\u003c/h3\u003e\u003cp\u003eThe industry has seen a shift towards \u003ca href=\"https://cloud.google.com/multicloud\"\u003emulticloud\u003c/a\u003e. Organizations have adopted multicloud strategies to minimize vendor lock-in, take advantage of best-in-class solutions, improve cost-efficiencies, and increase flexibility through choice. \u003c/p\u003e\u003cp\u003eAt ForgeRock, we’re big proponents of multicloud. Part of that comes from the fact that our identity and access management product works across Google Cloud, AWS, and Azure. Developing products using open-source technologies such as Kubernetes has been particularly helpful in driving this interoperability. \u003c/p\u003e\u003cp\u003eTekton has been another critical project that has allowed us to prevent vendor lock-in. Thanks to Tekton, our continuous delivery pipelines can deploy across any Kubernetes cluster. Most importantly, since Tekton pipelines run on Kubernetes, these pipelines can be decoupled from the runtime. Like Tekton and Kubernetes, both Cloud Build and Container Registry are based on open technologies. \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-builders-community\" target=\"_blank\"\u003eCommunity-contributed builders\u003c/a\u003eand \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-builders\" target=\"_blank\"\u003eofficial builder images\u003c/a\u003e allow us to connect to a variety of tools as a part of the build process. And finally, with support for open technologies like \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/google-cloud-now-supports-buildpacks\"\u003eGoogle Cloud buildpacks\u003c/a\u003e within Cloud Build, we can build containers without even knowing Docker. \u003c/p\u003e\u003cp\u003eMaking it easier for developers and operators to build, deploy and manage applications is critical for the success of any organization. Driving developer productivity within IDEs, leveraging end-to-end automation, and support for multi-cloud patterns and practices are just some of the ways we are trying to achieve this at ForgeRock. To learn more about ForgeRock, and to deploy the ForgeRock Identity Platform into your Kubernetes cluster, check out our open-source \u003ca href=\"https://github.com/ForgeRock/forgeops\" target=\"_blank\"\u003eForgeOps\u003c/a\u003e repository on GitHub.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/appdev.max-2200x2200.jpg",
      "date_published": "2021-06-01T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eWarren Strange\u003c/name\u003e\u003ctitle\u003eEngineering Director, ForgeRock\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice/",
      "title": "Four steps to jumpstarting your SRE practice",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#DevOps\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003carticle-cta _nghost-c17=\"\"\u003e\u003cdiv _ngcontent-c17=\"\"\u003e\u003ch4 _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eTry GCP\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eStart building on Google Cloud with $300 in free credits and 20+ always free products.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c17=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"free trial\" track-metadata-eventdetail=\"https://cloud.google.com/free/\" href=\"https://cloud.google.com/free/\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eFree Trial\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;A few months ago, we wrote about how the first step to implementing Site Reliability Engineering (SRE) in an organization is \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\u0026#34;\u0026gt;getting leadership on board\u0026lt;/a\u0026gt;. So, let\u0026amp;#8217;s assume that you\u0026amp;#8217;ve gone ahead and done that. Now what? What are some concrete steps you can take to get the SRE ball rolling? In this blog post, we\u0026amp;#8217;ll take a look at what you as an IT leader can do to fast-track SRE within your team.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Step 1: Start small and iterate\u0026amp;#160;\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#34;Rome wasn\u0026#39;t built in a day,\u0026amp;#34; the saying goes, but you do need to start somewhere. When it comes to implementing SRE principles, the approach that I (and my team) found to be the most effective is to start with a proof of concept, learn from our mistakes, and iterate!\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Start by identifying a relevant application and/or team\u0026amp;#160;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;There are many factors that go into choosing a specific team or application for your SRE proof of concept. Most of the time, though, this is a strategic decision for the organization, which is outside the scope of this article. Possible candidates can be a team shifting from traditional operations or DevOps to SRE, or a need to increase reliability to a business-critical product. No matter the reason, it\u0026amp;#8217;s crucial to select an application that is:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Critical to the business. Your customers should care deeply about its uptime and reliability.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Currently in development. Pick an application in which the business is actively investing resources.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;In a perfect world, the application provides data and metrics regarding its behaviour.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;Conversely, stay away from proprietary software. If the application wasn\u0026amp;#8217;t built by you, it\u0026#39;s not a good candidate for SRE! You need the ability to make strategic decisions about\u0026amp;#8212;and engineering changes to\u0026amp;#8212;the application as needed.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Pro tip\u0026lt;/b\u0026gt;: In general, if you have workloads both on-premises and in the cloud, try to start with the cloud-based app. If your engineers come from a traditional operations environment, changing their thinking away from \u0026#39;bare metal\u0026#39; and infrastructure metrics will be easier for a cloud-based app, as managed infrastructure turns practitioners into users and forces them to consume it like developers (APIs, infrastructure as code, etc.)\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Remember\u0026lt;/b\u0026gt;: Set realistic goals. Discouraging your team with unrealistic expectations early on will have a negative effect on the initiative.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Step 2: Empower your teams\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;Implementing SRE principles requires fostering a learning culture, and in that regard, \u0026lt;b\u0026gt;team enablement\u0026lt;/b\u0026gt; means both training them, i.e., in regards to knowledge, as well as \u0026lt;i\u0026gt;empowering\u0026lt;/i\u0026gt; them.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Building a training program is a topic in and of itself, but it\u0026amp;#8217;s important to think about an \u0026lt;b\u0026gt;enablement strategy\u0026lt;/b\u0026gt; at an early stage. Especially in large organizations, you need to address topics like internal upskilling, hiring and scaling the team as well as onboarding and creating a learning community.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Your enablement strategy should also accommodate employees at different levels and in different functions. For example, higher leadership\u0026#39;s training will look very different from practitioners\u0026amp;#8217; training. Leadership\u0026#39;s education should be sufficient to get buy-in and to be able to make organizational decisions. To drive change in the entire organization, additional training to leadership on cultural concepts and practices might be required.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eA few months ago, we wrote about how the first step to implementing Site Reliability Engineering (SRE) in an organization is \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\" track-metadata-module=\"post\"\u003egetting leadership on board\u003c/a\u003e. So, let’s assume that you’ve gone ahead and done that. Now what? What are some concrete steps you can take to get the SRE ball rolling? In this blog post, we’ll take a look at what you as an IT leader can do to fast-track SRE within your team. \u003c/p\u003e\u003ch2\u003eStep 1: Start small and iterate \u003c/h2\u003e\u003cp\u003e\u0026#34;Rome wasn\u0026#39;t built in a day,\u0026#34; the saying goes, but you do need to start somewhere. When it comes to implementing SRE principles, the approach that I (and my team) found to be the most effective is to start with a proof of concept, learn from our mistakes, and iterate!\u003c/p\u003e\u003ch3\u003eStart by identifying a relevant application and/or team \u003c/h3\u003e\u003cp\u003eThere are many factors that go into choosing a specific team or application for your SRE proof of concept. Most of the time, though, this is a strategic decision for the organization, which is outside the scope of this article. Possible candidates can be a team shifting from traditional operations or DevOps to SRE, or a need to increase reliability to a business-critical product. No matter the reason, it’s crucial to select an application that is:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eCritical to the business. Your customers should care deeply about its uptime and reliability. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCurrently in development. Pick an application in which the business is actively investing resources. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn a perfect world, the application provides data and metrics regarding its behaviour. \u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eConversely, stay away from proprietary software. If the application wasn’t built by you, it\u0026#39;s not a good candidate for SRE! You need the ability to make strategic decisions about—and engineering changes to—the application as needed. \u003c/p\u003e\u003cp\u003e\u003cb\u003ePro tip\u003c/b\u003e: In general, if you have workloads both on-premises and in the cloud, try to start with the cloud-based app. If your engineers come from a traditional operations environment, changing their thinking away from \u0026#39;bare metal\u0026#39; and infrastructure metrics will be easier for a cloud-based app, as managed infrastructure turns practitioners into users and forces them to consume it like developers (APIs, infrastructure as code, etc.)\u003c/p\u003e\u003cp\u003e\u003cb\u003eRemember\u003c/b\u003e: Set realistic goals. Discouraging your team with unrealistic expectations early on will have a negative effect on the initiative. \u003c/p\u003e\u003ch2\u003eStep 2: Empower your teams\u003c/h2\u003e\u003cp\u003eImplementing SRE principles requires fostering a learning culture, and in that regard, \u003cb\u003eteam enablement\u003c/b\u003e means both training them, i.e., in regards to knowledge, as well as \u003ci\u003eempowering\u003c/i\u003e them.\u003c/p\u003e\u003cp\u003eBuilding a training program is a topic in and of itself, but it’s important to think about an \u003cb\u003eenablement strategy\u003c/b\u003e at an early stage. Especially in large organizations, you need to address topics like internal upskilling, hiring and scaling the team as well as onboarding and creating a learning community. \u003c/p\u003e\u003cp\u003eYour enablement strategy should also accommodate employees at different levels and in different functions. For example, higher leadership\u0026#39;s training will look very different from practitioners’ training. Leadership\u0026#39;s education should be sufficient to get buy-in and to be able to make organizational decisions. To drive change in the entire organization, additional training to leadership on cultural concepts and practices might be required.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;When it comes to engineering leadership and/or middle management (managers that manage managers), training should be a combination of\u0026amp;#160; high-level cultural concepts to help foster the required culture, and technical SRE practices that are deep enough to understand prioritization, resource allocation, process creation, and future needs.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When it comes to practitioners, ideally you want the entire organization to be aligned both from a knowledge perspective as well as culturally. But as we\u0026amp;#8217;ve mentioned earlier, it\u0026amp;#8217;s best to start simple, with just one team.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The starting point for those teams should be to understand reliability and key concepts like \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos\u0026#34;\u0026gt;SLAs, SLOs, SLIs\u0026lt;/a\u0026gt; and \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows\u0026#34;\u0026gt;error budgets\u0026lt;/a\u0026gt;. These are important because SRE is focused on the customer experience. Measuring whether systems meet customer expectations requires a shift in mindset and can take time.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eWhen it comes to engineering leadership and/or middle management (managers that manage managers), training should be a combination of  high-level cultural concepts to help foster the required culture, and technical SRE practices that are deep enough to understand prioritization, resource allocation, process creation, and future needs.\u003c/p\u003e\u003cp\u003eWhen it comes to practitioners, ideally you want the entire organization to be aligned both from a knowledge perspective as well as culturally. But as we’ve mentioned earlier, it’s best to start simple, with just one team.\u003c/p\u003e\u003cp\u003eThe starting point for those teams should be to understand reliability and key concepts like \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos\" track-metadata-module=\"post\"\u003eSLAs, SLOs, SLIs\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows\" track-metadata-module=\"post\"\u003eerror budgets\u003c/a\u003e. These are important because SRE is focused on the customer experience. Measuring whether systems meet customer expectations requires a shift in mindset and can take time.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eAfter identifying your first application and/or the team responsible for it, it\u0026#39;s time to identify the app’s user journeys, the set of interactions a user has with a service to achieve a single goal—for example, a single click or a multi-step pipeline, and rank them according to business impact. The most critical ones are called Critical User Journeys (CUJ), and these are where you should start  drafting SLO/SLIs.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Pro tip\u0026lt;/b\u0026gt;: There are some general technical practices that can help you embrace SRE faster. For example, using less repos rather than more can help you reduce silos within the organization and better utilize resources.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Likewise, prioritizing automatic processes and self-healing systems can benefit reliability, but also team satisfaction, helping the organization retain talent.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003ePro tip\u003c/b\u003e: There are some general technical practices that can help you embrace SRE faster. For example, using less repos rather than more can help you reduce silos within the organization and better utilize resources. \u003c/p\u003e\u003cp\u003eLikewise, prioritizing automatic processes and self-healing systems can benefit reliability, but also team satisfaction, helping the organization retain talent.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Final note\u0026lt;/b\u0026gt;: Similar to the way that you make architecture decisions, your chosen technology, solutions and implementation tools should enable you to do what you are trying to do and not vice versa.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Step 3: Scale those learnings\u0026amp;#160;\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;After you establish these SRE practices with one or a few teams, the next step is to think about building an SRE community and formalized processes across the organization. In some organizations, you can do this in parallel to the end of step 2, and in some organizations, only after you have a few successful implementations under your belt.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In this phase, you\u0026amp;#8217;ll probably want to address \u0026lt;b\u0026gt;community, culture, enablement and processes.\u0026lt;/b\u0026gt; You will need to address them all, especially as they are intertwined, but which one you prioritize will depend on your organization.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Creating an SRE \u0026lt;b\u0026gt;community\u0026lt;/b\u0026gt; in the organization is important both from a learning perspective, but also to establish a knowledge base of best practices, train subject-matter experts, help create needed guardrails, and align processes.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Building a community goes hand in hand with fostering an \u0026lt;b\u0026gt;empowered culture and training teams\u0026lt;/b\u0026gt;. The idea is that early adopters are ambassadors for SRE who share their learnings and train other teams in the organization.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It is also useful to identify potential ambassadors or champions in individual development teams who are passionate about SRE and will help with the adoption of those practices.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It is also crucial to create repeatable trainings for each functional role, including onboarding sessions. Onboarding new team members is a critical aspect of training and fostering an empowered SRE culture. Therefore it is vital to be mindful about your onboarding process and make sure that the knowledge is not lost when team members change roles.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003cb\u003eFinal note\u003c/b\u003e: Similar to the way that you make architecture decisions, your chosen technology, solutions and implementation tools should enable you to do what you are trying to do and not vice versa. \u003c/p\u003e\u003ch2\u003eStep 3: Scale those learnings \u003c/h2\u003e\u003cp\u003eAfter you establish these SRE practices with one or a few teams, the next step is to think about building an SRE community and formalized processes across the organization. In some organizations, you can do this in parallel to the end of step 2, and in some organizations, only after you have a few successful implementations under your belt.\u003c/p\u003e\u003cp\u003eIn this phase, you’ll probably want to address \u003cb\u003ecommunity, culture, enablement and processes.\u003c/b\u003e You will need to address them all, especially as they are intertwined, but which one you prioritize will depend on your organization.\u003c/p\u003e\u003cp\u003eCreating an SRE \u003cb\u003ecommunity\u003c/b\u003e in the organization is important both from a learning perspective, but also to establish a knowledge base of best practices, train subject-matter experts, help create needed guardrails, and align processes. \u003c/p\u003e\u003cp\u003eBuilding a community goes hand in hand with fostering an \u003cb\u003eempowered culture and training teams\u003c/b\u003e. The idea is that early adopters are ambassadors for SRE who share their learnings and train other teams in the organization. \u003c/p\u003e\u003cp\u003eIt is also useful to identify potential ambassadors or champions in individual development teams who are passionate about SRE and will help with the adoption of those practices.\u003c/p\u003e\u003cp\u003eIt is also crucial to create repeatable trainings for each functional role, including onboarding sessions. Onboarding new team members is a critical aspect of training and fostering an empowered SRE culture. Therefore it is vital to be mindful about your onboarding process and make sure that the knowledge is not lost when team members change roles.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;During this phase, you also want to foster an org-wide culture that promotes psychological safety, accepts failure as normal and enables the team to learn from mistakes. For that, leadership must model the desired culture and promote transparency.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Finally, having structured and formalized processes can help reduce the stress around emergency response\u0026amp;#8212;especially being on-call. Processes can also provide clarity and make teams more collaborative and effective.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In order to have the most impact, start by prioritizing the most painful areas under your team\u0026amp;#8217;s remit\u0026amp;#8212;for example, clean up noisy alerts to avoid (or address) alert fatigue, automate your change management processes and involve only the necessary people to save team bandwidth. Team members shouldn\u0026#39;t work on software engineering projects while doing on-call incident management, and vice-versa. Make sure they have enough bandwidth to do both, separately.\u0026amp;#160; Similar to other areas, you want to use data to drive your decisions.\u0026amp;#160; As such, identify where your teams spend the most time, and for how long.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you find that it is challenging to collect this kind of data, be it quantitative or qualitative, a good starting point is often your emergency response processes, as those have a direct impact on the business, especially around the escalation process, incident management and related policies.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Pro tip\u0026lt;/b\u0026gt;: All the above practices contribute to reducing silos and align goals across the organization; those should include also your vendors and engineering partners. To that end, make sure your contracts with them capture those goals as well.\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Step 4: Embody a data-driven mindset\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;Starting the SRE journey can take time, even if you\u0026#39;re just implementing it for one team. Two quick wins that you can start with that will make a positive impact are collecting data and doing blameless postmortems.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In SRE we try to be as \u0026lt;b\u0026gt;data-driven\u0026lt;/b\u0026gt; as possible, so creating a measurement culture in your organization is crucial. When prioritizing data collection, ideally look for data that represents the customer experience. Collecting that data will help you identify your gaps and help you prioritize according to business needs and by extension your customer expectations.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eDuring this phase, you also want to foster an org-wide culture that promotes psychological safety, accepts failure as normal and enables the team to learn from mistakes. For that, leadership must model the desired culture and promote transparency. \u003c/p\u003e\u003cp\u003eFinally, having structured and formalized processes can help reduce the stress around emergency response—especially being on-call. Processes can also provide clarity and make teams more collaborative and effective. \u003c/p\u003e\u003cp\u003eIn order to have the most impact, start by prioritizing the most painful areas under your team’s remit—for example, clean up noisy alerts to avoid (or address) alert fatigue, automate your change management processes and involve only the necessary people to save team bandwidth. Team members shouldn\u0026#39;t work on software engineering projects while doing on-call incident management, and vice-versa. Make sure they have enough bandwidth to do both, separately.  Similar to other areas, you want to use data to drive your decisions.  As such, identify where your teams spend the most time, and for how long. \u003c/p\u003e\u003cp\u003eIf you find that it is challenging to collect this kind of data, be it quantitative or qualitative, a good starting point is often your emergency response processes, as those have a direct impact on the business, especially around the escalation process, incident management and related policies. \u003c/p\u003e\u003cp\u003e\u003cb\u003ePro tip\u003c/b\u003e: All the above practices contribute to reducing silos and align goals across the organization; those should include also your vendors and engineering partners. To that end, make sure your contracts with them capture those goals as well.\u003c/p\u003e\u003ch2\u003eStep 4: Embody a data-driven mindset\u003c/h2\u003e\u003cp\u003eStarting the SRE journey can take time, even if you\u0026#39;re just implementing it for one team. Two quick wins that you can start with that will make a positive impact are collecting data and doing blameless postmortems.\u003c/p\u003e\u003cp\u003eIn SRE we try to be as \u003cb\u003edata-driven\u003c/b\u003e as possible, so creating a measurement culture in your organization is crucial. When prioritizing data collection, ideally look for data that represents the customer experience. Collecting that data will help you identify your gaps and help you prioritize according to business needs and by extension your customer expectations.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Another thing that you can do is run or improve \u0026lt;a href=\u0026#34;https://sre.google/sre-book/postmortem-culture/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;postmortems\u0026lt;/a\u0026gt;, which are an essential way of learning from failure and fostering a strong SRE culture. From our experience, even organizations that do run postmortems can benefit from them much more with a few minor improvements. It is important to remember that postmortems should be blameless in order to make the team feel safe to share and learn from failures. And to make tomorrow better than today, i.e., not repeat the same problems, it\u0026amp;#8217;s important that postmortems include action items and are assigned to an owner.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons\u0026#34;\u0026gt;Creating a shared repository\u0026lt;/a\u0026gt; for postmortems can have a tremendous impact on the team: it increases transparency, reduces silos, and contributes to the \u0026lt;a href=\u0026#34;https://cloud.google.com/solutions/devops/devops-culture-learning-culture\u0026#34;\u0026gt;learning culture\u0026lt;/a\u0026gt;. It also shows the team that the organization \u0026amp;#8220;practices what it preaches.\u0026amp;#8221; Implementing a repository can be as easy as creating a shared drive.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Pro ti\u0026lt;/b\u0026gt;p: Postmortems should be blameless and actionable.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eAnother thing that you can do is run or improve \u003ca href=\"https://sre.google/sre-book/postmortem-culture/\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003epostmortems\u003c/a\u003e, which are an essential way of learning from failure and fostering a strong SRE culture. From our experience, even organizations that do run postmortems can benefit from them much more with a few minor improvements. It is important to remember that postmortems should be blameless in order to make the team feel safe to share and learn from failures. And to make tomorrow better than today, i.e., not repeat the same problems, it’s important that postmortems include action items and are assigned to an owner. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons\" track-metadata-module=\"post\"\u003eCreating a shared repository\u003c/a\u003e for postmortems can have a tremendous impact on the team: it increases transparency, reduces silos, and contributes to the \u003ca href=\"https://cloud.google.com/solutions/devops/devops-culture-learning-culture\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/solutions/devops/devops-culture-learning-culture\" track-metadata-module=\"post\"\u003elearning culture\u003c/a\u003e. It also shows the team that the organization “practices what it preaches.” Implementing a repository can be as easy as creating a shared drive.\u003c/p\u003e\u003cp\u003e\u003cb\u003ePro ti\u003c/b\u003ep: Postmortems should be blameless and actionable.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c18=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eA few months ago, we wrote about how the first step to implementing Site Reliability Engineering (SRE) in an organization is \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\"\u003egetting leadership on board\u003c/a\u003e. So, let’s assume that you’ve gone ahead and done that. Now what? What are some concrete steps you can take to get the SRE ball rolling? In this blog post, we’ll take a look at what you as an IT leader can do to fast-track SRE within your team. \u003c/p\u003e\u003ch2\u003eStep 1: Start small and iterate \u003c/h2\u003e\u003cp\u003e\"Rome wasn't built in a day,\" the saying goes, but you do need to start somewhere. When it comes to implementing SRE principles, the approach that I (and my team) found to be the most effective is to start with a proof of concept, learn from our mistakes, and iterate!\u003c/p\u003e\u003ch3\u003eStart by identifying a relevant application and/or team \u003c/h3\u003e\u003cp\u003eThere are many factors that go into choosing a specific team or application for your SRE proof of concept. Most of the time, though, this is a strategic decision for the organization, which is outside the scope of this article. Possible candidates can be a team shifting from traditional operations or DevOps to SRE, or a need to increase reliability to a business-critical product. No matter the reason, it’s crucial to select an application that is:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eCritical to the business. Your customers should care deeply about its uptime and reliability. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCurrently in development. Pick an application in which the business is actively investing resources. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eIn a perfect world, the application provides data and metrics regarding its behaviour. \u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eConversely, stay away from proprietary software. If the application wasn’t built by you, it's not a good candidate for SRE! You need the ability to make strategic decisions about—and engineering changes to—the application as needed. \u003c/p\u003e\u003cp\u003e\u003cb\u003ePro tip\u003c/b\u003e: In general, if you have workloads both on-premises and in the cloud, try to start with the cloud-based app. If your engineers come from a traditional operations environment, changing their thinking away from 'bare metal' and infrastructure metrics will be easier for a cloud-based app, as managed infrastructure turns practitioners into users and forces them to consume it like developers (APIs, infrastructure as code, etc.)\u003c/p\u003e\u003cp\u003e\u003cb\u003eRemember\u003c/b\u003e: Set realistic goals. Discouraging your team with unrealistic expectations early on will have a negative effect on the initiative. \u003c/p\u003e\u003ch2\u003eStep 2: Empower your teams\u003c/h2\u003e\u003cp\u003eImplementing SRE principles requires fostering a learning culture, and in that regard, \u003cb\u003eteam enablement\u003c/b\u003e means both training them, i.e., in regards to knowledge, as well as \u003ci\u003eempowering\u003c/i\u003e them.\u003c/p\u003e\u003cp\u003eBuilding a training program is a topic in and of itself, but it’s important to think about an \u003cb\u003eenablement strategy\u003c/b\u003e at an early stage. Especially in large organizations, you need to address topics like internal upskilling, hiring and scaling the team as well as onboarding and creating a learning community. \u003c/p\u003e\u003cp\u003eYour enablement strategy should also accommodate employees at different levels and in different functions. For example, higher leadership's training will look very different from practitioners’ training. Leadership's education should be sufficient to get buy-in and to be able to make organizational decisions. To drive change in the entire organization, additional training to leadership on cultural concepts and practices might be required.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout_external\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003e\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhen it comes to engineering leadership and/or middle management (managers that manage managers), training should be a combination of  high-level cultural concepts to help foster the required culture, and technical SRE practices that are deep enough to understand prioritization, resource allocation, process creation, and future needs.\u003c/p\u003e\u003cp\u003eWhen it comes to practitioners, ideally you want the entire organization to be aligned both from a knowledge perspective as well as culturally. But as we’ve mentioned earlier, it’s best to start simple, with just one team.\u003c/p\u003e\u003cp\u003eThe starting point for those teams should be to understand reliability and key concepts like \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos\"\u003eSLAs, SLOs, SLIs\u003c/a\u003e and \u003ca href=\"https://cloud.google.com/blog/products/management-tools/sre-error-budgets-and-maintenance-windows\"\u003eerror budgets\u003c/a\u003e. These are important because SRE is focused on the customer experience. Measuring whether systems meet customer expectations requires a shift in mindset and can take time.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout_external\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003e\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAfter identifying your first application and/or the team responsible for it, it's time to identify the app’s user journeys, the set of interactions a user has with a service to achieve a single goal—for example, a single click or a multi-step pipeline, and rank them according to business impact. The most critical ones are called Critical User Journeys (CUJ), and these are where you should start  drafting SLO/SLIs.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_kqBt6Vr.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"image1.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/image1_kqBt6Vr.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003ePro tip\u003c/b\u003e: There are some general technical practices that can help you embrace SRE faster. For example, using less repos rather than more can help you reduce silos within the organization and better utilize resources. \u003c/p\u003e\u003cp\u003eLikewise, prioritizing automatic processes and self-healing systems can benefit reliability, but also team satisfaction, helping the organization retain talent.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout_external\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003e\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003cb\u003eFinal note\u003c/b\u003e: Similar to the way that you make architecture decisions, your chosen technology, solutions and implementation tools should enable you to do what you are trying to do and not vice versa. \u003c/p\u003e\u003ch2\u003eStep 3: Scale those learnings \u003c/h2\u003e\u003cp\u003eAfter you establish these SRE practices with one or a few teams, the next step is to think about building an SRE community and formalized processes across the organization. In some organizations, you can do this in parallel to the end of step 2, and in some organizations, only after you have a few successful implementations under your belt.\u003c/p\u003e\u003cp\u003eIn this phase, you’ll probably want to address \u003cb\u003ecommunity, culture, enablement and processes.\u003c/b\u003e You will need to address them all, especially as they are intertwined, but which one you prioritize will depend on your organization.\u003c/p\u003e\u003cp\u003eCreating an SRE \u003cb\u003ecommunity\u003c/b\u003e in the organization is important both from a learning perspective, but also to establish a knowledge base of best practices, train subject-matter experts, help create needed guardrails, and align processes. \u003c/p\u003e\u003cp\u003eBuilding a community goes hand in hand with fostering an \u003cb\u003eempowered culture and training teams\u003c/b\u003e. The idea is that early adopters are ambassadors for SRE who share their learnings and train other teams in the organization. \u003c/p\u003e\u003cp\u003eIt is also useful to identify potential ambassadors or champions in individual development teams who are passionate about SRE and will help with the adoption of those practices.\u003c/p\u003e\u003cp\u003eIt is also crucial to create repeatable trainings for each functional role, including onboarding sessions. Onboarding new team members is a critical aspect of training and fostering an empowered SRE culture. Therefore it is vital to be mindful about your onboarding process and make sure that the knowledge is not lost when team members change roles.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout_external\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003e\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eDuring this phase, you also want to foster an org-wide culture that promotes psychological safety, accepts failure as normal and enables the team to learn from mistakes. For that, leadership must model the desired culture and promote transparency. \u003c/p\u003e\u003cp\u003eFinally, having structured and formalized processes can help reduce the stress around emergency response—especially being on-call. Processes can also provide clarity and make teams more collaborative and effective. \u003c/p\u003e\u003cp\u003eIn order to have the most impact, start by prioritizing the most painful areas under your team’s remit—for example, clean up noisy alerts to avoid (or address) alert fatigue, automate your change management processes and involve only the necessary people to save team bandwidth. Team members shouldn't work on software engineering projects while doing on-call incident management, and vice-versa. Make sure they have enough bandwidth to do both, separately.  Similar to other areas, you want to use data to drive your decisions.  As such, identify where your teams spend the most time, and for how long. \u003c/p\u003e\u003cp\u003eIf you find that it is challenging to collect this kind of data, be it quantitative or qualitative, a good starting point is often your emergency response processes, as those have a direct impact on the business, especially around the escalation process, incident management and related policies. \u003c/p\u003e\u003cp\u003e\u003cb\u003ePro tip\u003c/b\u003e: All the above practices contribute to reducing silos and align goals across the organization; those should include also your vendors and engineering partners. To that end, make sure your contracts with them capture those goals as well.\u003c/p\u003e\u003ch2\u003eStep 4: Embody a data-driven mindset\u003c/h2\u003e\u003cp\u003eStarting the SRE journey can take time, even if you're just implementing it for one team. Two quick wins that you can start with that will make a positive impact are collecting data and doing blameless postmortems.\u003c/p\u003e\u003cp\u003eIn SRE we try to be as \u003cb\u003edata-driven\u003c/b\u003e as possible, so creating a measurement culture in your organization is crucial. When prioritizing data collection, ideally look for data that represents the customer experience. Collecting that data will help you identify your gaps and help you prioritize according to business needs and by extension your customer expectations.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout_external\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003e\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAnother thing that you can do is run or improve \u003ca href=\"https://sre.google/sre-book/postmortem-culture/\" target=\"_blank\"\u003epostmortems\u003c/a\u003e, which are an essential way of learning from failure and fostering a strong SRE culture. From our experience, even organizations that do run postmortems can benefit from them much more with a few minor improvements. It is important to remember that postmortems should be blameless in order to make the team feel safe to share and learn from failures. And to make tomorrow better than today, i.e., not repeat the same problems, it’s important that postmortems include action items and are assigned to an owner. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons\"\u003eCreating a shared repository\u003c/a\u003e for postmortems can have a tremendous impact on the team: it increases transparency, reduces silos, and contributes to the \u003ca href=\"https://cloud.google.com/solutions/devops/devops-culture-learning-culture\"\u003elearning culture\u003c/a\u003e. It also shows the team that the organization “practices what it preaches.” Implementing a repository can be as easy as creating a shared drive.\u003c/p\u003e\u003cp\u003e\u003cb\u003ePro ti\u003c/b\u003ep: Postmortems should be blameless and actionable.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout_external\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003e\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch2\u003eOn the SRE fast track\u003c/h2\u003e\u003cp\u003eOf course, no two organizations are alike, and no two SRE teams are either. But by following these steps, you can help get your team on the path to SRE success faster. To learn more about developing an effective SRE practice, check out the following resources. \u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://medium.com/@ayeletsachti/sre-public-resources-for-gcp-customers-bab039444ad3\" target=\"_blank\"\u003eCollection of SRE Public resources\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/consulting\"\u003eGoogle Professional Services SRE packages\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/sre-success-starts-with-getting-leadership-on-board/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eWith SRE, failing to plan is planning to fail\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThe process of becoming a successful Site Reliability Engineering shop starts well before you take your first class or read your first ma...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-2200x2200.jpg",
      "date_published": "2021-05-25T18:30:00Z",
      "author": {
        "name": "\u003cname\u003eAyelet Sachto\u003c/name\u003e\u003ctitle\u003eStrategic Cloud Engineer, Infra, AppMod, SRE\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-sli-vs-slo-vs-sla/",
      "title": "SRE fundamentals 2021: SLIs vs SLAs vs SLOs",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;A big part of \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons\u0026#34;\u0026gt;ensuring the availability of your applications\u0026lt;/a\u0026gt; is establishing and monitoring service-level metrics\u0026amp;#8212;something that our \u0026lt;a href=\u0026#34;https://sre.google/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Site Reliability Engineering\u0026lt;/a\u0026gt; (SRE) team does every day here at Google Cloud. The end goal of our SRE principles is to improve services and in turn the user experience.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The concept of SRE starts with the idea that metrics should be closely tied to business objectives. In addition to business-level SLAs, we also use \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\u0026#34;\u0026gt;SLOs and SLIs\u0026lt;/a\u0026gt; in SRE planning and practice.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h2\u0026gt;Defining the terms of site reliability engineering\u0026lt;/h2\u0026gt;\u0026lt;p\u0026gt;These tools aren\u0026amp;#8217;t just useful abstractions. Without them, you won\u0026amp;#8217;t know if your system is reliable, available, or even useful. If the tools don\u0026amp;#8217;t tie back to your business objectives, then you\u0026amp;#8217;ll be missing data on whether your choices are helping or hurting your business.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As a refresher, here\u0026amp;#8217;s a look at SLOs, SLAs, and SLIS, as discussed by our Customer Reliability Engineering team in their blog post, \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\u0026#34;\u0026gt;SLOs, SLIs, SLAs, oh my - CRE life lessons\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;1. Service-Level Objective (SLO)\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;SRE begins with the idea that \u0026lt;a href=\u0026#34;https://sre.google/sre-book/embracing-risk/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;availability is a prerequisite for success\u0026lt;/a\u0026gt;. An unavailable system can\u0026amp;#8217;t perform its function and will fail by default. Availability, in SRE terms, defines whether a system is able to fulfill its intended function at a point in time. In addition to its use as a reporting tool, the historical availability measurement can also describe the probability that your system will perform as expected in the future.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When we set out to define the terms of SRE, we wanted to set a precise numerical target for system availability. We term this target the availability \u0026lt;a href=\u0026#34;https://sre.google/sre-book/service-level-objectives/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Service-Level Objective\u0026lt;/a\u0026gt; (SLO) of our system. Any future discussion about whether the system is running reliably and if any design or architectural changes to it are needed must be framed in terms of our system continuing to meet this SLO.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Keep in mind that the more reliable the service, the more it costs to operate. Define the lowest level of reliability that is acceptable for users of each service, then state that as your SLO. Every service should have an availability SLO\u0026amp;#8212;without it, your team and your stakeholders can\u0026amp;#8217;t make principled judgments about whether your service needs to be made more reliable (increasing cost and slowing development) or less reliable (allowing greater velocity of development). Excessive availability has become the expectation, which can lead to problems. Don\u0026amp;#8217;t make your system overly reliable if the user experience doesn\u0026amp;#8217;t necessitate it, and especially if you don\u0026amp;#8217;t intend to commit to always reaching that level. You can learn more about this by participating in \u0026lt;a href=\u0026#34;https://sre.google/resources/practices-and-processes/art-of-slos/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;The Art of SLOs\u0026lt;/a\u0026gt; training.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026amp;#160;Within Google Cloud, we implement periodic downtime in some services to prevent a service from being overly available. You could also try experimenting with occasional planned-downtime exercises with front-end servers, as we did with one of our internal systems. We found that these exercises can uncover services that are using those servers inappropriately. With that information, you can then move workloads to a more suitable place and keep servers at the right availability level.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;2. Service-Level Agreement (SLA)\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;At Google Cloud, \u0026lt;a href=\u0026#34;https://sre.google/sre-book/service-level-objectives/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;we distinguish between an SLO and a Service-Level Agreement (SLA)\u0026lt;/a\u0026gt;. An SLA normally involves a promise to a service user that the service availability SLO should meet a certain level over a certain period. Failing to do so then results in some kind of penalty. This might be a partial refund of the service subscription fee paid by customers for that period, or additional subscription time added for free. Going out of SLO will hurt the service team, so they will push hard to stay within SLO. If you\u0026amp;#8217;re charging your customers money, you\u0026amp;#8217;ll probably need an SLA.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Because of this, and because of the principle that availability shouldn\u0026amp;#8217;t be much better than the SLO, the availability SLO in the SLA is normally a looser objective than the internal availability SLO. This might be expressed in availability numbers: for instance, an availability SLO of 99.9% over one month, with an internal availability SLO of 99.95%. Alternatively, the SLA might only specify a subset of the metrics that make up the internal SLO.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you have an SLO in your SLA that is different from your internal SLO (as it almost always is), it\u0026amp;#8217;s important for your monitoring to explicitly measure SLO compliance. You want to be able to view your system\u0026amp;#8217;s availability over the SLA calendar period, and quickly see if it appears to be in danger of going out of SLO.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;You\u0026amp;#8217;ll also need a precise measurement of compliance, usually from logs analysis. Since we have an extra set of obligations (described in the SLA) to paying customers, we need to measure queries received from them separately from other queries. This is another benefit of establishing an SLA\u0026amp;#8212;it\u0026amp;#8217;s an unambiguous way to prioritize traffic.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When you define your SLA\u0026amp;#8217;s availability SLO, be careful about which queries you count as legitimate. For example, if a customer goes over quota because they released a buggy version of their mobile client, you may consider excluding all \u0026amp;#8220;out of quota\u0026amp;#8221; response codes from your SLA accounting.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;3. Service-Level Indicator (SLI)\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Our Service-Level Indicator (SLI) is a direct measurement of a service\u0026amp;#8217;s behavior, defined as the frequency of successful probes of our system. When we evaluate whether our system has been running within SLO for the past week, \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons\u0026#34;\u0026gt;we look at the SLI\u0026lt;/a\u0026gt; to get the service availability percentage. If it goes below the specified SLO, we have a problem and may need to make the system more available in some way, such as by running a second instance of the service in a different city and load-balancing between the two. If you want to know how reliable your service is, you must be able to measure the rates of successful and unsuccessful queries as your SLIs.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;If you\u0026amp;#8217;re building a system from scratch, make sure that SLIs and SLOs are part of your system requirements. If you already have a production system but don\u0026amp;#8217;t have them clearly defined, then that\u0026amp;#8217;s your highest priority work.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eA big part of \u003ca href=\"https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons\" track-metadata-module=\"post\"\u003eensuring the availability of your applications\u003c/a\u003e is establishing and monitoring service-level metrics—something that our \u003ca href=\"https://sre.google/\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSite Reliability Engineering\u003c/a\u003e (SRE) team does every day here at Google Cloud. The end goal of our SRE principles is to improve services and in turn the user experience.\u003c/p\u003e\u003cp\u003eThe concept of SRE starts with the idea that metrics should be closely tied to business objectives. In addition to business-level SLAs, we also use \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\" track-metadata-module=\"post\"\u003eSLOs and SLIs\u003c/a\u003e in SRE planning and practice. \u003c/p\u003e\u003ch2\u003eDefining the terms of site reliability engineering\u003c/h2\u003e\u003cp\u003eThese tools aren’t just useful abstractions. Without them, you won’t know if your system is reliable, available, or even useful. If the tools don’t tie back to your business objectives, then you’ll be missing data on whether your choices are helping or hurting your business.\u003c/p\u003e\u003cp\u003eAs a refresher, here’s a look at SLOs, SLAs, and SLIS, as discussed by our Customer Reliability Engineering team in their blog post, \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\" track-metadata-module=\"post\"\u003eSLOs, SLIs, SLAs, oh my - CRE life lessons\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003e1. Service-Level Objective (SLO)\u003c/h3\u003e\u003cp\u003eSRE begins with the idea that \u003ca href=\"https://sre.google/sre-book/embracing-risk/\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eavailability is a prerequisite for success\u003c/a\u003e. An unavailable system can’t perform its function and will fail by default. Availability, in SRE terms, defines whether a system is able to fulfill its intended function at a point in time. In addition to its use as a reporting tool, the historical availability measurement can also describe the probability that your system will perform as expected in the future.\u003c/p\u003e\u003cp\u003eWhen we set out to define the terms of SRE, we wanted to set a precise numerical target for system availability. We term this target the availability \u003ca href=\"https://sre.google/sre-book/service-level-objectives/\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eService-Level Objective\u003c/a\u003e (SLO) of our system. Any future discussion about whether the system is running reliably and if any design or architectural changes to it are needed must be framed in terms of our system continuing to meet this SLO.\u003c/p\u003e\u003cp\u003eKeep in mind that the more reliable the service, the more it costs to operate. Define the lowest level of reliability that is acceptable for users of each service, then state that as your SLO. Every service should have an availability SLO—without it, your team and your stakeholders can’t make principled judgments about whether your service needs to be made more reliable (increasing cost and slowing development) or less reliable (allowing greater velocity of development). Excessive availability has become the expectation, which can lead to problems. Don’t make your system overly reliable if the user experience doesn’t necessitate it, and especially if you don’t intend to commit to always reaching that level. You can learn more about this by participating in \u003ca href=\"https://sre.google/resources/practices-and-processes/art-of-slos/\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eThe Art of SLOs\u003c/a\u003e training.\u003c/p\u003e\u003cp\u003e Within Google Cloud, we implement periodic downtime in some services to prevent a service from being overly available. You could also try experimenting with occasional planned-downtime exercises with front-end servers, as we did with one of our internal systems. We found that these exercises can uncover services that are using those servers inappropriately. With that information, you can then move workloads to a more suitable place and keep servers at the right availability level.\u003c/p\u003e\u003ch3\u003e2. Service-Level Agreement (SLA)\u003c/h3\u003e\u003cp\u003eAt Google Cloud, \u003ca href=\"https://sre.google/sre-book/service-level-objectives/\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003ewe distinguish between an SLO and a Service-Level Agreement (SLA)\u003c/a\u003e. An SLA normally involves a promise to a service user that the service availability SLO should meet a certain level over a certain period. Failing to do so then results in some kind of penalty. This might be a partial refund of the service subscription fee paid by customers for that period, or additional subscription time added for free. Going out of SLO will hurt the service team, so they will push hard to stay within SLO. If you’re charging your customers money, you’ll probably need an SLA.\u003c/p\u003e\u003cp\u003eBecause of this, and because of the principle that availability shouldn’t be much better than the SLO, the availability SLO in the SLA is normally a looser objective than the internal availability SLO. This might be expressed in availability numbers: for instance, an availability SLO of 99.9% over one month, with an internal availability SLO of 99.95%. Alternatively, the SLA might only specify a subset of the metrics that make up the internal SLO.\u003c/p\u003e\u003cp\u003eIf you have an SLO in your SLA that is different from your internal SLO (as it almost always is), it’s important for your monitoring to explicitly measure SLO compliance. You want to be able to view your system’s availability over the SLA calendar period, and quickly see if it appears to be in danger of going out of SLO. \u003c/p\u003e\u003cp\u003eYou’ll also need a precise measurement of compliance, usually from logs analysis. Since we have an extra set of obligations (described in the SLA) to paying customers, we need to measure queries received from them separately from other queries. This is another benefit of establishing an SLA—it’s an unambiguous way to prioritize traffic.\u003c/p\u003e\u003cp\u003eWhen you define your SLA’s availability SLO, be careful about which queries you count as legitimate. For example, if a customer goes over quota because they released a buggy version of their mobile client, you may consider excluding all “out of quota” response codes from your SLA accounting.\u003c/p\u003e\u003ch3\u003e3. Service-Level Indicator (SLI)\u003c/h3\u003e\u003cp\u003eOur Service-Level Indicator (SLI) is a direct measurement of a service’s behavior, defined as the frequency of successful probes of our system. When we evaluate whether our system has been running within SLO for the past week, \u003ca href=\"https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons\" track-metadata-module=\"post\"\u003ewe look at the SLI\u003c/a\u003e to get the service availability percentage. If it goes below the specified SLO, we have a problem and may need to make the system more available in some way, such as by running a second instance of the service in a different city and load-balancing between the two. If you want to know how reliable your service is, you must be able to measure the rates of successful and unsuccessful queries as your SLIs.\u003c/p\u003e\u003cp\u003eIf you’re building a system from scratch, make sure that SLIs and SLOs are part of your system requirements. If you already have a production system but don’t have them clearly defined, then that’s your highest priority work.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eA big part of \u003ca href=\"https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons\"\u003eensuring the availability of your applications\u003c/a\u003e is establishing and monitoring service-level metrics—something that our \u003ca href=\"https://sre.google/\" target=\"_blank\"\u003eSite Reliability Engineering\u003c/a\u003e (SRE) team does every day here at Google Cloud. The end goal of our SRE principles is to improve services and in turn the user experience.\u003c/p\u003e\u003cp\u003eThe concept of SRE starts with the idea that metrics should be closely tied to business objectives. In addition to business-level SLAs, we also use \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\"\u003eSLOs and SLIs\u003c/a\u003e in SRE planning and practice. \u003c/p\u003e\u003ch2\u003eDefining the terms of site reliability engineering\u003c/h2\u003e\u003cp\u003eThese tools aren’t just useful abstractions. Without them, you won’t know if your system is reliable, available, or even useful. If the tools don’t tie back to your business objectives, then you’ll be missing data on whether your choices are helping or hurting your business.\u003c/p\u003e\u003cp\u003eAs a refresher, here’s a look at SLOs, SLAs, and SLIS, as discussed by our Customer Reliability Engineering team in their blog post, \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\"\u003eSLOs, SLIs, SLAs, oh my - CRE life lessons\u003c/a\u003e.\u003c/p\u003e\u003ch3\u003e1. Service-Level Objective (SLO)\u003c/h3\u003e\u003cp\u003eSRE begins with the idea that \u003ca href=\"https://sre.google/sre-book/embracing-risk/\" target=\"_blank\"\u003eavailability is a prerequisite for success\u003c/a\u003e. An unavailable system can’t perform its function and will fail by default. Availability, in SRE terms, defines whether a system is able to fulfill its intended function at a point in time. In addition to its use as a reporting tool, the historical availability measurement can also describe the probability that your system will perform as expected in the future.\u003c/p\u003e\u003cp\u003eWhen we set out to define the terms of SRE, we wanted to set a precise numerical target for system availability. We term this target the availability \u003ca href=\"https://sre.google/sre-book/service-level-objectives/\" target=\"_blank\"\u003eService-Level Objective\u003c/a\u003e (SLO) of our system. Any future discussion about whether the system is running reliably and if any design or architectural changes to it are needed must be framed in terms of our system continuing to meet this SLO.\u003c/p\u003e\u003cp\u003eKeep in mind that the more reliable the service, the more it costs to operate. Define the lowest level of reliability that is acceptable for users of each service, then state that as your SLO. Every service should have an availability SLO—without it, your team and your stakeholders can’t make principled judgments about whether your service needs to be made more reliable (increasing cost and slowing development) or less reliable (allowing greater velocity of development). Excessive availability has become the expectation, which can lead to problems. Don’t make your system overly reliable if the user experience doesn’t necessitate it, and especially if you don’t intend to commit to always reaching that level. You can learn more about this by participating in \u003ca href=\"https://sre.google/resources/practices-and-processes/art-of-slos/\" target=\"_blank\"\u003eThe Art of SLOs\u003c/a\u003e training.\u003c/p\u003e\u003cp\u003e Within Google Cloud, we implement periodic downtime in some services to prevent a service from being overly available. You could also try experimenting with occasional planned-downtime exercises with front-end servers, as we did with one of our internal systems. We found that these exercises can uncover services that are using those servers inappropriately. With that information, you can then move workloads to a more suitable place and keep servers at the right availability level.\u003c/p\u003e\u003ch3\u003e2. Service-Level Agreement (SLA)\u003c/h3\u003e\u003cp\u003eAt Google Cloud, \u003ca href=\"https://sre.google/sre-book/service-level-objectives/\" target=\"_blank\"\u003ewe distinguish between an SLO and a Service-Level Agreement (SLA)\u003c/a\u003e. An SLA normally involves a promise to a service user that the service availability SLO should meet a certain level over a certain period. Failing to do so then results in some kind of penalty. This might be a partial refund of the service subscription fee paid by customers for that period, or additional subscription time added for free. Going out of SLO will hurt the service team, so they will push hard to stay within SLO. If you’re charging your customers money, you’ll probably need an SLA.\u003c/p\u003e\u003cp\u003eBecause of this, and because of the principle that availability shouldn’t be much better than the SLO, the availability SLO in the SLA is normally a looser objective than the internal availability SLO. This might be expressed in availability numbers: for instance, an availability SLO of 99.9% over one month, with an internal availability SLO of 99.95%. Alternatively, the SLA might only specify a subset of the metrics that make up the internal SLO.\u003c/p\u003e\u003cp\u003eIf you have an SLO in your SLA that is different from your internal SLO (as it almost always is), it’s important for your monitoring to explicitly measure SLO compliance. You want to be able to view your system’s availability over the SLA calendar period, and quickly see if it appears to be in danger of going out of SLO. \u003c/p\u003e\u003cp\u003eYou’ll also need a precise measurement of compliance, usually from logs analysis. Since we have an extra set of obligations (described in the SLA) to paying customers, we need to measure queries received from them separately from other queries. This is another benefit of establishing an SLA—it’s an unambiguous way to prioritize traffic.\u003c/p\u003e\u003cp\u003eWhen you define your SLA’s availability SLO, be careful about which queries you count as legitimate. For example, if a customer goes over quota because they released a buggy version of their mobile client, you may consider excluding all “out of quota” response codes from your SLA accounting.\u003c/p\u003e\u003ch3\u003e3. Service-Level Indicator (SLI)\u003c/h3\u003e\u003cp\u003eOur Service-Level Indicator (SLI) is a direct measurement of a service’s behavior, defined as the frequency of successful probes of our system. When we evaluate whether our system has been running within SLO for the past week, \u003ca href=\"https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons\"\u003ewe look at the SLI\u003c/a\u003e to get the service availability percentage. If it goes below the specified SLO, we have a problem and may need to make the system more available in some way, such as by running a second instance of the service in a different city and load-balancing between the two. If you want to know how reliable your service is, you must be able to measure the rates of successful and unsuccessful queries as your SLIs.\u003c/p\u003e\u003cp\u003eIf you’re building a system from scratch, make sure that SLIs and SLOs are part of your system requirements. If you already have a production system but don’t have them clearly defined, then that’s your highest priority work.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_sZG2gQO.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Cloud Monitoring.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Cloud_Monitoring_sZG2gQO.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003cfigcaption class=\"article-image__caption \"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ci\u003e\u003ca href=\"https://cloud.google.com/monitoring\"\u003eCloud Monitoring\u003c/a\u003e provides predefined dashboards for the Google Cloud services that you use. These dashboards require no setup or configuration effort. Learn how to set SLOs in Cloud Monitoring \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/ui/create-slo\"\u003ehere\u003c/a\u003e.\u003c/i\u003e\u003c/div\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eLearn more about these concepts in our \u003ca href=\"https://cloud.google.com/blog/products/management-tools/practical-guide-to-setting-slos\"\u003epractical guide to setting SLOs\u003c/a\u003e, and make use of our \u003ca href=\"https://sre.google/resources/practices-and-processes/art-of-slos/\" target=\"_blank\"\u003eshared training materials\u003c/a\u003e to teach others in your organization.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/sre-at-google-our-complete-list-of-cre-life-lessons/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eSRE at Google: Our complete list of CRE life lessons\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eFind links to blog posts that share Google’s SRE best practices in one handy location.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-2200x2200.jpg",
      "date_published": "2021-05-07T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eAdrian Hilton\u003c/name\u003e\u003ctitle\u003eCustomer Reliability Engineer, SRE\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/take-2021-state-devops-survey-shape-future-devops/",
      "title": "Take the 2021 State of DevOps survey: Shape the future of DevOps",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Today, Google Cloud and the \u0026lt;a href=\u0026#34;https://www.devops-research.com/research.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DORA\u0026lt;/a\u0026gt; research team are excited to announce the launch of the \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;2021 State of DevOps survey\u0026lt;/a\u0026gt;. The \u0026lt;a href=\u0026#34;https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;survey\u0026lt;/a\u0026gt; takes approximately 25 minutes to complete and we\u0026amp;#8217;d love to hear from you. Your answers will allow us to better understand the practices that teams are employing to improve software delivery performance and inturn generate powerful business outcomes.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The \u0026lt;a href=\u0026#34;https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\u0026#34;\u0026gt;State of DevOps report\u0026lt;/a\u0026gt; by Google Cloud and the DORA research team is the largest and longest running research of its kind. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region can employ to drive better performance.\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Like the past six research reports, our goal this year is to perform detailed analysis to help various teams benchmark their performance against the industry as elite, high, medium, or low performers. We also look to show specific strategies that teams can employ to improve their performance.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The table below highlights elite, high, medium, and low performers at a glance from the \u0026lt;a href=\u0026#34;https://cloud.google.com/devops/state-of-devops\u0026#34;\u0026gt;last report.\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eToday, Google Cloud and the \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDORA\u003c/a\u003e research team are excited to announce the launch of the \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003e2021 State of DevOps survey\u003c/a\u003e. The \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://google.qualtrics.com\" track-metadata-module=\"post\"\u003esurvey\u003c/a\u003e takes approximately 25 minutes to complete and we’d love to hear from you. Your answers will allow us to better understand the practices that teams are employing to improve software delivery performance and inturn generate powerful business outcomes.\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\" track-metadata-module=\"post\"\u003eState of DevOps report\u003c/a\u003e by Google Cloud and the DORA research team is the largest and longest running research of its kind. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region can employ to drive better performance.  \u003c/p\u003e\u003cp\u003eLike the past six research reports, our goal this year is to perform detailed analysis to help various teams benchmark their performance against the industry as elite, high, medium, or low performers. We also look to show specific strategies that teams can employ to improve their performance. \u003c/p\u003e\u003cp\u003eThe table below highlights elite, high, medium, and low performers at a glance from the \u003ca href=\"https://cloud.google.com/devops/state-of-devops\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/devops/state-of-devops\" track-metadata-module=\"post\"\u003elast report.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eToday, Google Cloud and the \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\"\u003eDORA\u003c/a\u003e research team are excited to announce the launch of the \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\"\u003e2021 State of DevOps survey\u003c/a\u003e. The \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\"\u003esurvey\u003c/a\u003e takes approximately 25 minutes to complete and we’d love to hear from you. Your answers will allow us to better understand the practices that teams are employing to improve software delivery performance and inturn generate powerful business outcomes.\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://cloud.google.com/devops#read-dora%E2%80%99s-state-of-devops-reports-and-devops-roi-whitepaper\"\u003eState of DevOps report\u003c/a\u003e by Google Cloud and the DORA research team is the largest and longest running research of its kind. It provides an independent view into the practices and capabilities that organizations, irrespective of their size, industry, and region can employ to drive better performance.  \u003c/p\u003e\u003cp\u003eLike the past six research reports, our goal this year is to perform detailed analysis to help various teams benchmark their performance against the industry as elite, high, medium, or low performers. We also look to show specific strategies that teams can employ to improve their performance. \u003c/p\u003e\u003cp\u003eThe table below highlights elite, high, medium, and low performers at a glance from the \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003elast report.\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"dora2\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Screen_Shot_2021-05-03_at_12.43.32_AM.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAchieving elite performance is a team endeavor and diverse, inclusive teams drive the best performance. The research program benefits from the participation of a diverse group of people. Please help us encourage more voices by sharing this survey with your network, especially with your colleagues from underrepresented parts of our industry. \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\"\u003esurvey\u003c/a\u003e is for everyone, regardless of where you are on your DevOps journey, the size of your organization, or your organization's industry. There are no right or wrong answers, in fact we often hear feedback that questions in the survey prompt ideas for improvement. Many of these ideas can be put into practice immediately. \u003c/p\u003e\u003cp\u003eSome of the key topics we look to deep dive into this year include:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eMetrics and Measurement: Practices employed by high performing teams \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eSRE and DevOps: How do they fit together and how they impact performance\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eHow to best integrate security \u0026amp; compliance as a part of your app development \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe impact of cloud, monitoring \u0026amp; observability, open source, and documentation on performance\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDistributed teams: Practices to improve work/life balance and reduce burnout\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eThe state of multi-cloud computing \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHelp us shape the future of DevOps and make your voice heard by completing the \u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\"\u003esurvey now\u003c/a\u003e. \u003c/p\u003e\u003cp\u003e\u003ca href=\"https://google.qualtrics.com/jfe/form/SV_cIb0SmhJPfm8H7n\" target=\"_blank\"\u003eThe survey\u003c/a\u003e will remain open until midnight PST on July 2, 2021. We look forward to hearing from you. \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/WorkplaceTransformation-01_TjiwJTd.max-1000x1000.png",
      "date_published": "2021-05-03T13:50:00Z",
      "author": {
        "name": "\u003cname\u003eDustin Smith\u003c/name\u003e\u003ctitle\u003eDORA Research Lead\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/sre-at-google-our-complete-list-of-cre-life-lessons/",
      "title": "SRE at Google: Our complete list of CRE life lessons",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;In 2016 we announced a new discipline at Google, \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\u0026#34;\u0026gt;Customer Reliability Engineering\u0026lt;/a\u0026gt;, an offshoot of \u0026lt;a href=\u0026#34;https://sre.google/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Site Reliability Engineering\u0026lt;/a\u0026gt; (SRE). Our goal with CRE was (and still is) to create a shared operational fate between Google and our Google Cloud customers, to give you more control over the critical applications you\u0026#39;re entrusting to us. Since then, here on the Google Cloud blog, we\u0026amp;#8217;ve published a wealth of resources to help you take the best practices we\u0026amp;#8217;ve learned from SRE teams at Google and apply them in your own environments.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Below is the complete list of CRE life lessons posts we\u0026amp;#8217;ve published \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/topics/cre-life-lessons\u0026#34;\u0026gt;in the past five years\u0026lt;/a\u0026gt; in one convenient location.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Common pitfalls\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\u0026#34;\u0026gt;Know thy enemy: How to prioritize and communicate risks\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/how-to-avoid-a-self-inflicted-ddos-attack-cre-life-lessons\u0026#34;\u0026gt;How to avoid a self-inflicted DDoS Attack\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons\u0026#34;\u0026gt;Using load shedding to survive a success disaster\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Service-level metrics\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons\u0026#34;\u0026gt;Available . . . or not? That is the question\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\u0026#34;\u0026gt;SLOs, SLIs, SLAs, oh my\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/building-good-slos-cre-life-lessons\u0026#34;\u0026gt;Building good SLOs\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/consequences-of-slo-violations-cre-life-lessons\u0026#34;\u0026gt;Consequences of SLO violations\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/an-example-escalation-policy-cre-life-lessons\u0026#34;\u0026gt;An example escalation policy\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/applying-the-escalation-policy-cre-life-lessons\u0026#34;\u0026gt;Applying the escalation policy\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/defining-slos-for-services-with-dependencies-cre-life-lessons\u0026#34;\u0026gt;Defining SLOs for services with dependencies\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons\u0026#34;\u0026gt;Tune up your SLI metrics\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/learn-how-to-set-slos-for-an-sre-or-cre-practice\u0026#34;\u0026gt;Learning\u0026amp;#8212;and teaching\u0026amp;#8212;the art of service-level objectives\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/using-deemed-slis-to-measure-customer-reliability\u0026#34;\u0026gt;Using deemed SLIs to measure customer reliability\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Releases\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons\u0026#34;\u0026gt;Reliable releases and rollbacks\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/how-release-canaries-can-save-your-bacon-cre-life-lessons\u0026#34;\u0026gt;How release canaries can save your bacon\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;SRE support\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/why-should-your-app-get-sre-support-cre-life-lessons\u0026#34;\u0026gt;Why should your app get SRE support?\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons\u0026#34;\u0026gt;How SREs find the landmines in a service\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/making-the-most-of-an-sre-service-takeover-cre-life-lessons\u0026#34;\u0026gt;Making the most of an SRE service takeover\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Dark launches\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/cre-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me\u0026#34;\u0026gt;What is a dark launch, and what does it do for me?\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/cre-life-lessons-practicalities-of-dark-launches\u0026#34;\u0026gt;The practicalities of dark launching\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Postmortems\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons\u0026#34;\u0026gt;Fearless shared postmortems\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons\u0026#34;\u0026gt;Getting the most out of shared postmortems\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Error budgets\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/good-housekeeping-error-budgetscre-life-lessons\u0026#34;\u0026gt;Good housekeeping for error budgets\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons\u0026#34;\u0026gt;Understanding error budget overspend\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Production incidents\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons\u0026#34;\u0026gt;Shrinking the impact of production incidents using SRE principles\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents\u0026#34;\u0026gt;Shrinking the time to mitigate production incidents\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Becoming an SRE team\u0026lt;/h3\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\u0026#34;\u0026gt;How to implement a successful SRE practice from the outset\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice\u0026#34;\u0026gt;Jumpstarting your SRE practice\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum\u0026#34;\u0026gt;Assessing an SRE team\u0026amp;#8217;s maturity\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We still have plenty more articles to come, so keep your eye on our \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre\u0026#34;\u0026gt;DevOps \u0026amp;amp; SRE channel\u0026lt;/a\u0026gt;. You can also check out \u0026lt;a href=\u0026#34;http://sre.google\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;sre.google\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://sre.google/books/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;read our SRE books online\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eIn 2016 we announced a new discipline at Google, \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\" track-metadata-module=\"post\"\u003eCustomer Reliability Engineering\u003c/a\u003e, an offshoot of \u003ca href=\"https://sre.google/\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSite Reliability Engineering\u003c/a\u003e (SRE). Our goal with CRE was (and still is) to create a shared operational fate between Google and our Google Cloud customers, to give you more control over the critical applications you\u0026#39;re entrusting to us. Since then, here on the Google Cloud blog, we’ve published a wealth of resources to help you take the best practices we’ve learned from SRE teams at Google and apply them in your own environments. \u003c/p\u003e\u003cp\u003eBelow is the complete list of CRE life lessons posts we’ve published \u003ca href=\"https://cloud.google.com/blog/topics/cre-life-lessons\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/topics/cre-life-lessons\" track-metadata-module=\"post\"\u003ein the past five years\u003c/a\u003e in one convenient location.\u003c/p\u003e\u003ch3\u003eCommon pitfalls\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\" track-metadata-module=\"post\"\u003eKnow thy enemy: How to prioritize and communicate risks\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/how-to-avoid-a-self-inflicted-ddos-attack-cre-life-lessons\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/how-to-avoid-a-self-inflicted-ddos-attack-cre-life-lessons\" track-metadata-module=\"post\"\u003eHow to avoid a self-inflicted DDoS Attack\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons\" track-metadata-module=\"post\"\u003eUsing load shedding to survive a success disaster\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eService-level metrics\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons\" track-metadata-module=\"post\"\u003eAvailable . . . or not? That is the question\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\" track-metadata-module=\"post\"\u003eSLOs, SLIs, SLAs, oh my\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/building-good-slos-cre-life-lessons\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/building-good-slos-cre-life-lessons\" track-metadata-module=\"post\"\u003eBuilding good SLOs\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/consequences-of-slo-violations-cre-life-lessons\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/consequences-of-slo-violations-cre-life-lessons\" track-metadata-module=\"post\"\u003eConsequences of SLO violations\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/an-example-escalation-policy-cre-life-lessons\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/an-example-escalation-policy-cre-life-lessons\" track-metadata-module=\"post\"\u003eAn example escalation policy\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/applying-the-escalation-policy-cre-life-lessons\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/applying-the-escalation-policy-cre-life-lessons\" track-metadata-module=\"post\"\u003eApplying the escalation policy\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/defining-slos-for-services-with-dependencies-cre-life-lessons\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/defining-slos-for-services-with-dependencies-cre-life-lessons\" track-metadata-module=\"post\"\u003eDefining SLOs for services with dependencies\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons\" track-type=\"inline link\" track-name=\"14\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons\" track-metadata-module=\"post\"\u003eTune up your SLI metrics\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/management-tools/learn-how-to-set-slos-for-an-sre-or-cre-practice\" track-type=\"inline link\" track-name=\"15\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/learn-how-to-set-slos-for-an-sre-or-cre-practice\" track-metadata-module=\"post\"\u003eLearning—and teaching—the art of service-level objectives\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/management-tools/using-deemed-slis-to-measure-customer-reliability\" track-type=\"inline link\" track-name=\"16\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/using-deemed-slis-to-measure-customer-reliability\" track-metadata-module=\"post\"\u003eUsing deemed SLIs to measure customer reliability\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eReleases\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons\" track-type=\"inline link\" track-name=\"17\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons\" track-metadata-module=\"post\"\u003eReliable releases and rollbacks\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/how-release-canaries-can-save-your-bacon-cre-life-lessons\" track-type=\"inline link\" track-name=\"18\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/how-release-canaries-can-save-your-bacon-cre-life-lessons\" track-metadata-module=\"post\"\u003eHow release canaries can save your bacon\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eSRE support\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/why-should-your-app-get-sre-support-cre-life-lessons\" track-type=\"inline link\" track-name=\"19\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/why-should-your-app-get-sre-support-cre-life-lessons\" track-metadata-module=\"post\"\u003eWhy should your app get SRE support?\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons\" track-type=\"inline link\" track-name=\"20\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons\" track-metadata-module=\"post\"\u003eHow SREs find the landmines in a service\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/making-the-most-of-an-sre-service-takeover-cre-life-lessons\" track-type=\"inline link\" track-name=\"21\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/making-the-most-of-an-sre-service-takeover-cre-life-lessons\" track-metadata-module=\"post\"\u003eMaking the most of an SRE service takeover\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eDark launches\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/cre-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me\" track-type=\"inline link\" track-name=\"22\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/cre-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me\" track-metadata-module=\"post\"\u003eWhat is a dark launch, and what does it do for me?\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/cre-life-lessons-practicalities-of-dark-launches\" track-type=\"inline link\" track-name=\"23\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/cre-life-lessons-practicalities-of-dark-launches\" track-metadata-module=\"post\"\u003eThe practicalities of dark launching\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003ePostmortems\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons\" track-type=\"inline link\" track-name=\"24\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons\" track-metadata-module=\"post\"\u003eFearless shared postmortems\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons\" track-type=\"inline link\" track-name=\"25\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons\" track-metadata-module=\"post\"\u003eGetting the most out of shared postmortems\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eError budgets\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/good-housekeeping-error-budgetscre-life-lessons\" track-type=\"inline link\" track-name=\"26\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/good-housekeeping-error-budgetscre-life-lessons\" track-metadata-module=\"post\"\u003eGood housekeeping for error budgets\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons\" track-type=\"inline link\" track-name=\"27\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons\" track-metadata-module=\"post\"\u003eUnderstanding error budget overspend\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eProduction incidents\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons\" track-type=\"inline link\" track-name=\"28\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons\" track-metadata-module=\"post\"\u003eShrinking the impact of production incidents using SRE principles\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents\" track-type=\"inline link\" track-name=\"29\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents\" track-metadata-module=\"post\"\u003eShrinking the time to mitigate production incidents\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eBecoming an SRE team\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\" track-type=\"inline link\" track-name=\"30\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\" track-metadata-module=\"post\"\u003eHow to implement a successful SRE practice from the outset\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice\" track-type=\"inline link\" track-name=\"31\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice\" track-metadata-module=\"post\"\u003eJumpstarting your SRE practice\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum\" track-type=\"inline link\" track-name=\"32\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum\" track-metadata-module=\"post\"\u003eAssessing an SRE team’s maturity\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe still have plenty more articles to come, so keep your eye on our \u003ca href=\"https://cloud.google.com/blog/products/devops-sre\" track-type=\"inline link\" track-name=\"33\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre\" track-metadata-module=\"post\"\u003eDevOps \u0026amp; SRE channel\u003c/a\u003e. You can also check out \u003ca href=\"http://sre.google\" target=\"_blank\" track-type=\"inline link\" track-name=\"34\" track-metadata-eventdetail=\"http://sre.google\" track-metadata-module=\"post\"\u003esre.google\u003c/a\u003e or \u003ca href=\"https://sre.google/books/\" target=\"_blank\" track-type=\"inline link\" track-name=\"35\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eread our SRE books online\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIn 2016 we announced a new discipline at Google, \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering\"\u003eCustomer Reliability Engineering\u003c/a\u003e, an offshoot of \u003ca href=\"https://sre.google/\" target=\"_blank\"\u003eSite Reliability Engineering\u003c/a\u003e (SRE). Our goal with CRE was (and still is) to create a shared operational fate between Google and our Google Cloud customers, to give you more control over the critical applications you're entrusting to us. Since then, here on the Google Cloud blog, we’ve published a wealth of resources to help you take the best practices we’ve learned from SRE teams at Google and apply them in your own environments. \u003c/p\u003e\u003cp\u003eBelow is the complete list of CRE life lessons posts we’ve published \u003ca href=\"https://cloud.google.com/blog/topics/cre-life-lessons\"\u003ein the past five years\u003c/a\u003e in one convenient location.\u003c/p\u003e\u003ch3\u003eCommon pitfalls\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/know-thy-enemy-how-to-prioritize-and-communicate-risks-cre-life-lessons\"\u003eKnow thy enemy: How to prioritize and communicate risks\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/how-to-avoid-a-self-inflicted-ddos-attack-cre-life-lessons\"\u003eHow to avoid a self-inflicted DDoS Attack\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/using-load-shedding-to-survive-a-success-disaster-cre-life-lessons\"\u003eUsing load shedding to survive a success disaster\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eService-level metrics\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/available-or-not-that-is-the-question-cre-life-lessons\"\u003eAvailable . . . or not? That is the question\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/availability-part-deux-cre-life-lessons\"\u003eSLOs, SLIs, SLAs, oh my\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/building-good-slos-cre-life-lessons\"\u003eBuilding good SLOs\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/consequences-of-slo-violations-cre-life-lessons\"\u003eConsequences of SLO violations\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/an-example-escalation-policy-cre-life-lessons\"\u003eAn example escalation policy\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/applying-the-escalation-policy-cre-life-lessons\"\u003eApplying the escalation policy\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/defining-slos-for-services-with-dependencies-cre-life-lessons\"\u003eDefining SLOs for services with dependencies\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons\"\u003eTune up your SLI metrics\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/management-tools/learn-how-to-set-slos-for-an-sre-or-cre-practice\"\u003eLearning—and teaching—the art of service-level objectives\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/management-tools/using-deemed-slis-to-measure-customer-reliability\"\u003eUsing deemed SLIs to measure customer reliability\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eReleases\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons\"\u003eReliable releases and rollbacks\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/how-release-canaries-can-save-your-bacon-cre-life-lessons\"\u003eHow release canaries can save your bacon\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eSRE support\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/why-should-your-app-get-sre-support-cre-life-lessons\"\u003eWhy should your app get SRE support?\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/how-sres-find-the-landmines-in-a-service-cre-life-lessons\"\u003eHow SREs find the landmines in a service\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/making-the-most-of-an-sre-service-takeover-cre-life-lessons\"\u003eMaking the most of an SRE service takeover\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eDark launches\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/cre-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me\"\u003eWhat is a dark launch, and what does it do for me?\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/cre-life-lessons-practicalities-of-dark-launches\"\u003eThe practicalities of dark launching\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003ePostmortems\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/fearless-shared-postmortems-cre-life-lessons\"\u003eFearless shared postmortems\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/getting-the-most-out-of-shared-postmortems-cre-life-lessons\"\u003eGetting the most out of shared postmortems\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eError budgets\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/good-housekeeping-error-budgetscre-life-lessons\"\u003eGood housekeeping for error budgets\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/gcp/understanding-error-budget-overspend-cre-life-lessons\"\u003eUnderstanding error budget overspend\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eProduction incidents\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/shrinking-the-impact-of-production-incidents-using-sre-principles-cre-life-lessons\"\u003eShrinking the impact of production incidents using SRE principles\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/management-tools/shrinking-the-time-to-mitigate-production-incidents\"\u003eShrinking the time to mitigate production incidents\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003ch3\u003eBecoming an SRE team\u003c/h3\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board\"\u003eHow to implement a successful SRE practice from the outset\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice\"\u003eJumpstarting your SRE practice\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/blog/products/devops-sre/evaluating-where-your-team-lies-on-the-sre-spectrum\"\u003eAssessing an SRE team’s maturity\u003c/a\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003c/p\u003e\u003cp\u003eWe still have plenty more articles to come, so keep your eye on our \u003ca href=\"https://cloud.google.com/blog/products/devops-sre\"\u003eDevOps \u0026amp; SRE channel\u003c/a\u003e. You can also check out \u003ca href=\"http://sre.google\" target=\"_blank\"\u003esre.google\u003c/a\u003e or \u003ca href=\"https://sre.google/books/\" target=\"_blank\"\u003eread our SRE books online\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/sre-fundamentals-sli-vs-slo-vs-sla/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eSRE fundamentals 2021: SLIs vs SLAs vs SLOs\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eWhat’s the difference between an SLI, an SLO and an SLA? Google Site Reliability Engineers (SRE) explain.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-2200x2200.jpg",
      "date_published": "2021-04-27T20:00:00Z",
      "author": {
        "name": "\u003cname\u003eThe Google Cloud content marketing team \u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/containers-kubernetes/how-configuration-as-data-impacts-policy/",
      "title": "Sign here! Creating a policy contract with Configuration as Data",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003crouter-outlet\u003e\u003c/router-outlet\u003e\u003cdynamic-page\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#containers\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-author-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cp\u003e Mark Balch \u003c/p\u003e\u003cp\u003e Senior Product Manager, Google Cloud \u003c/p\u003e\u003c/div\u003e\u003cp\u003e\u003cspan\u003e April 26, 2021 \u003c/span\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-author-block\u003e\u003c/div\u003e\u003carticle-cta _nghost-c17=\"\"\u003e\u003cdiv _ngcontent-c17=\"\"\u003e\u003ch4 _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eTry GCP\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eStart building on Google Cloud with $300 in free credits and 20+ always free products.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c17=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"free trial\" track-metadata-eventdetail=\"https://cloud.google.com/free/\" href=\"https://cloud.google.com/free/\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eFree Trial\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Configuration as Data is an emerging cloud infrastructure management paradigm that allows developers to \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes\u0026#34;\u0026gt;declare the desired state\u0026lt;/a\u0026gt; of their applications and infrastructure, without specifying the precise actions or steps for how to achieve it. However, declaring a configuration is only half the battle: you also want policy that defines how a configuration is to be used.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Configuration as Data enables a normalized policy contract across all your cloud resources. That contract, knowing how your deployment will operate, can be inspected and enforced throughout a CI/CD pipeline, from upstream in your development environment to deployment time, and ongoing in the live runtime environment. This consistency is possible by expressing configuration as data throughout the development and operations lifecycle.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://cloud.google.com/config-connector/docs/overview\u0026#34;\u0026gt;Config Connector\u0026lt;/a\u0026gt; is the tool that allows you to express configuration as data in Google Cloud. In this model, configuration is what you want to deploy, such as \u0026amp;#8220;a storage bucket named \u0026lt;code\u0026gt;my-bucket\u0026lt;/code\u0026gt; with a standard storage class and uniform access control.\u0026amp;#8221;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Policy, meanwhile, typically specifies what you\u0026amp;#8217;re allowed to deploy, usually in conformance with your organization\u0026amp;#8217;s compliance needs. For example, \u0026amp;#8220;all resources must be deployed in Google Cloud\u0026amp;#8217;s \u0026lt;code\u0026gt;LONDON\u0026lt;/code\u0026gt; region.\u0026amp;#8221;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When each stage in your pipeline treats configuration as data, you can use any tool or language to manipulate configuration as data, knowing they will interoperate and that policy can be consistently enforced at any or all stages. And while a policy engine won\u0026amp;#8217;t be able to understand every tool, it can validate the data generated by each tool. It\u0026amp;#8217;s just like data in a database can be inspected by anyone who knows the schema regardless of the tool that wrote into the database.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Contrast that with pipelines today, where policy is manually validated, hard coded in scripts within the pipeline logic itself, or post-processed on raw deployment artifacts after rendering configuration templates into specific instances. In each case, policy is siloed\u0026amp;#8212;you can\u0026amp;#8217;t take the same policy and apply it anywhere in your pipeline because formats differ from tool to tool.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Helm, for example, contains code \u0026lt;a href=\u0026#34;https://helm.sh/docs/chart_template_guide/control_structures/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;specific to its own format\u0026lt;/a\u0026gt;.\u0026lt;sup\u0026gt;1\u0026lt;/sup\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eConfiguration as Data is an emerging cloud infrastructure management paradigm that allows developers to \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes\" track-metadata-module=\"post\"\u003edeclare the desired state\u003c/a\u003e of their applications and infrastructure, without specifying the precise actions or steps for how to achieve it. However, declaring a configuration is only half the battle: you also want policy that defines how a configuration is to be used. \u003c/p\u003e\u003cp\u003eConfiguration as Data enables a normalized policy contract across all your cloud resources. That contract, knowing how your deployment will operate, can be inspected and enforced throughout a CI/CD pipeline, from upstream in your development environment to deployment time, and ongoing in the live runtime environment. This consistency is possible by expressing configuration as data throughout the development and operations lifecycle.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/config-connector/docs/overview\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/config-connector/docs/overview\" track-metadata-module=\"post\"\u003eConfig Connector\u003c/a\u003e is the tool that allows you to express configuration as data in Google Cloud. In this model, configuration is what you want to deploy, such as “a storage bucket named \u003ccode\u003emy-bucket\u003c/code\u003e with a standard storage class and uniform access control.” \u003c/p\u003e\u003cp\u003ePolicy, meanwhile, typically specifies what you’re allowed to deploy, usually in conformance with your organization’s compliance needs. For example, “all resources must be deployed in Google Cloud’s \u003ccode\u003eLONDON\u003c/code\u003e region.” \u003c/p\u003e\u003cp\u003eWhen each stage in your pipeline treats configuration as data, you can use any tool or language to manipulate configuration as data, knowing they will interoperate and that policy can be consistently enforced at any or all stages. And while a policy engine won’t be able to understand every tool, it can validate the data generated by each tool. It’s just like data in a database can be inspected by anyone who knows the schema regardless of the tool that wrote into the database.\u003c/p\u003e\u003cp\u003eContrast that with pipelines today, where policy is manually validated, hard coded in scripts within the pipeline logic itself, or post-processed on raw deployment artifacts after rendering configuration templates into specific instances. In each case, policy is siloed—you can’t take the same policy and apply it anywhere in your pipeline because formats differ from tool to tool. \u003c/p\u003e\u003cp\u003eHelm, for example, contains code \u003ca href=\"https://helm.sh/docs/chart_template_guide/control_structures/\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://helm.sh\" track-metadata-module=\"post\"\u003especific to its own format\u003c/a\u003e.\u003csup\u003e1\u003c/sup\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003e{{- if .Values.master.usePodSecurityContext }}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      securityContext:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e        runAsUser: {{ default 0 .Values.master.runAsUser }}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e{{- if and (.Values.master.runAsUser) (.Values.master.fsGroup) }}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e{{- if not (eq (int .Values.master.runAsUser) 0) }}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e        fsGroup: {{ .Values.master.fsGroup }}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e{{- end }}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e{{- end }}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e{{- end }}\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003edata \u0026#34;helm_repository\u0026#34; \u0026#34;stable\u0026#34; {\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  name = \u0026#34;stable\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  url  = \u0026#34;https://kubernetes-charts.storage.googleapis.com/\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003eresource \u0026#34;helm_release\u0026#34; \u0026#34;default\u0026#34; {\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  name  = \u0026#34;spin\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  chart = \u0026#34;stable/spinnaker\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  values = [local.helm_chart_values]\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  timeout = 1200\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e}\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eThe HCL becomes a \u003ca href=\"https://www.terraform.io/docs/internals/json-format.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://www.terraform.io\" track-metadata-module=\"post\"\u003eJSON plan\u003c/a\u003e, where the deployment-ready configuration may be validated before being applied to the live environment.\u003csup\u003e3\u003c/sup\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003e# google_compute_instance.vm_instance will be created\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  + resource \u0026#34;google_compute_instance\u0026#34; \u0026#34;vm_instance\u0026#34; {\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      + can_ip_forward       = false\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      + cpu_platform         = (known after apply)\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      + deletion_protection  = false\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      + guest_accelerator    = (known after apply)\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      + id                   = (known after apply)\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      + instance_id          = (known after apply)\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      + label_fingerprint    = (known after apply)\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      + machine_type         = \u0026#34;f1-micro\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e...\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e        }\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;These examples show three disparate data formats across two different tools representing different portions of a desired end state. Add in Python scripting, gcloud CLI, or \u0026lt;code\u0026gt;kubectl\u0026lt;/code\u0026gt; commands and you start approaching ten different formats\u0026amp;#8212;all for the same deployment!\u0026amp;#160; Reliably enforcing a policy contract requires you to inject tool- and format-specific validation logic on case-by-case basis. If you decide to move a config step from Python to Terraform or from Terraform to \u0026lt;code\u0026gt;kubectl\u0026lt;/code\u0026gt;, you\u0026amp;#8217;ll need to re-evaluate your contract and probably re-implement some of that policy validation.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Why don\u0026amp;#8217;t these tools work together cleanly? Why does policy validation change depending on the development tools you\u0026amp;#8217;re using? Each tool can do a good job enforcing policy within itself. As long as you use that tool everywhere, things will probably work ok. But we all know that\u0026amp;#8217;s not how development works. People tend to choose tools that fit their needs and figure out integration later on.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;A Rosetta Stone for policy contracts\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Imagine that everyone is defining their configuration as data, while using tools and formats of their choice. Terraform or Python for orchestration. Helm for application packaging. Java or Go for data transformation and validation. Once the data format is understood (because it is open source and extensible), your pipeline becomes a bus that anyone can push configuration onto and pull configuration from.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThese examples show three disparate data formats across two different tools representing different portions of a desired end state. Add in Python scripting, gcloud CLI, or \u003ccode\u003ekubectl\u003c/code\u003e commands and you start approaching ten different formats—all for the same deployment!  Reliably enforcing a policy contract requires you to inject tool- and format-specific validation logic on case-by-case basis. If you decide to move a config step from Python to Terraform or from Terraform to \u003ccode\u003ekubectl\u003c/code\u003e, you’ll need to re-evaluate your contract and probably re-implement some of that policy validation. \u003c/p\u003e\u003cp\u003eWhy don’t these tools work together cleanly? Why does policy validation change depending on the development tools you’re using? Each tool can do a good job enforcing policy within itself. As long as you use that tool everywhere, things will probably work ok. But we all know that’s not how development works. People tend to choose tools that fit their needs and figure out integration later on.\u003c/p\u003e\u003ch3\u003eA Rosetta Stone for policy contracts\u003c/h3\u003e\u003cp\u003eImagine that everyone is defining their configuration as data, while using tools and formats of their choice. Terraform or Python for orchestration. Helm for application packaging. Java or Go for data transformation and validation. Once the data format is understood (because it is open source and extensible), your pipeline becomes a bus that anyone can push configuration onto and pull configuration from.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Policies can be automatically validated at commit or build time using custom and \u0026lt;a href=\u0026#34;https://googlecontainertools.github.io/kpt/guides/consumer/function/catalog/validators/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;off-the-shelf functions\u0026lt;/a\u0026gt; that operate on YAML. You can manage commit and merge permissions separately for config and policy to separate these distinct concerns. You can have folders and unique permissions for org-wide policy, team-wide policy, or app-specific policy. Therein lies the dream.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;The most common way to generate configuration is to simply write a YAML file describing how Kubernetes should create a resource for you. The resulting YAML file is then stored in a git repository where it can be versioned and picked up by another tool and applied to a Kubernetes cluster. Policies can be enforced on the git repo side to limit who can push changes to the repository and ultimately reference them at deploy time.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For most users this is not where policy enforcement ends. While code reviews can catch a lot of things, it\u0026amp;#8217;s considered best practice to \u0026amp;#8220;trust but verify\u0026amp;#8221; at all layers in the stack. That\u0026amp;#8217;s where admission controllers come in, which can be considered to be the last mile of policy enforcement. \u0026lt;a href=\u0026#34;https://www.openpolicyagent.org/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Gatekeeper\u0026lt;/a\u0026gt; serves as an admission controller inside of a Kubernetes cluster. Only configurations that meet defined constraints will be admitted to the live cloud environment.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Let\u0026amp;#8217;s tie these concepts together \u0026lt;a href=\u0026#34;https://github.com/kelseyhightower/config-connector-policy-demo\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;with an example\u0026lt;/a\u0026gt;. Imagine you want to enable users to create Cloud Storage buckets, but you don\u0026amp;#8217;t want them doing so using the Google Cloud Console or the gcloud command line tool. You want all users to declare what they want and push those changes to a git repository for review before the underlying Cloud Storage buckets are created with \u0026lt;a href=\u0026#34;https://cloud.google.com/config-connector/docs/overview\u0026#34;\u0026gt;Config Connector\u0026lt;/a\u0026gt;. Essentially you want users to be able to submit a YAML file that looks like this:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003ePolicies can be automatically validated at commit or build time using custom and \u003ca href=\"https://googlecontainertools.github.io/kpt/guides/consumer/function/catalog/validators/\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://googlecontainertools.github.io\" track-metadata-module=\"post\"\u003eoff-the-shelf functions\u003c/a\u003e that operate on YAML. You can manage commit and merge permissions separately for config and policy to separate these distinct concerns. You can have folders and unique permissions for org-wide policy, team-wide policy, or app-specific policy. Therein lies the dream. \u003c/p\u003e\u003cp\u003eThe most common way to generate configuration is to simply write a YAML file describing how Kubernetes should create a resource for you. The resulting YAML file is then stored in a git repository where it can be versioned and picked up by another tool and applied to a Kubernetes cluster. Policies can be enforced on the git repo side to limit who can push changes to the repository and ultimately reference them at deploy time.\u003c/p\u003e\u003cp\u003eFor most users this is not where policy enforcement ends. While code reviews can catch a lot of things, it’s considered best practice to “trust but verify” at all layers in the stack. That’s where admission controllers come in, which can be considered to be the last mile of policy enforcement. \u003ca href=\"https://www.openpolicyagent.org/\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://www.openpolicyagent.org\" track-metadata-module=\"post\"\u003eGatekeeper\u003c/a\u003e serves as an admission controller inside of a Kubernetes cluster. Only configurations that meet defined constraints will be admitted to the live cloud environment.\u003c/p\u003e\u003cp\u003eLet’s tie these concepts together \u003ca href=\"https://github.com/kelseyhightower/config-connector-policy-demo\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ewith an example\u003c/a\u003e. Imagine you want to enable users to create Cloud Storage buckets, but you don’t want them doing so using the Google Cloud Console or the gcloud command line tool. You want all users to declare what they want and push those changes to a git repository for review before the underlying Cloud Storage buckets are created with \u003ca href=\"https://cloud.google.com/config-connector/docs/overview\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/config-connector/docs/overview\" track-metadata-module=\"post\"\u003eConfig Connector\u003c/a\u003e. Essentially you want users to be able to submit a YAML file that looks like this:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003eapiVersion: storage.cnrm.cloud.google.com/v1beta1\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003ekind: StorageBucket\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003emetadata:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  name: ${BUCKET_NAME}\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;This creates a storage bucket in a default location. There is one problem with this: users can create buckets in any location even if company policy dictates otherwise. Sure, you can catch people using forbidden bucket locations during code review, but that\u0026amp;#8217;s prone to human error.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;This is where Gatekeeper comes in. You want the ability to limit which Cloud Storage bucket location can be used. Ideally you can write policies that look like this:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThis creates a storage bucket in a default location. There is one problem with this: users can create buckets in any location even if company policy dictates otherwise. Sure, you can catch people using forbidden bucket locations during code review, but that’s prone to human error.\u003c/p\u003e\u003cp\u003eThis is where Gatekeeper comes in. You want the ability to limit which Cloud Storage bucket location can be used. Ideally you can write policies that look like this:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003eapiVersion: constraints.gatekeeper.sh/v1beta1\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003ekind: StorageBucketAllowedLocations\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003emetadata:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  name: allowmultiregions\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003espec:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  match:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e    kinds:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      - apiGroups: [\u0026#34;storage.cnrm.cloud.google.com\u0026#34;]\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e        kinds: [\u0026#34;StorageBucket\u0026#34;]\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  parameters:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e    locations:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      - \u0026#34;ASIA\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      - \u0026#34;EU\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e      - \u0026#34;US\u0026#34;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;The above \u0026lt;code\u0026gt;StorageBucketAllowedLocation\u0026lt;/code\u0026gt; policy rejects \u0026lt;code\u0026gt;StorageBucket\u0026lt;/code\u0026gt; objects with the \u0026lt;code\u0026gt;spec.location\u0026lt;/code\u0026gt; field set to any value other than one of the Cloud Storage multi-region locations: ASIA, EU, US. You decide where to validate policy without being limited by your tool of choice and anywhere in your pipeline.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Now you have the last stage of your configuration pipeline.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Testing the contract\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;How does this work in practice? Let\u0026amp;#8217;s say someone managed to check in \u0026lt;code\u0026gt;StorageBucket\u0026lt;/code\u0026gt; resource with the following config:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThe above \u003ccode\u003eStorageBucketAllowedLocation\u003c/code\u003e policy rejects \u003ccode\u003eStorageBucket\u003c/code\u003e objects with the \u003ccode\u003espec.location\u003c/code\u003e field set to any value other than one of the Cloud Storage multi-region locations: ASIA, EU, US. You decide where to validate policy without being limited by your tool of choice and anywhere in your pipeline.\u003c/p\u003e\u003cp\u003eNow you have the last stage of your configuration pipeline. \u003c/p\u003e\u003ch3\u003eTesting the contract\u003c/h3\u003e\u003cp\u003eHow does this work in practice? Let’s say someone managed to check in \u003ccode\u003eStorageBucket\u003c/code\u003e resource with the following config:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003eapiVersion: storage.cnrm.cloud.google.com/v1beta1\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003ekind: StorageBucket\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003emetadata:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  annotations:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e    cnrm.cloud.google.com/force-destroy: \u0026#34;false\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  name: ${BUCKET_NAME}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003espec:\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eOur policy would reject the bucket because an empty location is not allowed. What happens if configuration was set to a Cloud Storage location not allowed by the policy, \u003ccode\u003eUS-WEST1\u003c/code\u003e for example?\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003eapiVersion: storage.cnrm.cloud.google.com/v1beta1\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003ekind: StorageBucket\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003emetadata:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  annotations:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e    cnrm.cloud.google.com/force-destroy: \u0026#34;false\u0026#34;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  name: ${BUCKET_NAME}\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003espec:\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003e  location: \u0026#34;US-WEST1\u0026#34;\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Ideally you would catch this during the code review process before the config is committed to a git repo, but as mentioned above, that\u0026amp;#8217;s error prone.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Luckily, the configuration will fail because the \u0026lt;code\u0026gt;allowmultiregions\u0026lt;/code\u0026gt; policy constraint only allows multi-region bucket locations including ASIA, EU, and US, and will reject the configuration. So, now, if you set location to \u0026amp;#8220;US\u0026amp;#8221; you can deploy the Cloud Storage bucket. You can also apply this type of location policy or any other like it to all of your resource types\u0026amp;#8212;Redis instances, Compute Engine virtual machines, even Google Kubernetes Engine (GKE) clusters. Beyond admission control, you can apply the same constraint anywhere in your pipeline, by \u0026amp;#8221;shifting left\u0026amp;#8221; policy validation at any stage.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;One contract to rule them all\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;When config is managed in silos\u0026amp;#8212;whether across many tools, pipelines, graphical interfaces, and command lines\u0026amp;#8212;you can\u0026amp;#8217;t inject logic without building bespoke tools for every interface. You may be able to define policies built for your front-end tools and hope nothing changes on the backend. Or you can wait until deployment time to scan for deviations and hope nothing appears during crunch time.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Compare that with configuration as data contracts, which are transparent and normalized across resource types, which has facilitated a rich ecosystem of tooling built around Kubernetes with varied syntax (YAML, JSON) and languages including Ruby, Typescript, Go, Jinja, Mustache, Jsonnet, Starlark, and many others. This isn\u0026amp;#8217;t possible without a data model.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Configuration-as-Data-inspired tools such as \u0026lt;a href=\u0026#34;https://cloud.google.com/config-connector/docs/overview\u0026#34;\u0026gt;Config Connector\u0026lt;/a\u0026gt; and Gatekeeper let you enforce policy and governance as natural parts of your existing git-based workflow rather than creating manual processes and approvals. Configuration as data normalizes your contract across resource types and even cloud providers. You don\u0026amp;#8217;t need to reverse engineer scripts and code paths to know if your contract is being met\u0026amp;#8212;just look at the data.\u0026lt;/p\u0026gt;\u0026lt;hr\u0026gt;\u0026lt;sup\u0026gt;\u0026lt;i\u0026gt;1.\u0026amp;#160;\u0026lt;a href=\u0026#34;https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml\u0026lt;/a\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/sup\u0026gt;\u0026lt;p\u0026gt;\u0026lt;sup\u0026gt;\u0026lt;i\u0026gt;2.\u0026amp;#160;\u0026lt;a href=\u0026#34;https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55\u0026lt;/a\u0026gt;\u0026lt;br\u0026gt;\u0026lt;/i\u0026gt;\u0026lt;/sup\u0026gt;\u0026lt;sup\u0026gt;\u0026lt;i\u0026gt;3.\u0026amp;#160;\u0026lt;/i\u0026gt;\u0026lt;/sup\u0026gt;\u0026lt;a href=\u0026#34;https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;sup\u0026gt;\u0026lt;i\u0026gt;https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md\u0026lt;/i\u0026gt;\u0026lt;/sup\u0026gt;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eIdeally you would catch this during the code review process before the config is committed to a git repo, but as mentioned above, that’s error prone. \u003c/p\u003e\u003cp\u003eLuckily, the configuration will fail because the \u003ccode\u003eallowmultiregions\u003c/code\u003e policy constraint only allows multi-region bucket locations including ASIA, EU, and US, and will reject the configuration. So, now, if you set location to “US” you can deploy the Cloud Storage bucket. You can also apply this type of location policy or any other like it to all of your resource types—Redis instances, Compute Engine virtual machines, even Google Kubernetes Engine (GKE) clusters. Beyond admission control, you can apply the same constraint anywhere in your pipeline, by ”shifting left” policy validation at any stage. \u003c/p\u003e\u003ch3\u003eOne contract to rule them all\u003c/h3\u003e\u003cp\u003eWhen config is managed in silos—whether across many tools, pipelines, graphical interfaces, and command lines—you can’t inject logic without building bespoke tools for every interface. You may be able to define policies built for your front-end tools and hope nothing changes on the backend. Or you can wait until deployment time to scan for deviations and hope nothing appears during crunch time. \u003c/p\u003e\u003cp\u003eCompare that with configuration as data contracts, which are transparent and normalized across resource types, which has facilitated a rich ecosystem of tooling built around Kubernetes with varied syntax (YAML, JSON) and languages including Ruby, Typescript, Go, Jinja, Mustache, Jsonnet, Starlark, and many others. This isn’t possible without a data model. \u003c/p\u003e\u003cp\u003eConfiguration-as-Data-inspired tools such as \u003ca href=\"https://cloud.google.com/config-connector/docs/overview\" track-type=\"inline link\" track-name=\"10\" track-metadata-eventdetail=\"https://cloud.google.com/config-connector/docs/overview\" track-metadata-module=\"post\"\u003eConfig Connector\u003c/a\u003e and Gatekeeper let you enforce policy and governance as natural parts of your existing git-based workflow rather than creating manual processes and approvals. Configuration as data normalizes your contract across resource types and even cloud providers. You don’t need to reverse engineer scripts and code paths to know if your contract is being met—just look at the data.\u003c/p\u003e\u003chr/\u003e\u003cp\u003e\u003csup\u003e\u003ci\u003e1. \u003ca href=\"https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml\" target=\"_blank\" track-type=\"inline link\" track-name=\"11\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ehttps://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml\u003c/a\u003e\u003c/i\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e\u003csup\u003e\u003ci\u003e2. \u003ca href=\"https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55\" target=\"_blank\" track-type=\"inline link\" track-name=\"12\" track-metadata-eventdetail=\"https://medium.com\" track-metadata-module=\"post\"\u003ehttps://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55\u003c/a\u003e\u003cbr/\u003e\u003c/i\u003e\u003c/sup\u003e\u003csup\u003e\u003ci\u003e3. \u003c/i\u003e\u003c/sup\u003e\u003ca href=\"https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md\" target=\"_blank\" track-type=\"inline link\" track-name=\"13\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003e\u003csup\u003e\u003ci\u003ehttps://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md\u003c/i\u003e\u003c/sup\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c18=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/dynamic-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eConfiguration as Data is an emerging cloud infrastructure management paradigm that allows developers to \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes\"\u003edeclare the desired state\u003c/a\u003e of their applications and infrastructure, without specifying the precise actions or steps for how to achieve it. However, declaring a configuration is only half the battle: you also want policy that defines how a configuration is to be used. \u003c/p\u003e\u003cp\u003eConfiguration as Data enables a normalized policy contract across all your cloud resources. That contract, knowing how your deployment will operate, can be inspected and enforced throughout a CI/CD pipeline, from upstream in your development environment to deployment time, and ongoing in the live runtime environment. This consistency is possible by expressing configuration as data throughout the development and operations lifecycle.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://cloud.google.com/config-connector/docs/overview\"\u003eConfig Connector\u003c/a\u003e is the tool that allows you to express configuration as data in Google Cloud. In this model, configuration is what you want to deploy, such as “a storage bucket named \u003ccode\u003emy-bucket\u003c/code\u003e with a standard storage class and uniform access control.” \u003c/p\u003e\u003cp\u003ePolicy, meanwhile, typically specifies what you’re allowed to deploy, usually in conformance with your organization’s compliance needs. For example, “all resources must be deployed in Google Cloud’s \u003ccode\u003eLONDON\u003c/code\u003e region.” \u003c/p\u003e\u003cp\u003eWhen each stage in your pipeline treats configuration as data, you can use any tool or language to manipulate configuration as data, knowing they will interoperate and that policy can be consistently enforced at any or all stages. And while a policy engine won’t be able to understand every tool, it can validate the data generated by each tool. It’s just like data in a database can be inspected by anyone who knows the schema regardless of the tool that wrote into the database.\u003c/p\u003e\u003cp\u003eContrast that with pipelines today, where policy is manually validated, hard coded in scripts within the pipeline logic itself, or post-processed on raw deployment artifacts after rendering configuration templates into specific instances. In each case, policy is siloed—you can’t take the same policy and apply it anywhere in your pipeline because formats differ from tool to tool. \u003c/p\u003e\u003cp\u003eHelm, for example, contains code \u003ca href=\"https://helm.sh/docs/chart_template_guide/control_structures/\" target=\"_blank\"\u003especific to its own format\u003c/a\u003e.\u003csup\u003e1\u003c/sup\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ca href=\"https://www.terraform.io/docs/language/syntax/configuration.html\" target=\"_blank\"\u003eTerraform HCL\u003c/a\u003e may then deploy the Helm chart.\u003csup\u003e2\u003c/sup\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe HCL becomes a \u003ca href=\"https://www.terraform.io/docs/internals/json-format.html\" target=\"_blank\"\u003eJSON plan\u003c/a\u003e, where the deployment-ready configuration may be validated before being applied to the live environment.\u003csup\u003e3\u003c/sup\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThese examples show three disparate data formats across two different tools representing different portions of a desired end state. Add in Python scripting, gcloud CLI, or \u003ccode\u003ekubectl\u003c/code\u003e commands and you start approaching ten different formats—all for the same deployment!  Reliably enforcing a policy contract requires you to inject tool- and format-specific validation logic on case-by-case basis. If you decide to move a config step from Python to Terraform or from Terraform to \u003ccode\u003ekubectl\u003c/code\u003e, you’ll need to re-evaluate your contract and probably re-implement some of that policy validation. \u003c/p\u003e\u003cp\u003eWhy don’t these tools work together cleanly? Why does policy validation change depending on the development tools you’re using? Each tool can do a good job enforcing policy within itself. As long as you use that tool everywhere, things will probably work ok. But we all know that’s not how development works. People tend to choose tools that fit their needs and figure out integration later on.\u003c/p\u003e\u003ch3\u003eA Rosetta Stone for policy contracts\u003c/h3\u003e\u003cp\u003eImagine that everyone is defining their configuration as data, while using tools and formats of their choice. Terraform or Python for orchestration. Helm for application packaging. Java or Go for data transformation and validation. Once the data format is understood (because it is open source and extensible), your pipeline becomes a bus that anyone can push configuration onto and pull configuration from.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/policy_contracts.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"policy contracts.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/policy_contracts.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003ePolicies can be automatically validated at commit or build time using custom and \u003ca href=\"https://googlecontainertools.github.io/kpt/guides/consumer/function/catalog/validators/\" target=\"_blank\"\u003eoff-the-shelf functions\u003c/a\u003e that operate on YAML. You can manage commit and merge permissions separately for config and policy to separate these distinct concerns. You can have folders and unique permissions for org-wide policy, team-wide policy, or app-specific policy. Therein lies the dream. \u003c/p\u003e\u003cp\u003eThe most common way to generate configuration is to simply write a YAML file describing how Kubernetes should create a resource for you. The resulting YAML file is then stored in a git repository where it can be versioned and picked up by another tool and applied to a Kubernetes cluster. Policies can be enforced on the git repo side to limit who can push changes to the repository and ultimately reference them at deploy time.\u003c/p\u003e\u003cp\u003eFor most users this is not where policy enforcement ends. While code reviews can catch a lot of things, it’s considered best practice to “trust but verify” at all layers in the stack. That’s where admission controllers come in, which can be considered to be the last mile of policy enforcement. \u003ca href=\"https://www.openpolicyagent.org/\" target=\"_blank\"\u003eGatekeeper\u003c/a\u003e serves as an admission controller inside of a Kubernetes cluster. Only configurations that meet defined constraints will be admitted to the live cloud environment.\u003c/p\u003e\u003cp\u003eLet’s tie these concepts together \u003ca href=\"https://github.com/kelseyhightower/config-connector-policy-demo\" target=\"_blank\"\u003ewith an example\u003c/a\u003e. Imagine you want to enable users to create Cloud Storage buckets, but you don’t want them doing so using the Google Cloud Console or the gcloud command line tool. You want all users to declare what they want and push those changes to a git repository for review before the underlying Cloud Storage buckets are created with \u003ca href=\"https://cloud.google.com/config-connector/docs/overview\"\u003eConfig Connector\u003c/a\u003e. Essentially you want users to be able to submit a YAML file that looks like this:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis creates a storage bucket in a default location. There is one problem with this: users can create buckets in any location even if company policy dictates otherwise. Sure, you can catch people using forbidden bucket locations during code review, but that’s prone to human error.\u003c/p\u003e\u003cp\u003eThis is where Gatekeeper comes in. You want the ability to limit which Cloud Storage bucket location can be used. Ideally you can write policies that look like this:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThe above \u003ccode\u003eStorageBucketAllowedLocation\u003c/code\u003e policy rejects \u003ccode\u003eStorageBucket\u003c/code\u003e objects with the \u003ccode\u003espec.location\u003c/code\u003e field set to any value other than one of the Cloud Storage multi-region locations: ASIA, EU, US. You decide where to validate policy without being limited by your tool of choice and anywhere in your pipeline.\u003c/p\u003e\u003cp\u003eNow you have the last stage of your configuration pipeline. \u003c/p\u003e\u003ch3\u003eTesting the contract\u003c/h3\u003e\u003cp\u003eHow does this work in practice? Let’s say someone managed to check in \u003ccode\u003eStorageBucket\u003c/code\u003e resource with the following config:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eOur policy would reject the bucket because an empty location is not allowed. What happens if configuration was set to a Cloud Storage location not allowed by the policy, \u003ccode\u003eUS-WEST1\u003c/code\u003e for example?\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIdeally you would catch this during the code review process before the config is committed to a git repo, but as mentioned above, that’s error prone. \u003c/p\u003e\u003cp\u003eLuckily, the configuration will fail because the \u003ccode\u003eallowmultiregions\u003c/code\u003e policy constraint only allows multi-region bucket locations including ASIA, EU, and US, and will reject the configuration. So, now, if you set location to “US” you can deploy the Cloud Storage bucket. You can also apply this type of location policy or any other like it to all of your resource types—Redis instances, Compute Engine virtual machines, even Google Kubernetes Engine (GKE) clusters. Beyond admission control, you can apply the same constraint anywhere in your pipeline, by ”shifting left” policy validation at any stage. \u003c/p\u003e\u003ch3\u003eOne contract to rule them all\u003c/h3\u003e\u003cp\u003eWhen config is managed in silos—whether across many tools, pipelines, graphical interfaces, and command lines—you can’t inject logic without building bespoke tools for every interface. You may be able to define policies built for your front-end tools and hope nothing changes on the backend. Or you can wait until deployment time to scan for deviations and hope nothing appears during crunch time. \u003c/p\u003e\u003cp\u003eCompare that with configuration as data contracts, which are transparent and normalized across resource types, which has facilitated a rich ecosystem of tooling built around Kubernetes with varied syntax (YAML, JSON) and languages including Ruby, Typescript, Go, Jinja, Mustache, Jsonnet, Starlark, and many others. This isn’t possible without a data model. \u003c/p\u003e\u003cp\u003eConfiguration-as-Data-inspired tools such as \u003ca href=\"https://cloud.google.com/config-connector/docs/overview\"\u003eConfig Connector\u003c/a\u003e and Gatekeeper let you enforce policy and governance as natural parts of your existing git-based workflow rather than creating manual processes and approvals. Configuration as data normalizes your contract across resource types and even cloud providers. You don’t need to reverse engineer scripts and code paths to know if your contract is being met—just look at the data.\u003c/p\u003e\u003chr/\u003e\u003csup\u003e\u003ci\u003e1. \u003ca href=\"https://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml\" target=\"_blank\"\u003ehttps://github.com/helm/charts/blob/master/stable/jenkins/templates/jenkins-master-deployment.yaml\u003c/a\u003e\u003c/i\u003e\u003c/sup\u003e\u003cp\u003e\u003csup\u003e\u003ci\u003e2. \u003ca href=\"https://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55\" target=\"_blank\"\u003ehttps://medium.com/swlh/deploying-helm-charts-w-terraform-58bd3a690e55\u003c/a\u003e\u003cbr/\u003e\u003c/i\u003e\u003c/sup\u003e\u003csup\u003e\u003ci\u003e3. \u003c/i\u003e\u003c/sup\u003e\u003ca href=\"https://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md\" target=\"_blank\"\u003e\u003csup\u003e\u003ci\u003ehttps://github.com/hashicorp/terraform-getting-started-gcp-cloud-shell/blob/master/tutorial/cloudshell_tutorial.md\u003c/i\u003e\u003c/sup\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/containers-kubernetes/understanding-configuration-as-data-in-kubernetes/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Blog_header_opensource_2.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eI do declare! Infrastructure automation with Configuration as Data\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eConfiguration as Data enables operational consistency, security, and velocity on Google Cloud with products like Config Connector.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_x_GKE.max-2200x2200.jpg",
      "date_published": "2021-04-26T16:00:00Z",
      "author": {
        "name": "\u003cname\u003eMark Balch\u003c/name\u003e\u003ctitle\u003eSenior Product Manager, Google Cloud\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/5-google-sre-resources-to-get-started/",
      "title": "5 resources to help you get started with SRE",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;https://sre.google/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Site reliability engineering\u0026lt;/a\u0026gt; (SRE) is an essential part of engineering at Google\u0026amp;#8212;it\u0026amp;#8217;s a mindset, and a set of practices, metrics, and prescriptive ways to ensure systems reliability. But not everyone knows the best places to start to implement SRE in their own organizations. Here are our top resources at Google Cloud for getting started.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;1. Do you have an SRE team yet? How to start and assess your journey\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;We\u0026amp;#8217;re often asked what implementing SRE means in practice, since our customers face challenges quantifying their success when setting up their own SRE practices. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey\u0026#34;\u0026gt;In this post\u0026lt;/a\u0026gt;, we share a couple of checklists to be used by members of an organization responsible for any high-reliability services. These will be useful when you\u0026amp;#8217;re trying to move your team toward an SRE model. Implementing this model at your organization can benefit both your services and teams due to higher service reliability, lower operational cost, and higher-value work for everyone on the team.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e\u003ca href=\"https://sre.google/\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003eSite reliability engineering\u003c/a\u003e (SRE) is an essential part of engineering at Google—it’s a mindset, and a set of practices, metrics, and prescriptive ways to ensure systems reliability. But not everyone knows the best places to start to implement SRE in their own organizations. Here are our top resources at Google Cloud for getting started.\u003c/p\u003e\u003ch3\u003e1. Do you have an SRE team yet? How to start and assess your journey\u003c/h3\u003e\u003cp\u003eWe’re often asked what implementing SRE means in practice, since our customers face challenges quantifying their success when setting up their own SRE practices. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey\" track-metadata-module=\"post\"\u003eIn this post\u003c/a\u003e, we share a couple of checklists to be used by members of an organization responsible for any high-reliability services. These will be useful when you’re trying to move your team toward an SRE model. Implementing this model at your organization can benefit both your services and teams due to higher service reliability, lower operational cost, and higher-value work for everyone on the team.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;h3\u0026gt;2. SRE fundamentals: SLIs, SLAs and SLOs\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Core to the definition of SRE is the idea that metrics should be closely tied to business objectives. Thus, a big part of the day-to-day of SREs is establishing and monitoring these service-level metrics. At Google, we use several essential measurements\u0026amp;#8212;SLO, SLA and SLI\u0026amp;#8212;in SRE planning and practice. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos\u0026#34;\u0026gt;This post\u0026lt;/a\u0026gt; gives you an overview of what each of these acronyms are, what they mean, and how to incorporate them.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003e2. SRE fundamentals: SLIs, SLAs and SLOs\u003c/h3\u003e\u003cp\u003eCore to the definition of SRE is the idea that metrics should be closely tied to business objectives. Thus, a big part of the day-to-day of SREs is establishing and monitoring these service-level metrics. At Google, we use several essential measurements—SLO, SLA and SLI—in SRE planning and practice. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos\" track-metadata-module=\"post\"\u003eThis post\u003c/a\u003e gives you an overview of what each of these acronyms are, what they mean, and how to incorporate them.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;h3\u0026gt;3. How SRE teams are organized, and how to get started\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;You know what SREs do and understand which best practices should be implemented at various levels of SRE maturity. Now you\u0026amp;#8217;re ready to take the next step by setting up your own SRE team. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started\u0026#34;\u0026gt;In this post\u0026lt;/a\u0026gt;, we\u0026amp;#8217;ll cover how different implementations of SRE teams establish boundaries to achieve their goals. We describe six different implementations that we\u0026amp;#8217;ve experienced, and what we have observed to be their most important pros and cons.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003e3. How SRE teams are organized, and how to get started\u003c/h3\u003e\u003cp\u003eYou know what SREs do and understand which best practices should be implemented at various levels of SRE maturity. Now you’re ready to take the next step by setting up your own SRE team. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started\" track-metadata-module=\"post\"\u003eIn this post\u003c/a\u003e, we’ll cover how different implementations of SRE teams establish boundaries to achieve their goals. We describe six different implementations that we’ve experienced, and what we have observed to be their most important pros and cons.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;h3\u0026gt;4. Meeting reliability challenges with SRE principles\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Through years of work using SRE principles, we\u0026amp;#8217;ve found there are a few common challenges that teams face, and some important ways to meet or avoid those challenges. Learn what we at Google think are the \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles\u0026#34;\u0026gt;three top sources of production stress\u0026lt;/a\u0026gt; and how we recommend addressing them.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003e4. Meeting reliability challenges with SRE principles\u003c/h3\u003e\u003cp\u003eThrough years of work using SRE principles, we’ve found there are a few common challenges that teams face, and some important ways to meet or avoid those challenges. Learn what we at Google think are the \u003ca href=\"https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles\" track-metadata-module=\"post\"\u003ethree top sources of production stress\u003c/a\u003e and how we recommend addressing them.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;h3\u0026gt;5. Transitioning a typical engineering ops team into an SRE powerhouse\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Perpetually adding engineers to ops teams to meet customer growth doesn\u0026amp;#8217;t scale. Google\u0026amp;#8217;s SRE principles can help, bringing software engineering solutions to operational problems. \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse\u0026#34;\u0026gt;In this post\u0026lt;/a\u0026gt;, we\u0026amp;#8217;ll take a look at how we transformed our global network ops team by abandoning traditional network engineering orthodoxy and replacing it with SRE. You\u0026amp;#8217;ll learn how Google\u0026amp;#8217;s production networking team tackled this problem and consider how you might incorporate SRE principles in your own organization.\u0026lt;/p\u0026gt;\"\u003e\u003ch3\u003e5. Transitioning a typical engineering ops team into an SRE powerhouse\u003c/h3\u003e\u003cp\u003ePerpetually adding engineers to ops teams to meet customer growth doesn’t scale. Google’s SRE principles can help, bringing software engineering solutions to operational problems. \u003ca href=\"https://cloud.google.com/blog/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse\" track-metadata-module=\"post\"\u003eIn this post\u003c/a\u003e, we’ll take a look at how we transformed our global network ops team by abandoning traditional network engineering orthodoxy and replacing it with SRE. You’ll learn how Google’s production networking team tackled this problem and consider how you might incorporate SRE principles in your own organization.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ca href=\"https://sre.google/\" target=\"_blank\"\u003eSite reliability engineering\u003c/a\u003e (SRE) is an essential part of engineering at Google—it’s a mindset, and a set of practices, metrics, and prescriptive ways to ensure systems reliability. But not everyone knows the best places to start to implement SRE in their own organizations. Here are our top resources at Google Cloud for getting started.\u003c/p\u003e\u003ch3\u003e1. Do you have an SRE team yet? How to start and assess your journey\u003c/h3\u003e\u003cp\u003eWe’re often asked what implementing SRE means in practice, since our customers face challenges quantifying their success when setting up their own SRE practices. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-to-start-and-assess-your-sre-journey\"\u003eIn this post\u003c/a\u003e, we share a couple of checklists to be used by members of an organization responsible for any high-reliability services. These will be useful when you’re trying to move your team toward an SRE model. Implementing this model at your organization can benefit both your services and teams due to higher service reliability, lower operational cost, and higher-value work for everyone on the team.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/how-to-start-and-assess-your-sre-journey/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eDo you have an SRE team yet? How to start and assess your journey\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eThis post shares checklists you can use when you’re trying to move your team toward an SRE model. These checklists can be useful as a for...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003e2. SRE fundamentals: SLIs, SLAs and SLOs\u003c/h3\u003e\u003cp\u003eCore to the definition of SRE is the idea that metrics should be closely tied to business objectives. Thus, a big part of the day-to-day of SREs is establishing and monitoring these service-level metrics. At Google, we use several essential measurements—SLO, SLA and SLI—in SRE planning and practice. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/sre-fundamentals-slis-slas-and-slos\"\u003eThis post\u003c/a\u003e gives you an overview of what each of these acronyms are, what they mean, and how to incorporate them.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/sre-fundamentals-slis-slas-and-slos/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eSRE fundamentals: SLIs, SLAs and SLOs\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eA big part of SRE is establishing and monitoring service-level metrics like SLOs, SLAs and SLIs. This post gives you an overview of what ...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003e3. How SRE teams are organized, and how to get started\u003c/h3\u003e\u003cp\u003eYou know what SREs do and understand which best practices should be implemented at various levels of SRE maturity. Now you’re ready to take the next step by setting up your own SRE team. \u003ca href=\"https://cloud.google.com/blog/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started\"\u003eIn this post\u003c/a\u003e, we’ll cover how different implementations of SRE teams establish boundaries to achieve their goals. We describe six different implementations that we’ve experienced, and what we have observed to be their most important pros and cons.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_A_Rnd3.max-2800x2800.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eHow SRE teams are organized, and how to get started\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eLearn six different implementations of SRE teams you can apply in your organization, as well as how to establish boundaries to achieve th...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003e4. Meeting reliability challenges with SRE principles\u003c/h3\u003e\u003cp\u003eThrough years of work using SRE principles, we’ve found there are a few common challenges that teams face, and some important ways to meet or avoid those challenges. Learn what we at Google think are the \u003ca href=\"https://cloud.google.com/blog/products/management-tools/meeting-reliability-challenges-with-sre-principles\"\u003ethree top sources of production stress\u003c/a\u003e and how we recommend addressing them.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/management-tools/meeting-reliability-challenges-with-sre-principles/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_GCP.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eMeeting reliability challenges with SRE principles\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eFollowing SRE principles can help you build reliable production systems. When getting started, you may encounter three common challenges....\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003e5. Transitioning a typical engineering ops team into an SRE powerhouse\u003c/h3\u003e\u003cp\u003ePerpetually adding engineers to ops teams to meet customer growth doesn’t scale. Google’s SRE principles can help, bringing software engineering solutions to operational problems. \u003ca href=\"https://cloud.google.com/blog/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse\"\u003eIn this post\u003c/a\u003e, we’ll take a look at how we transformed our global network ops team by abandoning traditional network engineering orthodoxy and replacing it with SRE. You’ll learn how Google’s production networking team tackled this problem and consider how you might incorporate SRE principles in your own organization.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/management-tools/transitioning-a-typical-engineering-ops-team-into-an-sre-powerhouse/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eTransitioning a typical engineering ops team into an SRE powerhouse\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eMoving a network operations team to an SRE-driven model took some time, but was well worth the effort, as teams can focus on reliability ...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eLots more to read\u003c/h3\u003e\u003cp\u003eCan’t wait to read more about SRE? We wrote \u003ca href=\"https://sre.google/sre-book/table-of-contents/\" target=\"_blank\"\u003ean entire book on SRE\u003c/a\u003e to help you get started (actually, we’ve written \u003ca href=\"https://sre.google/books/\" target=\"_blank\"\u003emore than one\u003c/a\u003e). You can also find all our \u003ca href=\"https://cloud.google.com/blog/products/devops-sre\"\u003eDevOps and SRE blog content\u003c/a\u003e or follow our columns on \u003ca href=\"https://cloud.google.com/blog/topics/cre-life-lessons\"\u003eCustomer Reliability Engineering\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/gcp/sre-vs-devops-competing-standards-or-close-friends/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/SREvsDevOps.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eSRE vs. DevOps: Competing standards or close friends?\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eWhat exactly is SRE and how does it relate to DevOps? This post helps answer questions and reduce friction between the communities.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_C_Rnd3_n7MW7mI.max-2200x2200.jpg",
      "date_published": "2021-04-23T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eThe Google Cloud content marketing team \u003c/name\u003e\u003ctitle\u003e\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/a-practical-guide-to-cloud-migration-from-google-cloud-sres/",
      "title": "How do you eat an elephant? Google SREs talk digital transformation",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Today, everything from payroll software to specialized machine-learning systems is available \u0026amp;#8220;as a service\u0026amp;#8221; in the cloud, addressing a vast range of needs across businesses, enabling rapid growth and scale while allowing a business to focus on its core competencies.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;But moving to the cloud can generate tension, which is inevitably challenging for everyone involved\u0026amp;#8212;especially if that transformation creates \u0026amp;#34;winners\u0026amp;#34; and \u0026amp;#34;losers\u0026amp;#34; or frames individuals as \u0026amp;#34;old\u0026amp;#34; or \u0026amp;#34;new.\u0026amp;#34; The good news; however, is that a cloud transformation doesn\u0026#39;t have to be this way.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As Google Cloud has grown, so too has the team of Googlers who build and support the platform, and many of us have sat in the same seat as our customers. We\u0026#39;ve experienced firsthand how empowering it can be to shape the future of an organization, help one another grow, as well as unlock the business opportunities that a transformation provides. Our own personal experiences, and those of our peers, have led us to conclude one thing we know to be true for every company - the story of digital transformation is a human story\u0026amp;#8212;one that involves as much cultural transformation as technological transformation. It\u0026amp;#8217;s with this realization we have identified the deeper factors behind a successful transformation. That\u0026#39;s why we recently published a guide, reflecting on the nature of these changes and how you can take action in your own organization to drive a migration to the cloud.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Posing challenging questions helps you reflect on your own organization\u0026amp;#8217;s journey and the unique path you will need to take to lead to meaningful change. We wrote this guide to share key tenets that underpin the change philosophy you need to instill in your own organization. In \u0026lt;a href=\u0026#34;https://googlesre.page.link/cloud-migration-guide\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;A Practical Guide to Moving to Cloud\u0026lt;/a\u0026gt;, we present the following calls to action:\u0026lt;/p\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Understand who in the organization you need to enlist to move to cloud.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Create a psychologically safe culture in which you can grow together.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Define clear objectives for your organization. Document measurable steps towards these goals and understand that each step must, in and of itself, deliver value.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Review your existing organizational behaviors and set principles/policies which influence and direct every future decision related to your transformation.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Use your new culture to refine how decisions are made, and provide meaningful autonomy across the organization.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Build structures that empower practitioners to share best practices and solve common problems. Use these structures to empower your peers.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Build guardrails into your cloud platform that support transformation, at pace, without negatively impacting others. Support safe experimentation.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Understand what types of cloud platforms are the best fit for your business needs and determine your multi-cloud strategy in anticipation of your evolving business needs (e.g. acquisitions, new revenue streams, competitive responses).\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Recognize that everything is now software, and understand what this means for your existing IT infrastructure functions.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Don\u0026amp;#8217;t be afraid to revisit existing, hallowed, security policies. Making them fit-for-purpose is crucial.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Continuously measure and apply your new policies through software.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Be bold; build a new way of operating your business products with a customer-centric perspective.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Love your developers.\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;p\u0026gt;At Google Cloud, we\u0026amp;#8217;re here to help you craft the right migration for you and your business. \u0026lt;a href=\u0026#34;https://googlesre.page.link/cloud-migration-guide\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;A Practical Guide to Moving to Cloud\u0026lt;/a\u0026gt; is available as a free download. You can also learn more about our \u0026lt;a href=\u0026#34;https://cloud.google.com/solutions/migration-center\u0026#34;\u0026gt;data center migration solutions\u0026lt;/a\u0026gt; or \u0026lt;a href=\u0026#34;https://inthecloud.withgoogle.com/tco-assessment-19/form.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;sign up for a free migration cost assessment\u0026lt;/a\u0026gt;. Let\u0026amp;#8217;s get migrating!\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;Visit \u0026lt;a href=\u0026#34;https://sre.google\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;sre.google\u0026lt;/a\u0026gt; to learn more about SRE and industry-leading practices for service reliability.\u0026lt;/i\u0026gt;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eToday, everything from payroll software to specialized machine-learning systems is available “as a service” in the cloud, addressing a vast range of needs across businesses, enabling rapid growth and scale while allowing a business to focus on its core competencies.\u003c/p\u003e\u003cp\u003eBut moving to the cloud can generate tension, which is inevitably challenging for everyone involved—especially if that transformation creates \u0026#34;winners\u0026#34; and \u0026#34;losers\u0026#34; or frames individuals as \u0026#34;old\u0026#34; or \u0026#34;new.\u0026#34; The good news; however, is that a cloud transformation doesn\u0026#39;t have to be this way.\u003c/p\u003e\u003cp\u003eAs Google Cloud has grown, so too has the team of Googlers who build and support the platform, and many of us have sat in the same seat as our customers. We\u0026#39;ve experienced firsthand how empowering it can be to shape the future of an organization, help one another grow, as well as unlock the business opportunities that a transformation provides. Our own personal experiences, and those of our peers, have led us to conclude one thing we know to be true for every company - the story of digital transformation is a human story—one that involves as much cultural transformation as technological transformation. It’s with this realization we have identified the deeper factors behind a successful transformation. That\u0026#39;s why we recently published a guide, reflecting on the nature of these changes and how you can take action in your own organization to drive a migration to the cloud. \u003c/p\u003e\u003cp\u003ePosing challenging questions helps you reflect on your own organization’s journey and the unique path you will need to take to lead to meaningful change. We wrote this guide to share key tenets that underpin the change philosophy you need to instill in your own organization. In \u003ca href=\"https://googlesre.page.link/cloud-migration-guide\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://googlesre.page.link\" track-metadata-module=\"post\"\u003eA Practical Guide to Moving to Cloud\u003c/a\u003e, we present the following calls to action:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eUnderstand who in the organization you need to enlist to move to cloud.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCreate a psychologically safe culture in which you can grow together.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDefine clear objectives for your organization. Document measurable steps towards these goals and understand that each step must, in and of itself, deliver value.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eReview your existing organizational behaviors and set principles/policies which influence and direct every future decision related to your transformation.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse your new culture to refine how decisions are made, and provide meaningful autonomy across the organization.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBuild structures that empower practitioners to share best practices and solve common problems. Use these structures to empower your peers.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBuild guardrails into your cloud platform that support transformation, at pace, without negatively impacting others. Support safe experimentation.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUnderstand what types of cloud platforms are the best fit for your business needs and determine your multi-cloud strategy in anticipation of your evolving business needs (e.g. acquisitions, new revenue streams, competitive responses).\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRecognize that everything is now software, and understand what this means for your existing IT infrastructure functions.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDon’t be afraid to revisit existing, hallowed, security policies. Making them fit-for-purpose is crucial.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eContinuously measure and apply your new policies through software.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBe bold; build a new way of operating your business products with a customer-centric perspective. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLove your developers.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAt Google Cloud, we’re here to help you craft the right migration for you and your business. \u003ca href=\"https://googlesre.page.link/cloud-migration-guide\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://googlesre.page.link\" track-metadata-module=\"post\"\u003eA Practical Guide to Moving to Cloud\u003c/a\u003e is available as a free download. You can also learn more about our \u003ca href=\"https://cloud.google.com/solutions/migration-center\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/solutions/migration-center\" track-metadata-module=\"post\"\u003edata center migration solutions\u003c/a\u003e or \u003ca href=\"https://inthecloud.withgoogle.com/tco-assessment-19/form.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://inthecloud.withgoogle.com\" track-metadata-module=\"post\"\u003esign up for a free migration cost assessment\u003c/a\u003e. Let’s get migrating! \u003c/p\u003e\u003cp\u003e\u003ci\u003eVisit \u003ca href=\"https://sre.google\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://sre.google\" track-metadata-module=\"post\"\u003esre.google\u003c/a\u003e to learn more about SRE and industry-leading practices for service reliability.\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eToday, everything from payroll software to specialized machine-learning systems is available “as a service” in the cloud, addressing a vast range of needs across businesses, enabling rapid growth and scale while allowing a business to focus on its core competencies.\u003c/p\u003e\u003cp\u003eBut moving to the cloud can generate tension, which is inevitably challenging for everyone involved—especially if that transformation creates \"winners\" and \"losers\" or frames individuals as \"old\" or \"new.\" The good news; however, is that a cloud transformation doesn't have to be this way.\u003c/p\u003e\u003cp\u003eAs Google Cloud has grown, so too has the team of Googlers who build and support the platform, and many of us have sat in the same seat as our customers. We've experienced firsthand how empowering it can be to shape the future of an organization, help one another grow, as well as unlock the business opportunities that a transformation provides. Our own personal experiences, and those of our peers, have led us to conclude one thing we know to be true for every company - the story of digital transformation is a human story—one that involves as much cultural transformation as technological transformation. It’s with this realization we have identified the deeper factors behind a successful transformation. That's why we recently published a guide, reflecting on the nature of these changes and how you can take action in your own organization to drive a migration to the cloud. \u003c/p\u003e\u003cp\u003ePosing challenging questions helps you reflect on your own organization’s journey and the unique path you will need to take to lead to meaningful change. We wrote this guide to share key tenets that underpin the change philosophy you need to instill in your own organization. In \u003ca href=\"https://googlesre.page.link/cloud-migration-guide\" target=\"_blank\"\u003eA Practical Guide to Moving to Cloud\u003c/a\u003e, we present the following calls to action:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cp\u003eUnderstand who in the organization you need to enlist to move to cloud.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCreate a psychologically safe culture in which you can grow together.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDefine clear objectives for your organization. Document measurable steps towards these goals and understand that each step must, in and of itself, deliver value.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eReview your existing organizational behaviors and set principles/policies which influence and direct every future decision related to your transformation.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUse your new culture to refine how decisions are made, and provide meaningful autonomy across the organization.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBuild structures that empower practitioners to share best practices and solve common problems. Use these structures to empower your peers.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBuild guardrails into your cloud platform that support transformation, at pace, without negatively impacting others. Support safe experimentation.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eUnderstand what types of cloud platforms are the best fit for your business needs and determine your multi-cloud strategy in anticipation of your evolving business needs (e.g. acquisitions, new revenue streams, competitive responses).\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eRecognize that everything is now software, and understand what this means for your existing IT infrastructure functions.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eDon’t be afraid to revisit existing, hallowed, security policies. Making them fit-for-purpose is crucial.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eContinuously measure and apply your new policies through software.\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eBe bold; build a new way of operating your business products with a customer-centric perspective. \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eLove your developers.\u003c/p\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAt Google Cloud, we’re here to help you craft the right migration for you and your business. \u003ca href=\"https://googlesre.page.link/cloud-migration-guide\" target=\"_blank\"\u003eA Practical Guide to Moving to Cloud\u003c/a\u003e is available as a free download. You can also learn more about our \u003ca href=\"https://cloud.google.com/solutions/migration-center\"\u003edata center migration solutions\u003c/a\u003e or \u003ca href=\"https://inthecloud.withgoogle.com/tco-assessment-19/form.html\" target=\"_blank\"\u003esign up for a free migration cost assessment\u003c/a\u003e. Let’s get migrating! \u003c/p\u003e\u003cp\u003e\u003ci\u003eVisit \u003ca href=\"https://sre.google\" target=\"_blank\"\u003esre.google\u003c/a\u003e to learn more about SRE and industry-leading practices for service reliability.\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/how-sre-teams-are-organized-and-how-to-get-started/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_B_Rnd3.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eHow SRE teams are organized, and how to get started\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eLearn six different implementations of SRE teams you can apply in your organization, as well as how to establish boundaries to achieve th...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Blog_CloudMigration_D.max-2200x2200.jpg",
      "date_published": "2021-03-12T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eKieran Broadfoot\u003c/name\u003e\u003ctitle\u003eDirector, Site Reliability Engineering\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/devops-sre/sre-success-starts-with-getting-leadership-on-board/",
      "title": "With SRE, failing to plan is planning to fail",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;People sometimes think that implementing \u0026lt;a href=\u0026#34;http://sre.google\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Site Reliability Engineering\u0026lt;/a\u0026gt; (or DevOps for that matter) will magically make everything better. Just sprinkle a little bit of SRE fairy dust on your organization and your services will be more reliable, more profitable, and your IT, product and engineering teams will be happy.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It\u0026amp;#8217;s easy to see why people think this way. Some of the world\u0026amp;#8217;s most reliable and scalable services run with the help of an SRE team, Google being the prime example.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;For almost two decades, I\u0026amp;#8217;ve lived and breathed running production systems at large scale. I had to think about tradeoffs, reliability, costs, implementing a variety of architectures with different constraints and requirements\u0026amp;#8212;all while getting paged in the middle of the night. More recently, I\u0026amp;#8217;ve had the privilege to leverage that experience and knowledge to help Google Cloud customers modernize their infrastructure and applications, including implementing an SRE practice. While these learnings look different from organization to organization, there are common lessons learned that will impact the success of your deployment.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;When problems do arise, it\u0026amp;#8217;s usually not because of technical challenges. A stalled SRE culture is usually a business process failure\u0026amp;#8212;goals weren\u0026amp;#8217;t properly defined up front and stakeholders weren\u0026amp;#8217;t properly engaged. After watching this play out repeatedly, I\u0026amp;#8217;ve developed some advice for technology leaders about how to implement a successful SRE practice.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Before you start\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Your SRE journey should start well before you read your first manual, or put in your first call to an SRE advisor. As a technology leader within your organization, your first job is to answer a few key questions and gather some basic facts.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;What problem are you trying to solve?\u0026amp;#160;\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Most organizations will readily admit they\u0026amp;#8217;re not perfect. Perhaps you need to \u0026lt;a href=\u0026#34;https://www.youtube.com/watch?v=IvQ-15-yE_c\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;reduce toil\u0026lt;/a\u0026gt;, be more innovative, or release software faster. SRE, as a framework for operating large scale systems reliably, can certainly help with those goals. To do that, it\u0026amp;#8217;s important to understand your motivations and what gaps or needs exist in your organization.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Ask yourself what the organization is trying to achieve from the transformation. What worries the organization about reliability? For SRE to be successful and efficient, it is crucial to \u0026lt;b\u0026gt;start with the pain\u0026lt;/b\u0026gt;. Starting by identifying what you are trying to solve will not just help you solve it; it will help your organization be more focused, align the relevant parties to a common goal, and make it easier to gain decision-makers\u0026amp;#8217; buy-in (and much more).\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Once you understand the problem you are trying to solve, you need to know when you have \u0026amp;#8220;solved\u0026amp;#8221; it (e.g. how you will define success). Setting goals is critical\u0026amp;#8212;otherwise, how will you know if you have improved? We\u0026amp;#8217;ll discuss how to set up metrics to help in this self-evaluation in a later post.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Who are the key decision-makers in the organization?\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Even though implementing SRE principles involves engineering at its core, it\u0026amp;#8217;s actually more of a transformation process than a technological challenge. As such, it will likely require procedural and cultural changes.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As with any business transformation, you need to identify the relevant decision-makers up front. Who those people are depends on the organization, but it usually includes stakeholders from product, operations, and engineering leadership, though these can be named differently in various organizations and can even be separated under multiple organizations. Identifying those decision makers can be especially difficult in a siloed organization. It is important to take the time and reach out to different groups to identify the key stakeholders and influencers (it will save you a lot of time later on). Make sure that you are throwing a wide enough net. It is important to get input from different groups with different requirements (e.g., security).\u0026amp;#160;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;At the same time, try to be \u0026lt;b\u0026gt;flexible\u0026lt;/b\u0026gt;. It\u0026amp;#8217;s okay if your list of decision makers gets updated and fine-tuned during the process. Like in other engineering domains, the goal is to start simple and iterate.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Get buy-in and build trust\u0026lt;/b\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Once you\u0026amp;#8217;ve identified the relevant decision makers, make sure you have support from your colleagues, and the rest of the organization\u0026amp;#8217;s leaders. \u0026lt;b\u0026gt;Creating an empowered culture\u0026lt;/b\u0026gt; is critical for implementing the core principles of SRE: a \u0026lt;a href=\u0026#34;https://cloud.google.com/solutions/devops/devops-culture-learning-culture\u0026#34;\u0026gt;learning culture\u0026lt;/a\u0026gt; that accepts failures, that facilitates blamelessness and creates psychological safety, all while prioritizing gradual changes and automation.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;From my experience, you cannot drive real change in an organization without widespread support and buy-in from leadership and decision-makers\u0026amp;#8212;and that\u0026amp;#8217;s especially true for SRE.\u0026amp;#160; Implementing SRE, similar to DevOps, requires collaboration between different functions in the organization (product, operations and development). In most organizations, those functions fall under separate leadership chains, each with its own processes. If you\u0026amp;#8217;re going to align those goals and procedures, leadership needs to prioritize the change. At the same time, driving cultural change from the bottom up can be more challenging and take longer than top-down mandates, and in some cultures will be impossible. In short, leading by example and enabling the people in the organization are critical for driving change and fostering the \u0026#39;right\u0026#39; culture.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Remember: it\u0026#39;s a marathon, not a sprint\u0026amp;#160;\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The journey to SRE combines several challenges, both from technical and human (culture, process, extra) perspectives, and those are intertwined. To be successful, leadership needs to prioritize organizational changes, allocating resources for \u0026lt;b\u0026gt;engineering excellence\u0026lt;/b\u0026gt; (quality and reliability) and \u0026lt;a href=\u0026#34;https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture#how_to_implement_organizational_culture\u0026#34;\u0026gt;\u0026lt;b\u0026gt;fostering cultural principles\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt; like reducing silos, blamelessness and accepting failure as normal.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Align expectations! All parties involved in an SRE implementation\u0026amp;#8212;from product and engineering to leadership\u0026amp;#8212;will need to recognize that change takes time and effort, and in the short term\u0026amp;#8212;resources. Daunting as it may be, SRE\u0026amp;#8217;s goal is to solve hard problems and build for a better tomorrow.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;i\u0026gt;Interested in getting deeper with SRE principles? Check out this Coursera course for leaders, \u0026lt;a href=\u0026#34;https://www.coursera.org/learn/developing-a-google-sre-culture\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Developing a Google SRE Culture\u0026lt;/a\u0026gt;.\u0026lt;/i\u0026gt; And stay tuned for my next post, where I outline some tactical considerations for teams that are early on their SRE journey, from identifying the right teams to start with, enablement and building community.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003ePeople sometimes think that implementing \u003ca href=\"http://sre.google\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"http://sre.google\" track-metadata-module=\"post\"\u003eSite Reliability Engineering\u003c/a\u003e (or DevOps for that matter) will magically make everything better. Just sprinkle a little bit of SRE fairy dust on your organization and your services will be more reliable, more profitable, and your IT, product and engineering teams will be happy.\u003c/p\u003e\u003cp\u003eIt’s easy to see why people think this way. Some of the world’s most reliable and scalable services run with the help of an SRE team, Google being the prime example. \u003c/p\u003e\u003cp\u003eFor almost two decades, I’ve lived and breathed running production systems at large scale. I had to think about tradeoffs, reliability, costs, implementing a variety of architectures with different constraints and requirements—all while getting paged in the middle of the night. More recently, I’ve had the privilege to leverage that experience and knowledge to help Google Cloud customers modernize their infrastructure and applications, including implementing an SRE practice. While these learnings look different from organization to organization, there are common lessons learned that will impact the success of your deployment.\u003c/p\u003e\u003cp\u003eWhen problems do arise, it’s usually not because of technical challenges. A stalled SRE culture is usually a business process failure—goals weren’t properly defined up front and stakeholders weren’t properly engaged. After watching this play out repeatedly, I’ve developed some advice for technology leaders about how to implement a successful SRE practice. \u003c/p\u003e\u003ch3\u003eBefore you start\u003c/h3\u003e\u003cp\u003eYour SRE journey should start well before you read your first manual, or put in your first call to an SRE advisor. As a technology leader within your organization, your first job is to answer a few key questions and gather some basic facts. \u003c/p\u003e\u003cp\u003e\u003cb\u003eWhat problem are you trying to solve? \u003c/b\u003e\u003c/p\u003e\u003cp\u003eMost organizations will readily admit they’re not perfect. Perhaps you need to \u003ca href=\"https://www.youtube.com/watch?v=IvQ-15-yE_c\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://www.youtube.com\" track-metadata-module=\"post\"\u003ereduce toil\u003c/a\u003e, be more innovative, or release software faster. SRE, as a framework for operating large scale systems reliably, can certainly help with those goals. To do that, it’s important to understand your motivations and what gaps or needs exist in your organization.\u003c/p\u003e\u003cp\u003eAsk yourself what the organization is trying to achieve from the transformation. What worries the organization about reliability? For SRE to be successful and efficient, it is crucial to \u003cb\u003estart with the pain\u003c/b\u003e. Starting by identifying what you are trying to solve will not just help you solve it; it will help your organization be more focused, align the relevant parties to a common goal, and make it easier to gain decision-makers’ buy-in (and much more).\u003c/p\u003e\u003cp\u003eOnce you understand the problem you are trying to solve, you need to know when you have “solved” it (e.g. how you will define success). Setting goals is critical—otherwise, how will you know if you have improved? We’ll discuss how to set up metrics to help in this self-evaluation in a later post.\u003c/p\u003e\u003cp\u003e\u003cb\u003eWho are the key decision-makers in the organization?\u003c/b\u003e\u003c/p\u003e\u003cp\u003eEven though implementing SRE principles involves engineering at its core, it’s actually more of a transformation process than a technological challenge. As such, it will likely require procedural and cultural changes.\u003c/p\u003e\u003cp\u003eAs with any business transformation, you need to identify the relevant decision-makers up front. Who those people are depends on the organization, but it usually includes stakeholders from product, operations, and engineering leadership, though these can be named differently in various organizations and can even be separated under multiple organizations. Identifying those decision makers can be especially difficult in a siloed organization. It is important to take the time and reach out to different groups to identify the key stakeholders and influencers (it will save you a lot of time later on). Make sure that you are throwing a wide enough net. It is important to get input from different groups with different requirements (e.g., security).  \u003c/p\u003e\u003cp\u003eAt the same time, try to be \u003cb\u003eflexible\u003c/b\u003e. It’s okay if your list of decision makers gets updated and fine-tuned during the process. Like in other engineering domains, the goal is to start simple and iterate. \u003c/p\u003e\u003cp\u003e\u003cb\u003eGet buy-in and build trust\u003c/b\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified the relevant decision makers, make sure you have support from your colleagues, and the rest of the organization’s leaders. \u003cb\u003eCreating an empowered culture\u003c/b\u003e is critical for implementing the core principles of SRE: a \u003ca href=\"https://cloud.google.com/solutions/devops/devops-culture-learning-culture\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/solutions/devops/devops-culture-learning-culture\" track-metadata-module=\"post\"\u003elearning culture\u003c/a\u003e that accepts failures, that facilitates blamelessness and creates psychological safety, all while prioritizing gradual changes and automation.\u003c/p\u003e\u003cp\u003eFrom my experience, you cannot drive real change in an organization without widespread support and buy-in from leadership and decision-makers—and that’s especially true for SRE.  Implementing SRE, similar to DevOps, requires collaboration between different functions in the organization (product, operations and development). In most organizations, those functions fall under separate leadership chains, each with its own processes. If you’re going to align those goals and procedures, leadership needs to prioritize the change. At the same time, driving cultural change from the bottom up can be more challenging and take longer than top-down mandates, and in some cultures will be impossible. In short, leading by example and enabling the people in the organization are critical for driving change and fostering the \u0026#39;right\u0026#39; culture.\u003c/p\u003e\u003ch3\u003eRemember: it\u0026#39;s a marathon, not a sprint \u003c/h3\u003e\u003cp\u003eThe journey to SRE combines several challenges, both from technical and human (culture, process, extra) perspectives, and those are intertwined. To be successful, leadership needs to prioritize organizational changes, allocating resources for \u003cb\u003eengineering excellence\u003c/b\u003e (quality and reliability) and \u003ca href=\"https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture#how_to_implement_organizational_culture\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture#how_to_implement_organizational_culture\" track-metadata-module=\"post\"\u003e\u003cb\u003efostering cultural principles\u003c/b\u003e\u003c/a\u003e like reducing silos, blamelessness and accepting failure as normal.\u003c/p\u003e\u003cp\u003eAlign expectations! All parties involved in an SRE implementation—from product and engineering to leadership—will need to recognize that change takes time and effort, and in the short term—resources. Daunting as it may be, SRE’s goal is to solve hard problems and build for a better tomorrow. \u003c/p\u003e\u003cp\u003e\u003ci\u003eInterested in getting deeper with SRE principles? Check out this Coursera course for leaders, \u003ca href=\"https://www.coursera.org/learn/developing-a-google-sre-culture\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://www.coursera.org\" track-metadata-module=\"post\"\u003eDeveloping a Google SRE Culture\u003c/a\u003e.\u003c/i\u003e And stay tuned for my next post, where I outline some tactical considerations for teams that are early on their SRE journey, from identifying the right teams to start with, enablement and building community.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003ePeople sometimes think that implementing \u003ca href=\"http://sre.google\" target=\"_blank\"\u003eSite Reliability Engineering\u003c/a\u003e (or DevOps for that matter) will magically make everything better. Just sprinkle a little bit of SRE fairy dust on your organization and your services will be more reliable, more profitable, and your IT, product and engineering teams will be happy.\u003c/p\u003e\u003cp\u003eIt’s easy to see why people think this way. Some of the world’s most reliable and scalable services run with the help of an SRE team, Google being the prime example. \u003c/p\u003e\u003cp\u003eFor almost two decades, I’ve lived and breathed running production systems at large scale. I had to think about tradeoffs, reliability, costs, implementing a variety of architectures with different constraints and requirements—all while getting paged in the middle of the night. More recently, I’ve had the privilege to leverage that experience and knowledge to help Google Cloud customers modernize their infrastructure and applications, including implementing an SRE practice. While these learnings look different from organization to organization, there are common lessons learned that will impact the success of your deployment.\u003c/p\u003e\u003cp\u003eWhen problems do arise, it’s usually not because of technical challenges. A stalled SRE culture is usually a business process failure—goals weren’t properly defined up front and stakeholders weren’t properly engaged. After watching this play out repeatedly, I’ve developed some advice for technology leaders about how to implement a successful SRE practice. \u003c/p\u003e\u003ch3\u003eBefore you start\u003c/h3\u003e\u003cp\u003eYour SRE journey should start well before you read your first manual, or put in your first call to an SRE advisor. As a technology leader within your organization, your first job is to answer a few key questions and gather some basic facts. \u003c/p\u003e\u003cp\u003e\u003cb\u003eWhat problem are you trying to solve? \u003c/b\u003e\u003c/p\u003e\u003cp\u003eMost organizations will readily admit they’re not perfect. Perhaps you need to \u003ca href=\"https://www.youtube.com/watch?v=IvQ-15-yE_c\" target=\"_blank\"\u003ereduce toil\u003c/a\u003e, be more innovative, or release software faster. SRE, as a framework for operating large scale systems reliably, can certainly help with those goals. To do that, it’s important to understand your motivations and what gaps or needs exist in your organization.\u003c/p\u003e\u003cp\u003eAsk yourself what the organization is trying to achieve from the transformation. What worries the organization about reliability? For SRE to be successful and efficient, it is crucial to \u003cb\u003estart with the pain\u003c/b\u003e. Starting by identifying what you are trying to solve will not just help you solve it; it will help your organization be more focused, align the relevant parties to a common goal, and make it easier to gain decision-makers’ buy-in (and much more).\u003c/p\u003e\u003cp\u003eOnce you understand the problem you are trying to solve, you need to know when you have “solved” it (e.g. how you will define success). Setting goals is critical—otherwise, how will you know if you have improved? We’ll discuss how to set up metrics to help in this self-evaluation in a later post.\u003c/p\u003e\u003cp\u003e\u003cb\u003eWho are the key decision-makers in the organization?\u003c/b\u003e\u003c/p\u003e\u003cp\u003eEven though implementing SRE principles involves engineering at its core, it’s actually more of a transformation process than a technological challenge. As such, it will likely require procedural and cultural changes.\u003c/p\u003e\u003cp\u003eAs with any business transformation, you need to identify the relevant decision-makers up front. Who those people are depends on the organization, but it usually includes stakeholders from product, operations, and engineering leadership, though these can be named differently in various organizations and can even be separated under multiple organizations. Identifying those decision makers can be especially difficult in a siloed organization. It is important to take the time and reach out to different groups to identify the key stakeholders and influencers (it will save you a lot of time later on). Make sure that you are throwing a wide enough net. It is important to get input from different groups with different requirements (e.g., security).  \u003c/p\u003e\u003cp\u003eAt the same time, try to be \u003cb\u003eflexible\u003c/b\u003e. It’s okay if your list of decision makers gets updated and fine-tuned during the process. Like in other engineering domains, the goal is to start simple and iterate. \u003c/p\u003e\u003cp\u003e\u003cb\u003eGet buy-in and build trust\u003c/b\u003e\u003c/p\u003e\u003cp\u003eOnce you’ve identified the relevant decision makers, make sure you have support from your colleagues, and the rest of the organization’s leaders. \u003cb\u003eCreating an empowered culture\u003c/b\u003e is critical for implementing the core principles of SRE: a \u003ca href=\"https://cloud.google.com/solutions/devops/devops-culture-learning-culture\"\u003elearning culture\u003c/a\u003e that accepts failures, that facilitates blamelessness and creates psychological safety, all while prioritizing gradual changes and automation.\u003c/p\u003e\u003cp\u003eFrom my experience, you cannot drive real change in an organization without widespread support and buy-in from leadership and decision-makers—and that’s especially true for SRE.  Implementing SRE, similar to DevOps, requires collaboration between different functions in the organization (product, operations and development). In most organizations, those functions fall under separate leadership chains, each with its own processes. If you’re going to align those goals and procedures, leadership needs to prioritize the change. At the same time, driving cultural change from the bottom up can be more challenging and take longer than top-down mandates, and in some cultures will be impossible. In short, leading by example and enabling the people in the organization are critical for driving change and fostering the 'right' culture.\u003c/p\u003e\u003ch3\u003eRemember: it's a marathon, not a sprint \u003c/h3\u003e\u003cp\u003eThe journey to SRE combines several challenges, both from technical and human (culture, process, extra) perspectives, and those are intertwined. To be successful, leadership needs to prioritize organizational changes, allocating resources for \u003cb\u003eengineering excellence\u003c/b\u003e (quality and reliability) and \u003ca href=\"https://cloud.google.com/solutions/devops/devops-culture-westrum-organizational-culture#how_to_implement_organizational_culture\"\u003e\u003cb\u003efostering cultural principles\u003c/b\u003e\u003c/a\u003e like reducing silos, blamelessness and accepting failure as normal.\u003c/p\u003e\u003cp\u003eAlign expectations! All parties involved in an SRE implementation—from product and engineering to leadership—will need to recognize that change takes time and effort, and in the short term—resources. Daunting as it may be, SRE’s goal is to solve hard problems and build for a better tomorrow. \u003c/p\u003e\u003cp\u003e\u003ci\u003eInterested in getting deeper with SRE principles? Check out this Coursera course for leaders, \u003ca href=\"https://www.coursera.org/learn/developing-a-google-sre-culture\" target=\"_blank\"\u003eDeveloping a Google SRE Culture\u003c/a\u003e.\u003c/i\u003e And stay tuned for my next post, where I outline some tactical considerations for teams that are early on their SRE journey, from identifying the right teams to start with, enablement and building community.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/devops-sre/four-steps-to-jumpstarting-your-sre-practice/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_A_Rnd3.max-2800x2800.max-500x500.jpg')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eFour steps to jumpstarting your SRE practice\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eOnce you have leadership buy-in, there are some things you can do to get the SRE ball rolling, fast.\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/DevOps_BlogHeader_D_Rnd3.max-2200x2200.jpg",
      "date_published": "2021-02-26T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eAyelet Sachto\u003c/name\u003e\u003ctitle\u003eStrategic Cloud Engineer, Infra, AppMod, SRE\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/operations/troubleshooting-services-on-google-kubernetes-engine/",
      "title": "Troubleshooting services on Google Kubernetes Engine by example",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003carticle-page\u003e\u003cmain id=\"jump-content\"\u003e\u003carticle\u003e\u003carticle-header-block\u003e\u003c/article-header-block\u003e\u003carticle-aspect-image-block\u003e\u003cfigure\u003e\u003cp\u003e\u003cspan\u003e#containers\u003c/span\u003e\u003c/p\u003e\u003c/figure\u003e\u003c/article-aspect-image-block\u003e\u003cdiv\u003e\u003carticle-cta _nghost-c17=\"\"\u003e\u003cdiv _ngcontent-c17=\"\"\u003e\u003ch4 _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eTry GCP\u003c/span\u003e\u003c/h4\u003e\u003cp _ngcontent-c17=\"\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eStart building on Google Cloud with $300 in free credits and 20+ always free products.\u003c/span\u003e\u003c/p\u003e\u003cp\u003e\u003ca _ngcontent-c17=\"\" clicktracker=\"\" rel=\"external\" track-metadata-module=\"article cta\" track-type=\"button\" track-name=\"free trial\" track-metadata-eventdetail=\"https://cloud.google.com/free/\" href=\"https://cloud.google.com/free/\"\u003e\u003cspan _ngcontent-c17=\"\"\u003eFree Trial\u003c/span\u003e\u003c/a\u003e\u003c/p\u003e\u003c/div\u003e\u003c/article-cta\u003e\u003c/div\u003e\u003carticle-share-block\u003e\u003c/article-share-block\u003e\u003carticle-sticky-share-block\u003e\u003c/article-sticky-share-block\u003e\u003cdiv\u003e\u003cdiv\u003e\u003carticle-content-stream-block\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;Applications fail. Containers crash. It\u0026amp;#8217;s a fact of life that SRE and DevOps teams know all too well. To help navigate life\u0026amp;#8217;s hiccups, we\u0026amp;#8217;ve previously shared \u0026lt;a href=\u0026#34;https://cloud.google.com/blog/products/containers-kubernetes/tools-for-debugging-apps-on-google-kubernetes-engine\u0026#34;\u0026gt;how to debug applications running on Google Kubernetes Engine (GKE)\u0026lt;/a\u0026gt;. We\u0026amp;#8217;ve also updated the GKE \u0026lt;a href=\u0026#34;https://cloud.google.com/stackdriver/docs/solutions/gke/observing\u0026#34;\u0026gt;dashboard\u0026lt;/a\u0026gt; with new easier-to-use troubleshooting flows. Today, we go one step further and show you how you can use these flows to quickly find and resolve issues in your applications and infrastructure.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In this blog, we\u0026#39;ll walk through deploying a sample app to your cluster and configuring an alerting policy that will notify you if there are any container restarts observed. From there, we\u0026#39;ll trigger the alert and explore how the new GKE dashboard makes it easy to identify the issue and determine exactly what\u0026#39;s going on with your workload or infrastructure that may be causing it.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Setting up\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Deploy the app\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;This example uses a \u0026lt;a href=\u0026#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/main.go\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;demo app\u0026lt;/a\u0026gt; that exposes two endpoints: an endpoint at /, which is just a \u0026amp;#34;hello world\u0026amp;#34;, and a /crashme endpoint, which uses Go\u0026#39;s \u0026lt;code\u0026gt;os.Exit(1)\u0026lt;/code\u0026gt; to terminate the process. To deploy the app in your own cluster, create a container image using Cloud Build and \u0026lt;a href=\u0026#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;deploy\u0026lt;/a\u0026gt; it to GKE. Then, expose the service with a load balancer.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Once the service is deployed, check the running pods:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eApplications fail. Containers crash. It’s a fact of life that SRE and DevOps teams know all too well. To help navigate life’s hiccups, we’ve previously shared \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/tools-for-debugging-apps-on-google-kubernetes-engine\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/blog/products/containers-kubernetes/tools-for-debugging-apps-on-google-kubernetes-engine\" track-metadata-module=\"post\"\u003ehow to debug applications running on Google Kubernetes Engine (GKE)\u003c/a\u003e. We’ve also updated the GKE \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/observing\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://cloud.google.com/stackdriver/docs/solutions/gke/observing\" track-metadata-module=\"post\"\u003edashboard\u003c/a\u003e with new easier-to-use troubleshooting flows. Today, we go one step further and show you how you can use these flows to quickly find and resolve issues in your applications and infrastructure. \u003c/p\u003e\u003cp\u003eIn this blog, we\u0026#39;ll walk through deploying a sample app to your cluster and configuring an alerting policy that will notify you if there are any container restarts observed. From there, we\u0026#39;ll trigger the alert and explore how the new GKE dashboard makes it easy to identify the issue and determine exactly what\u0026#39;s going on with your workload or infrastructure that may be causing it.\u003c/p\u003e\u003ch3\u003eSetting up\u003c/h3\u003e\u003cp\u003e\u003cb\u003eDeploy the app\u003cbr/\u003e\u003c/b\u003eThis example uses a \u003ca href=\"https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/main.go\" target=\"_blank\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003edemo app\u003c/a\u003e that exposes two endpoints: an endpoint at /, which is just a \u0026#34;hello world\u0026#34;, and a /crashme endpoint, which uses Go\u0026#39;s \u003ccode\u003eos.Exit(1)\u003c/code\u003e to terminate the process. To deploy the app in your own cluster, create a container image using Cloud Build and \u003ca href=\"https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003edeploy\u003c/a\u003e it to GKE. Then, expose the service with a load balancer. \u003c/p\u003e\u003cp\u003eOnce the service is deployed, check the running pods:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003e✗ kubectl get pods\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003eNAME                                     READY   STATUS    RESTARTS   AGE\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003erestarting-deployment-54c8678f79-gjh2v   1/1     Running   0          6m38s\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003erestarting-deployment-54c8678f79-l8tsm   1/1     Running   0          6m38s\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003erestarting-deployment-54c8678f79-qjrcb   1/1     Running   0          6m38s\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eNotice that RESTARTS is initially at zero for each pod. Use a browser or a command line tool like curl to access the /crashme endpoint. At this point, you should see a restart:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003e✗ kubectl get pods\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003eNAME                                     READY   STATUS    RESTARTS   AGE\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003erestarting-deployment-54c8678f79-gjh2v   1/1     Running   1          9m28s\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003erestarting-deployment-54c8678f79-l8tsm   1/1     Running   0          9m28s\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003erestarting-deployment-54c8678f79-qjrcb   1/1     Running   0          9m28s\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eEach request to that endpoint will result in a restart. However, be careful to not do this more often than every 30 seconds or so, otherwise, the containers will go into CrashLoopBackOff, and it will take time for the service to be available again. You can use this simple shell script to trigger restarts when as needed:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-code-block _nghost-c20=\"\"\u003e\u003cpre _ngcontent-c20=\"\"\u003e  \u003ccode _ngcontent-c20=\"\"\u003ewhile true;\n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003edo \n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003ecurl http://$IP_ADDRESS:8080/crashme; \n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003esleep 45; \n\u003c/code\u003e\u003ccode _ngcontent-c20=\"\"\u003edone\u003c/code\u003e\n\u003c/pre\u003e\u003c/article-code-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;where $IP_ADDRESS is the IP address of the load balancer you\u0026#39;ve already created.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Why do container restarts matter? Well, restarts, to a certain degree, are an expected part of a container\u0026amp;#8217;s typical lifecycle in Kubernetes. Too many container restarts, however, could affect the availability of your service, especially when expanded over a larger number of replicas for a given Pod. Not only do excessive restarts degrade the service in question, but they also risks affecting other services downstream that use it as a dependency.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;In real life,the culprit for a large number of restarts could be a poorly designed liveness probe, issues like \u0026lt;a href=\u0026#34;https://en.wikipedia.org/wiki/Deadlock\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;deadlocks\u0026lt;/a\u0026gt; in the application itself, or misconfigured memory requests that result in \u0026lt;a href=\u0026#34;https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;\u0026lt;code\u0026gt;OOMkilled\u0026lt;/code\u0026gt;\u0026lt;/a\u0026gt; errors. So, it is important for you to proactively alert on container restarts to preempt potential degradation that can cascade across multiple services.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Configure the alert\u0026lt;br\u0026gt;\u0026lt;/b\u0026gt;Now, you\u0026#39;re ready to configure the alert that will notify you when restarts are detected. Here\u0026#39;s how to set up your \u0026lt;a href=\u0026#34;https://cloud.google.com/monitoring/alerts\u0026#34;\u0026gt;alerting policy\u0026lt;/a\u0026gt;:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003ewhere $IP_ADDRESS is the IP address of the load balancer you\u0026#39;ve already created. \u003c/p\u003e\u003cp\u003eWhy do container restarts matter? Well, restarts, to a certain degree, are an expected part of a container’s typical lifecycle in Kubernetes. Too many container restarts, however, could affect the availability of your service, especially when expanded over a larger number of replicas for a given Pod. Not only do excessive restarts degrade the service in question, but they also risks affecting other services downstream that use it as a dependency.\u003c/p\u003e\u003cp\u003eIn real life,the culprit for a large number of restarts could be a poorly designed liveness probe, issues like \u003ca href=\"https://en.wikipedia.org/wiki/Deadlock\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"https://en.wikipedia.org\" track-metadata-module=\"post\"\u003edeadlocks\u003c/a\u003e in the application itself, or misconfigured memory requests that result in \u003ca href=\"https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://kubernetes.io\" track-metadata-module=\"post\"\u003e\u003ccode\u003eOOMkilled\u003c/code\u003e\u003c/a\u003e errors. So, it is important for you to proactively alert on container restarts to preempt potential degradation that can cascade across multiple services. \u003c/p\u003e\u003cp\u003e\u003cb\u003eConfigure the alert\u003cbr/\u003e\u003c/b\u003eNow, you\u0026#39;re ready to configure the alert that will notify you when restarts are detected. Here\u0026#39;s how to set up your \u003ca href=\"https://cloud.google.com/monitoring/alerts\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://cloud.google.com/monitoring/alerts\" track-metadata-module=\"post\"\u003ealerting policy\u003c/a\u003e:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;You can use the \u0026lt;code\u0026gt;kubernetes.io/container/restart_count\u0026lt;/code\u0026gt; metric, filtered to the specific container name (as specified in the deployment yaml \u0026lt;a href=\u0026#34;https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;file\u0026lt;/a\u0026gt;). Configure the alert to trigger if any timeseries exceeded zero\u0026amp;#8212;meaning if any container restarts are observed.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;With the setup done, you are ready to test and see what happens!\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Testing the alert\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;When you\u0026amp;#8217;re ready, start the looped script to hit the /crashme endpoint every 45 seconds. The restart_count metric is sampled every 60 seconds, so it shouldn\u0026#39;t take very long for an alert to show up on the dashboard:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eYou can use the \u003ccode\u003ekubernetes.io/container/restart_count\u003c/code\u003e metric, filtered to the specific container name (as specified in the deployment yaml \u003ca href=\"https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml\" target=\"_blank\" track-type=\"inline link\" track-name=\"8\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003efile\u003c/a\u003e). Configure the alert to trigger if any timeseries exceeded zero—meaning if any container restarts are observed. \u003c/p\u003e\u003cp\u003eWith the setup done, you are ready to test and see what happens!\u003c/p\u003e\u003ch3\u003eTesting the alert\u003c/h3\u003e\u003cp\u003eWhen you’re ready, start the looped script to hit the /crashme endpoint every 45 seconds. The restart_count metric is sampled every 60 seconds, so it shouldn\u0026#39;t take very long for an alert to show up on the dashboard:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eYou can mouse-over the incident to get more information about it:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eThen click on \u0026#34;View Incident\u0026#34;. This takes you to the Incident details screen, where you can see the specific resources that triggered it—in this case, the incident is generated by the container.\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003eNext, you can click on View Logs to see the logs (in the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-viewer-preview\" track-type=\"inline link\" track-name=\"9\" track-metadata-eventdetail=\"https://cloud.google.com/logging/docs/view/logs-viewer-preview\" track-metadata-module=\"post\"\u003enew Logs Viewer\u003c/a\u003e!)—it\u0026#39;s immediately apparently that the alert is triggered by the containers restarting:\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;This is all very nicely tied together and makes troubleshooting during an incident much easier!\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;In summary\u0026amp;#8230;.\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The latest GKE dashboard includes many improvements over previous iterations. The new alerts timeline is intuitive, and incidents are clearly marked so that you can interact with them to get the full details of exactly what happened, all the way down to the container logs that tell you the actual problem.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;As an oncall SRE or DevOps engineer for a service running on GKE, the GKE dashboard makes it easier for you to respond to incidents. You\u0026#39;re now able to go from an incident all the way to debug logs quickly and easily and reduce the time it takes to triage and mitigate incidents. For a short overview on how to troubleshoot services on GKE, check out this video:\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eThis is all very nicely tied together and makes troubleshooting during an incident much easier!\u003c/p\u003e\u003ch3\u003eIn summary….\u003c/h3\u003e\u003cp\u003eThe latest GKE dashboard includes many improvements over previous iterations. The new alerts timeline is intuitive, and incidents are clearly marked so that you can interact with them to get the full details of exactly what happened, all the way down to the container logs that tell you the actual problem.\u003c/p\u003e\u003cp\u003eAs an oncall SRE or DevOps engineer for a service running on GKE, the GKE dashboard makes it easier for you to respond to incidents. You\u0026#39;re now able to go from an incident all the way to debug logs quickly and easily and reduce the time it takes to triage and mitigate incidents. For a short overview on how to troubleshoot services on GKE, check out this video:\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003carticle-video-block _nghost-c16=\"\"\u003e\u003cp _ngcontent-c16=\"\"\u003e\u003ciframe _ngcontent-c16=\"\" allow=\"encrypted-media\" allowfullscreen=\"\" frameborder=\"0\" height=\"100%\" position=\"absolute\" width=\"100%\" src=\"https://www.youtube.com/embed/--4WWwx4Log?enablejsapi=1\u0026amp;\"\u003e\u003c/iframe\u003e\u003c/p\u003e\u003c/article-video-block\u003e\u003c/div\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cp\u003e\u003ci\u003e\u003csup\u003eA special thanks to Anthony Bushong, Specialist Customer Engineer, for his contributions to this blog post.\u003c/sup\u003e\u003c/i\u003e\u003c/p\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/article-content-stream-block\u003e\u003carticle-tag-list-block\u003e\u003c/article-tag-list-block\u003e\u003c/div\u003e\u003csection\u003e\u003carticle-up-1to3-block _nghost-c18=\"\"\u003e\u003c/article-up-1to3-block\u003e\u003c/section\u003e\u003c/div\u003e\u003c/article\u003e\u003c/main\u003e\u003c/article-page\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eApplications fail. Containers crash. It’s a fact of life that SRE and DevOps teams know all too well. To help navigate life’s hiccups, we’ve previously shared \u003ca href=\"https://cloud.google.com/blog/products/containers-kubernetes/tools-for-debugging-apps-on-google-kubernetes-engine\"\u003ehow to debug applications running on Google Kubernetes Engine (GKE)\u003c/a\u003e. We’ve also updated the GKE \u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/gke/observing\"\u003edashboard\u003c/a\u003e with new easier-to-use troubleshooting flows. Today, we go one step further and show you how you can use these flows to quickly find and resolve issues in your applications and infrastructure. \u003c/p\u003e\u003cp\u003eIn this blog, we'll walk through deploying a sample app to your cluster and configuring an alerting policy that will notify you if there are any container restarts observed. From there, we'll trigger the alert and explore how the new GKE dashboard makes it easy to identify the issue and determine exactly what's going on with your workload or infrastructure that may be causing it.\u003c/p\u003e\u003ch3\u003eSetting up\u003c/h3\u003e\u003cp\u003e\u003cb\u003eDeploy the app\u003cbr/\u003e\u003c/b\u003eThis example uses a \u003ca href=\"https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/main.go\" target=\"_blank\"\u003edemo app\u003c/a\u003e that exposes two endpoints: an endpoint at /, which is just a \"hello world\", and a /crashme endpoint, which uses Go's \u003ccode\u003eos.Exit(1)\u003c/code\u003e to terminate the process. To deploy the app in your own cluster, create a container image using Cloud Build and \u003ca href=\"https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml\" target=\"_blank\"\u003edeploy\u003c/a\u003e it to GKE. Then, expose the service with a load balancer. \u003c/p\u003e\u003cp\u003eOnce the service is deployed, check the running pods:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eNotice that RESTARTS is initially at zero for each pod. Use a browser or a command line tool like curl to access the /crashme endpoint. At this point, you should see a restart:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eEach request to that endpoint will result in a restart. However, be careful to not do this more often than every 30 seconds or so, otherwise, the containers will go into CrashLoopBackOff, and it will take time for the service to be available again. You can use this simple shell script to trigger restarts when as needed:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003ewhere $IP_ADDRESS is the IP address of the load balancer you've already created. \u003c/p\u003e\u003cp\u003eWhy do container restarts matter? Well, restarts, to a certain degree, are an expected part of a container’s typical lifecycle in Kubernetes. Too many container restarts, however, could affect the availability of your service, especially when expanded over a larger number of replicas for a given Pod. Not only do excessive restarts degrade the service in question, but they also risks affecting other services downstream that use it as a dependency.\u003c/p\u003e\u003cp\u003eIn real life,the culprit for a large number of restarts could be a poorly designed liveness probe, issues like \u003ca href=\"https://en.wikipedia.org/wiki/Deadlock\" target=\"_blank\"\u003edeadlocks\u003c/a\u003e in the application itself, or misconfigured memory requests that result in \u003ca href=\"https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/\" target=\"_blank\"\u003e\u003ccode\u003eOOMkilled\u003c/code\u003e\u003c/a\u003e errors. So, it is important for you to proactively alert on container restarts to preempt potential degradation that can cascade across multiple services. \u003c/p\u003e\u003cp\u003e\u003cb\u003eConfigure the alert\u003cbr/\u003e\u003c/b\u003eNow, you're ready to configure the alert that will notify you when restarts are detected. Here's how to set up your \u003ca href=\"https://cloud.google.com/monitoring/alerts\"\u003ealerting policy\u003c/a\u003e:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"1 Configure the alert.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/1_Configure_the_alert.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou can use the \u003ccode\u003ekubernetes.io/container/restart_count\u003c/code\u003e metric, filtered to the specific container name (as specified in the deployment yaml \u003ca href=\"https://github.com/yuriatgoogle/stack-doctor/blob/master/crashing-pod-demo/deployment.yaml\" target=\"_blank\"\u003efile\u003c/a\u003e). Configure the alert to trigger if any timeseries exceeded zero—meaning if any container restarts are observed. \u003c/p\u003e\u003cp\u003eWith the setup done, you are ready to test and see what happens!\u003c/p\u003e\u003ch3\u003eTesting the alert\u003c/h3\u003e\u003cp\u003eWhen you’re ready, start the looped script to hit the /crashme endpoint every 45 seconds. The restart_count metric is sampled every 60 seconds, so it shouldn't take very long for an alert to show up on the dashboard:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"2 Testing the alert.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/2_Testing_the_alert.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou can mouse-over the incident to get more information about it:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"3 incident.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/3_incident.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThen click on \"View Incident\". This takes you to the Incident details screen, where you can see the specific resources that triggered it—in this case, the incident is generated by the container.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"4 View Incident.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/4_View_Incident.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eNext, you can click on View Logs to see the logs (in the \u003ca href=\"https://cloud.google.com/logging/docs/view/logs-viewer-preview\"\u003enew Logs Viewer\u003c/a\u003e!)—it's immediately apparently that the alert is triggered by the containers restarting:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"5 View Logs.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/5_View_Logs.max-1000x1000.jpg\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eThis is all very nicely tied together and makes troubleshooting during an incident much easier!\u003c/p\u003e\u003ch3\u003eIn summary….\u003c/h3\u003e\u003cp\u003eThe latest GKE dashboard includes many improvements over previous iterations. The new alerts timeline is intuitive, and incidents are clearly marked so that you can interact with them to get the full details of exactly what happened, all the way down to the container logs that tell you the actual problem.\u003c/p\u003e\u003cp\u003eAs an oncall SRE or DevOps engineer for a service running on GKE, the GKE dashboard makes it easier for you to respond to incidents. You're now able to go from an incident all the way to debug logs quickly and easily and reduce the time it takes to triage and mitigate incidents. For a short overview on how to troubleshoot services on GKE, check out this video:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal---4WWwx4Log-\" href=\"https://youtube.com/watch?v=--4WWwx4Log\"\u003e\u003cimg alt=\"In previous episodes, we’ve shown you how to set up monitoring and alerting for your GKE services, but what do you do when an alert fires? In this episode of Stack Doctor, we show you how to use the alerts timeline on your GKE monitoring dashboards to troubleshoot your services. Watch to learn how you can easily spot and resolve issues in your applications and infrastructure!\" src=\"//img.youtube.com/vi/--4WWwx4Log/maxresdefault.jpg\"/\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal---4WWwx4Log-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"--4WWwx4Log\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=--4WWwx4Log\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e\u003ci\u003e\u003csup\u003eA special thanks to Anthony Bushong, Specialist Customer Engineer, for his contributions to this blog post.\u003c/sup\u003e\u003c/i\u003e\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"https://gweb-cloudblog-publish.appspot.com/products/management-tools/shrinking-the-time-to-mitigate-production-incidents/\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud-01_xyGPYQS.max-500x500.png')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003eShrinking the time to mitigate production incidents—CRE life lessons\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003eSee how you can use SRE and CRE principles and tests from Google, including Wheel of Misfortune and DiRT, to reduce the time needed to mi...\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_Kubernetes_tJPVpVo.max-2200x2200.jpg",
      "date_published": "2021-02-26T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eYuri Grinshteyn\u003c/name\u003e\u003ctitle\u003eSite Reliability Engineer, CRE\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/application-development/show-me-the-money-how-you-can-see-returns-up-to-259m-with-a-devops-transformation/",
      "title": "Show me the money! How you can see returns up to $259M with a DevOps transformation",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;2020 challenged some of the best laid plans by enterprises. With nearly everything moving online, Covid-19 pushed forward years of digital transformation. DevOps was at the heart of this transformation journey. After all, delivering software quickly, reliably, and safely to meet the changing needs of customers was crucial to adapt to this new normal.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;It is unlikely that the pace of modernization will slow down in 2021. As IT and business leaders further drive digital adoption within their organizations via DevOps, the need to quantify the business benefit from a digital transformation remains top of mind. A reliable model is imperative to drive the right level of investments and measure the returns. This is precisely why we wrote \u0026lt;a href=\u0026#34;https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\u0026#34;\u0026gt;How to Measure ROI of DevOps Transformation\u0026lt;/a\u0026gt;. This white paper is backed with scientific studies conducted by \u0026lt;a href=\u0026#34;https://www.devops-research.com/research.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;DevOps Research and Assessment, DORA\u0026lt;/a\u0026gt;, with 31,000 professionals worldwide over 6 years to provide clear guidance based on impartial industry data. We found the financial savings of DevOps transformation varies from from $10M to $259M a year.\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Looking beyond cost to value\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;The most innovative companies undertake their technology transformations with a focus on the value they can deliver to their customers. Hence, in addition to measuring cost savings, we show how DevOps done right can be a value driver and innovation engine. Let\u0026#39;s look deeper into how we quantify the cost and value-generating power of DevOps.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Cost-driven category\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Here, we focus on quantifying the cost savings and efficiencies realized by implementing DevOps\u0026amp;#8212;for example, how an investment in DevOps reduces costs by cutting the time it takes to resolve outages and avoiding downtime as much as possible.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;However, focusing solely on reducing costs can rarely yield systemic, long-term gains; thereby increasing the importance of going beyond cost-driven strategies. The cost savings achieved in year one \u0026amp;#8220;no longer count\u0026amp;#8221; beyond year two as the organization adjusts to a new baseline of costs and performance. Worse, only focusing on cost savings signals to technical staff their job is potentially at risk due to automation rather than being liberated from drudge work to better drive business growth. This leads to negative effects on morale and productivity.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Value-driven category\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;There are two value drivers in a DevOps transformation, (1) improved efficiency through the reduction of unnecessary rework, and (2) the potential revenue gained by reinvesting the time saved in new offer capabilities.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Adding these cost and value driven categories together, IT and business decision makers can get an estimate of the potential value their organizations can expect to gain from a DevOps transformation. This helps justify the investment needed to implement the required changes. To quantify the impact, we leverage industry benchmark data across low, medium, high, and elite DevOps teams, as described by DORA in its annual \u0026lt;a href=\u0026#34;https://cloud.google.com/devops/state-of-devops\u0026#34;\u0026gt;Accelerate: State of DevOps report\u0026lt;/a\u0026gt;.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;h3\u0026gt;Combining cost and value\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;As an example, let\u0026#39;s consider the impact of a DevOps transformation on a large organization with 8,500 technical staff and a medium IT performer. Using the data gained from the DevOps report, we can calculate both the cost and value driven categories along with total impact.\u0026amp;#160;\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003e2020 challenged some of the best laid plans by enterprises. With nearly everything moving online, Covid-19 pushed forward years of digital transformation. DevOps was at the heart of this transformation journey. After all, delivering software quickly, reliably, and safely to meet the changing needs of customers was crucial to adapt to this new normal.\u003c/p\u003e\u003cp\u003eIt is unlikely that the pace of modernization will slow down in 2021. As IT and business leaders further drive digital adoption within their organizations via DevOps, the need to quantify the business benefit from a digital transformation remains top of mind. A reliable model is imperative to drive the right level of investments and measure the returns. This is precisely why we wrote \u003ca href=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\" track-metadata-module=\"post\"\u003eHow to Measure ROI of DevOps Transformation\u003c/a\u003e. This white paper is backed with scientific studies conducted by \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://www.devops-research.com\" track-metadata-module=\"post\"\u003eDevOps Research and Assessment, DORA\u003c/a\u003e, with 31,000 professionals worldwide over 6 years to provide clear guidance based on impartial industry data. We found the financial savings of DevOps transformation varies from from $10M to $259M a year.\u003c/p\u003e\u003ch3\u003eLooking beyond cost to value\u003c/h3\u003e\u003cp\u003eThe most innovative companies undertake their technology transformations with a focus on the value they can deliver to their customers. Hence, in addition to measuring cost savings, we show how DevOps done right can be a value driver and innovation engine. Let\u0026#39;s look deeper into how we quantify the cost and value-generating power of DevOps. \u003c/p\u003e\u003ch3\u003eCost-driven category\u003c/h3\u003e\u003cp\u003eHere, we focus on quantifying the cost savings and efficiencies realized by implementing DevOps—for example, how an investment in DevOps reduces costs by cutting the time it takes to resolve outages and avoiding downtime as much as possible. \u003c/p\u003e\u003cp\u003eHowever, focusing solely on reducing costs can rarely yield systemic, long-term gains; thereby increasing the importance of going beyond cost-driven strategies. The cost savings achieved in year one “no longer count” beyond year two as the organization adjusts to a new baseline of costs and performance. Worse, only focusing on cost savings signals to technical staff their job is potentially at risk due to automation rather than being liberated from drudge work to better drive business growth. This leads to negative effects on morale and productivity. \u003c/p\u003e\u003ch3\u003eValue-driven category\u003c/h3\u003e\u003cp\u003eThere are two value drivers in a DevOps transformation, (1) improved efficiency through the reduction of unnecessary rework, and (2) the potential revenue gained by reinvesting the time saved in new offer capabilities.\u003c/p\u003e\u003cp\u003eAdding these cost and value driven categories together, IT and business decision makers can get an estimate of the potential value their organizations can expect to gain from a DevOps transformation. This helps justify the investment needed to implement the required changes. To quantify the impact, we leverage industry benchmark data across low, medium, high, and elite DevOps teams, as described by DORA in its annual \u003ca href=\"https://cloud.google.com/devops/state-of-devops\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/devops/state-of-devops\" track-metadata-module=\"post\"\u003eAccelerate: State of DevOps report\u003c/a\u003e. \u003c/p\u003e\u003ch3\u003eCombining cost and value\u003c/h3\u003e\u003cp\u003eAs an example, let\u0026#39;s consider the impact of a DevOps transformation on a large organization with 8,500 technical staff and a medium IT performer. Using the data gained from the DevOps report, we can calculate both the cost and value driven categories along with total impact. \u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003e2020 challenged some of the best laid plans by enterprises. With nearly everything moving online, Covid-19 pushed forward years of digital transformation. DevOps was at the heart of this transformation journey. After all, delivering software quickly, reliably, and safely to meet the changing needs of customers was crucial to adapt to this new normal.\u003c/p\u003e\u003cp\u003eIt is unlikely that the pace of modernization will slow down in 2021. As IT and business leaders further drive digital adoption within their organizations via DevOps, the need to quantify the business benefit from a digital transformation remains top of mind. A reliable model is imperative to drive the right level of investments and measure the returns. This is precisely why we wrote \u003ca href=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\"\u003eHow to Measure ROI of DevOps Transformation\u003c/a\u003e. This white paper is backed with scientific studies conducted by \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\"\u003eDevOps Research and Assessment, DORA\u003c/a\u003e, with 31,000 professionals worldwide over 6 years to provide clear guidance based on impartial industry data. We found the financial savings of DevOps transformation varies from from $10M to $259M a year.\u003c/p\u003e\u003ch3\u003eLooking beyond cost to value\u003c/h3\u003e\u003cp\u003eThe most innovative companies undertake their technology transformations with a focus on the value they can deliver to their customers. Hence, in addition to measuring cost savings, we show how DevOps done right can be a value driver and innovation engine. Let's look deeper into how we quantify the cost and value-generating power of DevOps. \u003c/p\u003e\u003ch3\u003eCost-driven category\u003c/h3\u003e\u003cp\u003eHere, we focus on quantifying the cost savings and efficiencies realized by implementing DevOps—for example, how an investment in DevOps reduces costs by cutting the time it takes to resolve outages and avoiding downtime as much as possible. \u003c/p\u003e\u003cp\u003eHowever, focusing solely on reducing costs can rarely yield systemic, long-term gains; thereby increasing the importance of going beyond cost-driven strategies. The cost savings achieved in year one “no longer count” beyond year two as the organization adjusts to a new baseline of costs and performance. Worse, only focusing on cost savings signals to technical staff their job is potentially at risk due to automation rather than being liberated from drudge work to better drive business growth. This leads to negative effects on morale and productivity. \u003c/p\u003e\u003ch3\u003eValue-driven category\u003c/h3\u003e\u003cp\u003eThere are two value drivers in a DevOps transformation, (1) improved efficiency through the reduction of unnecessary rework, and (2) the potential revenue gained by reinvesting the time saved in new offer capabilities.\u003c/p\u003e\u003cp\u003eAdding these cost and value driven categories together, IT and business decision makers can get an estimate of the potential value their organizations can expect to gain from a DevOps transformation. This helps justify the investment needed to implement the required changes. To quantify the impact, we leverage industry benchmark data across low, medium, high, and elite DevOps teams, as described by DORA in its annual \u003ca href=\"https://cloud.google.com/devops/state-of-devops\"\u003eAccelerate: State of DevOps report\u003c/a\u003e. \u003c/p\u003e\u003ch3\u003eCombining cost and value\u003c/h3\u003e\u003cp\u003eAs an example, let's consider the impact of a DevOps transformation on a large organization with 8,500 technical staff and a medium IT performer. Using the data gained from the DevOps report, we can calculate both the cost and value driven categories along with total impact. \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003cimg alt=\"roi table\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Screen_Shot_2021-01-25_at_6.08.51_PM.max-1000x1000.png\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eWhile this example represents what a medium IT performer at a large organization might expect by investing in DevOps, companies of all sizes and performance profiles can leverage DevOps to drive performance. In the \u003ca href=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\"\u003ewhite paper\u003c/a\u003e, we calculate the impact of DevOps across organizations of different sizes—small, medium, and large—as well as across four distinct performance profiles—low, medium, high, elite. \u003c/p\u003e\u003cp\u003eThere will be variation in these measurements based on your team’s current performance, compensation, change fail rate, benefits multiplier, and deployments per year, so we share our methodology in the white paper and invite you to customize the approach based on your specific needs and constraints. \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-video\"\u003e\u003cdiv class=\"article-module article-video \"\u003e\u003cfigure\u003e\u003ca class=\"h-c-video h-c-video--marquee\" data-glue-modal-disabled-on-mobile=\"true\" data-glue-modal-trigger=\"uni-modal-gZ7GtQ8XzYo-\" href=\"https://youtube.com/watch?v=gZ7GtQ8XzYo\"\u003e\u003cdiv class=\"article-video__aspect-image\" style=\"background-image: url(https://storage.googleapis.com/gweb-cloudblog-publish/images/Screen_Shot_2021-01-12_at_2.49.11_PM.max-1000x1000.png);\"\u003e\u003cspan class=\"h-u-visually-hidden\"\u003eROI of DevOps Transformation\u003c/span\u003e\u003c/div\u003e\u003csvg class=\"h-c-video__play h-c-icon h-c-icon--color-white\" role=\"img\"\u003e\u003cuse xlink:href=\"#mi-youtube-icon\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/a\u003e\u003cfigcaption class=\"article-video__caption h-c-page\"\u003e\u003ch4 class=\"h-c-headline h-c-headline--four h-u-font-weight-medium h-u-mt-std\"\u003eROI of DevOps Transformation: How to quantify the impact of your modernization initiatives\u003c/h4\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\u003cdiv class=\"h-c-modal--video\" data-glue-modal=\"uni-modal-gZ7GtQ8XzYo-\" data-glue-modal-close-label=\"Close Dialog\"\u003e\u003ca class=\"glue-yt-video\" data-glue-yt-video-autoplay=\"true\" data-glue-yt-video-height=\"99%\" data-glue-yt-video-vid=\"gZ7GtQ8XzYo\" data-glue-yt-video-width=\"100%\" href=\"https://youtube.com/watch?v=gZ7GtQ8XzYo\" ng-cloak=\"\"\u003e\u003c/a\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYears of \u003ca href=\"https://www.devops-research.com/research.html\" target=\"_blank\"\u003eDORA research\u003c/a\u003e show that undertaking a technology transformation initiative can produce sizable returns for any organization. Our goal \u003ca href=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\"\u003ewith the white paper\u003c/a\u003e is to provide IT and business decision makers an industry backed, data driven foundational basis for determining their investment in DevOps. Download the white paper \u003ca href=\"https://cloud.google.com/resources/roi-of-devops-transformation-whitepaper\"\u003ehere\u003c/a\u003e to calculate the impact of DevOps on your organization, while driving your digital transformation. \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-related_article_tout_external\"\u003e\u003cdiv class=\"uni-related-article-tout h-c-page\"\u003e\u003csection class=\"h-c-grid\"\u003e\u003ca class=\"uni-related-article-tout__wrapper h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3 uni-click-tracker\" data-analytics='{\n                       \"event\": \"page interaction\",\n                       \"category\": \"article lead\",\n                       \"action\": \"related article - inline\",\n                       \"label\": \"article: {slug}\"\n                     }' href=\"\"\u003e\u003cdiv class=\"uni-related-article-tout__inner-wrapper\"\u003e\u003cp class=\"uni-related-article-tout__eyebrow h-c-eyebrow\"\u003eRelated Article\u003c/p\u003e\u003cdiv class=\"uni-related-article-tout__content-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image-wrapper\"\u003e\u003cdiv class=\"uni-related-article-tout__image\" style=\"background-image: url('')\"\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"uni-related-article-tout__content\"\u003e\u003ch4 class=\"uni-related-article-tout__header h-has-bottom-margin\"\u003e\u003c/h4\u003e\u003cp class=\"uni-related-article-tout__body\"\u003e\u003c/p\u003e\u003cdiv class=\"cta module-cta h-c-copy uni-related-article-tout__cta muted\"\u003e\u003cspan class=\"nowrap\"\u003eRead Article\u003csvg class=\"icon h-c-icon\" role=\"presentation\"\u003e\u003cuse xlink:href=\"#mi-arrow-forward\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\u003e\u003c/use\u003e\u003c/svg\u003e\u003c/span\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/a\u003e\u003c/section\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Google_Cloud_BigQuery_KHi78bE.max-2200x2200.jpg",
      "date_published": "2021-01-26T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eBrenna Washington\u003c/name\u003e\u003ctitle\u003eProduct Marketing Manager\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    },
    {
      "id": "",
      "url": "https://cloud.google.com/blog/products/operations/on-the-road-to-sre-with-cloud-operations-sandbox/",
      "title": "Take the first step toward SRE with Cloud Operations Sandbox",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\u003cparagraph-block _nghost-c19=\"\"\u003e\u003cdiv _ngcontent-c19=\"\" innerhtml=\"\u0026lt;p\u0026gt;At Google Cloud, we strive to bring Site Reliability Engineering (SRE) culture to our customers not only through training on organizational best practices, but also with the tools you need to run successful cloud services. Part and parcel of that is comprehensive observability tooling\u0026amp;#8212;logging, monitoring, tracing, profiling and debugging\u0026amp;#8212;which can help you troubleshoot production issues faster, increase release velocity and improve service reliability.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;We often hear that implementing observability is hard, especially for complex distributed applications that are implemented in different programming languages, deployed in a variety of environments, that have different operational costs, and many other factors. As a result, when migrating and modernizing workloads onto Google Cloud, observability is often an afterthought.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Nevertheless, being able to debug the system and gain insights into the system\u0026amp;#8217;s behavior is important for running reliable production systems. Customers want to learn how to instrument services for observability and implement SRE best practices using tools Google Cloud has to offer, but without risking production environments. With \u0026lt;a href=\u0026#34;http://cloud-ops-sandbox.dev\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Cloud Operations Sandbox\u0026lt;/a\u0026gt;, you can learn in practice how to kickstart your observability journey and answer the question, \u0026amp;#8220;Will it work for my use-case?\u0026amp;#8221;\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;Cloud Operations Sandbox is an \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;open-source tool\u0026lt;/a\u0026gt; that helps you learn SRE practices from Google and apply them on cloud services using \u0026lt;a href=\u0026#34;https://cloud.google.com/products/operations\u0026#34;\u0026gt;Google Cloud\u0026amp;#8217;s operations suite\u0026lt;/a\u0026gt; (formerly Stackdriver). Cloud Operations Sandbox has everything you need to get started in one click:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Demo service\u0026lt;/b\u0026gt; - an application built using microservices architecture on modern, cloud-native stack (a modified fork of a \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/microservices-demo\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;Online Boutique microservices\u0026lt;/a\u0026gt; demo app)\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;One-click deployment\u0026lt;/b\u0026gt; - automated script that deploys and configures the service to Google Cloud, including:\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Service Monitoring configuration\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Tracing with OpenTelemetry\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Cloud Profiling, Logging, Error Reporting, Debugging and more\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;Load generator\u0026lt;/b\u0026gt; - a component that produces synthetic traffic on the demo service\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;b\u0026gt;SRE recipes\u0026lt;/b\u0026gt; - pre-built tasks that manufacture intentional errors in the demo app so you can use Cloud Operations tools to find the root cause of problems like you would in production\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;An \u0026lt;b\u0026gt;interactive walkthrough\u0026lt;/b\u0026gt; to get started with Cloud Operations\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;h3\u0026gt;Getting started\u0026lt;/h3\u0026gt;\u0026lt;p\u0026gt;Launching the Cloud Operations Sandbox is as easy as can be. Simply:\u0026lt;/p\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Go to \u0026lt;a href=\u0026#34;http://cloud-ops-sandbox.dev\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;cloud-ops-sandbox.dev\u0026lt;/a\u0026gt;\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;Click on the \u0026amp;#8220;Open in Google Cloud Shell\u0026amp;#8221; button.\u0026amp;#160;\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;p\u0026gt;This creates a new Google Cloud project. Within that project, a Terraform script creates a Google Kubernetes Engine (GKE) cluster and deploys a sample application to it. \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;The microservices that make up the demo app\u0026lt;/a\u0026gt; are pre-instrumented with logging, monitoring, tracing, debugging and profiling as appropriate for each microservices language runtime. As such, sending traffic to the demo app generates telemetry that can be useful for diagnosing the cloud service\u0026amp;#8217;s operation. In order to generate production-like traffic to the demo app, \u0026lt;a href=\u0026#34;https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src/loadgenerator\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;an automated script\u0026lt;/a\u0026gt; deploys a synthetic load generator in a different geo-location than the demo app.\u0026lt;/p\u0026gt;\"\u003e\u003cp\u003eAt Google Cloud, we strive to bring Site Reliability Engineering (SRE) culture to our customers not only through training on organizational best practices, but also with the tools you need to run successful cloud services. Part and parcel of that is comprehensive observability tooling—logging, monitoring, tracing, profiling and debugging—which can help you troubleshoot production issues faster, increase release velocity and improve service reliability. \u003c/p\u003e\u003cp\u003eWe often hear that implementing observability is hard, especially for complex distributed applications that are implemented in different programming languages, deployed in a variety of environments, that have different operational costs, and many other factors. As a result, when migrating and modernizing workloads onto Google Cloud, observability is often an afterthought. \u003c/p\u003e\u003cp\u003eNevertheless, being able to debug the system and gain insights into the system’s behavior is important for running reliable production systems. Customers want to learn how to instrument services for observability and implement SRE best practices using tools Google Cloud has to offer, but without risking production environments. With \u003ca href=\"http://cloud-ops-sandbox.dev\" target=\"_blank\" track-type=\"inline link\" track-name=\"1\" track-metadata-eventdetail=\"http://cloud-ops-sandbox.dev\" track-metadata-module=\"post\"\u003eCloud Operations Sandbox\u003c/a\u003e, you can learn in practice how to kickstart your observability journey and answer the question, “Will it work for my use-case?”\u003c/p\u003e\u003cp\u003eCloud Operations Sandbox is an \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox\" target=\"_blank\" track-type=\"inline link\" track-name=\"2\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eopen-source tool\u003c/a\u003e that helps you learn SRE practices from Google and apply them on cloud services using \u003ca href=\"https://cloud.google.com/products/operations\" track-type=\"inline link\" track-name=\"3\" track-metadata-eventdetail=\"https://cloud.google.com/products/operations\" track-metadata-module=\"post\"\u003eGoogle Cloud’s operations suite\u003c/a\u003e (formerly Stackdriver). Cloud Operations Sandbox has everything you need to get started in one click:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDemo service\u003c/b\u003e - an application built using microservices architecture on modern, cloud-native stack (a modified fork of a \u003ca href=\"https://github.com/GoogleCloudPlatform/microservices-demo\" target=\"_blank\" track-type=\"inline link\" track-name=\"4\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eOnline Boutique microservices\u003c/a\u003e demo app)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eOne-click deployment\u003c/b\u003e - automated script that deploys and configures the service to Google Cloud, including:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eService Monitoring configuration\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTracing with OpenTelemetry\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Profiling, Logging, Error Reporting, Debugging and more\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eLoad generator\u003c/b\u003e - a component that produces synthetic traffic on the demo service\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSRE recipes\u003c/b\u003e - pre-built tasks that manufacture intentional errors in the demo app so you can use Cloud Operations tools to find the root cause of problems like you would in production\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAn \u003cb\u003einteractive walkthrough\u003c/b\u003e to get started with Cloud Operations \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eGetting started\u003c/h3\u003e\u003cp\u003eLaunching the Cloud Operations Sandbox is as easy as can be. Simply:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eGo to \u003ca href=\"http://cloud-ops-sandbox.dev\" target=\"_blank\" track-type=\"inline link\" track-name=\"5\" track-metadata-eventdetail=\"http://cloud-ops-sandbox.dev\" track-metadata-module=\"post\"\u003ecloud-ops-sandbox.dev\u003c/a\u003e \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick on the “Open in Google Cloud Shell” button. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis creates a new Google Cloud project. Within that project, a Terraform script creates a Google Kubernetes Engine (GKE) cluster and deploys a sample application to it. \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src\" target=\"_blank\" track-type=\"inline link\" track-name=\"6\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003eThe microservices that make up the demo app\u003c/a\u003e are pre-instrumented with logging, monitoring, tracing, debugging and profiling as appropriate for each microservices language runtime. As such, sending traffic to the demo app generates telemetry that can be useful for diagnosing the cloud service’s operation. In order to generate production-like traffic to the demo app, \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src/loadgenerator\" target=\"_blank\" track-type=\"inline link\" track-name=\"7\" track-metadata-eventdetail=\"https://github.com\" track-metadata-module=\"post\"\u003ean automated script\u003c/a\u003e deploys a synthetic load generator in a different geo-location than the demo app.\u003c/p\u003e\u003c/div\u003e\u003c/paragraph-block\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003chtml\u003e\u003chead\u003e\u003c/head\u003e\u003cbody\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAt Google Cloud, we strive to bring Site Reliability Engineering (SRE) culture to our customers not only through training on organizational best practices, but also with the tools you need to run successful cloud services. Part and parcel of that is comprehensive observability tooling—logging, monitoring, tracing, profiling and debugging—which can help you troubleshoot production issues faster, increase release velocity and improve service reliability. \u003c/p\u003e\u003cp\u003eWe often hear that implementing observability is hard, especially for complex distributed applications that are implemented in different programming languages, deployed in a variety of environments, that have different operational costs, and many other factors. As a result, when migrating and modernizing workloads onto Google Cloud, observability is often an afterthought. \u003c/p\u003e\u003cp\u003eNevertheless, being able to debug the system and gain insights into the system’s behavior is important for running reliable production systems. Customers want to learn how to instrument services for observability and implement SRE best practices using tools Google Cloud has to offer, but without risking production environments. With \u003ca href=\"http://cloud-ops-sandbox.dev\" target=\"_blank\"\u003eCloud Operations Sandbox\u003c/a\u003e, you can learn in practice how to kickstart your observability journey and answer the question, “Will it work for my use-case?”\u003c/p\u003e\u003cp\u003eCloud Operations Sandbox is an \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox\" target=\"_blank\"\u003eopen-source tool\u003c/a\u003e that helps you learn SRE practices from Google and apply them on cloud services using \u003ca href=\"https://cloud.google.com/products/operations\"\u003eGoogle Cloud’s operations suite\u003c/a\u003e (formerly Stackdriver). Cloud Operations Sandbox has everything you need to get started in one click:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eDemo service\u003c/b\u003e - an application built using microservices architecture on modern, cloud-native stack (a modified fork of a \u003ca href=\"https://github.com/GoogleCloudPlatform/microservices-demo\" target=\"_blank\"\u003eOnline Boutique microservices\u003c/a\u003e demo app)\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eOne-click deployment\u003c/b\u003e - automated script that deploys and configures the service to Google Cloud, including:\u003c/p\u003e\u003c/li\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eService Monitoring configuration\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eTracing with OpenTelemetry\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eCloud Profiling, Logging, Error Reporting, Debugging and more\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eLoad generator\u003c/b\u003e - a component that produces synthetic traffic on the demo service\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003e\u003cb\u003eSRE recipes\u003c/b\u003e - pre-built tasks that manufacture intentional errors in the demo app so you can use Cloud Operations tools to find the root cause of problems like you would in production\u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eAn \u003cb\u003einteractive walkthrough\u003c/b\u003e to get started with Cloud Operations \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch3\u003eGetting started\u003c/h3\u003e\u003cp\u003eLaunching the Cloud Operations Sandbox is as easy as can be. Simply:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cp\u003eGo to \u003ca href=\"http://cloud-ops-sandbox.dev\" target=\"_blank\"\u003ecloud-ops-sandbox.dev\u003c/a\u003e \u003c/p\u003e\u003c/li\u003e\u003cli\u003e\u003cp\u003eClick on the “Open in Google Cloud Shell” button. \u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis creates a new Google Cloud project. Within that project, a Terraform script creates a Google Kubernetes Engine (GKE) cluster and deploys a sample application to it. \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src\" target=\"_blank\"\u003eThe microservices that make up the demo app\u003c/a\u003e are pre-instrumented with logging, monitoring, tracing, debugging and profiling as appropriate for each microservices language runtime. As such, sending traffic to the demo app generates telemetry that can be useful for diagnosing the cloud service’s operation. In order to generate production-like traffic to the demo app, \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/src/loadgenerator\" target=\"_blank\"\u003ean automated script\u003c/a\u003e deploys a synthetic load generator in a different geo-location than the demo app.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Terraform_script_creates_a_GKE_cluste.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"Terraform script creates a GKE cluste.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/Terraform_script_creates_a_GKE_cluste.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIt creates 11 custom dashboards (one for each microservice) to illustrate \u003ca href=\"https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals\" target=\"_blank\"\u003ethe four golden signals\u003c/a\u003e of monitoring \u003ca href=\"https://sre.google/sre-book/monitoring-distributed-systems/\" target=\"_blank\"\u003eas described in Google’s SRE book\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/creates_11_custom_dashboards.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"creates 11 custom dashboards.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/creates_11_custom_dashboards.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eIt also adds and automatically configures uptime checks, service monitoring (\u003ca href=\"https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring\"\u003eSLOs and SLIs\u003c/a\u003e), log-based metrics, alerting policies and more.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/checkout_service.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"checkout service.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/checkout_service.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eAt the end of the provisioning script you’ll get a few URLs of the newly created project:\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-image_full_width\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid\"\u003e\u003cfigure class=\"article-image--large h-c-grid__col h-c-grid__col--6 h-c-grid__col--offset-3 \"\u003e\u003ca href=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/provisioning_script.max-2800x2800.jpg\" rel=\"external\" target=\"_blank\"\u003e\u003cimg alt=\"provisioning script.jpg\" src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/provisioning_script.max-1000x1000.jpg\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003cp\u003eYou can \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/blob/master/docs/README.md\" target=\"_blank\"\u003efollow the user guide\u003c/a\u003e to learn about the entire Cloud Operations suite of tools, including tracking microservices interactions in \u003ca href=\"https://cloud.google.com/trace\"\u003eCloud Trace\u003c/a\u003e (thanks to the \u003ca href=\"https://cloud.google.com/learn/what-is-opentelemetry\"\u003eOpenTelemetry\u003c/a\u003e instrumentation of the demo app) and see how to \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/tree/master/terraform/monitoring\" target=\"_blank\"\u003eapply the learnings to your scenario\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eFinally, to remove the Sandbox once you’re finished using it, you can run\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-code\"\u003e\u003cdiv class=\"article-module h-c-page\"\u003e\u003cdiv class=\"h-c-grid uni-paragraph-wrap\"\u003e\u003cdiv class=\"uni-paragraph h-c-grid__col h-c-grid__col--8 h-c-grid__col-m--6 h-c-grid__col-l--6 h-c-grid__col--offset-2 h-c-grid__col-m--offset-3 h-c-grid__col-l--offset-3\"\u003e\u003cpre\u003e\u003ccode\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"block-paragraph\"\u003e\u003cdiv class=\"rich-text\"\u003e\u003ch3\u003eNext steps\u003c/h3\u003e\u003cp\u003eFollowing SRE principles is a proven method for running highly reliable applications in the cloud. We hope that the Cloud Operations Sandbox gives you the understanding and confidence you need to jumpstart your SRE practice. \u003c/p\u003e\u003cp\u003eTo get started, visit  \u003ca href=\"http://cloud-ops-sandbox.dev\" target=\"_blank\"\u003ecloud-ops-sandbox.dev\u003c/a\u003e, explore the \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox\" target=\"_blank\"\u003eproject repo\u003c/a\u003e, and follow along in the \u003ca href=\"https://github.com/GoogleCloudPlatform/cloud-ops-sandbox/blob/master/docs/README.md\" target=\"_blank\"\u003euser guide\u003c/a\u003e.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/body\u003e\u003c/html\u003e",
      "image": "https://storage.googleapis.com/gweb-cloudblog-publish/images/Public-Sector-Momentum.max-1000x1000.png",
      "date_published": "2021-01-22T17:00:00Z",
      "author": {
        "name": "\u003cname\u003eDaniel Sanche\u003c/name\u003e\u003ctitle\u003eDeveloper Programs Engineer\u003c/title\u003e\u003cdepartment\u003e\u003c/department\u003e\u003ccompany\u003e\u003c/company\u003e"
      }
    }
  ]
}
