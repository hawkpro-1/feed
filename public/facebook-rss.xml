<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Facebook</title>
    <link>https://engineering.fb.com/feed</link>
    <description></description>
    <item>
      <title>How we built a general purpose key value store for Facebook with ZippyDB</title>
      <link>https://engineering.fb.com/2021/08/06/core-data/zippydb/</link>
      <description>&lt;p&gt;ZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. Since we first deployed ZippyDB in 2012, this key-value store has expanded rapidly, and today, ZippyDB serves a number of use cases, ranging from metadata for a distributed filesystem, counting events for both internal and external purposes, to product data that’s used for [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/08/06/core-data/zippydb/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/08/06/core-data/zippydb/&#34;&gt;How we built a general purpose key value store for Facebook with ZippyDB&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>ZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. Since we first deployed ZippyDB in 2012, this key-value store has expanded rapidly, and today, ZippyDB serves a number of use cases, ranging from metadata for a distributed filesystem, counting events for both internal and external purposes, to product data that’s used for various app features. ZippyDB offers a lot of flexibility to applications in terms of tunable durability, consistency, availability, and latency guarantees, which has made the service a popular choice within Facebook for storing both ephemeral and nonephemeral small key-value </span><span>data. In this post, we are sharing for the first time the history and evolution of ZippyDB and some of the unique design choices and trade-offs made in building this service that addressed the majority of key-value store scenarios at Facebook.</span></p>
<h2><span>History of ZippyDB</span></h2>
<p><span>ZippyDB uses</span><a href="https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-and-open-sourcing-rocksdb/10151822347683920/"> <span>RocksDB</span></a><span> as the underlying storage engine. Before ZippyDB, various teams across Facebook used RocksDB directly to manage their data. This resulted, however, in a duplication of efforts in terms of each team solving similar challenges such as consistency, fault tolerance, failure recovery, replication, and capacity management. To address the needs of these various teams, we built ZippyDB to provide a highly durable and consistent key-value data store that allowed products to move a lot faster by offloading all the data and the challenges associated with managing this data at scale to ZippyDB.</span></p>
<p><span>One of the significant design decisions we made early during the development of ZippyDB was to reuse as much of the existing infrastructure as possible. Consequently, most of our initial efforts were focused on building a reusable and flexible data replication library called Data Shuttle. We built a fully managed distributed key-value store by combining Data Shuttle with a preexisting and well-established storage engine (RocksDB) and layering this on top of our existing shard management (</span><a href="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/"><span>Shard Manager</span></a><span>)</span> <span>and </span><span>distributed </span><span>configuration service (built on </span><a href="https://zookeeper.apache.org/"><span>ZooKeeper</span></a><span>), that together solves load balancing, shard placement, failure detection, and service discovery.</span></p>
<h2><span>Architecture</span></h2>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>ZippyDB is deployed in units known as tiers. A tier consists of compute and storage resources spread across several geographic areas known as regions</span> <span>worldwide, which makes it resilient to failures. There are only a handful of ZippyDB tiers that exist today, including the default “wildcard” tier and specialized tiers for distributed filesystem metadata and other product groups within Facebook. Each tier hosts multiple use cases. Normally, use cases are created on the wildcard tier, which is our generic multitenant tier. This is the preferred tier because of its better utilization of hardware and lower operational overhead, but we occasionally bring up dedicated tiers if there is a need, usually due to stricter isolation requirements.</span></p>
<p><span>The data belonging to a use case on a tier is split into units known as shards,</span> <span>which are</span> <span>the basic units of data management on the server side. Each shard is replicated across multiple regions (for fault tolerance) using Data Shuttle, which uses either <a href="https://dl.acm.org/doi/10.1145/279227.279229">Paxos</a> or</span> <span>async replication to</span> <span>replicate</span> <span>data, depending on the configuration. Within a shard, a subset of replicas are configured to be a part of the Paxos quorum group, also known as global scope,</span> <span>where data is synchronously replicated using Multi-Paxos to provide high durability and availability in case of failures. The remaining replicas, if any, are configured as followers.</span> <span>These are similar to learners in Paxos terminology and receive data asynchronously. Followers allow applications to have many in-region replicas to support low-latency reads with relaxed consistency, while keeping the quorum size small for lower write latency. This flexibility in replica role configuration within a shard allows applications to strike a balance between durability, write performance, and read performance depending on their needs.</span></p>
<p><span>In addition to the sync or async replication strategy, applications also have the option to provide “hints” to the service about the regions in which the replicas of a shard must be placed. These hints, also known as stickiness</span> <span>constraints, allow applications to have some control over the latency of reads and writes by having replicas built in regions from where they expect most of the access to come. ZippyDB also provides a caching layer and integrates with a pub-sub system allowing subscriptions to data mutations on shards, both of which are opt-ins depending on the requirements of the use case.</span></p>
<h2><span>Data model</span></h2>
<p><span>ZippyDB supports a simple key-value</span> <span>data model with APIs to get, put, and delete keys along with their batch variants. It supports iterating over key prefixes and deleting a range of keys. These APIs are very similar to the API exposed by the underlying RocksDB storage engine. In addition, we also support a test-and-set API for basic read-modify-write operations and transactions, conditional writes for more generic read-modify-write operations (more about this later). This minimal API set has proved to be sufficient for most use cases to manage their data on ZippyDB. For ephemeral data, ZippyDB has native TTL support where the client can optionally specify the expiry time for an object at the time of the write. We piggyback on RocksDB’s periodic compaction support to clean up all the expired keys efficiently while filtering out dead keys on the read side in between compaction runs. Many applications actually access data on ZippyDB through an ORM layer on top of ZippyDB, which translates these accesses into ZippyDB API. Among other things, this layer serves to abstract the details of the underlying storage service.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Shard is the unit of data management on the server side. The optimal assignment of shards to servers needs to take into account load, failure domains, user constraints, etc., and this is handled by ShardManager</span><i><span>. </span></i><span>ShardManager</span> <span>is responsible for monitoring servers for load imbalance, failures, and initiating shard movement between servers. </span></p>
<p><span>Shard, often referred to as physical shard or p-shard, is a server-side concept and isn’t exposed to applications directly. Instead, we allow use cases to partition their key space into smaller units of related data known as μshards (micro-shards)</span><i><span>. </span></i><span>A typical</span> <span>physical</span> <span>shard</span> <span>has a size of 50–100 GB, hosting several tens of thousands of μshards. This additional layer of abstraction allows ZippyDB to reshard the data transparently without any changes on the client.</span></p>
<p><span>ZippyDB supports two kinds of mappings from </span><i><span>μ</span></i><span>shards to physical shards: compact mapping and Akkio mapping. Compact mapping</span> <span>is used</span> <span>when the assignment is fairly static and mapping is only changed when there is a need to split shards that have become too large or hot. In practice, this is a fairly infrequent operation when compared with Akkio mapping, where mapping of </span><i><span>μ</span></i><span>shards is managed by a service known as</span><a href="https://engineering.fb.com/2018/10/08/core-data/akkio/"> <span>Akkio</span></a><span>. Akkio splits use cases’ key space into μshards and places these μshards in regions where the information is typically accessed. Akkio helps reduce data set duplication and provides a significantly more efficient solution for low latency access than having to place data in every region.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>As we mentioned earlier, Data Shuttle uses Multi-Paxos to synchronously replicate data to all replicas in the global scope</span><i><span>. </span></i><span>Conceptually, time is subdivided into units known as epochs. Each epoch has a unique leader, whose role is assigned using an external shard management service called ShardManager. Once a leader is assigned, it has a lease for the entire duration of the epoch. Periodic heartbeats used to keep a lease active until ShardManager bumps up the epoch on the shard (e.g., for failover, primary load balancing, etc.). When a failure occurs, ShardManager detects the failure, assigns a new leader with a higher epoch and restores write availability. Within each epoch, the leader generates a total ordering of all writes to the shard, by assigning each write a monotonically increasing sequence number. The writes are then written to a replicated durable log using Multi-Paxos</span> <span>to achieve consensus on the ordering. Once the writes have reached consensus, they are drained in-order across all replicas.</span></p>
<p><span>We chose to use an external service to detect failures and assign leaders to keep the design of the service simple in the initial implementation. However, in the future we plan to move towards detecting failures entirely within</span> <span>Data Shuttle (“in-band”) and reelecting the leaders more proactively without having to wait for ShardManager and incurring delays.</span></p>
<h2><span>Consistency</span></h2>
<p><span>ZippyDB provides configurable consistency and durability levels to applications, which can be specified as options in read and write APIs. This allows applications to make durability, consistency, and performance trade-offs dynamically on a per-request level.</span></p>
<p><span>By default, a write involves persisting the data on a majority of replicas’ Paxos logs and writing the data to RocksDB on the primary before acknowledging the write to the client. With the default write mode, a read on primary will always see the most recent write. Some applications cannot tolerate cross-region latencies for every write, so ZippyDB supports a fast-acknowledge</span> <span>mode, where writes are acknowledged as soon as they are enqueued on the primary for replication. The durability and consistency guarantees for this mode are obviously lower, which is the trade-off for higher performance.</span></p>
<p><span>On the read side, the three most popular consistency levels are eventual, read-your-writes, and strong. The eventual consistency level supported by ZippyDB is actually a much stronger consistency level than the more well-known eventual consistency. ZippyDB provides total ordering for all writes within a shard and ensures that reads aren’t served by replicas that are lagging behind primary/quorum beyond a certain configurable threshold (heartbeats are used to detect lag), so eventual reads supported by ZippyDB are closer to bounded staleness consistency in literature.</span></p>
<p><span>For read-your-writes, the clients cache the latest sequence number returned by the server for writes and use the version to run at-or-later</span> <span>queries</span> <span>while reading. The cache of versions is within the same client process.</span></p>
<p><span>ZippyDB also provides strong consistency or </span><a href="https://dl.acm.org/doi/10.1145/78969.78972"><span>linearizability</span></a><i><span>, </span></i><span>where</span> <span>clients can see the effects of the most recent writes regardless of where the writes or reads come from. Strong reads today are implemented by routing the reads to the primary in order to avoid the need to speak to a quorum, mostly for performance reasons. The primary relies on owning the lease to ensure that there is no other primary before serving reads. In certain outlier cases, where the primary hasn’t heard about the lease renewal, strong reads on primary turn into a quorum check and read.</span></p>
<h2><span>Transactions and conditional writes</span></h2>

<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB4.Revised.jpg?w=16" alt="" width="16" height="9"/><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg 2304w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/Screen-Shot-2021-05-25-at-10.20.41-AM.jpg?w=16" alt="" width="16" height="9"/></p>
<p><span>ZippyDB supports transactions and conditional writes for use cases that need atomic read-modify-write operations on a set of keys.</span></p>
<p><span>All transactions are serializable by default on a shard, and we don’t support lower isolation levels. This simplifies the server-side implementation and the reasoning about correctness of concurrently executing transactions on the client side. Transactions use optimistic concurrency control to detect and resolve </span><a href="https://dl.acm.org/doi/10.1145/568271.223787"><span>conflicts</span></a><span>, which works as shown in the figure above. The clients typically read from a secondary all of the data from a snapshot</span> <span>of the DB, compose the write</span> <span>set, and send both the read and write sets to the primary to commit. Upon receiving the read and write sets and the snapshot against which reads were performed, the primary checks whether there were conflicting writes by other concurrently executing transactions that have already been admitted. The transaction is admitted only if there are no conflicts, after which the transaction is guaranteed to succeed, assuming no server failures. Conflict resolution on the primary relies on tracking all of the recent</span> <span>writes performed by previously admitted transactions during the same epoch on the primary. Transactions spanning epochs are rejected, as this simplifies write set</span> <span>tracking without requiring replication. The history of writes maintained on the primary is also periodically purged to keep the space usage low. Since the complete history isn’t maintained, the primary needs to maintain a minimum tracked version</span> <span>and reject all transactions that have reads against a snapshot with lower version to guarantee serializability. Read-only transactions work exactly similar to read-write transactions, except that the write set is empty.</span></p>
<p><span>Conditional write is implemented using “server-side transactions”. It provides a more user friendly client side API for use cases where clients want to atomically modify a set of keys based on some common preconditions such as key_present, key_not_present, and value_matches_or_key_not_present. When a primary receives a conditional write request it sets up a transaction context and converts the preconditions and write set to a transaction on the server, reusing all of the machinery for transactions. The conditional-write API can be more efficient than the transaction API in cases where clients can compute the precondition without requiring a read.</span></p>
<h2><span>The future of ZippyDB</span></h2>
<p><span>Distributed key-value stores have many applications, and the need for them often comes up while building a variety of systems, from products to storing metadata for various infrastructure services. Building a scalable, strongly consistent, and fault-tolerant key-value store can be very challenging and often requires thinking through many trade-offs to provide a curated combination of system capabilities and guarantees that works well in practice for a variety of workloads. This blog post introduced ZippyDB, Facebook’s biggest key-value store, which has been in production for more than six years serving a lot of different workloads. Since its inception, the service has seen very steep adoption, mostly due to the flexibility that it offers in terms of making efficiency, availability, and performance trade-offs. The service also enables us to use engineering resources effectively as a company and use our key-value store capacity efficiently as a single pool. ZippyDB is still evolving and currently undergoing significant architectural changes, such as storage-compute disaggregation, fundamental changes to membership management, failure detection and recovery, and distributed transactions, in order to adapt to the changing ecosystem and product requirements.</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Sarang Masti</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Hero_.Image_.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 06 Aug 2021 16:52:02 +0000</pubDate>
    </item>
    <item>
      <title>Open sourcing Winterfell: A STARK prover and verifier</title>
      <link>https://engineering.fb.com/2021/08/04/open-source/winterfell/</link>
      <description>&lt;p&gt;We are releasing Winterfell, our implementation of a STARK prover/verifier to Crates.io  Winterfell is an easy to use open source implementation of STARKs for security and privacy applications. One potential application for Winterfell’s zero-knowledge proofs is blockchain privacy and scalability.  “Any sufficiently advanced technology is indistinguishable from magic.” —Clarke’s Third Law What if the average [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/08/04/open-source/winterfell/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/08/04/open-source/winterfell/&#34;&gt;Open sourcing Winterfell: A STARK prover and verifier&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>We are releasing Winterfell, our implementation of a STARK prover/verifier to Crates.io </span></li>
<li aria-level="1"><span>Winterfell is an easy to use open source implementation of STARKs for security and privacy applications.</span></li>
<li aria-level="1"><span>One potential application for Winterfell’s zero-knowledge proofs is blockchain privacy and scalability. </span></li>
</ul>
<p><i><span>“Any sufficiently advanced technology is indistinguishable from magic.” —Clarke’s Third Law</span></i></p>
<p><span>What if the average developer could benefit from proofs of computational integrity (CI) that would normally require an in-depth knowledge of cryptography to implement? </span></p>
<p><span>CI proofs, of which zero-knowledge proofs (ZKPs) are a subset, are a cryptographic technology that let you do seemingly impossible things. For example, you can run a computation and get some result. You can then use a CI proof to convince anyone that you did the computation correctly without their having to rerun the computation themselves.</span> <span>And they can verify this correctness in just a few milliseconds, regardless of how complex or long-running the original computation was.</span> <span>To bring the power of CI proofs to the masses, we’ve developed Winterfell, a general-purpose STARK (Scalable Transparent Arguments of Knowledge) prover and verifier. We are happy to be publishing the v0.1 version of the library to crates.io.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_FINAL.gif" alt="To bring the power of CI proofs to the masses, we’ve developed Winterfell, a general-purpose STARK prover and verifier. " width="1920" height="1080"/></p>
<p><span>Another important property of these CI proofs is the ability to hide some (or all) of the inputs that were used to run the computation. This is the zero-knowledge aspect. For example, you could prove that a number is in a given range without revealing the exact value of the number (these types of proofs are usually called range proofs). Or, you could do something as complex as comparing two number sequences, one public and one private (known only to yourself), and prove to anyone beyond a doubt that there is or isn’t a match between them.</span></p>
<h2><span>The magic of ZKPs</span></h2>
<p><span>The general ideas behind ZKPs were developed as early as the 1980s, but interest in this area of cryptography has </span><a href="https://nakamoto.com/cambrian-explosion-of-crypto-proofs/"><span>exploded</span></a><span> recently, driven in part by emergent applications in the blockchain space. In the last few years, over a dozen new proving systems have appeared. Some of them have even been deployed in production where tens of billions of dollars depend on their security properties. However, ZKPs are far from mainstream, primarily for two reasons:</span></p>
<ul>
<li aria-level="1"><span>Until recently, deploying ZKPs in applications required expert cryptographers with years of experience. The situation is somewhat better now, as there are plenty of relatively accessible materials available and more projects that try to make ZKPs accessible to the average developer. But even now, making sense of different proving systems and the trade-offs associated with them requires deep expertise and/or a steep learning curve, even for experienced software engineers.</span></li>
<li aria-level="1"><span>While verifying a ZK proof is extremely fast and requires very few compute resources, generating a proof is a computationally intensive process. It may take seconds or even minutes (or many CPU cores) to generate proofs for even relatively simple computations. Only relatively recent advances in cryptography and implementation improvements have brought a large segment of computations to within practical feasibility for ZKPs. And there is a lot of ongoing work to expand the set of computations for which proof generation is practical.</span></li>
</ul>
<p><span>We developed Winterfell to bridge these gaps and to bring ZKPs within reach of regular developers. </span></p>
<h2><span>Winterfell is here</span></h2>
<p><span>Winterfell is a general purpose STARK prover and verifier written in </span><a href="https://engineering.fb.com/2021/04/29/developer-tools/rust/"><span>Rust</span></a><span> at </span><a href="https://research.fb.com/category/blockchain-and-cryptoeconomics/"><span>Novi Research</span></a><span>. </span><i><span>General purpose</span></i><span> means that Winterfell can generate CI proofs for any computation. Basically, for any program that can be described with a Turing-complete language, we can generate a CI proof using Winterfell (though this would be much more straightforward for some programs than for others).</span></p>
<p><span>Winterfell uses STARKs, a proof-of-computation scheme developed by Eli Ben-Sasson, Michael Riabzev, et al. In comparison with many other CI proving systems, STARKs have a number of attractive properties, including:</span></p>
<ul>
<li aria-level="1"><span>STARKs rely on very few cryptographic assumptions. In fact, the only cryptographic primitive we need for STARKs to work is a collision resistant hash function (e.g., SHA256). This also makes STARKs resistant to potential attacks from adversaries with quantum computers.</span></li>
<li aria-level="1"><span>Unlike many other proving systems, STARKs are fully transparent. This means we don’t need to run complicated trusted setup ceremonies to start using STARKs. Trusted setups are a potential security weakness in other zero knowledge protocols, because a compromised trusted setup allows attackers to generate fake CI proofs. STARKs are immune to this.</span></li>
<li aria-level="1"><span>In comparison with other systems, STARK proof generation is extremely fast when we deal with uniform computations, or computations with regular structures. Fortunately, the vast majority of programs people write do possess such regular structures. Moreover, pretty much every single step of the STARK proof generation process is massively parallelizable. Thus, we can frequently speed up proof generation by distributing it across more and more CPU cores.</span></li>
</ul>
<p><span>None of the individual properties listed above are unique to STARKs. However, no other proving system combines lean cryptography, transparency, and performance to the extent STARKs do. Winterfell takes full advantage of these benefits while abstracting away most of the complexity. For example, proof generation can be distributed across multiple CPU cores to dramatically reduce proof generation time (see our benchmarks </span><a href="https://github.com/novifinancial/winterfell#performance"><span>here</span></a><span>). Moreover, we have plans to enable fully distributed proof generation across multiple machines and have already started work in this direction.</span></p>
<p><span>In addition to being performant, Winterfell is highly configurable. That is, you can dynamically tune almost all parameters of the STARK protocol to attain specific performance and security targets. We are able to achieve such high configurability without sacrificing performance or code clarity by relying on Rust’s zero-cost abstractions.</span></p>
<p><span>Last, and perhaps most important, you don’t need to be a cryptographer to use Winterfell. As mentioned previously, Winterfell abstracts away most of the complexity of the STARK protocol. The only thing the user is responsible for is describing their computation in a format that the STARK prover/verifier can understand. This format is called algebraic intermediate representation (AIR), and the step of translating a program into AIR is called arithmetization.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg" alt="" width="1920" height="847" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=916,404 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=768,339 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=1024,452 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=1536,678 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=96,42 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=192,85 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Using Winterfell</span></h2>
<p><span>Winterfell exposes a relatively simple interface for describing AIR for any computation. However, the concept of arithmetization is not something most developers are familiar with, so there is going to be a learning curve. </span></p>
<p><span>To help you get started, we’ve put together an end-to-end </span><a href="https://github.com/novifinancial/winterfell#usage"><span>tutorial</span></a><span> on how to define AIR for a very simple computation. We also have examples of more interesting computations in the </span><a href="https://github.com/novifinancial/winterfell/tree/main/examples"><span>examples crate</span></a><span>, ranging from something as simple as a Fibonacci sequence to something as sophisticated as aggregation of hash-based signatures. And if you would like to get a little bit deeper into theory, we recommend reading two excellent blog posts from StarkWare: </span><a href="https://medium.com/starkware/arithmetization-i-15c046390862"><span>Arithmetization I</span></a><span> and </span><a href="https://medium.com/starkware/arithmetization-ii-403c3b3f4355"><span>Arithmetization II</span></a><span>.</span></p>
<p><span>Once you are comfortable with writing AIRs, using Winterfell to generate STARK proofs becomes relatively easy. For example, AIR for a Fibonacci sequence requires less than 100 lines of code and can be put together in about 15 minutes. Even for the relatively complicated example of hash-based signature aggregation mentioned above, the AIR is described in about 600 lines of code (though it did take several days to put together).</span></p>
<p><span>Another point worth mentioning: We wrote Winterfell as a set of modular crates, all of which are being published to <a href="https://crates.io/users/irakliyk">Crates.io</a> today as well. While we use these crates to build a STARK proving system, many of them are general enough to be used as building blocks in other CI proving systems. For example, for low-degree testing, we use the FRI protocol implemented in the </span><a href="https://github.com/novifinancial/winterfell/tree/main/fri"><span>winter-fri</span></a><span> crate, which is also used as a building block for several other proof systems (e.g., </span><a href="https://eprint.iacr.org/2019/1076.pdf"><span>Fractal</span></a><span> and </span><a href="https://eprint.iacr.org/2018/828.pdf"><span>Aurora</span></a><span>) that aim to be transparent and post-quantum. Thus, we hope that our work will help implementers of these protocols get their job done more quickly and efficiently.</span></p>
<h2><span>Applications</span></h2>
<p><span>Recent advancements in ZKPs are driven by emergent use cases in the blockchain space. Specifically, ZKPs offer attractive solutions to perhaps two of the most pressing blockchain challenges: privacy and scalability. However, ZKPs have numerous potential applications outside of the blockchain space as well.</span></p>
<p><span>While there still remain some technical challenges to overcome before proofs of computational integrity can be considered practical at a large scale, we believe that Winterfell represents an important stepping stone for bringing a well-studied subject in academic research into practical deployments. And we hope that the security and privacy community will also benefit from an easy to use open source implementation of STARKs.</span></p>
<p><span>Please check out the </span><a href="https://github.com/novifinancial/winterfell"><span>Winterfell repository</span></a><span>, and feel free to open issues for comment and leave feedback!</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Irakliy Khaburzaniya, Kostas Chalkias, Kevin Lewi, Harjasleen Malvai</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_418_Open_Sourcing_Winterfell_HERO_FINAL.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 04 Aug 2021 16:00:43 +0000</pubDate>
    </item>
    <item>
      <title>A linear programming approach for optimizing features in ML models</title>
      <link>https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/</link>
      <description>&lt;p&gt;Whether it’s iterating on Facebook’s News Feed ranking algorithm or delivering the most relevant ads to users, we are constantly exploring new features to help improve our machine learning (ML) models. Every time we add new features, we create a challenging data engineering problem that requires us to think strategically about the choices we make. [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/&#34;&gt;A linear programming approach for optimizing features in ML models&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>Whether it’s iterating on </span><a href="https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/"><span>Facebook’s News Feed ranking algorithm</span></a><span> or delivering the most relevant ads to users, we are constantly exploring new features to help improve our machine learning (ML) models. Every time we add new features, we create a challenging data engineering problem that requires us to think strategically about the choices we make. More complex features and sophisticated techniques require additional storage space. Even at a company the size of Facebook, capacity isn’t infinite. If left unchecked, accepting all features would quickly overwhelm our capacity and slow down our iteration speed, decreasing the efficiency of running the models.</span></p>
<p><span>To better understand the relationship between these features and the capacity of the infrastructure that needs to support them, we can frame the system as a linear programming problem. By doing this, we can maximize a model’s performance, probe the sensitivity of its performance to different infrastructure constraints, and study the relationships between different services. </span><span>This work was done by data scientists embedded in our engineering team and demonstrates the value of analytics and data science in ML.</span></p>
<h2><span>Supporting feature development</span></h2>
<p><span>It’s important to continually introduce features that best leverage new data to maintain performant models. New features are responsible for the majority of incremental model improvements. These ML models are useful for our ad delivery system. They work together to predict a person’s likelihood of taking specific action on the ad. We work to continuously improve our models so our systems deliver only those ads that are relevant to a user.</span></p>
<p><span>As our techniques become more sophisticated, we develop more complex features that demand more of our infrastructure. A feature can leverage different services depending on its purpose. Some features have a higher memory cost, while others require extra CPU or take up more storage. It’s important to use our infrastructure wisely to maximize the performance of our models. We must be able to smartly allocate resources and be able to quantify the trade-offs of different scenarios.</span></p>
<p><span>To address these problems, we frame our system as a linear programming problem that maximizes our model’s metrics. We use this framework to better understand the interaction between our features and services. With this knowledge, we can automatically select the best features, identify infrastructure services to invest in, and maintain the health of both our models and services.</span></p>
<h2><span>Framing our problem</span></h2>
<p><span>To get a handle on our framework, we’ll first introduce a model problem. Say we have multiple features that all take up some amount of space (the height of the rectangles) and contribute some amount of gain to our models (the teal squares), and we are unable to accommodate them all in our limited capacity.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg" alt="Say we have multiple features that all take up some amount of space (the height of the rectangles) and contribute some amount of gain to our models (the teal squares), and we are unable to accommodate them all in our limited capacity." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>

<p><span>A naive solution would be to just start picking the features with the most gain (teal squares) until you are out of capacity. However, you might not be making the best use of your resources if you just prioritize the gain. For example, by taking in a big feature with a large gain, you could be taking up room that two smaller features with less gain could use instead. Together, those two smaller features would give you more bang for your buck than the big feature.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg" alt="by taking in a big feature with a large gain, you could be taking up room that two smaller features with less gain could use instead." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>If we get a little less naive, we could instead look to pick features that give us the most bang per buck — features that have the most gain per storage. However, if we pick features only from that perspective, we could end up leaving out some less efficient features that we would still have room to accommodate.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg" alt="we could instead look to pick features that give us the most bang per buck — features that have the most gain per storage" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>We’ve been looking at a very simplified view of infrastructure, but the reality is a bit more complex. For example, features often don’t take up just one resource but need many — such as memory, CPU, or storage in other services. We can make our example slightly more sophisticated by adding in Service B, and saying that orange features take up space in both Service A and Service B.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg" alt="We can make our example slightly more sophisticated by adding in Service B, and saying that orange features take up space in both Service A and Service B." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Picking which features we use is not the only way to control our infrastructure usage. We can also employ various techniques to increase the efficiency of our feature storage. This sometimes comes with a cost, either through the feature itself or capacity from a service. In this case, let’s say that we can halve the storage cost of some features (bordered in pink) but only at the cost of reducing the gain of the feature, and using some of the limited capacity in Service B.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg" alt="let’s say that we can halve the storage cost of some features (bordered in pink), but only at the cost of reducing the gain of the feature, and using some of the limited capacity in Service B." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>We’ll stop the example at this point, but this is enough for the general message to be clear — infrastructure can be a complicated interconnected system of different constraints. In reality, our capacity is not set in stone. We can move resources around if it is warranted. Features are also not the only thing we’re working on. There are plenty of other projects and workflows that compete for the same resources. We need to not only choose the features that maximize our gain but also be able to answer questions about how our system responds to changes:</span></p>
<ul>
<li aria-level="1"><span>Which features do we select to optimize the gain?</span></li>
<li aria-level="1"><span>Is feature compression worth it? More important, is it worth an engineer’s time to implement it?</span></li>
<li aria-level="1"><span>How does the gain change if we add more capacity to Service A?</span></li>
<li aria-level="1"><span>How do service dependencies interact? If we increase the capacity of Service B, can we use less of Service A?</span></li>
</ul>
<h2><span>Scaling the problem</span></h2>
<p><span>Let’s step back and review the conditions of our model problem:</span></p>
<ol>
<li aria-level="1"><span>We want to maximize our gain.</span></li>
<li aria-level="1"><span>We are limited by the capacity of Service A.</span></li>
<li aria-level="1"><span>We are also limited by the capacity of Service B, which only some features contribute to.</span></li>
<li aria-level="1"><span>Some features may be compressed, but:</span>
<ol>
<li aria-level="2"><span>They suffer a loss to their gain.</span></li>
<li aria-level="2"><span>Some of Service B’s capacity must be used.</span></li>
</ol>
</li>
</ol>
<p><span>We can express all these constraints as a system of linear equations.</span></p>
<p><span>Let 𝑥 be a vector that is 0 or 1 that signifies whether we select the feature, and let 𝑔 be a vector that stores the gain of the feature. The subscripts 𝑓 and 𝑐 denote whether we are specifying a full cost or compressed feature. For example, 𝑥</span><span>𝑓</span><span> denotes full, uncompressed features that we have selected to include, and 𝑔</span><span>𝑐</span><span> represents the cost of compressed features.</span></p>
<p><span>Given these definitions, our objective is to maximize:</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg" alt="linear programming equation" width="1920" height="244" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=916,116 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=768,98 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=1024,130 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=1536,195 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=96,12 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=192,24 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>We can now add our constraints that model the limitations of our infrastructure:</span></p>
<ol>
<li><span>Features will either be selected and compressed, selected but not compressed, or not selected. We should not select the compressed and uncompressed versions of the same feature.<br/>
<img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg" alt="linear programming equation" width="1920" height="894" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=916,427 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=768,358 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=1024,477 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=1536,715 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=192,89 192w" sizes="(max-width: 992px) 100vw, 62vw"/><br/>
</span></li>
<li><span>Let 𝑠 be the storage cost of the feature and the subscripts 𝐴 and 𝐵 represent Service A and B, respectively. For example, 𝑠</span><span>𝐴𝑐</span><span> represents the storage cost of compressed features in Service A. We are constrained by the capacity of the two services.<br/>
<img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg" alt="" width="1920" height="594" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=916,283 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=768,238 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=1024,317 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=1536,475 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=96,30 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=192,59 192w" sizes="(max-width: 992px) 100vw, 62vw"/><br/>
</span></li>
<li><span>Some of Service B must be utilized to enable compression. Let’s represent that as a few features that must be selected.<br/>
<img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg" alt="" width="1920" height="892" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=916,426 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=768,357 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=1024,476 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=1536,714 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=192,89 192w" sizes="(max-width: 992px) 100vw, 62vw"/><br/>
</span></li>
</ol>
<p><span>With this, we have now completely specified our problem in a few equations and can solve them using linear programming techniques. Of course, as we are interested in automating and productionalizing this, it can easily be specified in code. For this example, we accomplish this in Python using the excellent </span><a href="https://numpy.org/"><span>NumPy</span></a><span> and </span><a href="https://www.cvxpy.org/"><span>CVXPY</span></a><span> packages.</span></p>
<pre><code>import cvxpy as cp
import numpy as np
import pandas as pd
 
# Assuming data is a Pandas DataFrame that contains relevant feature data
data = pd.DataFrame(...)
# These variables contain the maximum capacity of various services
service_a = ...
service_b = ...
 
selected_full_features = cp.Variable(data.shape[0], boolean=True)
selected_compressed_features = cp.Variable(data.shape[0], boolean=True)
 
# Maximize the feature gain
feature_gain = (
   data.uncompressed_feature_gain.to_numpy() @ selected_full_features
   + data.compressed_feature_gain.to_numpy() @ selected_compressed_features
)
 
constraints = [
   # 1. We should not select the compressed and uncompressed version
   #    of the same feature
   selected_full_features + selected_compressed_features &lt;= np.ones(data.shape[0]),
   # 2. Features are restricted by the maximum capacity of the services
   data.full_storage_cost.to_numpy() @ selected_full_features
   + data.compressed_storage_cost.to_numpy() @ selected_full_features
   &lt;= service_a,
   data.full_memory_cost.to_numpy() @ selected_full_features
   + data.compressed_memory_cost.to_numpy() @ selected_compressed_features
   &lt;= service_b,
   # 3. Some features must be selected to enable compression
   selected_full_features &gt;= data.special_features.to_numpy(),
]
</code></pre>
<h2><span>Leveraging the framework</span></h2>
<p><span>Now we have a framework that we can use to express our questions and hypotheticals. If we want to find out how an increase in Service A translates to a feature gain, we can  run the optimization problem above at different values for Service A capacity and plot the gain. This way, we can directly quantify the return for each incremental increase in capacity. We can use this as a strong signal for what services we should invest in for the future and directly compare the return on investment on more feature memory, computing, or storage.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg" alt="chart showing return on service A capacity" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Similarly, we can look at the relationships between services. We simply vary the capacity of Services A and B while keeping the gain constant. We can see that as Service B’s capacity increases, less of Service A is needed to achieve the same gain. This can be leveraged if one service is overly stressed compared with another.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg" alt="chart showing the relationship between Service A and service B capacity" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Linear programming as a framework for automating decisions</span></h2>
<p><span>Previously, feature approval was a manual process where teams would spend valuable time calculating how many features we could support and analyzing what the return on investment was for increasing the capacity of our services. In a company like Facebook — where we have multiple models being continuously iterated on — this approach does not scale. By framing our services as a system of linear equations, we take a complex interconnected system and simplify it into basic relationships that are easily communicated. By doing this, we can make smarter decisions about the features we deploy and the infrastructure we invest in.</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Paulo Silva Costa</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/CD21-479_Framing-Feature-Resources_Hero-1920x1080-1.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 29 Jul 2021 15:57:28 +0000</pubDate>
    </item>
    <item>
      <title>Migrating Facebook to MySQL 8.0</title>
      <link>https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/</link>
      <description>&lt;p&gt;MySQL, an open source database developed by Oracle, powers some of Facebook’s most important workloads. We actively develop new features in MySQL to support our evolving requirements. These features change many different areas of MySQL, including client connectors, storage engine, optimizer, and replication. Each new major version of MySQL requires significant time and effort to [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/&#34;&gt;Migrating Facebook to MySQL 8.0&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><a href="https://github.com/facebook/mysql-5.6"><span>MySQL</span></a><span>, an open source database developed by Oracle, powers some of Facebook’s most important workloads. We actively develop new features in MySQL to support our evolving requirements. These features change many different areas of MySQL, including client connectors, storage engine, optimizer, and replication. Each new major version of MySQL requires significant time and effort to migrate our workloads. The challenges  include:</span></p>
<ul>
<li aria-level="1"><span>Porting our custom features to the new version</span></li>
<li aria-level="1"><span>Ensuring replication is compatible between the major versions</span></li>
<li aria-level="1"><span>Minimizing changes needed for existing application queries</span></li>
<li aria-level="1"><span>Fixing performance regressions that prevent the server from supporting our workloads</span></li>
</ul>
<p><span>Our last major version upgrade, to MySQL 5.6, took more than a year to roll out. When version 5.7 was released, we were still in the midst of developing our LSM-Tree storage engine, </span><a href="https://engineering.fb.com/2016/08/31/core-data/myrocks-a-space-and-write-optimized-mysql-database/"><span>MyRocks</span></a><span>, on version 5.6. Since upgrading to 5.7 while simultaneously building a new storage engine would have significantly slowed the progress on MyRocks, we opted to stay with 5.6 until MyRocks was complete. MySQL 8.0 was announced as we were finishing the rollout of MyRocks to our user database (UDB) service tier. </span></p>
<p><span>That version included compelling features like writeset-based parallel replication and a transactional data dictionary that provided atomic DDL support. For us, moving to 8.0 would also bring in the 5.7 features we had missed, including Document Store. Version 5.6 was approaching end of life, and we wanted to stay active within the MySQL community, especially with our work on the MyRocks storage engine. Enhancements in 8.0, like instant DDL, could speed up MyRocks schema changes, but we needed to be on the 8.0 codebase to use it. Given the benefits of the code update, we decided to migrate to 8.0. We’re sharing how we tackled our 8.0 migration project — and some of the surprises we discovered in the process. When we initially scoped out the project, it was clear that moving to 8.0 would be even more difficult than migrating to 5.6 or MyRocks.</span></p>
<ul>
<li aria-level="1"><span>At the time, our customized 5.6 branch had over 1,700 code patches to port to 8.0. As we were porting those changes, new Facebook MySQL features and fixes were added to the 5.6 codebase that moved the goalpost further away.</span></li>
<li aria-level="1"><span>We have many MySQL servers running in production, serving a large number of disparate applications. We also have extensive software infrastructure for managing MySQL instances. These applications perform operations like gathering statistics and managing server backups.</span></li>
<li aria-level="1"><span>Upgrading from 5.6 to 8.0 skipped over 5.7 entirely. APIs that were active in 5.6 would have been deprecated in 5.7 and possibly removed in 8.0, requiring us to update any application using the now-removed APIs.</span></li>
<li aria-level="1"><span>A number of Facebook features were not forward-compatible with similar ones in 8.0 and required a deprecation and migration path forward.</span></li>
<li aria-level="1"><span>MyRocks enhancements were needed to run in 8.0, including native partitioning and crash recovery.</span></li>
</ul>
<h2><span>Code patches</span></h2>
<p><span>We first set up the 8.0 branch for building and testing in our development environments. We then began the long journey to port the patches from our 5.6 branch. There were more than 1,700 patches when we started, but we were able to organize them into a few major categories. Most of our custom code had good comments and descriptions so we could easily determine whether it was still needed by the applications or if it could be dropped. Features that were enabled by special keywords or unique variable names also made it easy to determine relevance because we could search through our application codebases to find their use cases. A few patches were very obscure and required detective work — digging through old design documents, posts, and/or code review comments — to understand their history.</span></p>
<p><span>We sorted each patch into one of four buckets:</span></p>
<ol>
<li aria-level="1"><span>Drop: Features that were no longer used, or had equivalent functionality in 8.0, did not need to be ported.</span></li>
<li aria-level="1"><span>Build/Client: Non-server features that supported our build environment and modified MySQL tools like mysqlbinlog, or added functionality like the async client API, were ported.</span></li>
<li aria-level="1"><span>Non-MyRocks Server: Features in the mysqld server that were not related to our MyRocks storage engine were ported.</span></li>
<li aria-level="1"><span>MyRocks Server: Features that supported the MyRocks storage engine were ported.</span></li>
</ol>
<p><span>We tracked the status and relevant historical information of each patch using spreadsheets, and recorded our reasoning when dropping a patch. Multiple patches that updated the same feature were grouped together for porting. Patches ported and committed to the 8.0 branch were annotated with the 5.6 commit information. Discrepancies on porting status would inevitably arise due to the large number of patches we needed to sift through and these notes helped us resolve them.</span></p>
<p><span>Each of the client and server categories naturally became a software release milestone. With all client-related changes ported, we were able to update our client tooling and connector code to 8.0. Once all of the non-MyRocks server features were ported, we were able to deploy 8.0 mysqld for InnoDB servers. Finishing up the MyRocks server features enabled us to update MyRocks installations.</span></p>
<p><span>Some of the most complex features required significant changes for 8.0, and a few areas had major compatibility problems. For example, upstream 8.0 binlog event formats were incompatible with some of our custom 5.6 modifications. Error codes used by Facebook 5.6 features conflicted with those assigned to new features by upstream 8.0. We ultimately needed to patch our 5.6 server to be forward-compatible with 8.0.</span></p>
<p><span>It took a couple of years to complete porting all of these features. By the time we got to the end, we had evaluated more than 2,300 patches and ported 1,500 of those to 8.0.</span></p>
<h2><span>The migration path</span></h2>
<p><span>We group together multiple mysqld instances into a single MySQL replica set. Each instance in a replica set contains the same data but is geographically distributed to a different data center to provide data availability and failover support. Each replica set has one primary instance. The remaining instances are all secondaries. The primary handles all write traffic and replicates the data asynchronously to all secondaries.<br/>
</span></p>

<p><span>We started with replica sets consisting of 5.6 primary/5.6 secondaries and the end goal was replica sets with 8.0 primary/8.0 secondaries. We followed a plan similar to the</span><a href="https://engineering.fb.com/2017/09/25/core-data/migrating-a-database-from-innodb-to-myrocks/"> <span>UDB MyRocks migration plan</span></a><span>.</span></p>
<ol>
<li aria-level="1"><span>For each replica set, create and add 8.0 secondaries via a logical copy using mysqldump. These secondaries do not serve any application read traffic.</span></li>
<li aria-level="1"><span>Enable read traffic on the 8.0 secondaries.</span></li>
<li aria-level="1"><span>Allow the 8.0 instance to be promoted to primary.</span></li>
<li aria-level="1"><span>Disable the 5.6 instances for read traffic.</span></li>
<li aria-level="1"><span>Remove all the 5.6 instances.</span></li>
</ol>
<p><span>Each replica set could transition through each of the steps above independently and stay on a step as long as needed. We separated replica sets into much smaller groups, which we shepherded through each transition. If we found problems, we could rollback to the previous step. In some cases, replica sets were able to reach the last step before others started.</span></p>
<p><span>To automate the transition of a large number of replica sets, we needed to build new software infrastructure. We could group replica sets together and move them through each stage by simply changing a line in a configuration file. Any replica set that encountered problems could then be individually rolled back.</span></p>
<h3><span>Row-based replication</span></h3>
<p><span>As part of the 8.0 migration effort, we decided to standardize on using row-based replication (RBR). Some 8.0 features required RBR, and it simplified our MyRocks porting efforts. While most of our MySQL replica sets were already using RBR, those still running statement-based replication (SBR) could not be easily converted. These replica sets usually had tables without any high cardinality keys. Switching completely to RBR had been a goal, but the long tail of work needed to add primary keys was often prioritized lower than other projects.</span></p>
<p><span>Hence, we made RBR a requirement for 8.0. After evaluating and adding primary keys to every table, we switched over the last SBR replica set this year. Using RBR also gave us an alternative solution for resolving an application issue that we encountered when we moved some replica sets to 8.0 primaries, which will be discussed later.</span></p>
<h2><span>Automation validation</span></h2>
<p><span>Most of the 8.0 migration process involved testing and verifying the mysqld server with our automation infrastructure and application queries.</span></p>
<p><span>As our MySQL fleet grew, so did the automation infrastructure we use to manage the servers. In order to ensure all of our MySQL automation was compatible with the 8.0 version, we invested in building a test environment, which leveraged test replica sets with virtual machines to verify the behaviors. We wrote integration tests to canary each piece of automation to run on both the 5.6 version and the 8.0 version and verified their correctness. We found several bugs and behavior differences as we went through this exercise.</span></p>
<p><span>As each piece of MySQL infrastructure was validated against our 8.0 server, we found and fixed (or worked around) a number of interesting issues:</span></p>
<ol>
<li aria-level="1"><span>Software that parsed text output from error log, mysqldump output, or server show commands easily broke. Slight changes in the server output often revealed bugs in a tool’s parsing logic.</span></li>
<li aria-level="1"><span>The 8.0’s default </span><span>utf8mb4</span><span> collation settings resulted in collation mismatches between our 5.6 and 8.0 instances. 8.0 tables may use the new </span><span>utf8mb4_0900</span><span> collations even for create statements generated by 5.6’s show create table because the 5.6 schemas using </span><span>utf8mb4_general_ci</span><span> do not explicitly specify collation. These table differences often caused problems with replication and schema verification tools.</span></li>
<li aria-level="1"><span>The error codes for certain replication failures changed and we had to fix our automation to handle them correctly.</span></li>
<li aria-level="1"><span>The 8.0 version’s data dictionary obsoleted table .frm files, but some of our automation used them to detect table schema modifications.</span></li>
<li aria-level="1"><span>We had to update our automation to support the dynamic privs introduced in 8.0.</span></li>
</ol>
<h3><span>Application validation</span></h3>
<p><span>We wanted the transition for applications to be as transparent as possible, but some application queries hit performance regressions or would fail on 8.0.</span></p>
<p><span>For the MyRocks migration, we built a MySQL shadow testing framework that captured production traffic and replayed them to test instances. For each application workload, we constructed test instances on 8.0 and replayed shadow traffic queries to them. We captured and logged the errors returning from the 8.0 server and found some interesting problems. Unfortunately, not all of these problems were found during testing. For example, the transaction deadlock was discovered by applications during the migration. We were able to roll back these applications to 5.6 temporarily while we researched different solutions.</span></p>
<ul>
<li aria-level="1"><span>New reserved keywords were introduced in 8.0 and a few, such as groups and rank, conflicted with popular table column names and aliases used in application queries. These queries did not escape the names via backquotes, leading to parsing errors. Applications using software libraries that automatically escaped the column names in queries did not hit these issues, but not all applications used them. Fixing the problem was simple, but it took time to track down application owners and codebases generating these queries.</span></li>
<li aria-level="1"><span>A few REGEXP incompatibilities were also found between 5.6 and 8.0.</span></li>
<li aria-level="1"><span>A few applications hit </span><a href="https://bugs.mysql.com/bug.php?id=98324"><span>repeatable-read transaction deadlocks</span></a><span> involving </span><span>insert … on duplicate key</span><span> queries on InnoDB. 5.6 had a bug which was corrected in 8.0, but the fix increased the likelihood of transaction deadlocks. After analyzing our queries, we were able to resolve them by lowering the isolation level. This option was available to us since we had made the switch to row-based replication.</span></li>
<li aria-level="1"><span>Our custom 5.6 Document Store and JSON functions were not compatible with 8.0’s. Applications using Document Store needed to convert the document type to text for the migration. For the JSON functions, we added 5.6-compatible versions to the 8.0 server so that applications could migrate to the 8.0 API at a later time.</span></li>
</ul>
<p><span>Our query and performance testing of the 8.0 server uncovered a few problems that needed to be addressed almost immediately.</span></p>
<ul>
<li aria-level="1"><span>We found new mutex contention hotspots around the ACL cache. When a large number of connections were opened simultaneously, they could all block on checking ACLs.</span></li>
<li aria-level="1"><span>Similar contention was found with binlog index access when many binlog files are present and high binlog write rates rotate files frequently.</span></li>
<li aria-level="1"><span>Several queries involving temp tables were broken. The queries would return unexpected errors or take so long to run that they would time out.</span></li>
</ul>
<p><span>Memory usage compared with 5.6 had increased, especially for our MyRocks instances, because InnoDB in 8.0 must be loaded. The default performance_schema settings enabled all instruments and consumed significant memory. We limited the memory usage by only enabling a small number of instruments and making code changes to disable tables that could not be manually turned off. However, not all the increased memory was being allocated by performance_schema. We needed to examine and modify various InnoDB internal data structures to reduce the memory footprint further. This effort brought 8.0’s memory usage down to acceptable levels. </span></p>
<h2><span>What’s next</span></h2>
<p><span>The 8.0 migration has taken a few years so far. We have converted many of our InnoDB replica sets to running entirely on 8.0. Most of the remaining ones are at various stages along the migration path. Now that most of our custom features have been ported to 8.0, updating to Oracle’s minor releases has been comparatively easier and we plan to keep pace with the latest versions.</span></p>
<p><span>Skipping a major version like 5.7 introduced problems, which our migration needed to solve.</span></p>
<p><span>First, we could not upgrade servers in place and needed to use logical dump and restore to build a new server. However, for very large mysqld instances, this can take many days on a live production server and this fragile process will likely be interrupted before it can complete. For these large instances, we had to modify our backup and restore systems to handle the rebuild.</span></p>
<p><span>Second, it is much harder to detect API changes because 5.7 could have provided deprecation warnings to our application clients to fix potential issues. Instead, we needed to run additional shadow tests to find failures before we could migrate the production workloads. Using mysql client software that automatically escaped schema object names helps reduce the number of compatibility issues.</span></p>
<p><span>Supporting two major versions within a replica set is hard. Once a replica set promotes its primary to be an 8.0 instance, it is best to disable and remove the 5.6 ones as soon as possible. Application users tend to discover new features that are supported only by 8.0, like </span><span>utf8mb4_0900</span><span> collations, and using these can break the replication stream between 8.0 and 5.6 instances.</span></p>
<p><span>Despite all the hurdles in our migration path, we have already seen the benefits of running 8.0. Some applications have opted for early conversion to 8.0 to utilize features like Document Store and improved datetime support. We have been considering how to support storage engine features like Instant DDL on MyRocks. Overall, the new version greatly expands on what we can do with MySQL @ Facebook.</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Herman Lee, Pradeep Nayak</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/CD21_390_ENG_MySQL_HERO_FINAL_2x.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 22 Jul 2021 16:00:35 +0000</pubDate>
    </item>
    <item>
      <title>Fully Sharded Data Parallel: faster AI training with fewer GPUs</title>
      <link>https://engineering.fb.com/2021/07/15/open-source/fsdp/</link>
      <description>&lt;p&gt;Training AI models at a large scale isn’t easy. Aside from the need for large amounts of computing power and resources, there is also considerable engineering complexity behind training very large models. At Facebook AI Research (FAIR) Engineering, we have been working on building tools and infrastructure to make training large AI models easier. Our [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/15/open-source/fsdp/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/15/open-source/fsdp/&#34;&gt;Fully Sharded Data Parallel: faster AI training with fewer GPUs&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>Training AI models at a large scale isn’t easy. Aside from the need for large amounts of computing power and resources, there is also considerable engineering complexity behind training very large models. At Facebook AI Research (FAIR) Engineering, we have been working on building tools and infrastructure to make training large AI models easier. Our recent work in areas such as </span><a href="https://github.com/pytorch/fairseq/blob/master/examples/megatron_11b/README.md"><span>intra-layer model parallelism</span></a><span>, </span><a href="https://fairscale.readthedocs.io/en/latest/deep_dive/pipeline_parallelism.html"><span>pipeline model parallelism</span></a><span>, </span><a href="https://github.com/facebookresearch/fairscale#optimizer-state-sharding-zero"><span>optimizer state+gradient sharding</span></a><span>, and </span><a href="https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/moe_layer.py"><span>mixture of experts</span></a><span> is just part of our work to make training advanced AI models for any number of tasks more efficient.</span></p>
<p><span>Fully Sharded Data Parallel (FSDP) is the newest tool we’re introducing. It <a href="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/">shards</a> an AI model’s parameters across data parallel workers and can optionally offload part of the training computation to the CPUs. As its name suggests, FSDP is a type of data-parallel training algorithm. Although the parameters are sharded to different <a href="https://engineering.fb.com/2018/03/20/ml-applications/the-next-step-in-facebook-s-ai-hardware-infrastructure/">GPUs</a>, the computation for each microbatch of data is still local to each GPU worker. This conceptual simplicity makes FSDP easier to understand and more applicable to a wide range of usage scenarios (compared with intra-layer parallelism and pipeline parallelism). Compared with optimizer state+gradient sharding data parallel methods, FSDP shards parameters more uniformly and is capable of better performance via communication and computation overlapping during training.</span></p>
<p><span>With FSDP, it is now possible to more efficiently train models that are orders of magnitude larger using fewer GPUs. FSDP has been implemented in the </span><a href="https://github.com/facebookresearch/fairscale"><span>FairScale library</span></a><span> and allows engineers and developers to scale and optimize the training of their models with simple APIs. At Facebook, FSDP has already been integrated and tested for training some of our </span><a href="https://github.com/pytorch/fairseq"><span>NLP</span></a><span> and</span><a href="https://github.com/facebookresearch/vissl"><span> Vision</span></a><span> models.</span></p>
<h2><span>The high computational cost of large-scale training</span></h2>
<p><a href="https://arxiv.org/pdf/2001.08361.pdf"><span>NLP research</span></a><span> is one particular area where we can see the importance of efficiently leveraging compute for training AI. Last year, OpenAI announced that they had trained </span><a href="https://neurips.cc/virtual/2020/public/poster_1457c0d6bfcb4967418bfb8ac142f64a.html"><span>GPT-3</span></a><span>, the largest-ever neural language model, with 175 billion parameters. It is </span><a href="https://lambdalabs.com/blog/demystifying-gpt-3/"><span>estimated</span></a><span> to have taken roughly 355 GPU years to train GPT-3, or the equivalent of 1,000 GPUs working continuously for more than four months.</span></p>
<p><span>Besides requiring a lot of compute and engineering resources, most approaches to scaling like this introduce additional communication costs and require engineers to carefully evaluate trade-offs between memory use and computational efficiency. For example, typical data parallel training requires maintaining redundant copies of the model on each GPU, and model parallel training introduces additional communication costs to move activations between workers (GPUs).</span></p>
<p><span>FSDP is relatively free of trade-offs in comparison. It improves memory efficiency by sharding model parameters, gradients, and optimizer states across GPUs, and improves computational efficiency by decomposing the communication and overlapping it with both the forward and backward passes. FSDP produces identical results as standard distributed data parallel (DDP) training and is available in an easy-to-use interface that’s a drop-in replacement for PyTorch’s DistributedDataParallel module. Our early testing has shown that FSDP can enable scaling to trillions of parameters.</span></p>
<h2><span>How FSDP works</span></h2>
<p><span>In standard DDP training, every worker processes a separate batch and the gradients are summed across workers using an </span><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce"><span>all-reduce operation</span></a><span>. While DDP has become very popular, it takes more GPU memory than it needs because the model weights and optimizer states are replicated across all DDP workers.</span></p>
<p><span>One method to reduce replications is to apply a process called full parameter sharding, where only a subset of the model parameters, gradients, and optimizers needed for a local computation is made available. An implementation of this method, ZeRO-3, has already been popularized by Microsoft. </span></p>
<p><span>The key insight to unlock full parameter sharding is that we can decompose the </span><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce"><span>all-reduce</span></a><span> operations in DDP into separate </span><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#reducescatter"><span>reduce-scatter</span></a><span> and <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allgather">all-gather</a> operations:</span></p>
<figure id="attachment_17828" aria-describedby="caption-attachment-17828"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?w=1024" alt="Full Sharded Data Parallel graph" width="1024" height="562" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png 1264w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=916,503 916w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=768,422 768w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=1024,562 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=96,53 96w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=192,105 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17828">All-reduce as a combination of reduce-scatter and all-gather. The standard all-reduce operation to aggregate gradients can be decomposed into two separate phases: reduce-scatter and all-gather. During the reduce-scatter phase, the gradients are summed in equal blocks among ranks on each GPU based on their rank index. During the all-gather phase, the sharded portion of aggregated gradients available on each GPU are made available to all GPUs (see here for details on those operators).</figcaption></figure>
<p><span>We can then rearrange the reduce-scatter and all-gather so that each DDP worker needs to store only a single shard of parameters and optimizer states. The figure below illustrates standard DDP training (top) and FSDP training (bottom):</span></p>
<figure id="attachment_17812" aria-describedby="caption-attachment-17812"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?w=907" alt="Full Sharded Data Parallel graph" width="907" height="1024" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png 1566w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=811,916 811w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=768,867 768w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=907,1024 907w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=1361,1536 1361w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=96,108 96w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=192,217 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17812">A comparison of standard data parallel training and fully sharded data parallel training. In standard data parallel training methods, a copy of the model is present on each GPU and a sequence of forward and backward passes are evaluated on only a shard of the data. After these local computations, the parameters and optimizers for each local process are shared with the other GPUs in order to calculate the global weight update. In FSDP, only a shard of the model is present on a GPU. Then, locally, all weights are gathered from the other GPUs — by means of an all-gather step — to calculate the forward pass. This gathering of weights is then performed again before the backward pass. After that backward pass, the local gradients are averaged and sharded across the GPUs by means of a reduce-scatter step, which allows each GPU to update its local weight shard.</figcaption></figure>
<p><span>To maximize memory efficiency, we can discard the full weights after each layer’s forward pass, saving memory for subsequent layers. This can be implemented by applying the FSDP wrapper to every layer in the network (with </span><span>reshard_after_forward=True</span><span>). </span></p>
<p><span>In pseudo-code:</span></p>
<pre><span>FSDP forward pass:</span>
<span>    for layer_i in layers:</span>
<span>        all-gather full weights for layer_i</span>
<span>        forward pass for layer_i</span>
<span>        discard full weights for layer_i</span>

<span>FSDP backward pass:</span>
<span>    for layer_i in layers:</span>
<span>        all-gather full weights for layer_i</span>
<span>        backward pass for layer_i</span>
<span>        discard full weights for layer_i</span>
<span>        reduce-scatter gradients for layer_i</span></pre>
<h2><span>How to use FSDP</span></h2>
<p><span>There are several ways to use FSDP in large-scale AI research.</span><span> At this time, we offer four solutions to adapt to different needs.</span></p>
<h3><span>1. Using FSDP in language models</span></h3>
<p><span>For language models, FSDP is supported in the </span><a href="https://github.com/pytorch/fairseq"><i><span>fairseq</span></i><span> framework</span></a><span> via the following new arguments:</span></p>
<ul>
<li aria-level="1"><span>–ddp-backend=fully_sharded</span><span>: enables full sharding via FSDP</span></li>
<li aria-level="1"><span>–cpu-offload</span><span>: offloads the optimizer state and FP32 model copy to CPU (combine with</span><span>–optimizer=cpu_adam</span><span>)</span></li>
<li aria-level="1"><span>–no-reshard-after-forward</span><span>: increases training speed for large models (1B+ params) and is similar to ZeRO stage 2</span></li>
<li aria-level="1">Other popular options (<span>–fp16</span><span>, </span><span>–update-freq</span><span>, </span><span>–checkpoint-activations</span><span>, </span><span>–offload-activations</span><span>, etc.) continue to work as normal</span></li>
</ul>
<p><span>See the </span><a href="https://github.com/pytorch/fairseq/tree/master/examples/fully_sharded_data_parallel"><span>fairseq tutorial</span></a><span> for instructions on using FSDP to train a 13B-parameter model on eight GPUs or on a single GPU with FSDP + CPU offloading.</span></p>
<h3><span>2. Using FSDP in computer vision models</span></h3>
<p><span>For computer vision models, FSDP is supported in </span><a href="https://github.com/facebookresearch/vissl"><span>VISSL</span></a><span> and tested on RegNets architectures. Layers like BatchNorm and ReLU are seamlessly handled and tested for convergence.</span></p>
<p><span>Use the following options to enable FSDP:</span></p>
<ul>
<li aria-level="1"><span>config.MODEL.FSDP_CONFIG.AUTO_SETUP_FSDP=True</span></li>
<li aria-level="1"><span>config.MODEL.SYNC_BN_CONFIG.SYNC_BN_TYPE=pytorch</span></li>
<li aria-level="1"><span>config.MODEL.AMP_PARAMS.AMP_TYPE=pytorch</span></li>
</ul>
<p><span>See </span><a href="https://github.com/facebookresearch/vissl/blob/40441123a6f7098500676ca8800025c1f02e28b3/vissl/config/defaults.yaml#L498-L513"><span>this section</span></a><span> of the yaml config for additional options to config FSDP within VISSL.</span></p>
<h3><span>3. Using FSDP from PyTorch Lightning</span></h3>
<p><span>For easier integration with more general use cases, FSDP is supported as a beta feature by PyTorch Lightning. </span><a href="https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced_gpu.html#fully-sharded-training"><span>This tutorial</span></a><span> contains a detailed example on how to use the FSDP plugin with PyTorch Lightning. At a high level, adding </span><span>plugins=’fsdp’</span><span> below can activate it.</span></p>
<pre><span>model = MyModel()</span>
<span>trainer = Trainer(gpus=4, </span><b>plugins=&#39;fsdp&#39;</b><span>, precision=16)</span>
<span>trainer.fit(model)
</span><span>
trainer.test()</span>
<span>trainer.predict()</span></pre>
<h3><span>4. Using the FSDP library directly from FairScale</span></h3>
<p><span>The main library where FSDP has been developed, and where you can find the latest updates, is </span><a href="https://fairscale.readthedocs.io/en/latest/deep_dive/oss_sdp_fsdp.html"><span>FairScale</span></a><span>. You can directly use FSDP from FairScale with the below example by simply replacing the </span><span>DDP(my_module)</span><span>:</span></p>
<pre><span>from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP</span>
<span>...</span>
<span>sharded_module = </span><span><del>DDP(my_module)</del></span><b>FSDP(my_module)</b>
<span>optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)</span>
<span>for sample, label in dataload.next_batch:</span>
<span>  out = sharded_module(x=sample, y=3, z=torch.Tensor([1]))</span>
<span>  loss = criterion(out, label)</span>
<span>  loss.backward()</span>
<span>  optim.step()</span></pre>
<p><span>The FSDP library in FairScale exposes the low-level options for many important aspects of large-scale training. Here are some few important areas to consider when you apply FSDP with its full power.</span></p>
<ol>
<li aria-level="1"><b>Model wrapping: </b><span>In order to minimize the transient GPU memory needs, users need to wrap a model in a nested fashion. This introduces additional complexity. The </span><a href="https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/wrap/auto_wrap.py"><span>auto_wrap</span></a><span> utility is useful in annotating existing PyTorch model code for nested wrapping purposes.</span></li>
<li aria-level="1"><b>Model initialization:</b><span> Unlike DDP, FSDP does </span><b>not</b><span> automatically synchronize model weights between GPU workers. This means model initialization must be done carefully so that all GPU workers have the identical initial weights.</span></li>
<li aria-level="1"><b>Optimizer settings:</b><span> Due to sharding and wrapping, only certain types of optimizer and optimizer settings are supported by FSDP. In particular, if a module is wrapped by FSDP and its parameters are flattened into a single tensor, users cannot use different hyperparameters for different parameter groups in such a module.</span></li>
<li aria-level="1"><b>Mixed precision:</b><span> FSDP supports advanced mixed precision training with FP16 master weights, as well as FP16 reduce and scatter on the gradients. Certain parts of a model may converge only if full precision is used. In those cases, additional wrapping is needed to selectively run parts of a model in full precision.</span></li>
<li aria-level="1"><b>State checkpointing and inference:</b><span> When the model scale is large, saving and loading the model state can become challenging. FSDP supports several ways to make that task possible, but it is by no means trivial.</span></li>
<li aria-level="1"><span>Finally, FSDP is often used together with </span><b>activation checkpointing</b><span> functions like </span><a href="https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/checkpoint/checkpoint_activations.py"><span>checkpoint_wrapper</span></a><span> from FairScale. Users may need to carefully tune the activation checkpointing strategy to fit a large model within limited GPU memory space.</span></li>
</ol>
<h2><span>Next steps</span></h2>
<p><span>FSDP is open source, and early users have tried it and contributed to it. We think it can benefit the entire research community, and we look forward to working with everyone in making it better. In particular, these are some of the important areas.</span></p>
<ol>
<li aria-level="1"><b>Making FSDP more general.</b><span> So far, FSDP has been used on both NLP and vision models with SGD and Adam optimizers. As newer models and optimizers emerge, FSDP needs to continue supporting them. Being a purely data-parallel training scheme, FSDP has the greatest potential to be general in supporting a wide range of AI algorithms.</span></li>
<li aria-level="1"><b>Making FSDP auto-tune. </b><span>There are many knobs that users can tune today with FSDP for both scaling and performance. We look forward to developing algorithms for auto-tuning both GPU memory usage and training performance.</span></li>
<li aria-level="1"><span>In addition to training, more </span><b>scalable inference</b><span> and model serving is an important use case that FSDP might need to support.</span></li>
<li aria-level="1"><span>Last but not least, refactoring and continuing to </span><b>modularize FSDP</b><span> and its core components is equally important to newer and better features.</span></li>
</ol>
<h2><span>Try it out and contribute!</span></h2>
<p><span>FSDP is currently available directly from the </span><a href="https://github.com/facebookresearch/fairscale"><span>FairScale library</span></a><span>.</span></p>
<p><span>Thanks for sticking with us thus far. Please try FSDP in your research or production work. We would love to hear your feedback, and, as always, pull requests are welcome! </span></p>

		
	</div></div>]]></content:encoded>
      <author>By Myle Ott, Sam Shleifer, Min Xu, Priya Goyal, Quentin Duval, Vittorio Caggiano</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Hero-FINAL-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 15 Jul 2021 16:00:47 +0000</pubDate>
    </item>
    <item>
      <title>How WhatsApp enables multi-device capability</title>
      <link>https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/</link>
      <description>&lt;p&gt;For years, people have been asking us to create a true multi-device experience that allows people to use WhatsApp on other devices without requiring a smartphone connection. Today, we’re announcing the rollout of a limited public beta test for WhatsApp’s updated multi-device capability.  With this new capability, you can now use WhatsApp on your phone [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/&#34;&gt;How WhatsApp enables multi-device capability&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>For years, people have been asking us to create a true multi-device experience that allows people to use WhatsApp on other devices without requiring a smartphone connection.</span></p>
<p><span>Today, we’re announcing the rollout of a limited public beta test for WhatsApp’s updated multi-device capability. </span></p>
<p><span>With this new capability, you can now use WhatsApp on your phone and up to four other nonphone devices simultaneously — even if your phone battery is dead. Each companion device will connect to your WhatsApp independently while maintaining the same level of privacy and security through end-to-end encryption that people who use WhatsApp have come to expect. Importantly, we have developed new technologies to maintain end-to-end encryption while still managing to sync your data — such as contact names, chat archives, starred messages, and more — across devices.</span></p>
<p><span>To achieve this, we had to rethink WhatsApp’s architecture and design new systems to enable a standalone multi-device experience while preserving <a href="https://engineering.fb.com/2021/04/16/security/dit/">privacy and end-to-end encryption</a>. </span></p>
<h2><span>Taking smartphones out of the equation</span></h2>
<p>The current WhatsApp experience for companion devices on web, macOS, Windows, and Portal uses a smartphone app as the primary device, making the phone the source of truth for all user data and the only device capable of end-to-end encrypting messages for another user, initiating calls, etc. Companion devices maintain a persistent secure connection with the phone and simply mirror its contents on their own UI.</p>
<p><span>This architecture makes it easy to deliver a seamlessly synchronized experience between a phone and companion device without compromising on security. However, it comes with some significant reliability trade-offs: By requiring the phone to perform all operations, companion devices are slower and frequently get disconnected — especially when the phone has a poor connection, its battery is running low, or the application process gets killed by the phone’s OS. It also allows for only a single companion device to be operative at a time, meaning people can’t be on a call in Portal while checking their messages on their PC, for example. </span></p>
<p><span>The new WhatsApp multi-device architecture removes these hurdles, no longer requiring a smartphone to be the source of truth while still keeping user data seamlessly and securely synchronized and private.</span></p>
<p><span>The challenge in accomplishing this was in maintaining the secure user experience across devices without having to store people’s private messages on our servers in new ways.</span></p>
<h2><span>Meeting the security challenges of multiple devices</span></h2>
<p><span>Prior to the introduction of multi-device, everyone on WhatsApp was identified by a single identity key from which all encrypted communication keys were derived. With multi-device, each device now has its own identity key.</span></p>
<p><span>The WhatsApp server maintains a mapping between each person’s account and all their device identities. When someone wants to send a message, they get their device list keys from the server.  </span></p>
<p><span>We have also addressed the challenge of preventing a malicious or compromised server from eavesdropping on someone’s communications by surreptitiously adding devices to someone’s account. We use a combination of technologies to solve this: First, we have extended security codes to now represent the combination of all of someone’s device identities so that anyone and their contact can always verify all the devices they are sending messages to. </span></p>
<p><span>Second, in order to reduce the number of times that someone needs to perform identity verifications, we have developed and will roll out a technology called Automatic Device Verification. This system allows for devices to automatically establish trust between each other in a way that someone needs to compare another user’s security code only if that user reregisters their entire account, rather than each time they link a new device to their account. </span></p>
<p><span>Finally, we also give people additional control and protections over which devices are linked to their account. First, everyone will continue to be required to link new companion devices by scanning a QR code from their phone. This process now requires biometric authentication before linking where people have enabled this feature on compatible devices. Finally, people will be able to see all the companion devices linked to their account as well as when they were last used, and will be able to log out of them remotely if needed. </span></p>
<h2><span>Maintaining message privacy</span></h2>
<p><span>When people message each other in a one-on-one chat, a pairwise encrypted session is established between each of the sender’s and recipient’s devices. WhatsApp multi-device uses a client-fanout approach</span><span>,</span> <span>where the WhatsApp client sending the message encrypts and transmits it N number of times to N number of different devices </span><span>— those in the sender and receiver’s device lists</span><span>. Each message is individually encrypted using the established pairwise encryption session with each device. M</span><span>essages are not stored on the server after they are delivered. For groups, we still use the same scalable Sender Key encryption scheme from the Signal Protocol.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?w=1024" alt="WhatsApp Multi-device graphic" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<figure id="attachment_17849" aria-describedby="caption-attachment-17849"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?w=1024" alt="WhatsApp Multi-device graphic" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17849">WhatsApp’s legacy architecture used a smartphone as the source of truth. But with the new multi-device capability, up to four other nonphone companion devices can connect to WhatsApp independently while still maintaining the same level of privacy and security.</figcaption></figure>
<h2><span>Adapting voice and video protocols for multi-device, end-to-end encryption  </span></h2>
<p><span>When someone on WhatsApp makes a voice or video call:</span><span><br/>
</span></p>
<ol>
<li aria-level="1"><span>The initiator generates a set of random 32-byte </span><span>SRTP</span><span> master secrets for each of the recipient’s devices.</span><span><br/>
</span></li>
<li aria-level="1"><span>The initiator sends an incoming call message (using the client-fanout approach described above) to each of the devices of the recipient. Each recipient’s device receives this message, which contains the encrypted </span><span>SRTP</span><span> master secret.</span></li>
<li aria-level="1">If the responder answers the call from one of the devices, a <span>SRTP</span><span> encrypted call is started, protected by the </span><span>SRTP</span><span> master secret generated for that device.</span></li>
</ol>
<p><span>The </span><span>SRTP</span><span> master secret persists in memory on the client device and is used only during the call. Our servers do not have access to the </span><span>SRTP</span><span> master secrets.</span></p>
<p><span>For group calls, the server randomly selects a participant device that is in the call (either the initiator or a device on which a user has accepted the call) to generate the </span><span>SRTP</span><span> master secret. That device generates the secret and sends it to other active participant devices through pairwise end-to-end encryption. This process is repeated, and the keys are reset whenever someone joins or leaves the call.</span></p>
<h2><span>Keeping message history and other application states in sync across devices</span></h2>
<p><span>We want to ensure that people have a consistent experience with WhatsApp no matter the device they are using. To achieve this, we synchronize message history as well as other application state data (such as contact names, whether a chat is archived, or if a message is starred) across devices. All of this data is synchronized and end-to-end encrypted between your devices.</span></p>
<p><span>For message history: When a companion device is linked, the primary device encrypts a bundle of the messages from recent chats and transfers them to the newly linked device. The key to this encrypted message history blob is delivered to the newly linked device via an end-to-end encrypted message. After the companion device downloads, decrypts, unpacks, and stores the messages securely, the keys are deleted. From that point forward, the companion device accesses the message history from its own local database.</span></p>
<p><span>Other application data requires more than an initial transfer from the phone. We also need an ongoing synchronization every time someone modifies their application state (e.g., when they add a new contact, mute a chat, or star a message).</span></p>
<p><span>To solve this, the WhatsApp server securely stores a copy of each application state that all of someone’s devices can access. To properly secure this, all the information, and even the metadata about the information (what kind of user data is stored or accessed), is end-to-end encrypted with constantly changing keys known only to that person’s devices. </span></p>
<h2><span>How to try WhatsApp multi-device beta </span></h2>
<p><span>We plan to initially test the experience with a small group of users from our existing beta program. We will continue optimizing performance and adding a few additional features before slowly rolling it out more broadly. Those who opt in can always opt back out.</span></p>
<p><span>For more information about the beta and to sign up, visit the <a href="https://faq.whatsapp.com/general/download-and-installation/about-multi-device-beta">WhatsApp Help Center</a>.</span></p>
<p><span>For more information about WhatsApp multi-device, read our updated <a href="https://www.whatsapp.com/security/WhatsApp_Security_Whitepaper_v4_Preview.pdf">whitepaper</a>.</span></p>

		
	</div></div>]]></content:encoded>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Hero-Image_FINAL.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 14 Jul 2021 18:59:40 +0000</pubDate>
    </item>
    <item>
      <title>Enforcing encryption at scale</title>
      <link>https://engineering.fb.com/2021/07/12/security/enforcing-encryption/</link>
      <description>&lt;p&gt;Our infrastructure supports thousands of services that handle billions of requests per second. We’ve previously discussed how we built our service encryption infrastructure to keep these globally distributed services operating securely and performantly. This post discusses the system we designed to enforce encryption policies within our network and shares some of the lessons we learned [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/12/security/enforcing-encryption/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/12/security/enforcing-encryption/&#34;&gt;Enforcing encryption at scale&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>Our infrastructure supports thousands of services that handle billions of requests per second. We’ve previously discussed how we built our </span><a href="https://engineering.fb.com/security/service-encryption/"><span>service encryption infrastructure</span></a><span> to keep these globally distributed services operating securely and performantly. This post discusses the system we designed to enforce encryption policies within our network and shares some of the lessons we learned in the process. The goal of this enforcement is to catch any regression quickly and shut it off, keeping our internal traffic secure at the application level via TLS.</span></p>
<h2><span>Organizational challenges</span></h2>
<p><span>Implementing a transit encryption enforcement policy at Facebook scale requires careful planning and communication, in addition to the technical challenges we’ll discuss in a bit. We want the site to stay up and remain reliable so the people using our services will be unaffected by and unaware of any changes to the infrastructure.</span></p>
<p><span>Communicating the intent, specific timelines, and rollout strategy went a long way toward minimizing any potential disruptions for the thousands of teams that run services at Facebook. We use </span><a href="https://www.facebook.com/workplace"><span>Workplace</span></a><span> within Facebook, which enables us to easily distribute that information across a variety of groups with a single share button and consolidate feedback and concerns in a single place for all employees to see. We made sure to include the following:</span></p>
<ul>
<li aria-level="1"><span>A description of the impact of our enforcement mechanism and how it might appear at the application layer</span></li>
<li aria-level="1"><span>A dashboard for engineers to see whether their traffic would be affected</span></li>
<li aria-level="1"><span>The rollout and monitoring plan</span></li>
<li aria-level="1"><span>Dedicated points of contact and a Workplace group where users could ask questions about impact and troubleshoot any issues</span></li>
</ul>
<p><span>The post required multiple discussions within the team to come up with a rollout plan, dashboard requirements, and realistic timelines to meet the goals of the project. This level of communication proved to be useful as the team gathered important feedback early in the process. </span></p>
<h2><span>Building our SSLWall</span></h2>
<p><span>Hardware choke points are a natural approach to providing transparent enforcement. There are options, such as layer 7 firewalls, that let us do deep packet inspection, but executing fine-grained rollouts and the complexities of Facebook’s network would make implementing such a solution a nightmare. Additionally, working at a network firewall level would introduce a much larger blast radius of impacted traffic, and a single configuration issue could end up killing off traffic that we weren’t meant to touch.</span></p>
<p><span>Our team decided to develop and deploy what is internally known as SSLWall, a system that cuts off non-SSL connections across various boundaries. Let’s dive a bit into the design decisions behind this solution.</span></p>
<h3><span>Requirements </span></h3>
<p><span>We needed to be thorough when considering the requirements of a system that would potentially block traffic at such a large scale. The team came up with the following requirements for SSLWall, all of which had an impact on our design decisions:</span></p>
<ul>
<li aria-level="1"><span>Visibility into what traffic is being blocked. Service owners needed a way to assess impacts, and our team needed to be proactive and reach out whenever we felt there was a problem brewing.</span></li>
<li aria-level="1"><span>A passive monitoring mode in which we could turn a knob to flip to active enforcement. This helps us determine impacts early on and prepare teams.</span></li>
<li aria-level="1"><span>A mechanism to allow certain use cases to bypass enforcement, such as BGP, SSH, and approved network diagnostic tools.</span></li>
<li aria-level="1"><span>Support for cases like HTTP CONNECT and STARTTLS. These are instances that do a little bit of work over plaintext before doing a TLS handshake. We have many use cases for these in our infrastructure, such as HTTP tunneling, MySQL security, and SMTP, so these must not break, especially since they eventually encrypt the data with TLS.</span></li>
<li aria-level="1"><span>Extensible configurability. We might have different requirements depending on the environment in which SSLWall operates. Additionally, having important knobs that can be tuned with little disruption means we can roll features forward or back at our own pace.</span></li>
<li aria-level="1"><span>Transparent to the application. Applications should not need to rebuild their code or incur any additional library dependencies for SSLWall to operate. The team needed the ability to iterate quickly and change configuration options independently. In addition, being transparent to the application means SSLWall needs to be performant and use minimal resources without having an impact on latencies.</span></li>
</ul>
<p><span>These requirements all led us down the path of managing a host-level daemon that had a user space and kernel-level component. We needed a low-compute way to inspect all connections transparently and act on them.  </span></p>
<h3><span>eBPF</span></h3>
<p><span>Since we wanted to inspect every connection without needing any changes at the application level, we needed to do some work in the kernel context. We </span><a href="https://ebpf.io/"><span>use eBPF</span></a><span> extensively, and it provides all of the capabilities needed for SSLWall to achieve its goals. We leveraged a number of technologies that eBPF provides:</span></p>
<ul>
<li aria-level="1"><a href="http://man7.org/linux/man-pages/man8/tc-bpf.8.html"><span>tc-bpf</span></a><span>: We leveraged Linux’s </span><a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html"><span>traffic control</span></a><span> (TC) facility and implemented a filter using eBPF.  At this layer, we are able to do some computation on a per-packet basis for packets flowing in and out of the box. TC allows us to operate on a broader range of kernels within Facebook’s fleet. It wasn’t the perfect solution, but it worked for our needs at the time.</span></li>
<li aria-level="1"><span>kprobes: eBPF allows us to attach programs to kprobes, so we can run some code within the kernel context whenever certain functions are called. We were interested in the </span><span>tcp_connect</span><span> and </span><span>tcp_v6_destroy_sock</span><span> functions. These functions are called when a tcp connection is established and torn down, respectively. Old kernels played a factor in our use of kprobes as well.</span></li>
<li aria-level="1"><span>maps: eBPF provides access to a number of map types, including arrays, bounded LRU maps, and perf events</span></li>
</ul>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg" alt="" width="1920" height="950" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=916,453 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=768,380 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=1024,507 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=1536,760 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=96,48 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=192,95 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<figure id="attachment_17786" aria-describedby="caption-attachment-17786"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg" alt="Diagrams showing how kprobes, the tc filter, and our maps interact with one another when determining whether a connection needs to be blocked." width="1920" height="882" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=916,421 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=768,353 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=1024,470 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=1536,706 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=96,44 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=192,88 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17786">Diagrams showing how kprobes, the tc filter, and our maps interact with one another when determining whether a connection needs to be blocked.</figcaption></figure>
<h3><span>The management daemon</span></h3>
<p><span>We built a daemon, which manages the eBPF programs we install and emits logs to </span><a href="https://engineering.fb.com/data-infrastructure/scribe/"><span>Scribe</span></a><span> from our perf events. The daemon also provides the ability to update our TC filter, handles configuration changes (leveraging </span><a href="https://research.fb.com/wp-content/uploads/2016/11/holistic-configuration-management-at-facebook.pdf"><span>Facebook’s Configerator</span></a><span>), and monitors health.</span></p>
<p><span>Our eBPF programs are also bundled with this daemon. This makes management of releases easier to deal with, as we only have one software unit to monitor instead of needing to track a daemon and eBPF release. Additionally, we can modify the schema of our BPF tables, which both user space and kernel space consult, without compatibility concerns between releases.</span></p>
<h3><span>Technical challenges</span></h3>
<p><span>As one would expect, we encountered a number of interesting technical challenges while rolling out SSLWall at Facebook’s scale. A few highlights include:</span><span><br/>
</span><span> </span></p>
<ul>
<li aria-level="1"><a href="https://en.wikipedia.org/wiki/TCP_Fast_Open"><span>TCP Fast Open (TFO)</span></a><span>: We hit an interesting challenge around kprobe and TC filter execution order that was exposed by our use of TFO within the infra. In particular, we needed to move some of our flow tracking code to a kprobe prehandler.</span></li>
<li aria-level="1"><span>BPF Program Size Limit: All BPF programs are subject to size and complexity limits, which may vary based on the kernel version.</span></li>
<li aria-level="1"><span>Performance: We spent many engineering cycles optimizing our BPF programs, particularly the TC filter, so that SSLWall’s CPU impact on some of our critical high QPS services with high fanout remained trivial. Identifying early exit conditions and using BPF arrays over LRUs where possible proved effective.</span></li>
</ul>
<h2><span>TransparentTLS and the long tail</span></h2>
<p><span>With enforcement in place, we needed a way to address noncompliant services without significant engineering time. This included things like torrent clients, open source message queues, and some Java applications. While most applications use common internal libraries where we could bake this logic in, the ones that do not need a different solution.</span></p>
<p><span>Essentially, the team was left with the following requirements for what we refer to as Transparent TLS (or TTLS for short):</span></p>
<ul>
<li aria-level="1"><span>Transparently encrypt connections without the need for application changes.</span></li>
<li aria-level="1"><span>Avoid double encryption for existing TLS connections.</span></li>
<li aria-level="1"><span>Performance can be suboptimal for this long tail.</span></li>
</ul>
<p><span>It’s clear that a proxy solution would have helped here, but we needed to ensure that the application code didn’t need to change and that configuration would be minimal.</span></p>
<p><span>We settled on the following architecture: </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg" alt="" width="1920" height="739" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=916,353 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=768,296 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=1024,394 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=1536,591 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=96,37 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=192,74 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The challenge with this approach is transparently redirecting application connections to the local proxy. Once again, we use BPF to solve this problem. Thanks to the cgroup/connect6 hook, we can intercept all </span><a href="https://man7.org/linux/man-pages/man2/connect.2.html"><span>connect(2)</span></a><span> calls made by the application and redirect them to the proxy as needed.</span></p>
<figure id="attachment_17788" aria-describedby="caption-attachment-17788"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg" alt="Diagram showing application and proxy logic for transparent connect." width="1920" height="1181" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=916,563 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=768,472 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=1024,630 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=1536,945 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=96,59 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=192,118 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17788">Diagram showing application and proxy logic for transparent connect.</figcaption></figure>
<p><span>Aside from the application remaining unchanged, the BPF program makes policy decisions about routing through the proxy. For instance, we optimized this flow to bypass the proxy for all TLS connections created by the application to avoid double encryption.</span></p>
<p><span>This work on enforcement has brought us to a state where we can confidently say that our traffic is encrypted at our scale. However, our work is not yet complete. For instance, there are many new facilities that have come about in BPF that we intend to leverage as we remove old kernel support. We can also improve our transparent proxy solutions and leverage custom protocols to multiplex connections and improve performance.</span></p>
<p><i><span>We’d like to thank Takshak Chahande, Lingnan Gao, Andrey Ignatov, Petr Lapukhov, Puneet Mehra, Kyle Nekritz, Deepak Ravikumar, Paul Saab, and Michael Shao for their work on this project.</span></i></p>

		
	</div></div>]]></content:encoded>
      <author>By Neel Goyal, Ajanthan Asogamoorthy, Mingtao Yang</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/encryption_hero.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 12 Jul 2021 16:00:56 +0000</pubDate>
    </item>
    <item>
      <title>Ribbon filter: Practically smaller than Bloom and Xor</title>
      <link>https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/</link>
      <description>&lt;p&gt;What the research is: The Ribbon filter is a new data structure that is more space-efficient than the popular Bloom filters that are widely used for optimizing data retrieval. One of the ways that Bloom, and now Ribbon, filters solve real engineering problems is by providing smooth configurability unmatched by other filters. Bloom filters work [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/&#34;&gt;Ribbon filter: Practically smaller than Bloom and Xor&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<h2><span>What the research is:</span></h2>
<p><span>The Ribbon filter is a new data structure that is more space-efficient than the popular Bloom filters that are widely used for optimizing data retrieval. One of the ways that Bloom, and now Ribbon, filters solve real engineering problems is by providing smooth configurability unmatched by other filters. Bloom filters work by overapproximating a set of keys associated with some data resource. With a Bloom filter, almost all negative queries to that resource can be skipped (filtered) because the Bloom filter rejects almost all query keys not associated with the resource.</span></p>
<p><span>With proper data layout and design, the Ribbon filter is the first Bloom alternative to match the near-continuous, hazard-free, accuracy-versus-space trade-off provided by Bloom filters.</span></p>
<p><span>Here, near-continuous means efficiently utilizing any amount of memory to represent any number of keys, so that wasted memory such as in internal fragmentation can be minimized to zero. The typical hazard to the accuracy-versus-space trade-off is bit alignment, where some data sizes (e.g., 4, 8, or 16 bits per key) are faster to access than others. Like Bloom, our data layout for Ribbon does not suffer this hazard in access times so it is more freely configurable. And Ribbon filters add new freedom of configurability to the space-versus-time trade-off.</span></p>
<p><span>Building on some prior lines of research, the Ribbon filter combines a simplified, faster, and more flexible construction algorithm; a data layout optimized for filter queries; and near-continuous configurability to make a practical alternative to static (immutable) Bloom filters.</span></p>
<p><span>While well-engineered Bloom filters are extremely fast, they use roughly 50 percent more space (overhead) than the information-theoretic lower bound for filters on arbitrary keys. When Bloom filters cannot meet an application’s space efficiency targets, Ribbon filter variants dominate in space-versus-time trade-offs with near continuous configurability and space overhead as low as 1 percent or less. Ribbon filters have O(1) query times and save roughly 1/3 of memory compared with Bloom filters.</span></p>
<h2><span>How it works: </span></h2>
<p><span>Like some related immutable structures used for perfect hashing and maps, Ribbon filters are constructed by solving a linear system given by hash functions applied to a set of keys. Each row in the linear system expresses that querying as some key, which involves XOR-ing the values at some set of array indices, must yield a prescribed value to indicate it is “in” the set of keys. </span></p>
<p><span>Despite using Boolean — GF(2) — arithmetic, the approach to solving this logical system of equations is to use Gaussian elimination, which fundamentally means subtracting equations from one another until you can isolate variables (unknowns). If a solution exists, this approach will find it.</span></p>
<p><span>The name Ribbon has dual meanings. First, we use a linear system from Dietzfelbinger and Walzer whose sorted coefficient matrix resembles a physical ribbon, or a wavy approximation of a band matrix. Gaussian elimination is fundamentally more efficient on this system because it is already close to a reduced form.</span></p>
<p><span>Ribbon also stands for Rapid Incremental Boolean Banding ON the fly, which is the name of our fast and flexible new Gaussian solver. Through an approach resembling insertion into a linear-probed hash table, Ribbon does Gaussian elimination on the fly. This saves time and space in construction because row reductions can be done in registers rather than in memory, and because the reduced form of the ribbon coefficient matrix — a band matrix — is more space-efficient than explicitly representing the ribbon form. On-the-fly construction also unlocks a solution to the core challenge of the Ribbon approach: scaling its space efficiency to very large numbers of keys.  </span></p>
<h2><span>Why it matters: </span></h2>
<p><span>At Facebook’s scale, we expect Ribbon filters to save several percent of RAM resources, with a tiny increase in CPU usage for some major storage systems. However, we do not implement efficiency gains at all engineering costs, so it’s also important to have a user-friendly data structure. This issue stalled implementation of other Bloom alternatives offering some space savings. </span></p>
<p><span>The Ribbon filter opens these new trade-offs without introducing notable discontinuities or hazards in the configuration space. In other words, there is some complexity to make Ribbon filters general and highly configurable, but these details can be hidden behind a relatively simple API. You have essentially free choice over any three of the four core performance dimensions — number of keys added to the set, memory usage, CPU efficiency, and accuracy — and the accuracy is automatically well optimized.</span></p>
<h2><span>Read the full paper: </span></h2>
<p><a href="https://arxiv.org/abs/2103.02515"><span>Ribbon filter: Practically smaller than Bloom and Xor</span></a></p>

		
	</div></div>]]></content:encoded>
      <author>By Peter Dillinger</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 09 Jul 2021 16:00:16 +0000</pubDate>
    </item>
    <item>
      <title>Asicmon: A platform agnostic observability system for AI accelerators</title>
      <link>https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/</link>
      <description>&lt;p&gt;We will be hosting a talk about our work on, “A Platform Agnostic Observability System for AI Accelerators” during our virtual Systems @Scale event at 10:20 a.m. PT on Wednesday, June 30, followed by a live Q&amp;#38;A session. Please submit any questions to systemsatscale@fb.com before the event. Accelerators are special-purpose hardware devices optimized for specific [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/&#34;&gt;Asicmon: A platform agnostic observability system for AI accelerators&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><b><i>We will be hosting a talk about our work on, “<a href="https://atscaleconference.com/events/systems-scale-summer-2021/">A Platform Agnostic Observability System for AI Accelerators</a>” during our virtual <a href="https://atscaleconference.com/events/systems-scale-summer-2021/">Systems @Scale</a> event at 10:20 a.m. PT on Wednesday, June 30, followed by a live Q&amp;A session. Please submit any questions to <a href="mailto:systemsatscale@fb.com">systemsatscale@fb.com</a> before the event.</i></b></p>
<p><span>Accelerators are special-purpose hardware devices optimized for specific applications, like AI prediction and video encoding. And Application-specific hardware platforms play an important role in meeting the growing latency and compute demands of workloads like deep learning, content understanding, and video encoding.</span></p>
<p><span>At Facebook, the inevitable rise in use of accelerators in our data centers has led to better performance and energy efficiency. However, it is challenging to operate these heterogeneous platforms efficiently at scale. To ensure that these complex accelerators operate smoothly, we need an excellent observability system with monitoring and tracing capabilities so we can understand the performance and interactions between CPUs and accelerators.</span></p>
<p><span>To meet these challenges, we’ve introduced three new tools:</span></p>
<ol>
<li><b>ASIC Monitoring (Asicmon)</b><span>, a scalable observability framework. Asicmon’s library abstracts an accelerator’s custom interfaces and provides a standard interface to our internal tools. Asicmon has facilitated load balancing, performance monitoring, and automated health checks for hundreds of thousands of accelerators running in our data centers.</span></li>
<li><b>Asimov</b><span>, a custom specification language that makes developing and rapid prototyping new accelerators easier. It has shrunk our development time for onboarding a new accelerator from a month to under a week.</span></li>
<li><b>Atrace</b><span>, an accelerator tracing solution that collects traces remotely on production servers. It allows us to inspect accelerator systems in detail and provides actionable trace summaries and analyses. An initial version of Atrace allowed us to close a 10 percent performance gap between </span><a href="https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/"><span>Caffe2 and PyTorch implementations</span></a><span> of a large AI model. </span></li>
</ol>
<h2><span>Background</span></h2>
<p><span>Facebook’s cloud infrastructure handles about 150 trillion AI predictions per day for tasks ranging from feed recommendations to combating harmful content. Running these AI models comes with heavy infrastructure demands. And as these models improve, so do their <a href="https://arxiv.org/pdf/2003.09518.pdf">computational requirements</a>.</span></p>
<p><span>The graph below of AI model adoption at Facebook illustrates this </span><a href="https://arxiv.org/pdf/2003.09518.pdf"><span>unmistakable pattern</span></a><span>.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?w=852" alt="" width="852" height="916" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png 988w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=852,916 852w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=768,826 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=953,1024 953w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=96,103 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=192,206 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>The need for accelerators</span></h2>
<p><span>Good old general-purpose processors (CPUs) offer versatility and have grown exponentially faster over the decades. However, CPUs fail to meet the rising</span><a href="https://openai.com/blog/ai-and-compute/"> <span>computational demands of AI applications</span></a><span> today. They also tend to exhibit inefficiency in terms of energy used per AI prediction. As investigated by the OpenAI community, we’ve seen </span><a href="https://openai.com/blog/ai-and-compute/"><span>two distinct eras of compute in AI models</span></a><span>. </span><span>In recent times, model complexity and compute requirements for AI have grown by roughly a factor of 10 each year. </span><span>This far outpaces improvements in CPU performance.   </span></p>
<p><span>How do we remedy this? By designing hardware that is customized to accelerate AI operations via application-specific integrated circuits (ASICs).  </span></p>
<p><span>Since 2019, Facebook has invested </span><a href="https://engineering.fb.com/2019/03/14/data-center-engineering/accelerating-infrastructure/"><span>heavily in deploying accelerator-based servers</span></a><span> to provide higher performance and energy efficiency. Today, our first-generation systems are 10-30x more performant on our largest AI models. They also delivered a 3-10x performance-per-watt improvement over a CPU.</span></p>
<p><span>We also invested in specialized hardware for </span><a href="https://engineering.fb.com/2021/04/05/video-engineering/how-facebook-encodes-your-videos/"><span>video encoding</span></a><span> and decoding. This enables Facebook to process the nearly 250 million videos uploaded to our app each day. These videos are viewable on any device and with varying internet bandwidth. Our first-generation video accelerators delivered a 10x performance-per-watt improvement in processing 4K videos.</span></p>
<p><span>The figure below illustrates the design of our AI inference server. As you can see, it consists of two Twin Lake CPUs and multiple accelerators (M.2 modules) connected to them using a PCIE switch</span><span>.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?w=1024" alt="" width="1024" height="615" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png 2000w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=916,550 916w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=768,461 768w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=1024,615 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=1536,922 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=96,58 96w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=192,115 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>The challenges of operating accelerators</span></h2>
<p><span>In your typical cloud server, the CPU represents the most complex component. We focus a lot on building software to efficiently operate the CPU and monitor its performance and availability. However, with an accelerator system, we can imagine the CPU now has a complicated and brawnier sibling! The accelerator, or ASIC, represents a complex hardware and software system in its own right.</span></p>
<p><span>To deliver an excellent user experience, the cloud infrastructure needs to keep hundreds of thousands of accelerators running reliably and efficiently. This is where observability systems come to our rescue. Observability allows us to understand what happens in the accelerator hardware and software when any issue arises. It is useful in multiple ways: </span></p>
<ol>
<li><b>Health monitoring:</b><span> Just like any other piece of hardware, accelerators can overheat or hit a faulty condition or a functional bug. We can track various health metrics for the ASICs and use them in automated systems. These systems can then (if needed) remediate the issue by rebooting the accelerator or moving it into a repair state.</span></li>
<li><b>Performance monitoring:</b><span> By monitoring the performance and system load on an accelerator, we can efficiently scale our AI jobs to meet variable demand throughout the day. It also enables us to detect regressions in performance with new software deployments.</span></li>
<li><b>Performance profiling:</b><span> When we encounter issues such as poor performance or time-outs, we need to look deeper into how the accelerator server is functioning. We also need to equip software developers with tools to understand the performance of their applications while they run on accelerators. </span></li>
</ol>
<h2><span>The accelerator zoo</span></h2>
<p><span>Specialization is both a boon and bane for accelerators. As a result, we end up running multiple types of accelerators in our data centers at any given point.</span></p>
<p><span>In 2020 we started</span><a href="https://engineering.fb.com/2019/03/14/data-center-engineering/accelerating-infrastructure/"> <span>deploying the first generation</span></a><span> of these accelerators. In the near future, we will be developing two to three new accelerators for the second generation. Each accelerator will have unique driver interfaces, making the task of operating them harder. But duplicating the observability software for each accelerator would not be feasible in the timeline we have set out. The observability framework must be easy to prototype and adapt to multiple types of accelerators in a short time. It also needs to be efficient to avoid interfering with the original application. </span></p>
<h2><span>How we developed Asicmon and Asimov</span></h2>
<p><span>Our first challenge involved finding a way to effectively monitor different types of accelerators without duplicating code (and developer time). As you may have guessed, we can leverage abstraction to achieve this. </span></p>
<p><span>For example, consider an abstract metric: </span><i><span>device_utilization</span></i><span> — the measure of how busy an accelerator is — which becomes useful for balancing load across accelerators. To compute this metric, we may need to understand the internal architecture of the accelerator. With an abstract counter, however, engineers working on load balancing can more easily use the metric without being aware of finer details.</span></p>
<p><i><span>device_utilization = max(compute_core_active_i) /  total_time </span></i></p>
<p><span>With the above in mind, we designed Asicmon with these design objectives: </span></p>
<ol>
<li><b>Abstraction:</b><span> We needed a simple and uniform interface for all of our internal monitoring and operational tools to use. This enables infrastructure engineers and hardware teams to effectively operate multiple accelerators in a common way.</span></li>
<li><b>Development velocity:</b><span> Accelerators are new. Interfaces can also change due to evolving requirements. The framework should be easy to learn and able to iterate quickly.</span></li>
<li><b>Performance:</b><span> Finally, any observability system should be lightweight in terms of resources. As a result, it diminishes interference with high-throughput video and AI applications.</span></li>
</ol>
<p><span>The diagram below illustrates the overall software stack for monitoring accelerators. Asicmon acts as a bridge between individual accelerator drivers and the rest of the internal monitoring software. The left top illustrates automated health check tools that spot bad health signals and</span><a href="https://engineering.fb.com/2020/12/09/data-center-engineering/how-facebook-keeps-its-large-scale-infrastructure-hardware-up-and-running/"> <span>automatically fix faulty ASICs</span></a><span>. On the right, a telemetry daemon periodically publishes performance metrics for engineers to inspect the accelerators. Furthermore, automated load balancing and auto-scaling systems like</span><a href="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/"> <span>Shard Manager</span></a><span> utilize these counters. </span><span> </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h3><span>How does Asicmon work?  </span></h3>
<p><span>Under the hood, Asicmon creates an instance of a monitoring module per accelerator device. It maintains a cache of statistics that it updates periodically by probing the accelerator driver and computing-derived metrics. Queries to Asicmon’s standard interface for counters get implemented as a lookup into this cache. This shields the system against accidental overload of counter requests.</span></p>
<h3><span>Enter Asimov</span></h3>
<p><span>All great so far! We used abstraction to address the scalability aspect of observability software layers above Asicmon. However, the problem of building the glue code between the accelerator driver and these standard metrics still eluded us. This has to be done separately for each of the accelerators that have aggressive and overlapping timelines. So, we needed a method to develop on Asicmon that was quick to iterate and easy to ramp up on, while also being efficient. That’s where Asimov comes in. </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png 1999w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Asimov is an expressive Python-like custom language to instrument the accelerator driver. It essentially allows developers to focus on how to probe the accelerator interfaces and express derived metrics using them. The Asimov compiler generates an efficient C++ implementation of the monitoring module. It also handles details like caching the metrics, periodically reading them, and providing thread safety.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The code snippets below show examples of Asimov being used to read system metrics using interfaces ranging from Linux sysfs files (a) to custom library C functions (b).</span></p>
<p><span>Asimov incorporates the same standard interface as Asicmon in its internal representation (the stats data structure, left hand side in the code). We can also invoke C-library functions provided by the device driver and express equations/conditions for derived metrics like any regular language.  </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Asimov is built with the</span><a href="https://www.antlr.org/"> <span>ANTLR</span></a><span> compiler framework under the hood to provide the lexer/parser logic for the language. We then emit C++ code using templates that manage all the essential parts, like initialization, thread safety, etc., so someone using Asimov doesn’t need to worry about it.</span></p>
<h2><span>Asicmon in action</span></h2>
<p><span>Let’s look at a few illustrative examples of how Asimov and Asicmon are beneficial for operating accelerators at scale.</span></p>
<p><span>For AI inference applications, we use a system called</span><a href="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/"> <span>Shard Manager</span></a><span> to automatically scale the inference service instances. A shard is essentially a copy of the AI model that can serve inferences. Asicmon measures the load on the device using an abstract metric — accelerator</span> <span>device</span> <span>utilization. This helps Shard Manager effectively balance the load among servers and automatically scale up or down the number of shards. The diagram below explains how the number of shards gets scaled automatically during model update rollouts and increases in traffic.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The figure below illustrates the advantages of building observability early on in a project’s development cycle. In our test deployment for video accelerators, we detected a memory leak using an Asicmon counter for available device memory. It took multiple fixes to the driver to finally resolve the issue, well in time before its debut in production.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Finally, let’s take a look at the ease of prototyping with Asimov. While we certainly took longer to build the first version of Asimov alongside the first video accelerator, supporting the second one (the AI inference accelerator) went incredibly fast. Bootstrapping basic metrics for the AI inference accelerator took less than a week. Since implementing Asicmon we’ve been able to increase our AI accelerator metrics support from ~30 percent to ~75 percent</span></p>
<h2><span>Atrace: Accelerator tracing at scale</span></h2>
<h3><span>Why tracing?</span></h3>
<p><span>Now that we can monitor the performance of accelerators in our data centers, the next step involves addressing why performance metrics like the latency and throughput change over time. The tried-and-tested method for CPUs involves leveraging a stack-based profiler to sample the running function call stack at periodic intervals. However, for inference accelerators, tracing is the best form of profiling. Why? Because accelerators use special hardware units and thus do not have an equivalent notion of a function stack on a core. </span></p>
<p><span>As shown in the figure below, a trace essentially consists of a time series of events occurring on different parts in a system. Events in a trace can represent, among many things, functions, execution of AI operators, or data transfers. Traces offer deeper insights into the operation of the system, including understanding the latency and scheduling of operators and how the CPU and accelerator interact with each other. </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h3><span>Designing the tracing system</span></h3>
<p><span>While AI inference accelerator vendors do provide tools and APIs to collect traces from the device. These tools are designed to work on a single server and are often hard to use. In order to profile production systems better, we set out building a layer on top of this native capability. This better scales out the collection, processing, and analysis of traces themselves. </span></p>
<p><span>We kept two target use cases in mind while developing Atrace: </span></p>
<ol>
<li><b>Model development:</b><span> Model developers would typically be attempting to target their AI models to new inference hardware. They can run the tracing tool locally. But by integrating it with internal visualization and summarization tools, we can provide quicker feedback to engineers to iteratively tune their model.</span></li>
<li><b>Production:</b><span> Debugging performance issues in production is an important use case for tracing. For instance, say a continuous integration (CI) test detects a regression in performance. By collecting traces remotely and on the fly, production engineers can quickly diagnose the problem.</span></li>
</ol>
<p><span>To develop a scalable and ubiquitous tracing solution, we built a set of components that remotely trigger and collect traces. We save each trace to a shared storage and post process and summarize it. The diagram below outlines this, starting on the left with the trace being triggered, to the trace collection and post processing on the right.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Insights from accelerator traces</span></h2>
<h3><span>Trace profiles and summaries</span></h3>
<p><span>Traces themselves can be enormous and overwhelming to dive into directly. However, we can learn a great deal about an AI program by summarizing the trace at a high level. To achieve this, we built a summary of trace statistics grouped by various AI operator types, as shown below.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>This operator breakdown shows our engineers which operators consume the most execution time and merit optimization. It also allows for comparisons and debugging of performance regressions between two software versions.</span></p>
<h3><span>Trace critical path analysis</span></h3>
<p><span>For advanced users, who might want to delve deeper into the traces, we added visualization support for both the open source</span><a href="https://www.chromium.org/developers/how-tos/trace-event-profiling-tool"> <span>Chrome trace viewer</span></a><span> and an internal trace visualization tool from Facebook. It works all from a single click. We can also run automated analysis on the trace to infer the critical path of operators. This uses the dependency graph of the AI model and trace statistics.</span></p>
<p><span>This analysis lets us optimize the latency of the AI prediction. It can also highlight issues like an imbalance in operators. Doing so closed a 10 percent latency gap between the Caffe2 and PyTorch versions of one of our AI models.</span></p>
<h3><span>Trace correlation</span></h3>
<p><span>Lastly, it is also noteworthy that several software layers exist to handle the processing of an inference request. These include the application layer, PyTorch framework, and</span><a href="https://engineering.fb.com/2018/09/13/ml-applications/glow-a-community-driven-approach-to-ai-infrastructure/"> <span>Glow</span></a><span>, an open source graph lowering compiler for accelerators.</span></p>
<p><span>For more complex models involving video understanding or natural language processing, we learned that the model may be run partially on a CPU and partially on an accelerator. Thus, tracing the operations across multiple layers on the CPU and correlating them with the accelerator becomes a necessity.</span></p>
<p><span>We developed a</span><a href="https://github.com/pytorch/glow/pull/5568"> <span>prototype of trace correlation</span></a><span> into Glow and PyTorch. This allowed us to connect operations on the CPU in the Glow runtime, to the accelerator. Trace correlation is important for examining the complex software stack used for AI inference.</span></p>
<h2><span>Next steps</span></h2>
<p><span>In addition to continuing to support next-generation AI and video accelerators using Asimov and the Asicmon we are also exploring:</span></p>
<ol>
<li><b>Open source specifications:</b><span> There are multitudes of companies building accelerator chips today. But the monitoring interfaces for accelerators lack standardization. We are collaborating with the</span><a href="https://www.opencompute.org/wiki/Server/ODSA"> <span>Open Domain-Specific Accelerators (ODSA)</span></a><span> project so the industry as whole can benefit from a common specification.</span></li>
<li><b>Trace visualization and analysis:</b><span> We are investigating ways to automatically generate optimization recommendations from the trace and support better visualizations, such as integrating with TensorBoard.</span></li>
<li><b>Distributed tracing:</b><span> Since microservices do not run in isolation, we plan on exploring how to correlate distributed traces collected by the Canopy distributed tracing tool with system-level accelerator traces. This would allow us to debug the end-to-end latency of microservices that use AI accelerators.</span></li>
</ol>
<h2><span>Thanks</span></h2>
<p><em><span>We would like to thank our many collaborators at Facebook, including Jerry Liu, Thiara Ortiz, Jeremy Yang, Ashwin Poojary, Deng Pan, Craig Ross, Ashwin Narasimha, Gisle Dankel, Michael Anderson, Allan Di Wu, Yinghai Lu, Satish Nadathur, Garret Catron, and Jack Montgomery for supporting us in creating this framework.</span></em></p>

		
	</div></div>]]></content:encoded>
      <author>By Brian Coutinho, Hao Wang, David Carrillo-Cisneros, Cynthia Liu, Parth Malani</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 28 Jun 2021 16:00:24 +0000</pubDate>
    </item>
  </channel>
</rss>