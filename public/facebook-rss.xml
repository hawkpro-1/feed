<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Facebook</title>
    <link>https://engineering.fb.com/rss</link>
    <description></description>
    <item>
      <title>Superpack: Pushing the limits of compression in Facebook’s mobile apps</title>
      <link>https://engineering.fb.com/2021/09/13/core-data/superpack/</link>
      <description>&lt;p&gt;Managing app size at Facebook is a unique challenge: Every day, developers check in large volumes of code, and each line of code translates into additional bits in the apps that people ultimately download onto their phones. Left unchecked, this added code would make the app bigger and bigger until eventually the time it takes [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/09/13/core-data/superpack/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/09/13/core-data/superpack/&#34;&gt;Superpack: Pushing the limits of compression in Facebook’s mobile apps&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>Managing app size at Facebook is a unique challenge: Every day, developers check in large volumes of code, and each line of code translates into additional bits in the apps that people ultimately download onto their phones. Left unchecked, this added code would make the app bigger and bigger until eventually the time it takes to download would become unacceptable. Compression is one of the methods we use to keep app size minimal. These compressed files take up less space, which means smaller apps that download faster and use less bandwidth for billions of users around the world. Such savings are especially important in regions where mobile bandwidth is limited, making it costly to download large apps. But compression alone isn’t enough to keep pace with all the updates we make and features we add to our apps. So we developed a technique called Superpack, which combines compiler analysis with data compression to uncover size optimizations beyond the capability of traditional compression tools. Superpack pushes the limits of compression to achieve significantly better compression ratios than existing compression tools.</span></p>
<p><span>Over the past two years, Superpack has been able to check developer-induced app size growth and keep our Android apps small. Superpack’s compression has helped reduce the size of our fleet of Android apps, which are substantially smaller in comparison to regular Android APK compression, with average savings of over 20 percent compared with Android’s default Zip compression. Some apps that use Superpack include Facebook, Instagram, WhatsApp, and Messenger. The reduction in the size of these apps thanks to Superpack is illustrated in the table below. </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg" alt="Table illustrating the reduction in the size of these apps thanks to Superpack" width="1920" height="1081" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=1024,577 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=1536,865 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_03_FINAL.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg" alt="Table showing percentage improvement in app size, thanks to Superpack " width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_02_FINAL.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Superpack: Compilers meet data compression</span></h2>
<p><span>While existing compression algorithms, such as Zip’s Deflate and Xz’s LZMA, work well with monolithic data, they weren’t enough to offset the pace of growth we were seeing in our apps, so we set out to develop our own solution. Compression is a mature field, and the techniques we’ve developed crosscut the entire compression spectrum, from data comprehension and Lempel-Ziv (LZ) parsing to statistical coding. </span></p>
<p><span>Superpack’s strength lies in compressing code, such as machine code and bytecode, as well as other types of structured data. The approach underlying Superpack is based on an insight in </span><a href="https://www.tandfonline.com/doi/abs/10.1080/00207166808803030"><span>Kolmogorov’s algorithmic measure of complexity</span></a><span>, which defines the information content of a piece of data as the length of the shortest program that can generate that data. In other words, data can be compressed by representing it as a program that generates the data. When that data is code to begin with, then it can be transformed into one with a smaller compressed representation. A program that generates Fibonacci numbers, coupled with a list of their indices, is a highly compressed representation of a file containing such numbers. The idea of reducing Kolmogorov complexity in itself is not new to the domain of compression. Superpack’s novel approach involves combining compiler methods with modern compression techniques to achieve this goal.</span></p>
<p><span>There is considerable benefit in formalizing compression as a generative process that produces small programs. It gives the data compression engineer access to a treasure trove of mature compiler tools and techniques that can be repurposed to the end of data compression. Superpack compression leverages common compiler techniques such as parsing and code generation, as well as more recent innovations such as </span><a href="https://dl.acm.org/doi/10.1145/1995376.1995394"><span>Satisfiability modulo theories (SMT) solvers</span></a><span> to find the smallest programs.</span></p>
<p><span>One important ingredient of Superpack’s effectiveness is its ability to marry these compiler techniques with those used in mainstream data compression. Semantic knowledge from the compiler half of Superpack leads to enhanced LZ parsing (the step in compression that eliminates redundancy), as well as improved entropy coding (the step that produces short codes for frequent pieces of information). </span></p>
<h3><span>Improved LZ parsing</span></h3>
<p><span>Compressors typically identify repeating sequences of bytes using an algorithm selected from the LZ family. Broadly, each such algorithm tries to substitute recurring sequences of data with pointers to their previous occurrences. The pointer consists of the distance in number of bytes to the previous occurrence, along with the length of the sequence. If the pointer can be represented in fewer bits than the actual data, then the substitution is a compressed-size win. Superpack improves the process of LZ parsing by enabling the discovery of longer repeating sequences while also reducing the number of bits to represent pointers.</span></p>
<p><span>In the programs being compressed, Superpack enables these improvements by grouping data based on its AST. For example, in the following sequence of instructions, the length of the longest repeating sequence is 2. However, when sorted into groups based on AST types, namely, the opcode and registers (Group 1 in the table below) and immediates (Group 2 in the table), the length increases to 4. In the raw parse of the original data, the distance between the repeated sequences is 2 instructions. But in the grouped version, the distance is 0. Smaller distances typically use fewer bits, and longer sequence matches save space by capturing more input data in a given pointer. Accordingly, the pointer that Superpack generates is smaller than the one computed naively.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg" alt="In the programs being compressed, Superpack enables these improvements by grouping data based on its AST. " width="1920" height="882" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=916,421 916w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=768,353 768w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=1024,470 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=1536,706 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=96,44 96w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-1-copy.jpg?resize=192,88 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>But how do we decide when to split the code stream and when to leave it intact? Recent work in Superpack introduces hierarchical compression, which incorporates this decision into the optimizing component of LZ parsing, called the optimal parse. In the edited code below, it is best to leave the last segment of the snippet in its original form, and generate a single match with a pointer to the first five instructions, while splitting the rest of the snippet. In the split-out remainder, the sparseness of register combinations is exploited to generate longer matches. Grouping the code in this manner also further reduces distances by counting the number of logical units between repeating occurrences, as measured along the AST, instead of measuring the number of bytes.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg" alt="Grouping the code in this manner also further reduces distances by counting the number of logical units between repeating occurrences, as measured along the AST, instead of measuring the number of bytes." width="1920" height="902" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=916,430 916w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=768,361 768w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=1024,481 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=1536,722 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-2-copy.jpg?resize=192,90 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h3><span>Improved entropy coding</span></h3>
<p><span>Repeating sequences of bytes are substituted efficiently with a pointer to the previous occurrence. But what does the compressor do for a nonrepeating sequence, or for short sequences that are cheaper to represent than a pointer? In such cases, compressors represent the data literally by coding the values in it</span><i><span>. </span></i><span>The number of bits used to represent a literal exploits the distribution of values that the literal can assume. Entropy coding is the process of representing a value using roughly as many bits as the entropy of the value in the data. Some well-known techniques that compressors use to this end include Huffman coding, arithmetic coding, range coding, and asymmetrical numeral systems (ANS).</span></p>
<p><span>Superpack has a built-in ANS coder, but also features a pluggable architecture that supports multiple such coding back ends. Superpack improves entropy coding by identifying contexts in which the literals to be represented have lower entropy. Like in the case of LZ parsing, the contexts are derived from Superpack’s knowledge of the structure of the data extracted via compiler analysis. In the reduced sequence of instructions below, there are seven different addresses, each with the prefix 0x. In a large volume of different arrangements of this code, the number of bits used by a regular coder to represent the address field would approach 3</span><i><span>.</span></i></p>
<p><span>However, we notice that three out of the seven</span> <span>addresses are paired with the BL opcode, while another</span> <span>three are associated with B. Only one is coupled with both. If this pattern were to hold true in the entire body of code, then the opcode can be used as a coding context. With this context, the number of bits to represent these seven addresses approaches 2</span> <span>instead of 3. The table below shows the coding with and without the context. In the Superpack-compressed case in the third column, the opcode can be seen as predicting the missing bit. This simple example was contrived to illustrate how compiler contexts can be used to improve coding. In real data, the number of bits gained is usually fractional, and the mappings between contexts and data are seldom as direct as in this example.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg" alt="n real data, the number of bits gained are usually fractional, and the mappings between contexts and data are seldom as direct as in this example." width="1920" height="773" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=916,369 916w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=768,309 768w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=1024,412 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=1536,618 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=96,39 96w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-3-copy.jpg?resize=192,77 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Programs as compressed representations</span></p>
<p><span>We explained how Superpack improves LZ parsing and entropy coding when the data being compressed consists of code. But what happens when the data contains a stream of unstructured values? In such cases, Superpack tries to lend the values structure by transforming them into programs at compression time. Then, at decompression time, the programs are interpreted to recover the original data. An example of this technique is the compression of Dex references,</span> <span>which are labels for well-known values in Dex code. Dex references have a high degree of locality. To exploit this locality, we transform references into a language that stores recent values in a logical register, and issues forthcoming values as deltas from the values that were pinned down.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg" alt="we transform references into a language that stores recent values in a logical register, and issues forthcoming values as deltas from the values that were pinned down." width="1920" height="353" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=916,168 916w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=768,141 768w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=1024,188 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=1536,282 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=96,18 96w, https://engineering.fb.com/wp-content/uploads/2021/09/superpack-blog-table-4-copy.jpg?resize=192,35 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Writing an efficient compressor for this representation reduces to the familiar register allocation problem in compilers, which decides when to evict values from registers to load new values. While this reduction is specific to reference bytecode,</span> <span>a general idea applies to any bytecode representation, namely, that the resulting code is amenable to the optimizations outlined in the previous two sections. In this example, LZ parsing is improved by cohorting the opcodes, MOV and PIN, in one group, collecting the deltas in a second group, and recent references in a third group.</span></p>
<h2><span>Superpack on real data</span></h2>
<p><span>There are three main payloads targeted by Superpack. The first is Dex bytecode, the format into which Java gets compiled in Android apps. The second is ARM machine code, which is code compiled for ARM processors. The third is Hermes bytecode, which is a specialized high performance bytecode representation of Javascript created at Facebook. All three representations use the full breadth of Superpack techniques, powered by compiler analysis based on a knowledge of the syntax and grammar of the code. In all three cases, there is one set of compression transforms that is applied to the stream of instructions and a different set that is applied to metadata. </span></p>
<p><span>The transforms applied to code are all alike. Metadata transforms have two parts. The first part leverages the structure of the data, by grouping items by type. The second part leverages organizing rules in the specification of the metadata, such as those that cause the data to be sorted or expose correlations between items that can be used to contextualize distances and literals.</span></p>
<p><span>The compression ratios yielded by Zip, Xz, and Superpack for these three formats are shown in the table below.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg" alt="The compression ratios yielded by Zip, Xz, and Superpack for these three formats are shown in the table below." width="1920" height="372" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=916,177 916w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=768,149 768w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=1024,198 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=1536,298 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=96,19 96w, https://engineering.fb.com/wp-content/uploads/2021/09/Screen-Shot-2021-09-07-at-6.09.14-PM-copy.jpg?resize=192,37 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Superpack architecture and implementation</span></h2>
<p><span>Superpack is a unique player in the compression space in that baked into it is knowledge of the types of data that it compresses. In order to scale the development and use of Superpack at Facebook, we developed a modular design with abstractions that could be reused across the different formats that we compress. Superpack is architected like an operating system, with a kernel that implements paged memory allocation, file and archive abstractions, abstractions for transforming and manipulating instructions, as well as interfaces to pluggable modules. </span></p>
<p><span>Compiler-oriented mechanisms fall into a dedicated compiler layer. Each format is implemented as a pluggable</span> <span>driver. Drivers exploit properties of the data being compressed, and label correlations in the code, to eventually be leveraged by the compression layer. The machinery that parses the input code uses automated inference based on an SMT solver. How we use SMT solvers to aid compression is beyond the scope of this post but will make a fascinating topic for a future blog post.</span></p>
<p><span>The compression layer also consists of pluggable modules. One of these modules is Superpack’s own compressor, which includes a custom LZ engine and an entropy coding back end. While we were in the process of building this compressor, we plugged in modules that leveraged existing compression tools to do the compression work. In that setting, Superpack’s role is reduced to reorganizing the data into uncorrelated streams. A best effort compression by an existing tool follows, which is effective but limited in the granularity at which it can identify and use compiler information. Superpack’s custom compression back end solves this problem through a fine-grained view of the internal representation of the data, which enables it to exploit logical correlations at the fine granularity of a single bit. Abstracting out the mechanism used to do the compression work as a module gives us a selection of a number of trade-offs between compression ratio and decompression speed.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg" alt="Abstracting out the mechanism used to do the compression work as a module gives us a selection of a number of tradeoffs between compression ratio and decompression speed." width="1920" height="1081" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=1024,577 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=1536,865 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/SuperPack_01_FINAL.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Superpack’s implementation contains a mix of code written in the OCaml programming language and C code. OCaml is used on the compression side to manipulate complex compiler-oriented data structures and to interface with an SMT solver. C is a natural choice for decompression logic because it tends to be simple and at the same time is highly sensitive to the parameters of the processor on which the decompression code runs, such as the size of the L1 cache.</span></p>
<h3><span>Limitations and related work</span></h3>
<p><span>Superpack is an asymmetric compressor, which means that decompression is fast but compression is allowed to be slow. Streaming compression, in which data is compressed at the rate at which it is transmitted, has been a nongoal of Superpack. Superpack is unable to fit the constraints for this use case, as its present compression speed is not able to keep up with modern data transfer rates. Superpack has been applied to structured data, code, integer, and string data. It does not currently target image, video, or sound files.</span></p>
<p><span>On the Android platform, there is a trade-off between using compression to reduce download time and a possible increase in disk footprint and update size. This trade-off is not a limitation of Superpack, rather that interoperability has not yet been established between the packaging tools used by Facebook and the distribution tools used on Android. For example, on Android, app updates are distributed as deltas between the contents of consecutive versions of an app. But such deltas can only be generated by tools that are able to decompress and recompress the app’s contents. Since the diffing process implemented in the current tooling is not able to interpret Superpack archives, the deltas come out to be larger for apps containing such archives. We believe that issues of this type could be addressed through finer-grained interfaces between Superpack and Android tools, increased customizability in Android’s distribution mechanisms, and a public documentation of Superpack’s file format and compression methods. Facebook’s apps are dominated by code of the type that Superpack excels at compressing, in a way that goes far beyond existing compression implemented as part of Google Play on Android. So, for the time being, our compression is beneficial to our users despite the trade-off.</span></p>
<p><span>Superpack leverages Jarek Duda’s work on </span><a href="https://arxiv.org/abs/0902.0271"><span>asymmetrical numeral systems</span></a><span> as its entropy coding back end. Superpack draws on ideas in </span><a href="https://dl.acm.org/doi/abs/10.1145/36177.36194"><span>superoptimization</span></a><span>, along with past work on </span><a href="https://dl.acm.org/doi/abs/10.1145/258916.258947"><span>code compression</span></a><span>. It leverages the X</span><a href="https://tukaani.org/xz/"><span>z</span></a><span>, </span><a href="https://facebook.github.io/zstd/"><span>Zstd</span></a><span>, and </span><a href="https://github.com/google/brotli"><span>Brotli</span></a><span> compressors as optional back ends to do its compression work. Finally, Superpack uses Microsoft’s </span><a href="https://github.com/Z3Prover/z3"><span>Z3 SMT solver</span></a><span> to automatically parse and restructure a wide range of code formats.</span></p>
<h2><span>What’s next</span></h2>
<p><span>Superpack combines compiler and data compression techniques to increase the density of packed data in a way that is especially applicable to code such as Dex bytecode and ARM machine code. Superpack has substantially cut the size of our Android apps, and consequently saved billions of users around the world download time. We have described some of the core ideas underlying Superpack but have only scratched the surface of our work in asymmetric compression. </span></p>
<p><span>Our journey has only just begun. Superpack continues to improve through enhancements to both its compiler and compression components. Superpack started out as a tool to cut mobile app size, but our success in improving the compression ratio of a variety of data types has led us to target other use cases of asymmetric compression. We are working on a new on-demand executable file format that saves disk space by keeping shared libraries compressed and decompressing them at load time. We are evaluating using Superpack for delta compression of code to reduce the size of software updates. We are also investigating using Superpack as a cold-storage compressor, to compress log data and files that are rarely used.</span></p>
<p><span>Until now, our mobile deployment has been limited to our Android apps. However, our work is equally applicable to other platforms, such as iOS, and we are looking into porting our implementation to those platforms. Presently, Superpack is available only to our engineers, but we aspire to bring the benefits of Superpack to everyone. To this end, we are exploring ways to improve the compatibility of our compression work with the Android ecosystem. This blog post is a step in this direction. We may someday consider open sourcing Superpack.</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Sapan Bhatia</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/09/Superpack_Hero_FINAL.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 13 Sep 2021 13:00:26 +0000</pubDate>
    </item>
    <item>
      <title>How WhatsApp is enabling end-to-end encrypted backups</title>
      <link>https://engineering.fb.com/2021/09/10/security/whatsapp-e2ee-backups/</link>
      <description>&lt;p&gt;For years, in order to safeguard the privacy of people’s messages, WhatsApp has provided end-to-end encryption by default ​​so messages can be seen only by the sender and recipient, and no one in between. Now, we’re planning to give people the option to protect their WhatsApp backups using end-to-end encryption as well. People can already [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/09/10/security/whatsapp-e2ee-backups/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/09/10/security/whatsapp-e2ee-backups/&#34;&gt;How WhatsApp is enabling end-to-end encrypted backups&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>For years, in order to safeguard the privacy of people’s messages, WhatsApp has provided <a href="https://engineering.fb.com/2021/04/16/security/dit/">end-to-end encryption</a> by default ​​so messages can be seen only by the sender and recipient, and no one in between. Now, we’re planning to give people the option to protect their WhatsApp backups using end-to-end encryption as well.</span></p>
<p><span>People can already back up their WhatsApp message history via cloud-based services like Google Drive and iCloud. WhatsApp does not have access to these backups, and they are secured by the individual cloud-based storage services. </span></p>
<p><span>But now, if people choose to enable end-to-end encrypted (E2EE) backups once available, neither WhatsApp nor the backup service provider will be able to access their backup or their backup encryption key. </span></p>
<h2><span>How E2EE backups work</span></h2>
<h3><span>Generating encryption keys and passwords </span></h3>
<p><span>To enable E2EE backups, we developed an entirely new system for encryption key storage that works with both iOS and Android. With E2EE backups enabled, backups will be encrypted with a unique, randomly generated encryption key. People can choose to secure the key manually or with a user password. When someone opts for a password, the key is stored in a Backup Key Vault that is built based on a component called a hardware security module (HSM) </span>— <span>specialized, secure hardware that can be used to securely store encryption keys. When the account owner needs access to their backup, they can access it with their encryption key, or they can use their personal password to retrieve their encryption key from the HSM-based Backup Key Vault and decrypt their backup. </span></p>
<p><span>The HSM-based Backup Key Vault will be responsible for enforcing password verification attempts and rendering the key permanently inaccessible after a limited number of unsuccessful attempts to access it. These security measures provide protection against brute-force attempts to retrieve the key. WhatsApp will know only that a key exists in the HSM. It will not know the key itself.</span></p>
<h3><span>Storing keys in the Backup Key Vault</span></h3>
<p><span>WhatsApp’s front-end service, ChatD, handles client connections and client-server authentication, and will implement a protocol that sends the keys to the backups to and from WhatsApp’s servers. The client and HSM-based Backup Key Vault will exchange encrypted messages, the contents of which will not be accessible to ChatD itself.</span><span><br/>
</span></p>
<p><span>The HSM-based Backup Key Vault will sit behind ChatD and provide highly available and secure storage for the encryption keys to the backups. The backups themselves will be generated as a continuous stream of data that is encrypted using symmetric encryption with the generated key. With E2EE backups enabled, upon being encrypted, a backup can then be stored off device (e.g., to iCloud or Google Drive). </span></p>
<p><span>WhatsApp serves over 2 billion people, and one of the core challenges of this product was to make sure the HSM-based Backup Key Vault operates reliably. To help ensure that the system is always available, the HSM-based Backup Key Vault service will be geographically distributed across multiple data centers to keep it up and running in case of a data center <a href="https://engineering.fb.com/2021/06/02/data-center-engineering/how-facebook-deals-with-pcie-faults-to-keep-our-data-centers-running-reliably/">outage</a>.</span></p>
<figure id="attachment_18102" aria-describedby="caption-attachment-18102"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?w=1024" alt="WhatsApp end-to-end encrypted backups " width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg 3840w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_64-digit-encryption.jpeg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-18102">Backups can be end-to-end encrypted using a 64-digit encryption key.</figcaption></figure>
<figure id="attachment_18101" aria-describedby="caption-attachment-18101"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?w=1024" alt="WhatsApp end-to-end encrypted backups " width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg 3840w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/09/WhatsApp_E2EE-Backups_user-password.jpeg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-18101">Backups can also be secured with a password, in which case the encryption key is saved to the HSM-based Backup Key Vault.</figcaption></figure>
<h3><span>The HSM-based Backup Key Vault and the encryption and decryption process</span></h3>
<p>When the account owner uses a personal password to protect their end-to-end encrypted backup, the HSM-based Backup Key Vault will store and safeguard it.</p>
<p><span>When someone wants to retrieve their backup:</span></p>
<ol>
<li><span>They enter their password, which is encrypted and then verified by the Backup Key Vault.</span></li>
<li><span>Once the password is verified, the Backup Key Vault will send the encryption key back to the WhatsApp client.</span></li>
<li><span>With the key in hand, the WhatsApp client can then decrypt the backups.</span></li>
</ol>
<p><span>Alternatively, if an account owner has chosen to use the 64-digit key alone, they will have to manually enter the key themselves to decrypt and access their backups. </span></p>
<p><span>E2EE backups will be available on iOS and Android in the coming weeks. Check out the <a href="https://www.whatsapp.com/security/WhatsApp_Security_Encrypted_Backups_Whitepaper.pdf">end-to-end encrypted backups white paper</a> to learn more about the technical details. </span></p>

		
	</div></div>]]></content:encoded>
      <author>By Slavik Krassovsky, Gabriel Cadden</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/09/Whatsapp_E2EE-Backups_EngBlog-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Fri, 10 Sep 2021 15:00:12 +0000</pubDate>
    </item>
    <item>
      <title>CacheLib, Facebook’s open source caching engine for web-scale services</title>
      <link>https://engineering.fb.com/2021/09/02/open-source/cachelib/</link>
      <description>&lt;p&gt;Caching plays an important role in helping people access their information efficiently. For example, when an email app loads, it temporarily caches some messages, so the user can refresh the page without the app retrieving the same messages. However, large-scale caching has long been a complex engineering challenge. Companies must balance the fast experience people [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/09/02/open-source/cachelib/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/09/02/open-source/cachelib/&#34;&gt;CacheLib, Facebook’s open source caching engine for web-scale services&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div id="single-wrapper" tabindex="-1">
		<main id="main">

			
				
<article id="post-18046">

	

			<figure id="post-feat-image-container">
				<img width="1920" height="1080" src="https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_hero.jpg" alt="CacheLib, Facebook’s open source caching engine for web-scale services" loading="lazy" title="CacheLib_hero"/>						</figure>
		
	

	
	<div>

		<p><span>Caching plays an important role in helping people access their information efficiently. For example, when an email app loads, it temporarily caches some messages, so the user can refresh the page without the app retrieving the same messages. However, large-scale caching has long been a complex engineering challenge. Companies must balance the fast experience people have come to expect from caching with keeping systems highly performant and cost-effective. Traditionally, each cache implementation is created and maintained independently by different engineering teams. This approach isn’t efficient, since it ignores different caching systems’ shared challenges, from deployment to maintenance. </span></p>
<p><span>As traditional dynamic random-access memory (DRAM) caches become more expensive and require more power to scale, companies like Facebook are exploring hardware choices such as non-volatile memory (NVM) drives to augment their caching systems. This DRAM and NVM hybrid model is a step forward, but innovative caching designs are needed to harness the full potential of the hybrid cache. This includes new caching heuristics research that must push the boundaries of traditional systems by identifying the relevant content to cache for the right duration. We have consolidated these innovations and taken them a step further through collaborations and open source work. </span></p>
<p><span>Today, we’re announcing the release of </span><a href="https://github.com/facebookincubator/CacheLib"><span>CacheLib</span></a><span>, a pluggable in-process caching engine to </span><a href="https://www.usenix.org/conference/osdi20/presentation/berg"><span>build and scale high-performance services</span></a><span> collaboratively. </span><span>CacheLib’s C++ library</span> <span>enables developers to build and customize scalable and concurrent caches through its simple API. We are also open-sourcing </span><a href="https://cachelib.org/docs/Cache_Library_User_Guides/Cachebench_Overview/"><span>CacheBench</span></a><span>, a benchmarking tool for evaluating caching performance on diverse production workloads.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg" alt="CacheLib’s C++ library enables developers to build and customize scalable and concurrent caches through its simple API." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_chart-copy.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>CacheLib is leveraged as an in-process cache in more than 70 large-scale systems at Facebook, including the social graph, content delivery network, storage, and </span><a href="https://research.fb.com/publications/scaling-memcache-at-facebook/"><span>look-aside key-value caches</span></a><span>. This existing scale and the potential for open source adoption make CacheLib an aggregation point for optimizations and CacheBench an effective benchmarking tool for evaluating new ideas across diverse caching applications.</span></p>
<h2><span>Enabling innovation through partnerships</span></h2>
<p><span>As an open source platform, CacheLib and CacheBench have the potential to become an industry standard for caching innovations and benchmarking. To date, our collaborations with research universities, hardware manufacturers, and software companies have yielded substantial results that show the value of this toolkit.  </span></p>
<p><span>Over the past two years, we have partnered with many well-known organizations to push the boundaries of caching innovation. Today, we are working with Twitter on integrating CacheLib into </span><a href="https://github.com/twitter/pelikan"><span>Pelikan.io</span></a><span> to enable SSDs for caching objects within the Twitter infrastructure. Pinterest is evaluating the adoption of CacheLib within its machine learning infrastructure systems to improve prediction performance and system stability.</span></p>
<p><span>In academia, researchers at Carnegie Mellon University, Princeton University, and Yale University are using CacheLib and CacheBench to </span><a href="https://www.pdl.cmu.edu/PDL-FTP/NVM/McAllister-SOSP21.shtml"><span>prototype research ideas</span></a><span>. By evaluating their prototypes against industry caching workloads, these researchers can iterate on their projects much more quickly and accurately than before.</span></p>
<p><span>We have also collaborated with hardware industry partners like Intel, KIOXIA, Samsung, and Western </span><span>Digital</span><span> to standardize and enhance SSD technologies which enable improved caching solutions. This work is now part of the </span><a href="https://www.opencompute.org/documents/nvme-cloud-ssd-specification-v1-0-3-pdf"><span>Open Compute Project (OCP) NVMe Cloud SSD Specification</span></a><span>, which we discussed in </span><a href="https://www.opencompute.org/events/past-events/webinar-data-center-nvme-ssd-and-edsff-presented-by-facebook-sk-hynix-kioxia-intel-snia"><span>this webinar</span></a><span>. This specification, along with CacheLib, will help adapt future NVM technologies for caching workloads across the industry.  </span></p>
<p><span>CacheLib and CacheBench have enormous potential to shape the future of caching, thanks to its developer-friendly API, access to many benchmark workloads across the industry, and the collaborative nature of open source. We are thankful for our partners’ support and contributions in using the platform to drive innovation in such an important and complex area. We are open-sourcing this work in an effort to make building the future of caching a more collaborative and open space for sharing across the entire industry. <a href="http://www.cachelib.org/">Read more about the project at </a><span role="gridcell">Cachelib.org.</span></span></p>


		
	</div>


</article>








					
	 

		
				
			
		</main>

	</div></div>]]></content:encoded>
      <author>By Sathya Gunasekar, Snehal Khandkar, Dmitry Vinnik, Michael Cheng</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/08/CacheLib_hero.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 02 Sep 2021 16:00:27 +0000</pubDate>
    </item>
    <item>
      <title>RAMP-TAO: Layering atomic transactions on Facebook’s online graph store</title>
      <link>https://engineering.fb.com/2021/08/18/core-data/ramp-tao/</link>
      <description>&lt;p&gt;What the research is:  RAMP-TAO is a new protocol that improves the developer experience on TAO, Facebook’s online social graph store, by providing stronger transactional guarantees. It is the first protocol to provide transactional semantics over an eventually consistent massive-scale data store while still preserving the system’s overall reliability and performance. RAMP-TAO enables an intuitive [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/08/18/core-data/ramp-tao/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/08/18/core-data/ramp-tao/&#34;&gt;RAMP-TAO: Layering atomic transactions on Facebook’s online graph store&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><article id="post-18025">

	

			<figure id="post-feat-image-container">
				<img width="824" height="466" src="https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg" alt="HHVM Jump-Start: Boosting the performance of virtual machines" loading="lazy" title="RiB_LightNavy"/>						</figure>
		
	

	
	<div>

		<h2><span>What the research is: </span></h2>
<p><span>RAMP-TAO is a new protocol that improves the developer experience on </span><a href="https://engineering.fb.com/2013/06/25/core-data/tao-the-power-of-the-graph/"><span>TAO</span></a><span>, Facebook’s online social graph store, by providing stronger transactional guarantees. It is the first protocol to provide transactional semantics over an eventually consistent massive-scale data store while still preserving the system’s overall reliability and performance. RAMP-TAO enables an intuitive read transaction API, so developers do not have to investigate and handle rare anomalies.</span></p>
<p><span>Our research has demonstrated that RAMP-TAO can be feasibly deployed in production with 0.42 percent memory overhead and enables the vast majority (over 99.9 percent) of reads to complete in one round-trip to the local cache with tail latency on par with existing TAO reads.</span></p>
<h2><span>How it works:  </span></h2>
<p><span>RAMP-TAO draws inspiration from the </span><a href="https://people.eecs.berkeley.edu/~alig/papers/ramp.pdf"><span>Read Atomic Multi-Partition (RAMP) protocols</span></a><span>, which prevent fractured reads but impose unacceptably high performance and storage overheads for TAO.</span></p>
<p><span>The RAMP-TAO protocol provides stronger guarantees by layering them on top of TAO and is similar in spirit to the </span><a href="https://people.eecs.berkeley.edu/~alig/papers/bolt-on-causal-consistency.pdf"><span>“bolt-on” approach</span></a><span>. Since TAO, like most systems, ensures that all data is eventually consistent, we only need to guard against fractured reads for recent, transactionally updated data. We leverage this knowledge to minimize performance and storage overheads, especially for the existing TAO workloads that are not transactional.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?w=999" alt="Ramp tao" width="999" height="314" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png 999w, https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?resize=916,288 916w, https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?resize=768,241 768w, https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?resize=96,30 96w, https://engineering.fb.com/wp-content/uploads/2021/08/RAMP-TAO-chart.png?resize=192,60 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Why it matters: </span></h2>
<p><span>TAO serves billions of reads and millions of writes per second and supports many of Facebook’s applications. It has traditionally prioritized availability and scalability to serve its large, read-dominant workloads. By layering stronger transactional guarantees on top of the existing system, we can provide more intuitive system behavior while retaining its reliability and efficiency. This strategy also enables RAMP-TAO to be cache-friendly, hotspot tolerant, and extensible to different data stores. Moreover, we incur overhead only for applications that opt in, rather than causing every application to take a performance hit.</span></p>
<p><span>Although we focus on TAO in this work, these properties are crucial to large-scale, read-optimized systems. Our layered approach can be a practical solution for other systems, many of which have sought to strengthen their guarantees. </span></p>
<h2><span>Read the full paper:</span></h2>
<p><a href="http://www.vldb.org/pvldb/vol14/p3014-cheng.pdf"><span>RAMP-TAO: Layering atomic transactions on Facebook’s online TAO data store</span></a></p>

		
	</div>


</article></div>]]></content:encoded>
      <author>By Audrey Cheng</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 18 Aug 2021 16:00:37 +0000</pubDate>
    </item>
    <item>
      <title>Apricot subsea cable will boost internet capacity, speeds in the Asia-Pacific region</title>
      <link>https://engineering.fb.com/2021/08/15/connectivity/apricot-subsea-cable/</link>
      <description>&lt;p&gt;We are excited to announce our participation in the Apricot subsea cable system, together with leading regional and global partners. When completed, the project (which is still subject to regulatory approvals) will deliver much-needed internet capacity, redundancy, and reliability to expand connections in the Asia-Pacific region. The 12,000-kilometer-long cable will connect Japan, Taiwan, Guam, the [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/08/15/connectivity/apricot-subsea-cable/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/08/15/connectivity/apricot-subsea-cable/&#34;&gt;Apricot subsea cable will boost internet capacity, speeds in the Asia-Pacific region&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><article id="post-18020">

	

			<figure id="post-feat-image-container">
				<img width="1920" height="1280" src="https://engineering.fb.com/wp-content/uploads/2021/08/Apricot.jpg" alt="Apricot subsea cable will boost internet capacity, speeds in the Asia-Pacific region" loading="lazy"/>						</figure>
		
	

	
	<div>

		<p><span>We are excited to announce our participation in the Apricot subsea cable system, together with leading regional and global partners. </span><span>When completed, the project (which is still subject to regulatory approvals) will deliver much-needed internet capacity, redundancy, and reliability to expand connections in the Asia-Pacific region.</span></p>
<p><span>The 12,000-kilometer-long cable will connect Japan, Taiwan, Guam, the Philippines, Indonesia and Singapore. Apricot will feature a state of the art configuration allowing flexibility in trunk and branch capacity.</span></p>
<p><span>Expected to launch in 2024, the Apricot cable system will have an initial design capacity of more than 190 terabits per second to meet rising data demands in the region and support existing cable systems, such as the recently announced </span><a href="https://engineering.fb.com/2021/03/28/connectivity/echo-bifrost/"><span>Echo and Bifrost cable systems</span></a><span>. The cable will help meet the growing demand for 4G, 5G, and broadband access in the region.</span></p>
<p><span>The Apricot cable is part of our ongoing effort to expand global network infrastructure and better serve the more than 3.5 billion people around the world who use our services every month. To accomplish this, we collaborate with partners all over the world to build subsea fiber-optic cables. Apricot is the latest example of our innovative partnership model, in which all parties  benefit from developing scale infrastructure and shared technology expertise.</span></p>

		
	</div>


</article></div>]]></content:encoded>
      <author>By Nico Roehrich</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/08/Apricot.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 16 Aug 2021 00:30:20 +0000</pubDate>
    </item>
    <item>
      <title>Open-sourcing a more precise time appliance</title>
      <link>https://engineering.fb.com/2021/08/11/open-source/time-appliance/</link>
      <description>&lt;p&gt;Facebook engineers have built and open-sourced an Open Compute Time Appliance, an important component of the modern timing infrastructure. To make this possible, we came up with the Time Card — a PCI Express (PCIe) card that can turn almost any commodity server into a time appliance. With the help of the OCP community, we [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/08/11/open-source/time-appliance/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/08/11/open-source/time-appliance/&#34;&gt;Open-sourcing a more precise time appliance&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>Facebook engineers have built and open-sourced an Open Compute Time Appliance, an important component of the modern timing infrastructure.</span></li>
<li aria-level="1"><span>To make this possible, we came up with the Time Card — a PCI Express (PCIe) card that can turn almost any commodity server into a time appliance. </span></li>
<li aria-level="1"><span>With the help of the OCP community, we established the </span><a href="http://ocptap.com"><span>Open Compute Time Appliance Project</span></a><span> and open-sourced every aspect of the </span><a href="http://opentimeserver.com/"><span>Open Time Server</span></a><span>.</span></li>
</ul>
<p><span>In March 2020, we announced that we were in the process of switching over the servers in our data centers (together with our consumer products) to </span><a href="https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/"><span>a new timekeeping service based on the Network Time Protocol (NTP)</span></a><span>. The new service, built in-house and later open-sourced, was more scalable and improved the accuracy of timekeeping in the Facebook infrastructure from 10 milliseconds to 100 microseconds. </span><span>More accurate time keeping enables more advanced infrastructure management across our data centers, as well as faster performance of distributed databases.</span></p>
<p><span>The new NTP-based time architecture uses a </span><a href="https://engineering.fb.com/2020/03/18/production-engineering/ntp-service/"><span>Stratum 1</span></a><span> — an important component that is directly linked to an authoritative source of time, such as a global navigation satellite system (GNSS) or a cesium clock.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg" alt="Figure showing Stratums 0 through 3. Stratum 1 is an important component which is a device directly linked to an authoritative source of time, such as a Global Navigation Satellite System or a Cesium clock." width="1921" height="1081" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-1-v1.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Many companies rely on public NTP pools such as time.facebook.com to act as their Stratum 1. However, this approach has its drawbacks. These pools add dependency on internet connectivity and can impact overall security and reliability of the system. For instance, if connectivity is lost or an external service is down, it can result in outages or drift in timing for the dependent system.</span></p>
<p><span>To remove these dependencies, we’ve built a new dedicated piece of hardware called Time Appliance, which consists of a GNSS receiver and a miniaturized atomic clock (MAC). Users of time appliances can keep accurate time, even in the event of GNSS connectivity loss. While building our Time Appliance, we also invented a Time Card, a PCIe card that can turn any commodity server into a time appliance.</span></p>
<h2><span>Why do we need a new time device?</span></h2>
<p><span>Off-the-shelf time appliances have their own benefits. They work right out of the box and because many of these devices have been on the market for decades, they are battle-tested and generally stable enough to work without supervision for a long time.</span></p>
<p><span>However, these solutions also come with trade-offs:</span></p>
<ul>
<li aria-level="1"><span>In most cases, they are outdated and often vulnerable to software security concerns. Feature requests and security fixes may take months or even years to implement.</span></li>
<li aria-level="1"><span>These devices come with closed source software, which makes configuring and monitoring them limited and challenging. While configuration is done manually via a proprietary CLI or Web UI, monitoring often uses SNMP, a protocol that was not designed for this purpose.</span></li>
<li aria-level="1"><span>They include proprietary hardware that is not user-serviceable. When a single component breaks, there is no easy way to replace it. You have to either ship it to the vendor for repair or buy an entire new appliance.</span></li>
<li aria-level="1"><span>Since off-the-shelf devices are made in low quantities, they come with a higher markup and can become very costly to operate over time. The high cost associated with off-the-shelf devices create limitations for many in the industry. An open source version would open the door to broader applications.</span></li>
</ul>
<p><span>Until now, companies have had to accept these trade-offs and work within the constraints described above. We</span> <span>decided it was time to try something different, so we took a serious look at what it would take to build our new Time Appliance — specifically, one using the x86 architecture.</span></p>
<h2><span>Prototyping the Time Appliance </span></h2>
<p><span>Here’s a block diagram of what we envisioned:</span></p>
<p><span><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg" alt="Block diagram of the time appliance prototype" width="1921" height="1081" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-03-v3.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/> </span><span><br/>
</span><span>It all starts from a GNSS receiver that provides the time of day (ToD) as well as the one pulse per second (PPS). When the receiver is backed by a high-stability oscillator (e.g., an atomic clock or an oven-controlled crystal oscillator), it can provide time that is nanosecond-accurate. The time is delivered across the network via an off-the-shelf network card which supports PPS in/out and hardware time stamping of packets, such as the NVIDIA Mellanox ConnectX-6 Dx used in our initial appliance.</span></p>
<p><span>The output of the GPS disciplined oscillator (GPSDO) was fed into the EXT time-stamping of the ConnectX-6 Dx network card. In addition, the GNSS receiver provides the ToD via a serial port and a popular GPS reporting protocol called NMEA. Using </span><span>ts2phc</span><span> tool allowed us to synchronize the physical hardware clock of the network interface controller (NIC) down to a couple of tens of nanoseconds, as shown below:</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg" alt="block of code " width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/code-grid.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Our prototype gave us confidence that building such an appliance was possible. However, there was a lot of room for improvement.</span></p>
<p><span>To increase the reliability of the system, we divided it into two major parts: payload and delivery. The payload is the precision time that is essentially an interpolation system driven by a local oscillator to create nanoseconds of time measurement between consecutive PPS signals received by the GNSS receiver. We considered putting the GNSS receiver, the high-stability local oscillator, and the necessary processing logic into a PCIe form factor, and we called it the Time Card.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg" alt="Block diagram of the time card functionality" width="1921" height="1081" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-06-v3.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Here is the sketch of the Time Card we initially envisioned on a napkin:</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg" alt="Original design for the time appliance" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-7-v4.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>We used an onboard MAC, a multiband GNSS receiver, and a field-programmable gate array (FPGA) to implement the time engine. The time engine’s job is to interpolate in nanoseconds the granularity required between consecutive PPS signals. The GNSS receiver also provides a ToD in addition to a 1 PPS signal. In the event of the loss of GNSS reception, the time engine relies on the ongoing synchronization of the atomic clock based on an average ensemble of the consecutive PPS pulses. </span></p>
<p><span>The time engine consists of a set of processing blocks implemented on the FPGA of the Time Card. These processing blocks include various filtering, synchronization, error checking, time-stamping, and PCIe-related subsystems to allow the Time Card to perform as a system peripheral that provides precision time for the open time server.</span></p>
<p><span>It should be noted that the accuracy of a GNSS receiver is within tens of nanoseconds, while the required ongoing synchronization (calibration) of the MAC is within 10 picoseconds (1,000 times more accurate). </span></p>
<p><span>At first, this sounds impossible. However, the GNSS system provides timing based on continuous communication with standard time. This ability allows the GNSS onboard clock to be constantly synchronized with a source of time provided to its constellation, giving it virtually no long-term drifting error. Therefore, the MAC’s calibration is performed via a comparison of a MAC-driven counter and the GNSS-provided PPS pulse. Taking more time for the comparison allows us to achieve a higher precision of calibration for the MAC. Of course, this is with the consideration that the MAC is a linear time invariant system.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg" alt="In this block diagram, you can see a 10 MHz signal from the rubidium clock entering the time engine. The clock signal feeds into a digital clock module and a digital PLL, resulting in a 125 MHz frequency. The 125 MHz feeds into the ToD unit." width="1921" height="1479" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=916,705 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=768,591 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=1024,788 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=1536,1183 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=96,74 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-8-v3.jpg?resize=192,148 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>In this block diagram, you can see a 10 MHz signal from the rubidium clock entering the time engine. This clock signal can be replaced by a 10 MHz SMA input. The clock signal feeds into a digital clock module and a digital PLL (12.5x resulted from 25 up and divided by 2), resulting in a 125 MHz frequency. The 125 MHz (8-nanosecond periods) feeds into the ToD unit.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg" alt="Block diagram showing the filtering process" width="1921" height="1101" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=916,525 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=768,440 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=1024,587 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=1536,880 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=96,55 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-9-v3.jpg?resize=192,110 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The ToD unit associates the 8-nanosecond increments in digital values of 0b000001 since the LSB (least significant bit) is associated to 250 picoseconds (driven from 32 bits of subsecond accuracy on the gPTP).</span></p>
<p><span>On the other hand, the PPS signal coming filtered from the GNSS is used to snapshot the result of the increments. If the 125 MHz is accurate, the accumulated increments should result in exactly 1-second intervals. However, in reality, there is always a mismatch between the accumulated value and a theoretical 1-second interval.</span></p>
<p><span><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg" alt="" width="1921" height="850" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=916,405 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=768,340 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=1024,453 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=1536,680 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=96,42 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-10-v3.jpg?resize=192,85 192w" sizes="(max-width: 992px) 100vw, 62vw"/> </span><span><br/>
</span><span>The values can be adjusted using an internal PI (proportional and integral) control loop. The adjustment can be done by either altering the 0b000001 value by steps of 250 picoseconds or fine-tuning the 12.5x PPL. In addition, further (more finely tuned) adjustments can be applied by steering the rubidium oscillator. </span></p>
<p><span>The longer a GNSS isn’t available, the more time accuracy is lost. The rate of the time accuracy deterioration is called holdover. Usually, holdover is described as a timeframe for accuracy and how long it takes to exceed it. For example, the holdover of a MAC is within 1 microsecond for 24 hours. This means that after 24 hours, the time accuracy is nondeterministic but accurate within 1 microsecond.</span></p>
<p><span>As an alternative approach, we are counting on the new generation of chip-scale and miniaturized atomic clocks with their capability to receive PPS inputs. This allows the time engine of the Time Card to hand off the ultraprecision syntonization of the high-stability oscillator to the component rather than use digital resources to reach the target. </span></p>
<p><span>As a general principle, the more accurate the tuning, the better the holdover performance that can be achieved. In terms of delivery, using a NIC with precision timing ensures that network packets receive very accurate time stamps, which is critical for keeping the time precise as it is shared with other servers across the network. Such a NIC can also receive a PPS signal directly from the Time Card.</span></p>
<p><span>After conceptualizing the idea and various implementation iterations, we were able to put together a prototype. </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg" alt="Image of the time card prototype" width="1921" height="1430" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=916,682 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=768,572 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=1024,762 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=1536,1143 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=96,71 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-11-v2-1.jpg?resize=192,143 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>The Time Appliance in action</span></h2>
<p><span>The Time Card allows any x86 machine with a NIC capable of hardware time-stamping to be turned into a time appliance. This system is agnostic to whether it runs for NTP, PTP, SyncE, or any other time synchronization protocol, since the accuracy and stability provided by the Time Card is sufficient for almost any system. </span></p>
<p><span><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg" alt="Image showing the components to build a time appliance" width="1921" height="835" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=916,398 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=768,334 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=1024,445 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=1536,668 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=96,42 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-12-v4.jpg?resize=192,83 192w" sizes="(max-width: 992px) 100vw, 62vw"/></span><br/>
<span>The beauty of using PCIe cards is that the setup can be assembled even on a home PC, as long as it has enough PCIe slots available.</span></p>
<p><span>The next step would be to install Linux. The Time Card driver is included in Linux kernel </span><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=773bda96492153e11d21eb63ac814669b51fc701"><span>5.15</span></a><span> or newer. Or, it can be built from the</span><a href="https://github.com/opencomputeproject/Time-Appliance-Project/tree/master/Time-Card/DRV"> <span>OCP GitHub repository</span></a><span> on kernel 5.12 or newer.</span></p>
<p><span>The driver will expose several devices, including the PHC clock, GNSS, PPS, and atomic clock serial:</span></p>
<pre><code>$ ls -l /sys/class/timecard/ocp0/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 device -&gt; ../../../0000:04:00.0/
-r--r--r--. 1 root 4096 Aug  3 19:49 gnss_sync
lrwxrwxrwx. 1 root    0 Aug  3 19:49 i2c -&gt; ../../xiic-i2c.1024/i2c-2/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 pps -&gt; ../../../../../virtual/pps/pps1/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 ptp -&gt; ../../ptp/ptp2/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 ttyGNSS -&gt; ../../tty/ttyS7/
lrwxrwxrwx. 1 root    0 Aug  3 19:49 ttyMAC -&gt; ../../tty/ttyS8/
</code></pre>
<p><span>The driver also allows us to monitor the Time Card, the GNSS receiver, and the atomic clock status and flash a new FPGA bitstream using the <code>devlink</code> cli.</span></p>
<p><span>The only thing left to do is to configure the NTP and/or PTP server to use the Time Card as a reference clock. To configure chrony, one simply needs to specify <code>refclock</code> attribute:</span></p>
<pre><code>$ grep refclock /etc/chrony.conf
refclock PHC /dev/ptp2 tai poll 0 trust
</code></pre>
<p><span>And enjoy a very precise and stable NTP Stratum 1 server:</span></p>
<pre><code>$ chronyc sources
210 Number of sources = 1
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
#* PHC0                          0   0   377     1     +4ns[   +4ns] +/-   
36ns
</code></pre>
<p><span>For the PTP server (for example, </span><a href="https://github.com/facebookincubator/ptp"><span>ptp4u</span></a><span>) one will first need to synchronize Time Card PHC with the NIC PHC. This can be easily done by using the <code>phc2sys</code> tool which will sync the clock values with the high precision usually staying within single digits of nanoseconds:</span></p>
<pre><code>$ phc2sys -s /dev/ptp2 -c eth0 -O 0 -m</code></pre>
<p><span>For greater precision, it’s recommended to connect the Time Card and the NIC to the same CPU PCIe lane. For greater precision, one can connect the PPS output of the Time Card to the PPS input of the NIC.</span></p>
<p><span>To validate and confirm the precision, we’ve used an external validation device called Calnex Sentinel connected to the same network via several switches and an independent GNSS antenna. It can perform PPS testing as well as NTP and/or PTP protocols: </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg" alt="On this graph, the blue line represents NTP measurement results. The precision stays within ±40 microseconds throughout the 48-hour measurement interval. The orange line represents PTP measurement results. The offset is practically 0 ranging within nanoseconds range." width="1921" height="1081" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-14-v4.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The blue line represents NTP measurement results. The precision stays within ±40 microseconds throughout the 48-hour measurement interval.</span></p>
<p><span>The orange line represents PTP measurement results. The offset is practically 0 ranging within nanoseconds range.</span></p>
<p><span>Indeed, when we compare 1 PPS between Time Card output and the internal reference of the Calnex Sentinel, we see that the combined error ranges within ±200 nanoseconds:</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg" alt="when we compare 1 PPS between Time Card output and the internal reference of the Calnex Sentinel, we see that the combined error ranges within ±200 nanoseconds" width="1921" height="1081" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-16-v4.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>

<p><span>But what’s even more important is that these measurements demonstrate stability of the Time Appliance outputs.</span></p>
<p><span>In the event of the GNSS signal loss, we need to make sure the time drift (aka holdover) of the atomic-backed Time Card stays within 1 microsecond per 24 hours. Here is a graph showing the holdover of the atomic clock (SA.53s) over a 24-hour interval. As you can see, the PPS drift stays within 300 nanoseconds, which is within the atomic clock spec.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg" alt="Here is a graph showing the holdover of the atomic clock (SA.53s) over a 24-hour interval. As you can see, the PPS drift stays within 300 nanoseconds, which is within the atomic clock spec." width="1921" height="1081" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg 1921w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-diagram-15-v8.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The modular design of the Time Card allows the swap of the atomic clock with an oven-controlled crystal oscillator (OCXO) or a temperature compensated crystal oscillator (TCXO) for a budget solution with the compromise on the holdover capabilities. </span></p>
<h2><span>Open-sourcing the design of the Time Appliance</span></h2>
<p><span>Building a device that is very precise, inexpensive, and free from vendor lock was an achievement on its own. But we wanted to have a greater impact on the industry. We wanted to truly set it free and make it open and affordable for everyone, from a research scientist to a large cloud data center. That’s why we engaged with the Open Compute Project (OCP) to create a brand-new </span><a href="http://www.ocptap.com/"><span>Time Appliance Project</span></a><span> (TAP). Under the OCP umbrella, we open-sourced at the Time Appliance Project </span><a href="https://github.com/opencomputeproject/Time-Appliance-Project"><span>GitHub repository</span></a><span>, including the specs, schematics, mechanics, BOM, and the source code. Now, as long as printing the PCB and soldering tiny components does not sound scary, anyone can build their own Time Card for a fraction of the cost of a regular time appliance. We also worked with several vendors such as </span><a href="https://www.orolia.com/about-the-atomic-reference-time-card-art-card/"><span>Orolia</span></a><span> who will be building and selling time cards, and NVIDIA who are selling the precision timing-capable ConnectX-6 Dx (and the precision timing-capable BlueField-2 DPU).</span></p>
<p><span>We published an Open Time Server spec at </span><a href="http://www.opentimeserver.com"><span>www.opentimeserver.com</span></a><span>, which explains in great detail how to combine the hardware (Time Card, Network Card, and a commodity server) and the software (OS driver, NTP, and/or PTP server) to build the Time Appliance. Building an appliance based on this spec will give full control to the engineers maintaining the device, improving monitoring, configuration, management, and security.</span></p>
<p><span>The Time Appliance is an important step in the journey to improve the timing infrastructure for everyone, but there is more to be done. We will continue to work on other elements, including improving the precision and accuracy of the synchronization of our own servers, and we intend to continue sharing this work with the Open Compute community.  </span></p>

		
	</div></div>]]></content:encoded>
      <author>By Ahmad Byagowi, Oleg Obleukhov</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_391-Eng-Blog-Facebook-Time-Appliances-Hero-image-v2.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 11 Aug 2021 11:00:34 +0000</pubDate>
    </item>
    <item>
      <title>Risk-driven backbone management during COVID-19 and beyond</title>
      <link>https://engineering.fb.com/2021/08/09/connectivity/backbone-management/</link>
      <description>&lt;p&gt;What the research is:  A first-of-its-kind study detailing our backbone management strategy to ensure high service performance throughout the COVID-19 pandemic. The pandemic moved most social interactions online and caused an unprecedented stress test on our global network infrastructure with tens of data center regions. At this scale, failures such as fiber cuts, router misconfigurations, [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/08/09/connectivity/backbone-management/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/08/09/connectivity/backbone-management/&#34;&gt;Risk-driven backbone management during COVID-19 and beyond&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<h2><span>What the research is: </span></h2>
<p><span>A first-of-its-kind study detailing our backbone management strategy to ensure high service performance throughout the COVID-19 pandemic. The pandemic moved most social interactions online and caused an unprecedented stress test on our global network infrastructure with tens of data center regions. At this scale, failures such as fiber cuts, router misconfigurations, and power outages are a frequent occurrence.</span></p>
<p><span>We ran a simulation system that identifies possible failures and quantifies their potential severity with a set of metrics that measure network risk. The risk metrics, in turn, guided operational decisions for capacity deployment. Coupled with traffic priority management and proactive capacity enhancement, our backbone resiliently withstood the COVID-19 stress test while achieving high service availability and low latency, and efficiently handled traffic surges.</span></p>
<h2><span>How it works: </span></h2>
<p><span>To satisfy the network’s service-level objectives (SLO), we started by defining a set of risk metrics around demand loss, availability, and latency stretch. All these metrics are computed with respect to possible failure scenarios in the network, which can be enumerated by going through all the components making up the network. The goal of the failure modeling is to estimate the likelihood of a failure scenario as well as the duration of the failure event. Each component failure is characterized by its mean time between failures and mean time to repair. These statistics are estimated based on a combination of historical data and clustering followed by Bayesian regression modeling on common features such as vendor, ownership, and geographical region.</span></p>
<p><span>Our risk simulation system periodically computes the aforementioned risk metrics. It works by taking a fresh snapshot of the network topology and demand and the set of failure scenarios to consider together with their failure characteristics as its input. Due to the high number of failure scenarios, each is sharded onto a number of worker jobs that run the same code as our </span><a href="https://engineering.fb.com/2017/05/01/data-center-engineering/building-express-backbone-facebook-s-new-long-haul-network/"><span>SD-WAN controller</span></a><span> to compute the traffic engineering decision for the given failure scenario. The decisions are aggregated to derive the risk metrics and then logged for continuous monitoring.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/BackBoneRisk.1.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>During the onset of COVID-19, the risk metrics reported a significant increase in demand loss (which captures the highest traffic loss across all simulated failure scenarios), a decrease in availability and increase in latency for all quality of service (QoS) classes. The risk metrics guided us to the possible failure scenarios that, were they to occur, would degrade the network operating conditions for certain regions. Capacity was proactively deployed to mitigate these risks. Another helpful technique was looking at the traffic flows from the regions at risk, differentiating the traffic by criticality and then downgrading the QoS to a lower priority. The QoS classes are, in order of importance, infrastructure control (class 1), user traffic (class 2), internal applications (class 3), and bulk data transfer (class 4). We downgraded a lot of latency-insensitive traffic from class 3 to class 4. Less capacity is thus needed to guarantee the same level of SLO.</span></p>
<h2><span>Why it matters: </span></h2>
<p><span>There is a long lead time of months to years for building up capacity for backbone networks. As such, network operators typically procure capacity based on estimated traffic growth. When COVID-19 hit, there was a significant unplanned ramp-up in traffic within a short period of time, stressing backbone infrastructure all across the world. </span></p>
<p><span>Facebook was able to react swiftly thanks to its risk-driven backbone management strategy. Leveraging the risk metrics computed by our simulation systems, we quickly identified the operational pain points and prioritized capacity enhancements to bring the network back to normal. Our experience has shown that a metrics-centric approach to backbone management could adapt to rare adverse external shock. We hope our research can help operators looking to build a more resilient network. We would like to thank Ying Zhang, Guanqing Yan, Satyajeet Singh Ahuja, Alexander Nikolaidis, Soshant Bali, Bob Kamma and Gaya Nagarajan for their work on this project. </span></p>
<p><a href="https://www.usenix.org/conference/nsdi21/presentation/xia">To learn more, watch our presentation at NSDI 2021</a>.</p>
<h2><span>Read the full paper:</span></h2>
<p><a href="https://research.fb.com/publications/a-social-network-under-social-distancing-risk-driven-backbone-management-during-covid-19-and-beyond/"><span>A social network under social distancing: Risk-driven network management during COVID-19 and beyond</span></a></p>

		
	</div></div>]]></content:encoded>
      <author>By Chiun Lin Lim</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/08/RiB_RichTeal.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 09 Aug 2021 16:00:55 +0000</pubDate>
    </item>
    <item>
      <title>How we built a general purpose key value store for Facebook with ZippyDB</title>
      <link>https://engineering.fb.com/2021/08/06/core-data/zippydb/</link>
      <description>&lt;p&gt;ZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. Since we first deployed ZippyDB in 2012, this key-value store has expanded rapidly, and today, ZippyDB serves a number of use cases, ranging from metadata for a distributed filesystem, counting events for both internal and external purposes, to product data that’s used for [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/08/06/core-data/zippydb/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/08/06/core-data/zippydb/&#34;&gt;How we built a general purpose key value store for Facebook with ZippyDB&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>ZippyDB is the largest strongly consistent, geographically distributed key-value store at Facebook. Since we first deployed ZippyDB in 2012, this key-value store has expanded rapidly, and today, ZippyDB serves a number of use cases, ranging from metadata for a distributed filesystem, counting events for both internal and external purposes, to product data that’s used for various app features. ZippyDB offers a lot of flexibility to applications in terms of tunable durability, consistency, availability, and latency guarantees, which has made the service a popular choice within Facebook for storing both ephemeral and nonephemeral small key-value </span><span>data. In this post, we are sharing for the first time the history and evolution of ZippyDB and some of the unique design choices and trade-offs made in building this service that addressed the majority of key-value store scenarios at Facebook.</span></p>
<h2><span>History of ZippyDB</span></h2>
<p><span>ZippyDB uses</span><a href="https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-and-open-sourcing-rocksdb/10151822347683920/"> <span>RocksDB</span></a><span> as the underlying storage engine. Before ZippyDB, various teams across Facebook used RocksDB directly to manage their data. This resulted, however, in a duplication of efforts in terms of each team solving similar challenges such as consistency, fault tolerance, failure recovery, replication, and capacity management. To address the needs of these various teams, we built ZippyDB to provide a highly durable and consistent key-value data store that allowed products to move a lot faster by offloading all the data and the challenges associated with managing this data at scale to ZippyDB.</span></p>
<p><span>One of the significant design decisions we made early during the development of ZippyDB was to reuse as much of the existing infrastructure as possible. Consequently, most of our initial efforts were focused on building a reusable and flexible data replication library called Data Shuttle. We built a fully managed distributed key-value store by combining Data Shuttle with a preexisting and well-established storage engine (RocksDB) and layering this on top of our existing shard management (</span><a href="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/"><span>Shard Manager</span></a><span>)</span> <span>and </span><span>distributed </span><span>configuration service (built on </span><a href="https://zookeeper.apache.org/"><span>ZooKeeper</span></a><span>), that together solves load balancing, shard placement, failure detection, and service discovery.</span></p>
<h2><span>Architecture</span></h2>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDb.1.psd.BodyImage1.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>ZippyDB is deployed in units known as tiers. A tier consists of compute and storage resources spread across several geographic areas known as regions</span> <span>worldwide, which makes it resilient to failures. There are only a handful of ZippyDB tiers that exist today, including the default “wildcard” tier and specialized tiers for distributed filesystem metadata and other product groups within Facebook. Each tier hosts multiple use cases. Normally, use cases are created on the wildcard tier, which is our generic multitenant tier. This is the preferred tier because of its better utilization of hardware and lower operational overhead, but we occasionally bring up dedicated tiers if there is a need, usually due to stricter isolation requirements.</span></p>
<p><span>The data belonging to a use case on a tier is split into units known as shards,</span> <span>which are</span> <span>the basic units of data management on the server side. Each shard is replicated across multiple regions (for fault tolerance) using Data Shuttle, which uses either <a href="https://dl.acm.org/doi/10.1145/279227.279229">Paxos</a> or</span> <span>async replication to</span> <span>replicate</span> <span>data, depending on the configuration. Within a shard, a subset of replicas are configured to be a part of the Paxos quorum group, also known as global scope,</span> <span>where data is synchronously replicated using Multi-Paxos to provide high durability and availability in case of failures. The remaining replicas, if any, are configured as followers.</span> <span>These are similar to learners in Paxos terminology and receive data asynchronously. Followers allow applications to have many in-region replicas to support low-latency reads with relaxed consistency, while keeping the quorum size small for lower write latency. This flexibility in replica role configuration within a shard allows applications to strike a balance between durability, write performance, and read performance depending on their needs.</span></p>
<p><span>In addition to the sync or async replication strategy, applications also have the option to provide “hints” to the service about the regions in which the replicas of a shard must be placed. These hints, also known as stickiness</span> <span>constraints, allow applications to have some control over the latency of reads and writes by having replicas built in regions from where they expect most of the access to come. ZippyDB also provides a caching layer and integrates with a pub-sub system allowing subscriptions to data mutations on shards, both of which are opt-ins depending on the requirements of the use case.</span></p>
<h2><span>Data model</span></h2>
<p><span>ZippyDB supports a simple key-value</span> <span>data model with APIs to get, put, and delete keys along with their batch variants. It supports iterating over key prefixes and deleting a range of keys. These APIs are very similar to the API exposed by the underlying RocksDB storage engine. In addition, we also support a test-and-set API for basic read-modify-write operations and transactions, conditional writes for more generic read-modify-write operations (more about this later). This minimal API set has proved to be sufficient for most use cases to manage their data on ZippyDB. For ephemeral data, ZippyDB has native TTL support where the client can optionally specify the expiry time for an object at the time of the write. We piggyback on RocksDB’s periodic compaction support to clean up all the expired keys efficiently while filtering out dead keys on the read side in between compaction runs. Many applications actually access data on ZippyDB through an ORM layer on top of ZippyDB, which translates these accesses into ZippyDB API. Among other things, this layer serves to abstract the details of the underlying storage service.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB2.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Shard is the unit of data management on the server side. The optimal assignment of shards to servers needs to take into account load, failure domains, user constraints, etc., and this is handled by ShardManager</span><i><span>. </span></i><span>ShardManager</span> <span>is responsible for monitoring servers for load imbalance, failures, and initiating shard movement between servers. </span></p>
<p><span>Shard, often referred to as physical shard or p-shard, is a server-side concept and isn’t exposed to applications directly. Instead, we allow use cases to partition their key space into smaller units of related data known as μshards (micro-shards)</span><i><span>. </span></i><span>A typical</span> <span>physical</span> <span>shard</span> <span>has a size of 50–100 GB, hosting several tens of thousands of μshards. This additional layer of abstraction allows ZippyDB to reshard the data transparently without any changes on the client.</span></p>
<p><span>ZippyDB supports two kinds of mappings from </span><i><span>μ</span></i><span>shards to physical shards: compact mapping and Akkio mapping. Compact mapping</span> <span>is used</span> <span>when the assignment is fairly static and mapping is only changed when there is a need to split shards that have become too large or hot. In practice, this is a fairly infrequent operation when compared with Akkio mapping, where mapping of </span><i><span>μ</span></i><span>shards is managed by a service known as</span><a href="https://engineering.fb.com/2018/10/08/core-data/akkio/"> <span>Akkio</span></a><span>. Akkio splits use cases’ key space into μshards and places these μshards in regions where the information is typically accessed. Akkio helps reduce data set duplication and provides a significantly more efficient solution for low latency access than having to place data in every region.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg 4800w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZibbyDB3.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>As we mentioned earlier, Data Shuttle uses Multi-Paxos to synchronously replicate data to all replicas in the global scope</span><i><span>. </span></i><span>Conceptually, time is subdivided into units known as epochs. Each epoch has a unique leader, whose role is assigned using an external shard management service called ShardManager. Once a leader is assigned, it has a lease for the entire duration of the epoch. Periodic heartbeats used to keep a lease active until ShardManager bumps up the epoch on the shard (e.g., for failover, primary load balancing, etc.). When a failure occurs, ShardManager detects the failure, assigns a new leader with a higher epoch and restores write availability. Within each epoch, the leader generates a total ordering of all writes to the shard, by assigning each write a monotonically increasing sequence number. The writes are then written to a replicated durable log using Multi-Paxos</span> <span>to achieve consensus on the ordering. Once the writes have reached consensus, they are drained in-order across all replicas.</span></p>
<p><span>We chose to use an external service to detect failures and assign leaders to keep the design of the service simple in the initial implementation. However, in the future we plan to move towards detecting failures entirely within</span> <span>Data Shuttle (“in-band”) and reelecting the leaders more proactively without having to wait for ShardManager and incurring delays.</span></p>
<h2><span>Consistency</span></h2>
<p><span>ZippyDB provides configurable consistency and durability levels to applications, which can be specified as options in read and write APIs. This allows applications to make durability, consistency, and performance trade-offs dynamically on a per-request level.</span></p>
<p><span>By default, a write involves persisting the data on a majority of replicas’ Paxos logs and writing the data to RocksDB on the primary before acknowledging the write to the client. With the default write mode, a read on primary will always see the most recent write. Some applications cannot tolerate cross-region latencies for every write, so ZippyDB supports a fast-acknowledge</span> <span>mode, where writes are acknowledged as soon as they are enqueued on the primary for replication. The durability and consistency guarantees for this mode are obviously lower, which is the trade-off for higher performance.</span></p>
<p><span>On the read side, the three most popular consistency levels are eventual, read-your-writes, and strong. The eventual consistency level supported by ZippyDB is actually a much stronger consistency level than the more well-known eventual consistency. ZippyDB provides total ordering for all writes within a shard and ensures that reads aren’t served by replicas that are lagging behind primary/quorum beyond a certain configurable threshold (heartbeats are used to detect lag), so eventual reads supported by ZippyDB are closer to bounded staleness consistency in literature.</span></p>
<p><span>For read-your-writes, the clients cache the latest sequence number returned by the server for writes and use the version to run at-or-later</span> <span>queries</span> <span>while reading. The cache of versions is within the same client process.</span></p>
<p><span>ZippyDB also provides strong consistency or </span><a href="https://dl.acm.org/doi/10.1145/78969.78972"><span>linearizability</span></a><i><span>, </span></i><span>where</span> <span>clients can see the effects of the most recent writes regardless of where the writes or reads come from. Strong reads today are implemented by routing the reads to the primary in order to avoid the need to speak to a quorum, mostly for performance reasons. The primary relies on owning the lease to ensure that there is no other primary before serving reads. In certain outlier cases, where the primary hasn’t heard about the lease renewal, strong reads on primary turn into a quorum check and read.</span></p>
<h2><span>Transactions and conditional writes</span></h2>

<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB4.Revised.jpg?w=16" alt="" width="16" height="9"/><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?w=916" alt="" width="916" height="515" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg 2304w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Final_.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/Screen-Shot-2021-05-25-at-10.20.41-AM.jpg?w=16" alt="" width="16" height="9"/></p>
<p><span>ZippyDB supports transactions and conditional writes for use cases that need atomic read-modify-write operations on a set of keys.</span></p>
<p><span>All transactions are serializable by default on a shard, and we don’t support lower isolation levels. This simplifies the server-side implementation and the reasoning about correctness of concurrently executing transactions on the client side. Transactions use optimistic concurrency control to detect and resolve </span><a href="https://dl.acm.org/doi/10.1145/568271.223787"><span>conflicts</span></a><span>, which works as shown in the figure above. The clients typically read from a secondary all of the data from a snapshot</span> <span>of the DB, compose the write</span> <span>set, and send both the read and write sets to the primary to commit. Upon receiving the read and write sets and the snapshot against which reads were performed, the primary checks whether there were conflicting writes by other concurrently executing transactions that have already been admitted. The transaction is admitted only if there are no conflicts, after which the transaction is guaranteed to succeed, assuming no server failures. Conflict resolution on the primary relies on tracking all of the recent</span> <span>writes performed by previously admitted transactions during the same epoch on the primary. Transactions spanning epochs are rejected, as this simplifies write set</span> <span>tracking without requiring replication. The history of writes maintained on the primary is also periodically purged to keep the space usage low. Since the complete history isn’t maintained, the primary needs to maintain a minimum tracked version</span> <span>and reject all transactions that have reads against a snapshot with lower version to guarantee serializability. Read-only transactions work exactly similar to read-write transactions, except that the write set is empty.</span></p>
<p><span>Conditional write is implemented using “server-side transactions”. It provides a more user friendly client side API for use cases where clients want to atomically modify a set of keys based on some common preconditions such as key_present, key_not_present, and value_matches_or_key_not_present. When a primary receives a conditional write request it sets up a transaction context and converts the preconditions and write set to a transaction on the server, reusing all of the machinery for transactions. The conditional-write API can be more efficient than the transaction API in cases where clients can compute the precondition without requiring a read.</span></p>
<h2><span>The future of ZippyDB</span></h2>
<p><span>Distributed key-value stores have many applications, and the need for them often comes up while building a variety of systems, from products to storing metadata for various infrastructure services. Building a scalable, strongly consistent, and fault-tolerant key-value store can be very challenging and often requires thinking through many trade-offs to provide a curated combination of system capabilities and guarantees that works well in practice for a variety of workloads. This blog post introduced ZippyDB, Facebook’s biggest key-value store, which has been in production for more than six years serving a lot of different workloads. Since its inception, the service has seen very steep adoption, mostly due to the flexibility that it offers in terms of making efficiency, availability, and performance trade-offs. The service also enables us to use engineering resources effectively as a company and use our key-value store capacity efficiently as a single pool. ZippyDB is still evolving and currently undergoing significant architectural changes, such as storage-compute disaggregation, fundamental changes to membership management, failure detection and recovery, and distributed transactions, in order to adapt to the changing ecosystem and product requirements.</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Sarang Masti</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/08/ZippyDB.Hero_.Image_.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 06 Aug 2021 16:52:02 +0000</pubDate>
    </item>
    <item>
      <title>Open sourcing Winterfell: A STARK prover and verifier</title>
      <link>https://engineering.fb.com/2021/08/04/open-source/winterfell/</link>
      <description>&lt;p&gt;We are releasing Winterfell, our implementation of a STARK prover/verifier to Crates.io  Winterfell is an easy to use open source implementation of STARKs for security and privacy applications. One potential application for Winterfell’s zero-knowledge proofs is blockchain privacy and scalability.  “Any sufficiently advanced technology is indistinguishable from magic.” —Clarke’s Third Law What if the average [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/08/04/open-source/winterfell/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/08/04/open-source/winterfell/&#34;&gt;Open sourcing Winterfell: A STARK prover and verifier&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<ul>
<li aria-level="1"><span>We are releasing Winterfell, our implementation of a STARK prover/verifier to Crates.io </span></li>
<li aria-level="1"><span>Winterfell is an easy to use open source implementation of STARKs for security and privacy applications.</span></li>
<li aria-level="1"><span>One potential application for Winterfell’s zero-knowledge proofs is blockchain privacy and scalability. </span></li>
</ul>
<p><i><span>“Any sufficiently advanced technology is indistinguishable from magic.” —Clarke’s Third Law</span></i></p>
<p><span>What if the average developer could benefit from proofs of computational integrity (CI) that would normally require an in-depth knowledge of cryptography to implement? </span></p>
<p><span>CI proofs, of which zero-knowledge proofs (ZKPs) are a subset, are a cryptographic technology that let you do seemingly impossible things. For example, you can run a computation and get some result. You can then use a CI proof to convince anyone that you did the computation correctly without their having to rerun the computation themselves.</span> <span>And they can verify this correctness in just a few milliseconds, regardless of how complex or long-running the original computation was.</span> <span>To bring the power of CI proofs to the masses, we’ve developed Winterfell, a general-purpose STARK (Scalable Transparent Arguments of Knowledge) prover and verifier. We are happy to be publishing the v0.1 version of the library to crates.io.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_FINAL.gif" alt="To bring the power of CI proofs to the masses, we’ve developed Winterfell, a general-purpose STARK prover and verifier. " width="1920" height="1080"/></p>
<p><span>Another important property of these CI proofs is the ability to hide some (or all) of the inputs that were used to run the computation. This is the zero-knowledge aspect. For example, you could prove that a number is in a given range without revealing the exact value of the number (these types of proofs are usually called range proofs). Or, you could do something as complex as comparing two number sequences, one public and one private (known only to yourself), and prove to anyone beyond a doubt that there is or isn’t a match between them.</span></p>
<h2><span>The magic of ZKPs</span></h2>
<p><span>The general ideas behind ZKPs were developed as early as the 1980s, but interest in this area of cryptography has </span><a href="https://nakamoto.com/cambrian-explosion-of-crypto-proofs/"><span>exploded</span></a><span> recently, driven in part by emergent applications in the blockchain space. In the last few years, over a dozen new proving systems have appeared. Some of them have even been deployed in production where tens of billions of dollars depend on their security properties. However, ZKPs are far from mainstream, primarily for two reasons:</span></p>
<ul>
<li aria-level="1"><span>Until recently, deploying ZKPs in applications required expert cryptographers with years of experience. The situation is somewhat better now, as there are plenty of relatively accessible materials available and more projects that try to make ZKPs accessible to the average developer. But even now, making sense of different proving systems and the trade-offs associated with them requires deep expertise and/or a steep learning curve, even for experienced software engineers.</span></li>
<li aria-level="1"><span>While verifying a ZK proof is extremely fast and requires very few compute resources, generating a proof is a computationally intensive process. It may take seconds or even minutes (or many CPU cores) to generate proofs for even relatively simple computations. Only relatively recent advances in cryptography and implementation improvements have brought a large segment of computations to within practical feasibility for ZKPs. And there is a lot of ongoing work to expand the set of computations for which proof generation is practical.</span></li>
</ul>
<p><span>We developed Winterfell to bridge these gaps and to bring ZKPs within reach of regular developers. </span></p>
<h2><span>Winterfell is here</span></h2>
<p><span>Winterfell is a general purpose STARK prover and verifier written in </span><a href="https://engineering.fb.com/2021/04/29/developer-tools/rust/"><span>Rust</span></a><span> at </span><a href="https://research.fb.com/category/blockchain-and-cryptoeconomics/"><span>Novi Research</span></a><span>. </span><i><span>General purpose</span></i><span> means that Winterfell can generate CI proofs for any computation. Basically, for any program that can be described with a Turing-complete language, we can generate a CI proof using Winterfell (though this would be much more straightforward for some programs than for others).</span></p>
<p><span>Winterfell uses STARKs, a proof-of-computation scheme developed by Eli Ben-Sasson, Michael Riabzev, et al. In comparison with many other CI proving systems, STARKs have a number of attractive properties, including:</span></p>
<ul>
<li aria-level="1"><span>STARKs rely on very few cryptographic assumptions. In fact, the only cryptographic primitive we need for STARKs to work is a collision resistant hash function (e.g., SHA256). This also makes STARKs resistant to potential attacks from adversaries with quantum computers.</span></li>
<li aria-level="1"><span>Unlike many other proving systems, STARKs are fully transparent. This means we don’t need to run complicated trusted setup ceremonies to start using STARKs. Trusted setups are a potential security weakness in other zero knowledge protocols, because a compromised trusted setup allows attackers to generate fake CI proofs. STARKs are immune to this.</span></li>
<li aria-level="1"><span>In comparison with other systems, STARK proof generation is extremely fast when we deal with uniform computations, or computations with regular structures. Fortunately, the vast majority of programs people write do possess such regular structures. Moreover, pretty much every single step of the STARK proof generation process is massively parallelizable. Thus, we can frequently speed up proof generation by distributing it across more and more CPU cores.</span></li>
</ul>
<p><span>None of the individual properties listed above are unique to STARKs. However, no other proving system combines lean cryptography, transparency, and performance to the extent STARKs do. Winterfell takes full advantage of these benefits while abstracting away most of the complexity. For example, proof generation can be distributed across multiple CPU cores to dramatically reduce proof generation time (see our benchmarks </span><a href="https://github.com/novifinancial/winterfell#performance"><span>here</span></a><span>). Moreover, we have plans to enable fully distributed proof generation across multiple machines and have already started work in this direction.</span></p>
<p><span>In addition to being performant, Winterfell is highly configurable. That is, you can dynamically tune almost all parameters of the STARK protocol to attain specific performance and security targets. We are able to achieve such high configurability without sacrificing performance or code clarity by relying on Rust’s zero-cost abstractions.</span></p>
<p><span>Last, and perhaps most important, you don’t need to be a cryptographer to use Winterfell. As mentioned previously, Winterfell abstracts away most of the complexity of the STARK protocol. The only thing the user is responsible for is describing their computation in a format that the STARK prover/verifier can understand. This format is called algebraic intermediate representation (AIR), and the step of translating a program into AIR is called arithmetization.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg" alt="" width="1920" height="847" srcset="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=916,404 916w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=768,339 768w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=1024,452 1024w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=1536,678 1536w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=96,42 96w, https://engineering.fb.com/wp-content/uploads/2021/08/CD21_481_Open_Sourcing_Winterfell_flowchart_FINAL.jpg?resize=192,85 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Using Winterfell</span></h2>
<p><span>Winterfell exposes a relatively simple interface for describing AIR for any computation. However, the concept of arithmetization is not something most developers are familiar with, so there is going to be a learning curve. </span></p>
<p><span>To help you get started, we’ve put together an end-to-end </span><a href="https://github.com/novifinancial/winterfell#usage"><span>tutorial</span></a><span> on how to define AIR for a very simple computation. We also have examples of more interesting computations in the </span><a href="https://github.com/novifinancial/winterfell/tree/main/examples"><span>examples crate</span></a><span>, ranging from something as simple as a Fibonacci sequence to something as sophisticated as aggregation of hash-based signatures. And if you would like to get a little bit deeper into theory, we recommend reading two excellent blog posts from StarkWare: </span><a href="https://medium.com/starkware/arithmetization-i-15c046390862"><span>Arithmetization I</span></a><span> and </span><a href="https://medium.com/starkware/arithmetization-ii-403c3b3f4355"><span>Arithmetization II</span></a><span>.</span></p>
<p><span>Once you are comfortable with writing AIRs, using Winterfell to generate STARK proofs becomes relatively easy. For example, AIR for a Fibonacci sequence requires less than 100 lines of code and can be put together in about 15 minutes. Even for the relatively complicated example of hash-based signature aggregation mentioned above, the AIR is described in about 600 lines of code (though it did take several days to put together).</span></p>
<p><span>Another point worth mentioning: We wrote Winterfell as a set of modular crates, all of which are being published to <a href="https://crates.io/users/irakliyk">Crates.io</a> today as well. While we use these crates to build a STARK proving system, many of them are general enough to be used as building blocks in other CI proving systems. For example, for low-degree testing, we use the FRI protocol implemented in the </span><a href="https://github.com/novifinancial/winterfell/tree/main/fri"><span>winter-fri</span></a><span> crate, which is also used as a building block for several other proof systems (e.g., </span><a href="https://eprint.iacr.org/2019/1076.pdf"><span>Fractal</span></a><span> and </span><a href="https://eprint.iacr.org/2018/828.pdf"><span>Aurora</span></a><span>) that aim to be transparent and post-quantum. Thus, we hope that our work will help implementers of these protocols get their job done more quickly and efficiently.</span></p>
<h2><span>Applications</span></h2>
<p><span>Recent advancements in ZKPs are driven by emergent use cases in the blockchain space. Specifically, ZKPs offer attractive solutions to perhaps two of the most pressing blockchain challenges: privacy and scalability. However, ZKPs have numerous potential applications outside of the blockchain space as well.</span></p>
<p><span>While there still remain some technical challenges to overcome before proofs of computational integrity can be considered practical at a large scale, we believe that Winterfell represents an important stepping stone for bringing a well-studied subject in academic research into practical deployments. And we hope that the security and privacy community will also benefit from an easy to use open source implementation of STARKs.</span></p>
<p><span>Please check out the </span><a href="https://github.com/novifinancial/winterfell"><span>Winterfell repository</span></a><span>, and feel free to open issues for comment and leave feedback!</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Irakliy Khaburzaniya, Kostas Chalkias, Kevin Lewi, Harjasleen Malvai</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/08/CD21_418_Open_Sourcing_Winterfell_HERO_FINAL.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 04 Aug 2021 16:00:43 +0000</pubDate>
    </item>
    <item>
      <title>A linear programming approach for optimizing features in ML models</title>
      <link>https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/</link>
      <description>&lt;p&gt;Whether it’s iterating on Facebook’s News Feed ranking algorithm or delivering the most relevant ads to users, we are constantly exploring new features to help improve our machine learning (ML) models. Every time we add new features, we create a challenging data engineering problem that requires us to think strategically about the choices we make. [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/29/data-infrastructure/linear-programming/&#34;&gt;A linear programming approach for optimizing features in ML models&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>Whether it’s iterating on </span><a href="https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/"><span>Facebook’s News Feed ranking algorithm</span></a><span> or delivering the most relevant ads to users, we are constantly exploring new features to help improve our machine learning (ML) models. Every time we add new features, we create a challenging data engineering problem that requires us to think strategically about the choices we make. More complex features and sophisticated techniques require additional storage space. Even at a company the size of Facebook, capacity isn’t infinite. If left unchecked, accepting all features would quickly overwhelm our capacity and slow down our iteration speed, decreasing the efficiency of running the models.</span></p>
<p><span>To better understand the relationship between these features and the capacity of the infrastructure that needs to support them, we can frame the system as a linear programming problem. By doing this, we can maximize a model’s performance, probe the sensitivity of its performance to different infrastructure constraints, and study the relationships between different services. </span><span>This work was done by data scientists embedded in our engineering team and demonstrates the value of analytics and data science in ML.</span></p>
<h2><span>Supporting feature development</span></h2>
<p><span>It’s important to continually introduce features that best leverage new data to maintain performant models. New features are responsible for the majority of incremental model improvements. These ML models are useful for our ad delivery system. They work together to predict a person’s likelihood of taking specific action on the ad. We work to continuously improve our models so our systems deliver only those ads that are relevant to a user.</span></p>
<p><span>As our techniques become more sophisticated, we develop more complex features that demand more of our infrastructure. A feature can leverage different services depending on its purpose. Some features have a higher memory cost, while others require extra CPU or take up more storage. It’s important to use our infrastructure wisely to maximize the performance of our models. We must be able to smartly allocate resources and be able to quantify the trade-offs of different scenarios.</span></p>
<p><span>To address these problems, we frame our system as a linear programming problem that maximizes our model’s metrics. We use this framework to better understand the interaction between our features and services. With this knowledge, we can automatically select the best features, identify infrastructure services to invest in, and maintain the health of both our models and services.</span></p>
<h2><span>Framing our problem</span></h2>
<p><span>To get a handle on our framework, we’ll first introduce a model problem. Say we have multiple features that all take up some amount of space (the height of the rectangles) and contribute some amount of gain to our models (the teal squares), and we are unable to accommodate them all in our limited capacity.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg" alt="Say we have multiple features that all take up some amount of space (the height of the rectangles) and contribute some amount of gain to our models (the teal squares), and we are unable to accommodate them all in our limited capacity." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear1A.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>

<p><span>A naive solution would be to just start picking the features with the most gain (teal squares) until you are out of capacity. However, you might not be making the best use of your resources if you just prioritize the gain. For example, by taking in a big feature with a large gain, you could be taking up room that two smaller features with less gain could use instead. Together, those two smaller features would give you more bang for your buck than the big feature.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg" alt="by taking in a big feature with a large gain, you could be taking up room that two smaller features with less gain could use instead." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Linear1.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>If we get a little less naive, we could instead look to pick features that give us the most bang per buck — features that have the most gain per storage. However, if we pick features only from that perspective, we could end up leaving out some less efficient features that we would still have room to accommodate.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg" alt="we could instead look to pick features that give us the most bang per buck — features that have the most gain per storage" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear2.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>We’ve been looking at a very simplified view of infrastructure, but the reality is a bit more complex. For example, features often don’t take up just one resource but need many — such as memory, CPU, or storage in other services. We can make our example slightly more sophisticated by adding in Service B, and saying that orange features take up space in both Service A and Service B.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg" alt="We can make our example slightly more sophisticated by adding in Service B, and saying that orange features take up space in both Service A and Service B." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear3.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Picking which features we use is not the only way to control our infrastructure usage. We can also employ various techniques to increase the efficiency of our feature storage. This sometimes comes with a cost, either through the feature itself or capacity from a service. In this case, let’s say that we can halve the storage cost of some features (bordered in pink) but only at the cost of reducing the gain of the feature, and using some of the limited capacity in Service B.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg" alt="let’s say that we can halve the storage cost of some features (bordered in pink), but only at the cost of reducing the gain of the feature, and using some of the limited capacity in Service B." width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear4.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>We’ll stop the example at this point, but this is enough for the general message to be clear — infrastructure can be a complicated interconnected system of different constraints. In reality, our capacity is not set in stone. We can move resources around if it is warranted. Features are also not the only thing we’re working on. There are plenty of other projects and workflows that compete for the same resources. We need to not only choose the features that maximize our gain but also be able to answer questions about how our system responds to changes:</span></p>
<ul>
<li aria-level="1"><span>Which features do we select to optimize the gain?</span></li>
<li aria-level="1"><span>Is feature compression worth it? More important, is it worth an engineer’s time to implement it?</span></li>
<li aria-level="1"><span>How does the gain change if we add more capacity to Service A?</span></li>
<li aria-level="1"><span>How do service dependencies interact? If we increase the capacity of Service B, can we use less of Service A?</span></li>
</ul>
<h2><span>Scaling the problem</span></h2>
<p><span>Let’s step back and review the conditions of our model problem:</span></p>
<ol>
<li aria-level="1"><span>We want to maximize our gain.</span></li>
<li aria-level="1"><span>We are limited by the capacity of Service A.</span></li>
<li aria-level="1"><span>We are also limited by the capacity of Service B, which only some features contribute to.</span></li>
<li aria-level="1"><span>Some features may be compressed, but:</span>
<ol>
<li aria-level="2"><span>They suffer a loss to their gain.</span></li>
<li aria-level="2"><span>Some of Service B’s capacity must be used.</span></li>
</ol>
</li>
</ol>
<p><span>We can express all these constraints as a system of linear equations.</span></p>
<p><span>Let 𝑥 be a vector that is 0 or 1 that signifies whether we select the feature, and let 𝑔 be a vector that stores the gain of the feature. The subscripts 𝑓 and 𝑐 denote whether we are specifying a full cost or compressed feature. For example, 𝑥</span><span>𝑓</span><span> denotes full, uncompressed features that we have selected to include, and 𝑔</span><span>𝑐</span><span> represents the cost of compressed features.</span></p>
<p><span>Given these definitions, our objective is to maximize:</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg" alt="linear programming equation" width="1920" height="244" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=916,116 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=768,98 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=1024,130 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=1536,195 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=96,12 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-1.jpg?resize=192,24 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>We can now add our constraints that model the limitations of our infrastructure:</span></p>
<ol>
<li><span>Features will either be selected and compressed, selected but not compressed, or not selected. We should not select the compressed and uncompressed versions of the same feature.<br/>
<img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg" alt="linear programming equation" width="1920" height="894" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=916,427 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=768,358 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=1024,477 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=1536,715 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-2.jpg?resize=192,89 192w" sizes="(max-width: 992px) 100vw, 62vw"/><br/>
</span></li>
<li><span>Let 𝑠 be the storage cost of the feature and the subscripts 𝐴 and 𝐵 represent Service A and B, respectively. For example, 𝑠</span><span>𝐴𝑐</span><span> represents the storage cost of compressed features in Service A. We are constrained by the capacity of the two services.<br/>
<img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg" alt="" width="1920" height="594" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=916,283 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=768,238 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=1024,317 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=1536,475 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=96,30 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-3.jpg?resize=192,59 192w" sizes="(max-width: 992px) 100vw, 62vw"/><br/>
</span></li>
<li><span>Some of Service B must be utilized to enable compression. Let’s represent that as a few features that must be selected.<br/>
<img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg" alt="" width="1920" height="892" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=916,426 916w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=768,357 768w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=1024,476 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=1536,714 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=96,45 96w, https://engineering.fb.com/wp-content/uploads/2021/07/linear-equation-4.jpg?resize=192,89 192w" sizes="(max-width: 992px) 100vw, 62vw"/><br/>
</span></li>
</ol>
<p><span>With this, we have now completely specified our problem in a few equations and can solve them using linear programming techniques. Of course, as we are interested in automating and productionalizing this, it can easily be specified in code. For this example, we accomplish this in Python using the excellent </span><a href="https://numpy.org/"><span>NumPy</span></a><span> and </span><a href="https://www.cvxpy.org/"><span>CVXPY</span></a><span> packages.</span></p>
<pre><code>import cvxpy as cp
import numpy as np
import pandas as pd
 
# Assuming data is a Pandas DataFrame that contains relevant feature data
data = pd.DataFrame(...)
# These variables contain the maximum capacity of various services
service_a = ...
service_b = ...
 
selected_full_features = cp.Variable(data.shape[0], boolean=True)
selected_compressed_features = cp.Variable(data.shape[0], boolean=True)
 
# Maximize the feature gain
feature_gain = (
   data.uncompressed_feature_gain.to_numpy() @ selected_full_features
   + data.compressed_feature_gain.to_numpy() @ selected_compressed_features
)
 
constraints = [
   # 1. We should not select the compressed and uncompressed version
   #    of the same feature
   selected_full_features + selected_compressed_features &lt;= np.ones(data.shape[0]),
   # 2. Features are restricted by the maximum capacity of the services
   data.full_storage_cost.to_numpy() @ selected_full_features
   + data.compressed_storage_cost.to_numpy() @ selected_full_features
   &lt;= service_a,
   data.full_memory_cost.to_numpy() @ selected_full_features
   + data.compressed_memory_cost.to_numpy() @ selected_compressed_features
   &lt;= service_b,
   # 3. Some features must be selected to enable compression
   selected_full_features &gt;= data.special_features.to_numpy(),
]
</code></pre>
<h2><span>Leveraging the framework</span></h2>
<p><span>Now we have a framework that we can use to express our questions and hypotheticals. If we want to find out how an increase in Service A translates to a feature gain, we can  run the optimization problem above at different values for Service A capacity and plot the gain. This way, we can directly quantify the return for each incremental increase in capacity. We can use this as a strong signal for what services we should invest in for the future and directly compare the return on investment on more feature memory, computing, or storage.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg" alt="chart showing return on service A capacity" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-A-capacity.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Similarly, we can look at the relationships between services. We simply vary the capacity of Services A and B while keeping the gain constant. We can see that as Service B’s capacity increases, less of Service A is needed to achieve the same gain. This can be leveraged if one service is overly stressed compared with another.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg" alt="chart showing the relationship between Service A and service B capacity" width="1920" height="1080" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/Service-B-capacity.jpg?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Linear programming as a framework for automating decisions</span></h2>
<p><span>Previously, feature approval was a manual process where teams would spend valuable time calculating how many features we could support and analyzing what the return on investment was for increasing the capacity of our services. In a company like Facebook — where we have multiple models being continuously iterated on — this approach does not scale. By framing our services as a system of linear equations, we take a complex interconnected system and simplify it into basic relationships that are easily communicated. By doing this, we can make smarter decisions about the features we deploy and the infrastructure we invest in.</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Paulo Silva Costa</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/CD21-479_Framing-Feature-Resources_Hero-1920x1080-1.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 29 Jul 2021 15:57:28 +0000</pubDate>
    </item>
    <item>
      <title>Migrating Facebook to MySQL 8.0</title>
      <link>https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/</link>
      <description>&lt;p&gt;MySQL, an open source database developed by Oracle, powers some of Facebook’s most important workloads. We actively develop new features in MySQL to support our evolving requirements. These features change many different areas of MySQL, including client connectors, storage engine, optimizer, and replication. Each new major version of MySQL requires significant time and effort to [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/&#34;&gt;Migrating Facebook to MySQL 8.0&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><a href="https://github.com/facebook/mysql-5.6"><span>MySQL</span></a><span>, an open source database developed by Oracle, powers some of Facebook’s most important workloads. We actively develop new features in MySQL to support our evolving requirements. These features change many different areas of MySQL, including client connectors, storage engine, optimizer, and replication. Each new major version of MySQL requires significant time and effort to migrate our workloads. The challenges  include:</span></p>
<ul>
<li aria-level="1"><span>Porting our custom features to the new version</span></li>
<li aria-level="1"><span>Ensuring replication is compatible between the major versions</span></li>
<li aria-level="1"><span>Minimizing changes needed for existing application queries</span></li>
<li aria-level="1"><span>Fixing performance regressions that prevent the server from supporting our workloads</span></li>
</ul>
<p><span>Our last major version upgrade, to MySQL 5.6, took more than a year to roll out. When version 5.7 was released, we were still in the midst of developing our LSM-Tree storage engine, </span><a href="https://engineering.fb.com/2016/08/31/core-data/myrocks-a-space-and-write-optimized-mysql-database/"><span>MyRocks</span></a><span>, on version 5.6. Since upgrading to 5.7 while simultaneously building a new storage engine would have significantly slowed the progress on MyRocks, we opted to stay with 5.6 until MyRocks was complete. MySQL 8.0 was announced as we were finishing the rollout of MyRocks to our user database (UDB) service tier. </span></p>
<p><span>That version included compelling features like writeset-based parallel replication and a transactional data dictionary that provided atomic DDL support. For us, moving to 8.0 would also bring in the 5.7 features we had missed, including Document Store. Version 5.6 was approaching end of life, and we wanted to stay active within the MySQL community, especially with our work on the MyRocks storage engine. Enhancements in 8.0, like instant DDL, could speed up MyRocks schema changes, but we needed to be on the 8.0 codebase to use it. Given the benefits of the code update, we decided to migrate to 8.0. We’re sharing how we tackled our 8.0 migration project — and some of the surprises we discovered in the process. When we initially scoped out the project, it was clear that moving to 8.0 would be even more difficult than migrating to 5.6 or MyRocks.</span></p>
<ul>
<li aria-level="1"><span>At the time, our customized 5.6 branch had over 1,700 code patches to port to 8.0. As we were porting those changes, new Facebook MySQL features and fixes were added to the 5.6 codebase that moved the goalpost further away.</span></li>
<li aria-level="1"><span>We have many MySQL servers running in production, serving a large number of disparate applications. We also have extensive software infrastructure for managing MySQL instances. These applications perform operations like gathering statistics and managing server backups.</span></li>
<li aria-level="1"><span>Upgrading from 5.6 to 8.0 skipped over 5.7 entirely. APIs that were active in 5.6 would have been deprecated in 5.7 and possibly removed in 8.0, requiring us to update any application using the now-removed APIs.</span></li>
<li aria-level="1"><span>A number of Facebook features were not forward-compatible with similar ones in 8.0 and required a deprecation and migration path forward.</span></li>
<li aria-level="1"><span>MyRocks enhancements were needed to run in 8.0, including native partitioning and crash recovery.</span></li>
</ul>
<h2><span>Code patches</span></h2>
<p><span>We first set up the 8.0 branch for building and testing in our development environments. We then began the long journey to port the patches from our 5.6 branch. There were more than 1,700 patches when we started, but we were able to organize them into a few major categories. Most of our custom code had good comments and descriptions so we could easily determine whether it was still needed by the applications or if it could be dropped. Features that were enabled by special keywords or unique variable names also made it easy to determine relevance because we could search through our application codebases to find their use cases. A few patches were very obscure and required detective work — digging through old design documents, posts, and/or code review comments — to understand their history.</span></p>
<p><span>We sorted each patch into one of four buckets:</span></p>
<ol>
<li aria-level="1"><span>Drop: Features that were no longer used, or had equivalent functionality in 8.0, did not need to be ported.</span></li>
<li aria-level="1"><span>Build/Client: Non-server features that supported our build environment and modified MySQL tools like mysqlbinlog, or added functionality like the async client API, were ported.</span></li>
<li aria-level="1"><span>Non-MyRocks Server: Features in the mysqld server that were not related to our MyRocks storage engine were ported.</span></li>
<li aria-level="1"><span>MyRocks Server: Features that supported the MyRocks storage engine were ported.</span></li>
</ol>
<p><span>We tracked the status and relevant historical information of each patch using spreadsheets, and recorded our reasoning when dropping a patch. Multiple patches that updated the same feature were grouped together for porting. Patches ported and committed to the 8.0 branch were annotated with the 5.6 commit information. Discrepancies on porting status would inevitably arise due to the large number of patches we needed to sift through and these notes helped us resolve them.</span></p>
<p><span>Each of the client and server categories naturally became a software release milestone. With all client-related changes ported, we were able to update our client tooling and connector code to 8.0. Once all of the non-MyRocks server features were ported, we were able to deploy 8.0 mysqld for InnoDB servers. Finishing up the MyRocks server features enabled us to update MyRocks installations.</span></p>
<p><span>Some of the most complex features required significant changes for 8.0, and a few areas had major compatibility problems. For example, upstream 8.0 binlog event formats were incompatible with some of our custom 5.6 modifications. Error codes used by Facebook 5.6 features conflicted with those assigned to new features by upstream 8.0. We ultimately needed to patch our 5.6 server to be forward-compatible with 8.0.</span></p>
<p><span>It took a couple of years to complete porting all of these features. By the time we got to the end, we had evaluated more than 2,300 patches and ported 1,500 of those to 8.0.</span></p>
<h2><span>The migration path</span></h2>
<p><span>We group together multiple mysqld instances into a single MySQL replica set. Each instance in a replica set contains the same data but is geographically distributed to a different data center to provide data availability and failover support. Each replica set has one primary instance. The remaining instances are all secondaries. The primary handles all write traffic and replicates the data asynchronously to all secondaries.<br/>
</span></p>

<p><span>We started with replica sets consisting of 5.6 primary/5.6 secondaries and the end goal was replica sets with 8.0 primary/8.0 secondaries. We followed a plan similar to the</span><a href="https://engineering.fb.com/2017/09/25/core-data/migrating-a-database-from-innodb-to-myrocks/"> <span>UDB MyRocks migration plan</span></a><span>.</span></p>
<ol>
<li aria-level="1"><span>For each replica set, create and add 8.0 secondaries via a logical copy using mysqldump. These secondaries do not serve any application read traffic.</span></li>
<li aria-level="1"><span>Enable read traffic on the 8.0 secondaries.</span></li>
<li aria-level="1"><span>Allow the 8.0 instance to be promoted to primary.</span></li>
<li aria-level="1"><span>Disable the 5.6 instances for read traffic.</span></li>
<li aria-level="1"><span>Remove all the 5.6 instances.</span></li>
</ol>
<p><span>Each replica set could transition through each of the steps above independently and stay on a step as long as needed. We separated replica sets into much smaller groups, which we shepherded through each transition. If we found problems, we could rollback to the previous step. In some cases, replica sets were able to reach the last step before others started.</span></p>
<p><span>To automate the transition of a large number of replica sets, we needed to build new software infrastructure. We could group replica sets together and move them through each stage by simply changing a line in a configuration file. Any replica set that encountered problems could then be individually rolled back.</span></p>
<h3><span>Row-based replication</span></h3>
<p><span>As part of the 8.0 migration effort, we decided to standardize on using row-based replication (RBR). Some 8.0 features required RBR, and it simplified our MyRocks porting efforts. While most of our MySQL replica sets were already using RBR, those still running statement-based replication (SBR) could not be easily converted. These replica sets usually had tables without any high cardinality keys. Switching completely to RBR had been a goal, but the long tail of work needed to add primary keys was often prioritized lower than other projects.</span></p>
<p><span>Hence, we made RBR a requirement for 8.0. After evaluating and adding primary keys to every table, we switched over the last SBR replica set this year. Using RBR also gave us an alternative solution for resolving an application issue that we encountered when we moved some replica sets to 8.0 primaries, which will be discussed later.</span></p>
<h2><span>Automation validation</span></h2>
<p><span>Most of the 8.0 migration process involved testing and verifying the mysqld server with our automation infrastructure and application queries.</span></p>
<p><span>As our MySQL fleet grew, so did the automation infrastructure we use to manage the servers. In order to ensure all of our MySQL automation was compatible with the 8.0 version, we invested in building a test environment, which leveraged test replica sets with virtual machines to verify the behaviors. We wrote integration tests to canary each piece of automation to run on both the 5.6 version and the 8.0 version and verified their correctness. We found several bugs and behavior differences as we went through this exercise.</span></p>
<p><span>As each piece of MySQL infrastructure was validated against our 8.0 server, we found and fixed (or worked around) a number of interesting issues:</span></p>
<ol>
<li aria-level="1"><span>Software that parsed text output from error log, mysqldump output, or server show commands easily broke. Slight changes in the server output often revealed bugs in a tool’s parsing logic.</span></li>
<li aria-level="1"><span>The 8.0’s default </span><span>utf8mb4</span><span> collation settings resulted in collation mismatches between our 5.6 and 8.0 instances. 8.0 tables may use the new </span><span>utf8mb4_0900</span><span> collations even for create statements generated by 5.6’s show create table because the 5.6 schemas using </span><span>utf8mb4_general_ci</span><span> do not explicitly specify collation. These table differences often caused problems with replication and schema verification tools.</span></li>
<li aria-level="1"><span>The error codes for certain replication failures changed and we had to fix our automation to handle them correctly.</span></li>
<li aria-level="1"><span>The 8.0 version’s data dictionary obsoleted table .frm files, but some of our automation used them to detect table schema modifications.</span></li>
<li aria-level="1"><span>We had to update our automation to support the dynamic privs introduced in 8.0.</span></li>
</ol>
<h3><span>Application validation</span></h3>
<p><span>We wanted the transition for applications to be as transparent as possible, but some application queries hit performance regressions or would fail on 8.0.</span></p>
<p><span>For the MyRocks migration, we built a MySQL shadow testing framework that captured production traffic and replayed them to test instances. For each application workload, we constructed test instances on 8.0 and replayed shadow traffic queries to them. We captured and logged the errors returning from the 8.0 server and found some interesting problems. Unfortunately, not all of these problems were found during testing. For example, the transaction deadlock was discovered by applications during the migration. We were able to roll back these applications to 5.6 temporarily while we researched different solutions.</span></p>
<ul>
<li aria-level="1"><span>New reserved keywords were introduced in 8.0 and a few, such as groups and rank, conflicted with popular table column names and aliases used in application queries. These queries did not escape the names via backquotes, leading to parsing errors. Applications using software libraries that automatically escaped the column names in queries did not hit these issues, but not all applications used them. Fixing the problem was simple, but it took time to track down application owners and codebases generating these queries.</span></li>
<li aria-level="1"><span>A few REGEXP incompatibilities were also found between 5.6 and 8.0.</span></li>
<li aria-level="1"><span>A few applications hit </span><a href="https://bugs.mysql.com/bug.php?id=98324"><span>repeatable-read transaction deadlocks</span></a><span> involving </span><span>insert … on duplicate key</span><span> queries on InnoDB. 5.6 had a bug which was corrected in 8.0, but the fix increased the likelihood of transaction deadlocks. After analyzing our queries, we were able to resolve them by lowering the isolation level. This option was available to us since we had made the switch to row-based replication.</span></li>
<li aria-level="1"><span>Our custom 5.6 Document Store and JSON functions were not compatible with 8.0’s. Applications using Document Store needed to convert the document type to text for the migration. For the JSON functions, we added 5.6-compatible versions to the 8.0 server so that applications could migrate to the 8.0 API at a later time.</span></li>
</ul>
<p><span>Our query and performance testing of the 8.0 server uncovered a few problems that needed to be addressed almost immediately.</span></p>
<ul>
<li aria-level="1"><span>We found new mutex contention hotspots around the ACL cache. When a large number of connections were opened simultaneously, they could all block on checking ACLs.</span></li>
<li aria-level="1"><span>Similar contention was found with binlog index access when many binlog files are present and high binlog write rates rotate files frequently.</span></li>
<li aria-level="1"><span>Several queries involving temp tables were broken. The queries would return unexpected errors or take so long to run that they would time out.</span></li>
</ul>
<p><span>Memory usage compared with 5.6 had increased, especially for our MyRocks instances, because InnoDB in 8.0 must be loaded. The default performance_schema settings enabled all instruments and consumed significant memory. We limited the memory usage by only enabling a small number of instruments and making code changes to disable tables that could not be manually turned off. However, not all the increased memory was being allocated by performance_schema. We needed to examine and modify various InnoDB internal data structures to reduce the memory footprint further. This effort brought 8.0’s memory usage down to acceptable levels. </span></p>
<h2><span>What’s next</span></h2>
<p><span>The 8.0 migration has taken a few years so far. We have converted many of our InnoDB replica sets to running entirely on 8.0. Most of the remaining ones are at various stages along the migration path. Now that most of our custom features have been ported to 8.0, updating to Oracle’s minor releases has been comparatively easier and we plan to keep pace with the latest versions.</span></p>
<p><span>Skipping a major version like 5.7 introduced problems, which our migration needed to solve.</span></p>
<p><span>First, we could not upgrade servers in place and needed to use logical dump and restore to build a new server. However, for very large mysqld instances, this can take many days on a live production server and this fragile process will likely be interrupted before it can complete. For these large instances, we had to modify our backup and restore systems to handle the rebuild.</span></p>
<p><span>Second, it is much harder to detect API changes because 5.7 could have provided deprecation warnings to our application clients to fix potential issues. Instead, we needed to run additional shadow tests to find failures before we could migrate the production workloads. Using mysql client software that automatically escaped schema object names helps reduce the number of compatibility issues.</span></p>
<p><span>Supporting two major versions within a replica set is hard. Once a replica set promotes its primary to be an 8.0 instance, it is best to disable and remove the 5.6 ones as soon as possible. Application users tend to discover new features that are supported only by 8.0, like </span><span>utf8mb4_0900</span><span> collations, and using these can break the replication stream between 8.0 and 5.6 instances.</span></p>
<p><span>Despite all the hurdles in our migration path, we have already seen the benefits of running 8.0. Some applications have opted for early conversion to 8.0 to utilize features like Document Store and improved datetime support. We have been considering how to support storage engine features like Instant DDL on MyRocks. Overall, the new version greatly expands on what we can do with MySQL @ Facebook.</span></p>

		
	</div></div>]]></content:encoded>
      <author>By Herman Lee, Pradeep Nayak</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/CD21_390_ENG_MySQL_HERO_FINAL_2x.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Thu, 22 Jul 2021 16:00:35 +0000</pubDate>
    </item>
    <item>
      <title>Fully Sharded Data Parallel: faster AI training with fewer GPUs</title>
      <link>https://engineering.fb.com/2021/07/15/open-source/fsdp/</link>
      <description>&lt;p&gt;Training AI models at a large scale isn’t easy. Aside from the need for large amounts of computing power and resources, there is also considerable engineering complexity behind training very large models. At Facebook AI Research (FAIR) Engineering, we have been working on building tools and infrastructure to make training large AI models easier. Our [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/15/open-source/fsdp/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/15/open-source/fsdp/&#34;&gt;Fully Sharded Data Parallel: faster AI training with fewer GPUs&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>Training AI models at a large scale isn’t easy. Aside from the need for large amounts of computing power and resources, there is also considerable engineering complexity behind training very large models. At Facebook AI Research (FAIR) Engineering, we have been working on building tools and infrastructure to make training large AI models easier. Our recent work in areas such as </span><a href="https://github.com/pytorch/fairseq/blob/master/examples/megatron_11b/README.md"><span>intra-layer model parallelism</span></a><span>, </span><a href="https://fairscale.readthedocs.io/en/latest/deep_dive/pipeline_parallelism.html"><span>pipeline model parallelism</span></a><span>, </span><a href="https://github.com/facebookresearch/fairscale#optimizer-state-sharding-zero"><span>optimizer state+gradient sharding</span></a><span>, and </span><a href="https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/moe_layer.py"><span>mixture of experts</span></a><span> is just part of our work to make training advanced AI models for any number of tasks more efficient.</span></p>
<p><span>Fully Sharded Data Parallel (FSDP) is the newest tool we’re introducing. It <a href="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/">shards</a> an AI model’s parameters across data parallel workers and can optionally offload part of the training computation to the CPUs. As its name suggests, FSDP is a type of data-parallel training algorithm. Although the parameters are sharded to different <a href="https://engineering.fb.com/2018/03/20/ml-applications/the-next-step-in-facebook-s-ai-hardware-infrastructure/">GPUs</a>, the computation for each microbatch of data is still local to each GPU worker. This conceptual simplicity makes FSDP easier to understand and more applicable to a wide range of usage scenarios (compared with intra-layer parallelism and pipeline parallelism). Compared with optimizer state+gradient sharding data parallel methods, FSDP shards parameters more uniformly and is capable of better performance via communication and computation overlapping during training.</span></p>
<p><span>With FSDP, it is now possible to more efficiently train models that are orders of magnitude larger using fewer GPUs. FSDP has been implemented in the </span><a href="https://github.com/facebookresearch/fairscale"><span>FairScale library</span></a><span> and allows engineers and developers to scale and optimize the training of their models with simple APIs. At Facebook, FSDP has already been integrated and tested for training some of our </span><a href="https://github.com/pytorch/fairseq"><span>NLP</span></a><span> and</span><a href="https://github.com/facebookresearch/vissl"><span> Vision</span></a><span> models.</span></p>
<h2><span>The high computational cost of large-scale training</span></h2>
<p><a href="https://arxiv.org/pdf/2001.08361.pdf"><span>NLP research</span></a><span> is one particular area where we can see the importance of efficiently leveraging compute for training AI. Last year, OpenAI announced that they had trained </span><a href="https://neurips.cc/virtual/2020/public/poster_1457c0d6bfcb4967418bfb8ac142f64a.html"><span>GPT-3</span></a><span>, the largest-ever neural language model, with 175 billion parameters. It is </span><a href="https://lambdalabs.com/blog/demystifying-gpt-3/"><span>estimated</span></a><span> to have taken roughly 355 GPU years to train GPT-3, or the equivalent of 1,000 GPUs working continuously for more than four months.</span></p>
<p><span>Besides requiring a lot of compute and engineering resources, most approaches to scaling like this introduce additional communication costs and require engineers to carefully evaluate trade-offs between memory use and computational efficiency. For example, typical data parallel training requires maintaining redundant copies of the model on each GPU, and model parallel training introduces additional communication costs to move activations between workers (GPUs).</span></p>
<p><span>FSDP is relatively free of trade-offs in comparison. It improves memory efficiency by sharding model parameters, gradients, and optimizer states across GPUs, and improves computational efficiency by decomposing the communication and overlapping it with both the forward and backward passes. FSDP produces identical results as standard distributed data parallel (DDP) training and is available in an easy-to-use interface that’s a drop-in replacement for PyTorch’s DistributedDataParallel module. Our early testing has shown that FSDP can enable scaling to trillions of parameters.</span></p>
<h2><span>How FSDP works</span></h2>
<p><span>In standard DDP training, every worker processes a separate batch and the gradients are summed across workers using an </span><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce"><span>all-reduce operation</span></a><span>. While DDP has become very popular, it takes more GPU memory than it needs because the model weights and optimizer states are replicated across all DDP workers.</span></p>
<p><span>One method to reduce replications is to apply a process called full parameter sharding, where only a subset of the model parameters, gradients, and optimizers needed for a local computation is made available. An implementation of this method, ZeRO-3, has already been popularized by Microsoft. </span></p>
<p><span>The key insight to unlock full parameter sharding is that we can decompose the </span><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allreduce"><span>all-reduce</span></a><span> operations in DDP into separate </span><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#reducescatter"><span>reduce-scatter</span></a><span> and <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html#allgather">all-gather</a> operations:</span></p>
<figure id="attachment_17828" aria-describedby="caption-attachment-17828"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?w=1024" alt="Full Sharded Data Parallel graph" width="1024" height="562" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png 1264w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=916,503 916w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=768,422 768w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=1024,562 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=96,53 96w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-graph-2a.png?resize=192,105 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17828">All-reduce as a combination of reduce-scatter and all-gather. The standard all-reduce operation to aggregate gradients can be decomposed into two separate phases: reduce-scatter and all-gather. During the reduce-scatter phase, the gradients are summed in equal blocks among ranks on each GPU based on their rank index. During the all-gather phase, the sharded portion of aggregated gradients available on each GPU are made available to all GPUs (see here for details on those operators).</figcaption></figure>
<p><span>We can then rearrange the reduce-scatter and all-gather so that each DDP worker needs to store only a single shard of parameters and optimizer states. The figure below illustrates standard DDP training (top) and FSDP training (bottom):</span></p>
<figure id="attachment_17812" aria-describedby="caption-attachment-17812"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?w=907" alt="Full Sharded Data Parallel graph" width="907" height="1024" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png 1566w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=811,916 811w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=768,867 768w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=907,1024 907w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=1361,1536 1361w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=96,108 96w, https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Graph-2.png?resize=192,217 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17812">A comparison of standard data parallel training and fully sharded data parallel training. In standard data parallel training methods, a copy of the model is present on each GPU and a sequence of forward and backward passes are evaluated on only a shard of the data. After these local computations, the parameters and optimizers for each local process are shared with the other GPUs in order to calculate the global weight update. In FSDP, only a shard of the model is present on a GPU. Then, locally, all weights are gathered from the other GPUs — by means of an all-gather step — to calculate the forward pass. This gathering of weights is then performed again before the backward pass. After that backward pass, the local gradients are averaged and sharded across the GPUs by means of a reduce-scatter step, which allows each GPU to update its local weight shard.</figcaption></figure>
<p><span>To maximize memory efficiency, we can discard the full weights after each layer’s forward pass, saving memory for subsequent layers. This can be implemented by applying the FSDP wrapper to every layer in the network (with </span><span>reshard_after_forward=True</span><span>). </span></p>
<p><span>In pseudo-code:</span></p>
<pre><span>FSDP forward pass:</span>
<span>    for layer_i in layers:</span>
<span>        all-gather full weights for layer_i</span>
<span>        forward pass for layer_i</span>
<span>        discard full weights for layer_i</span>

<span>FSDP backward pass:</span>
<span>    for layer_i in layers:</span>
<span>        all-gather full weights for layer_i</span>
<span>        backward pass for layer_i</span>
<span>        discard full weights for layer_i</span>
<span>        reduce-scatter gradients for layer_i</span></pre>
<h2><span>How to use FSDP</span></h2>
<p><span>There are several ways to use FSDP in large-scale AI research.</span><span> At this time, we offer four solutions to adapt to different needs.</span></p>
<h3><span>1. Using FSDP in language models</span></h3>
<p><span>For language models, FSDP is supported in the </span><a href="https://github.com/pytorch/fairseq"><i><span>fairseq</span></i><span> framework</span></a><span> via the following new arguments:</span></p>
<ul>
<li aria-level="1"><span>–ddp-backend=fully_sharded</span><span>: enables full sharding via FSDP</span></li>
<li aria-level="1"><span>–cpu-offload</span><span>: offloads the optimizer state and FP32 model copy to CPU (combine with</span><span>–optimizer=cpu_adam</span><span>)</span></li>
<li aria-level="1"><span>–no-reshard-after-forward</span><span>: increases training speed for large models (1B+ params) and is similar to ZeRO stage 2</span></li>
<li aria-level="1">Other popular options (<span>–fp16</span><span>, </span><span>–update-freq</span><span>, </span><span>–checkpoint-activations</span><span>, </span><span>–offload-activations</span><span>, etc.) continue to work as normal</span></li>
</ul>
<p><span>See the </span><a href="https://github.com/pytorch/fairseq/tree/master/examples/fully_sharded_data_parallel"><span>fairseq tutorial</span></a><span> for instructions on using FSDP to train a 13B-parameter model on eight GPUs or on a single GPU with FSDP + CPU offloading.</span></p>
<h3><span>2. Using FSDP in computer vision models</span></h3>
<p><span>For computer vision models, FSDP is supported in </span><a href="https://github.com/facebookresearch/vissl"><span>VISSL</span></a><span> and tested on RegNets architectures. Layers like BatchNorm and ReLU are seamlessly handled and tested for convergence.</span></p>
<p><span>Use the following options to enable FSDP:</span></p>
<ul>
<li aria-level="1"><span>config.MODEL.FSDP_CONFIG.AUTO_SETUP_FSDP=True</span></li>
<li aria-level="1"><span>config.MODEL.SYNC_BN_CONFIG.SYNC_BN_TYPE=pytorch</span></li>
<li aria-level="1"><span>config.MODEL.AMP_PARAMS.AMP_TYPE=pytorch</span></li>
</ul>
<p><span>See </span><a href="https://github.com/facebookresearch/vissl/blob/40441123a6f7098500676ca8800025c1f02e28b3/vissl/config/defaults.yaml#L498-L513"><span>this section</span></a><span> of the yaml config for additional options to config FSDP within VISSL.</span></p>
<h3><span>3. Using FSDP from PyTorch Lightning</span></h3>
<p><span>For easier integration with more general use cases, FSDP is supported as a beta feature by PyTorch Lightning. </span><a href="https://pytorch-lightning.readthedocs.io/en/latest/advanced/advanced_gpu.html#fully-sharded-training"><span>This tutorial</span></a><span> contains a detailed example on how to use the FSDP plugin with PyTorch Lightning. At a high level, adding </span><span>plugins=’fsdp’</span><span> below can activate it.</span></p>
<pre><span>model = MyModel()</span>
<span>trainer = Trainer(gpus=4, </span><b>plugins=&#39;fsdp&#39;</b><span>, precision=16)</span>
<span>trainer.fit(model)
</span><span>
trainer.test()</span>
<span>trainer.predict()</span></pre>
<h3><span>4. Using the FSDP library directly from FairScale</span></h3>
<p><span>The main library where FSDP has been developed, and where you can find the latest updates, is </span><a href="https://fairscale.readthedocs.io/en/latest/deep_dive/oss_sdp_fsdp.html"><span>FairScale</span></a><span>. You can directly use FSDP from FairScale with the below example by simply replacing the </span><span>DDP(my_module)</span><span>:</span></p>
<pre><span>from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP</span>
<span>...</span>
<span>sharded_module = </span><span><del>DDP(my_module)</del></span><b>FSDP(my_module)</b>
<span>optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)</span>
<span>for sample, label in dataload.next_batch:</span>
<span>  out = sharded_module(x=sample, y=3, z=torch.Tensor([1]))</span>
<span>  loss = criterion(out, label)</span>
<span>  loss.backward()</span>
<span>  optim.step()</span></pre>
<p><span>The FSDP library in FairScale exposes the low-level options for many important aspects of large-scale training. Here are some few important areas to consider when you apply FSDP with its full power.</span></p>
<ol>
<li aria-level="1"><b>Model wrapping: </b><span>In order to minimize the transient GPU memory needs, users need to wrap a model in a nested fashion. This introduces additional complexity. The </span><a href="https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/wrap/auto_wrap.py"><span>auto_wrap</span></a><span> utility is useful in annotating existing PyTorch model code for nested wrapping purposes.</span></li>
<li aria-level="1"><b>Model initialization:</b><span> Unlike DDP, FSDP does </span><b>not</b><span> automatically synchronize model weights between GPU workers. This means model initialization must be done carefully so that all GPU workers have the identical initial weights.</span></li>
<li aria-level="1"><b>Optimizer settings:</b><span> Due to sharding and wrapping, only certain types of optimizer and optimizer settings are supported by FSDP. In particular, if a module is wrapped by FSDP and its parameters are flattened into a single tensor, users cannot use different hyperparameters for different parameter groups in such a module.</span></li>
<li aria-level="1"><b>Mixed precision:</b><span> FSDP supports advanced mixed precision training with FP16 master weights, as well as FP16 reduce and scatter on the gradients. Certain parts of a model may converge only if full precision is used. In those cases, additional wrapping is needed to selectively run parts of a model in full precision.</span></li>
<li aria-level="1"><b>State checkpointing and inference:</b><span> When the model scale is large, saving and loading the model state can become challenging. FSDP supports several ways to make that task possible, but it is by no means trivial.</span></li>
<li aria-level="1"><span>Finally, FSDP is often used together with </span><b>activation checkpointing</b><span> functions like </span><a href="https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/checkpoint/checkpoint_activations.py"><span>checkpoint_wrapper</span></a><span> from FairScale. Users may need to carefully tune the activation checkpointing strategy to fit a large model within limited GPU memory space.</span></li>
</ol>
<h2><span>Next steps</span></h2>
<p><span>FSDP is open source, and early users have tried it and contributed to it. We think it can benefit the entire research community, and we look forward to working with everyone in making it better. In particular, these are some of the important areas.</span></p>
<ol>
<li aria-level="1"><b>Making FSDP more general.</b><span> So far, FSDP has been used on both NLP and vision models with SGD and Adam optimizers. As newer models and optimizers emerge, FSDP needs to continue supporting them. Being a purely data-parallel training scheme, FSDP has the greatest potential to be general in supporting a wide range of AI algorithms.</span></li>
<li aria-level="1"><b>Making FSDP auto-tune. </b><span>There are many knobs that users can tune today with FSDP for both scaling and performance. We look forward to developing algorithms for auto-tuning both GPU memory usage and training performance.</span></li>
<li aria-level="1"><span>In addition to training, more </span><b>scalable inference</b><span> and model serving is an important use case that FSDP might need to support.</span></li>
<li aria-level="1"><span>Last but not least, refactoring and continuing to </span><b>modularize FSDP</b><span> and its core components is equally important to newer and better features.</span></li>
</ol>
<h2><span>Try it out and contribute!</span></h2>
<p><span>FSDP is currently available directly from the </span><a href="https://github.com/facebookresearch/fairscale"><span>FairScale library</span></a><span>.</span></p>
<p><span>Thanks for sticking with us thus far. Please try FSDP in your research or production work. We would love to hear your feedback, and, as always, pull requests are welcome! </span></p>

		
	</div></div>]]></content:encoded>
      <author>By Myle Ott, Sam Shleifer, Min Xu, Priya Goyal, Quentin Duval, Vittorio Caggiano</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/FSDP-Hero-FINAL-1.png" length="0" type="image/png"></enclosure>
      <pubDate>Thu, 15 Jul 2021 16:00:47 +0000</pubDate>
    </item>
    <item>
      <title>How WhatsApp enables multi-device capability</title>
      <link>https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/</link>
      <description>&lt;p&gt;For years, people have been asking us to create a true multi-device experience that allows people to use WhatsApp on other devices without requiring a smartphone connection. Today, we’re announcing the rollout of a limited public beta test for WhatsApp’s updated multi-device capability.  With this new capability, you can now use WhatsApp on your phone [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/14/security/whatsapp-multi-device/&#34;&gt;How WhatsApp enables multi-device capability&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>For years, people have been asking us to create a true multi-device experience that allows people to use WhatsApp on other devices without requiring a smartphone connection.</span></p>
<p><span>Today, we’re announcing the rollout of a limited public beta test for WhatsApp’s updated multi-device capability. </span></p>
<p><span>With this new capability, you can now use WhatsApp on your phone and up to four other nonphone devices simultaneously — even if your phone battery is dead. Each companion device will connect to your WhatsApp independently while maintaining the same level of privacy and security through end-to-end encryption that people who use WhatsApp have come to expect. Importantly, we have developed new technologies to maintain end-to-end encryption while still managing to sync your data — such as contact names, chat archives, starred messages, and more — across devices.</span></p>
<p><span>To achieve this, we had to rethink WhatsApp’s architecture and design new systems to enable a standalone multi-device experience while preserving <a href="https://engineering.fb.com/2021/04/16/security/dit/">privacy and end-to-end encryption</a>. </span></p>
<h2><span>Taking smartphones out of the equation</span></h2>
<p>The current WhatsApp experience for companion devices on web, macOS, Windows, and Portal uses a smartphone app as the primary device, making the phone the source of truth for all user data and the only device capable of end-to-end encrypting messages for another user, initiating calls, etc. Companion devices maintain a persistent secure connection with the phone and simply mirror its contents on their own UI.</p>
<p><span>This architecture makes it easy to deliver a seamlessly synchronized experience between a phone and companion device without compromising on security. However, it comes with some significant reliability trade-offs: By requiring the phone to perform all operations, companion devices are slower and frequently get disconnected — especially when the phone has a poor connection, its battery is running low, or the application process gets killed by the phone’s OS. It also allows for only a single companion device to be operative at a time, meaning people can’t be on a call in Portal while checking their messages on their PC, for example. </span></p>
<p><span>The new WhatsApp multi-device architecture removes these hurdles, no longer requiring a smartphone to be the source of truth while still keeping user data seamlessly and securely synchronized and private.</span></p>
<p><span>The challenge in accomplishing this was in maintaining the secure user experience across devices without having to store people’s private messages on our servers in new ways.</span></p>
<h2><span>Meeting the security challenges of multiple devices</span></h2>
<p><span>Prior to the introduction of multi-device, everyone on WhatsApp was identified by a single identity key from which all encrypted communication keys were derived. With multi-device, each device now has its own identity key.</span></p>
<p><span>The WhatsApp server maintains a mapping between each person’s account and all their device identities. When someone wants to send a message, they get their device list keys from the server.  </span></p>
<p><span>We have also addressed the challenge of preventing a malicious or compromised server from eavesdropping on someone’s communications by surreptitiously adding devices to someone’s account. We use a combination of technologies to solve this: First, we have extended security codes to now represent the combination of all of someone’s device identities so that anyone and their contact can always verify all the devices they are sending messages to. </span></p>
<p><span>Second, in order to reduce the number of times that someone needs to perform identity verifications, we have developed and will roll out a technology called Automatic Device Verification. This system allows for devices to automatically establish trust between each other in a way that someone needs to compare another user’s security code only if that user reregisters their entire account, rather than each time they link a new device to their account. </span></p>
<p><span>Finally, we also give people additional control and protections over which devices are linked to their account. First, everyone will continue to be required to link new companion devices by scanning a QR code from their phone. This process now requires biometric authentication before linking where people have enabled this feature on compatible devices. Finally, people will be able to see all the companion devices linked to their account as well as when they were last used, and will be able to log out of them remotely if needed. </span></p>
<h2><span>Maintaining message privacy</span></h2>
<p><span>When people message each other in a one-on-one chat, a pairwise encrypted session is established between each of the sender’s and recipient’s devices. WhatsApp multi-device uses a client-fanout approach</span><span>,</span> <span>where the WhatsApp client sending the message encrypts and transmits it N number of times to N number of different devices </span><span>— those in the sender and receiver’s device lists</span><span>. Each message is individually encrypted using the established pairwise encryption session with each device. M</span><span>essages are not stored on the server after they are delivered. For groups, we still use the same scalable Sender Key encryption scheme from the Signal Protocol.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?w=1024" alt="WhatsApp Multi-device graphic" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Legacy-architecture_FINAL.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<figure id="attachment_17849" aria-describedby="caption-attachment-17849"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?w=1024" alt="WhatsApp Multi-device graphic" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Multi-Device_New-graph.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17849">WhatsApp’s legacy architecture used a smartphone as the source of truth. But with the new multi-device capability, up to four other nonphone companion devices can connect to WhatsApp independently while still maintaining the same level of privacy and security.</figcaption></figure>
<h2><span>Adapting voice and video protocols for multi-device, end-to-end encryption  </span></h2>
<p><span>When someone on WhatsApp makes a voice or video call:</span><span><br/>
</span></p>
<ol>
<li aria-level="1"><span>The initiator generates a set of random 32-byte </span><span>SRTP</span><span> master secrets for each of the recipient’s devices.</span><span><br/>
</span></li>
<li aria-level="1"><span>The initiator sends an incoming call message (using the client-fanout approach described above) to each of the devices of the recipient. Each recipient’s device receives this message, which contains the encrypted </span><span>SRTP</span><span> master secret.</span></li>
<li aria-level="1">If the responder answers the call from one of the devices, a <span>SRTP</span><span> encrypted call is started, protected by the </span><span>SRTP</span><span> master secret generated for that device.</span></li>
</ol>
<p><span>The </span><span>SRTP</span><span> master secret persists in memory on the client device and is used only during the call. Our servers do not have access to the </span><span>SRTP</span><span> master secrets.</span></p>
<p><span>For group calls, the server randomly selects a participant device that is in the call (either the initiator or a device on which a user has accepted the call) to generate the </span><span>SRTP</span><span> master secret. That device generates the secret and sends it to other active participant devices through pairwise end-to-end encryption. This process is repeated, and the keys are reset whenever someone joins or leaves the call.</span></p>
<h2><span>Keeping message history and other application states in sync across devices</span></h2>
<p><span>We want to ensure that people have a consistent experience with WhatsApp no matter the device they are using. To achieve this, we synchronize message history as well as other application state data (such as contact names, whether a chat is archived, or if a message is starred) across devices. All of this data is synchronized and end-to-end encrypted between your devices.</span></p>
<p><span>For message history: When a companion device is linked, the primary device encrypts a bundle of the messages from recent chats and transfers them to the newly linked device. The key to this encrypted message history blob is delivered to the newly linked device via an end-to-end encrypted message. After the companion device downloads, decrypts, unpacks, and stores the messages securely, the keys are deleted. From that point forward, the companion device accesses the message history from its own local database.</span></p>
<p><span>Other application data requires more than an initial transfer from the phone. We also need an ongoing synchronization every time someone modifies their application state (e.g., when they add a new contact, mute a chat, or star a message).</span></p>
<p><span>To solve this, the WhatsApp server securely stores a copy of each application state that all of someone’s devices can access. To properly secure this, all the information, and even the metadata about the information (what kind of user data is stored or accessed), is end-to-end encrypted with constantly changing keys known only to that person’s devices. </span></p>
<h2><span>How to try WhatsApp multi-device beta </span></h2>
<p><span>We plan to initially test the experience with a small group of users from our existing beta program. We will continue optimizing performance and adding a few additional features before slowly rolling it out more broadly. Those who opt in can always opt back out.</span></p>
<p><span>For more information about the beta and to sign up, visit the <a href="https://faq.whatsapp.com/general/download-and-installation/about-multi-device-beta">WhatsApp Help Center</a>.</span></p>
<p><span>For more information about WhatsApp multi-device, read our updated <a href="https://www.whatsapp.com/security/WhatsApp_Security_Whitepaper_v4_Preview.pdf">whitepaper</a>.</span></p>

		
	</div></div>]]></content:encoded>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/WhatsApp-Multi-Device_Hero-Image_FINAL.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Wed, 14 Jul 2021 18:59:40 +0000</pubDate>
    </item>
    <item>
      <title>Enforcing encryption at scale</title>
      <link>https://engineering.fb.com/2021/07/12/security/enforcing-encryption/</link>
      <description>&lt;p&gt;Our infrastructure supports thousands of services that handle billions of requests per second. We’ve previously discussed how we built our service encryption infrastructure to keep these globally distributed services operating securely and performantly. This post discusses the system we designed to enforce encryption policies within our network and shares some of the lessons we learned [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/12/security/enforcing-encryption/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/12/security/enforcing-encryption/&#34;&gt;Enforcing encryption at scale&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><span>Our infrastructure supports thousands of services that handle billions of requests per second. We’ve previously discussed how we built our </span><a href="https://engineering.fb.com/security/service-encryption/"><span>service encryption infrastructure</span></a><span> to keep these globally distributed services operating securely and performantly. This post discusses the system we designed to enforce encryption policies within our network and shares some of the lessons we learned in the process. The goal of this enforcement is to catch any regression quickly and shut it off, keeping our internal traffic secure at the application level via TLS.</span></p>
<h2><span>Organizational challenges</span></h2>
<p><span>Implementing a transit encryption enforcement policy at Facebook scale requires careful planning and communication, in addition to the technical challenges we’ll discuss in a bit. We want the site to stay up and remain reliable so the people using our services will be unaffected by and unaware of any changes to the infrastructure.</span></p>
<p><span>Communicating the intent, specific timelines, and rollout strategy went a long way toward minimizing any potential disruptions for the thousands of teams that run services at Facebook. We use </span><a href="https://www.facebook.com/workplace"><span>Workplace</span></a><span> within Facebook, which enables us to easily distribute that information across a variety of groups with a single share button and consolidate feedback and concerns in a single place for all employees to see. We made sure to include the following:</span></p>
<ul>
<li aria-level="1"><span>A description of the impact of our enforcement mechanism and how it might appear at the application layer</span></li>
<li aria-level="1"><span>A dashboard for engineers to see whether their traffic would be affected</span></li>
<li aria-level="1"><span>The rollout and monitoring plan</span></li>
<li aria-level="1"><span>Dedicated points of contact and a Workplace group where users could ask questions about impact and troubleshoot any issues</span></li>
</ul>
<p><span>The post required multiple discussions within the team to come up with a rollout plan, dashboard requirements, and realistic timelines to meet the goals of the project. This level of communication proved to be useful as the team gathered important feedback early in the process. </span></p>
<h2><span>Building our SSLWall</span></h2>
<p><span>Hardware choke points are a natural approach to providing transparent enforcement. There are options, such as layer 7 firewalls, that let us do deep packet inspection, but executing fine-grained rollouts and the complexities of Facebook’s network would make implementing such a solution a nightmare. Additionally, working at a network firewall level would introduce a much larger blast radius of impacted traffic, and a single configuration issue could end up killing off traffic that we weren’t meant to touch.</span></p>
<p><span>Our team decided to develop and deploy what is internally known as SSLWall, a system that cuts off non-SSL connections across various boundaries. Let’s dive a bit into the design decisions behind this solution.</span></p>
<h3><span>Requirements </span></h3>
<p><span>We needed to be thorough when considering the requirements of a system that would potentially block traffic at such a large scale. The team came up with the following requirements for SSLWall, all of which had an impact on our design decisions:</span></p>
<ul>
<li aria-level="1"><span>Visibility into what traffic is being blocked. Service owners needed a way to assess impacts, and our team needed to be proactive and reach out whenever we felt there was a problem brewing.</span></li>
<li aria-level="1"><span>A passive monitoring mode in which we could turn a knob to flip to active enforcement. This helps us determine impacts early on and prepare teams.</span></li>
<li aria-level="1"><span>A mechanism to allow certain use cases to bypass enforcement, such as BGP, SSH, and approved network diagnostic tools.</span></li>
<li aria-level="1"><span>Support for cases like HTTP CONNECT and STARTTLS. These are instances that do a little bit of work over plaintext before doing a TLS handshake. We have many use cases for these in our infrastructure, such as HTTP tunneling, MySQL security, and SMTP, so these must not break, especially since they eventually encrypt the data with TLS.</span></li>
<li aria-level="1"><span>Extensible configurability. We might have different requirements depending on the environment in which SSLWall operates. Additionally, having important knobs that can be tuned with little disruption means we can roll features forward or back at our own pace.</span></li>
<li aria-level="1"><span>Transparent to the application. Applications should not need to rebuild their code or incur any additional library dependencies for SSLWall to operate. The team needed the ability to iterate quickly and change configuration options independently. In addition, being transparent to the application means SSLWall needs to be performant and use minimal resources without having an impact on latencies.</span></li>
</ul>
<p><span>These requirements all led us down the path of managing a host-level daemon that had a user space and kernel-level component. We needed a low-compute way to inspect all connections transparently and act on them.  </span></p>
<h3><span>eBPF</span></h3>
<p><span>Since we wanted to inspect every connection without needing any changes at the application level, we needed to do some work in the kernel context. We </span><a href="https://ebpf.io/"><span>use eBPF</span></a><span> extensively, and it provides all of the capabilities needed for SSLWall to achieve its goals. We leveraged a number of technologies that eBPF provides:</span></p>
<ul>
<li aria-level="1"><a href="http://man7.org/linux/man-pages/man8/tc-bpf.8.html"><span>tc-bpf</span></a><span>: We leveraged Linux’s </span><a href="http://tldp.org/HOWTO/Traffic-Control-HOWTO/intro.html"><span>traffic control</span></a><span> (TC) facility and implemented a filter using eBPF.  At this layer, we are able to do some computation on a per-packet basis for packets flowing in and out of the box. TC allows us to operate on a broader range of kernels within Facebook’s fleet. It wasn’t the perfect solution, but it worked for our needs at the time.</span></li>
<li aria-level="1"><span>kprobes: eBPF allows us to attach programs to kprobes, so we can run some code within the kernel context whenever certain functions are called. We were interested in the </span><span>tcp_connect</span><span> and </span><span>tcp_v6_destroy_sock</span><span> functions. These functions are called when a tcp connection is established and torn down, respectively. Old kernels played a factor in our use of kprobes as well.</span></li>
<li aria-level="1"><span>maps: eBPF provides access to a number of map types, including arrays, bounded LRU maps, and perf events</span></li>
</ul>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg" alt="" width="1920" height="950" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=916,453 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=768,380 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=1024,507 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=1536,760 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=96,48 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart1.jpg?resize=192,95 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<figure id="attachment_17786" aria-describedby="caption-attachment-17786"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg" alt="Diagrams showing how kprobes, the tc filter, and our maps interact with one another when determining whether a connection needs to be blocked." width="1920" height="882" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=916,421 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=768,353 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=1024,470 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=1536,706 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=96,44 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart2.jpg?resize=192,88 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17786">Diagrams showing how kprobes, the tc filter, and our maps interact with one another when determining whether a connection needs to be blocked.</figcaption></figure>
<h3><span>The management daemon</span></h3>
<p><span>We built a daemon, which manages the eBPF programs we install and emits logs to </span><a href="https://engineering.fb.com/data-infrastructure/scribe/"><span>Scribe</span></a><span> from our perf events. The daemon also provides the ability to update our TC filter, handles configuration changes (leveraging </span><a href="https://research.fb.com/wp-content/uploads/2016/11/holistic-configuration-management-at-facebook.pdf"><span>Facebook’s Configerator</span></a><span>), and monitors health.</span></p>
<p><span>Our eBPF programs are also bundled with this daemon. This makes management of releases easier to deal with, as we only have one software unit to monitor instead of needing to track a daemon and eBPF release. Additionally, we can modify the schema of our BPF tables, which both user space and kernel space consult, without compatibility concerns between releases.</span></p>
<h3><span>Technical challenges</span></h3>
<p><span>As one would expect, we encountered a number of interesting technical challenges while rolling out SSLWall at Facebook’s scale. A few highlights include:</span><span><br/>
</span><span> </span></p>
<ul>
<li aria-level="1"><a href="https://en.wikipedia.org/wiki/TCP_Fast_Open"><span>TCP Fast Open (TFO)</span></a><span>: We hit an interesting challenge around kprobe and TC filter execution order that was exposed by our use of TFO within the infra. In particular, we needed to move some of our flow tracking code to a kprobe prehandler.</span></li>
<li aria-level="1"><span>BPF Program Size Limit: All BPF programs are subject to size and complexity limits, which may vary based on the kernel version.</span></li>
<li aria-level="1"><span>Performance: We spent many engineering cycles optimizing our BPF programs, particularly the TC filter, so that SSLWall’s CPU impact on some of our critical high QPS services with high fanout remained trivial. Identifying early exit conditions and using BPF arrays over LRUs where possible proved effective.</span></li>
</ul>
<h2><span>TransparentTLS and the long tail</span></h2>
<p><span>With enforcement in place, we needed a way to address noncompliant services without significant engineering time. This included things like torrent clients, open source message queues, and some Java applications. While most applications use common internal libraries where we could bake this logic in, the ones that do not need a different solution.</span></p>
<p><span>Essentially, the team was left with the following requirements for what we refer to as Transparent TLS (or TTLS for short):</span></p>
<ul>
<li aria-level="1"><span>Transparently encrypt connections without the need for application changes.</span></li>
<li aria-level="1"><span>Avoid double encryption for existing TLS connections.</span></li>
<li aria-level="1"><span>Performance can be suboptimal for this long tail.</span></li>
</ul>
<p><span>It’s clear that a proxy solution would have helped here, but we needed to ensure that the application code didn’t need to change and that configuration would be minimal.</span></p>
<p><span>We settled on the following architecture: </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg" alt="" width="1920" height="739" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=916,353 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=768,296 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=1024,394 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=1536,591 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=96,37 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart3.jpg?resize=192,74 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The challenge with this approach is transparently redirecting application connections to the local proxy. Once again, we use BPF to solve this problem. Thanks to the cgroup/connect6 hook, we can intercept all </span><a href="https://man7.org/linux/man-pages/man2/connect.2.html"><span>connect(2)</span></a><span> calls made by the application and redirect them to the proxy as needed.</span></p>
<figure id="attachment_17788" aria-describedby="caption-attachment-17788"><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg" alt="Diagram showing application and proxy logic for transparent connect." width="1920" height="1181" srcset="https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg 1920w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=916,563 916w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=768,472 768w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=1024,630 1024w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=1536,945 1536w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=96,59 96w, https://engineering.fb.com/wp-content/uploads/2021/07/EE_chart4.jpg?resize=192,118 192w" sizes="(max-width: 992px) 100vw, 62vw"/><figcaption id="caption-attachment-17788">Diagram showing application and proxy logic for transparent connect.</figcaption></figure>
<p><span>Aside from the application remaining unchanged, the BPF program makes policy decisions about routing through the proxy. For instance, we optimized this flow to bypass the proxy for all TLS connections created by the application to avoid double encryption.</span></p>
<p><span>This work on enforcement has brought us to a state where we can confidently say that our traffic is encrypted at our scale. However, our work is not yet complete. For instance, there are many new facilities that have come about in BPF that we intend to leverage as we remove old kernel support. We can also improve our transparent proxy solutions and leverage custom protocols to multiplex connections and improve performance.</span></p>
<p><i><span>We’d like to thank Takshak Chahande, Lingnan Gao, Andrey Ignatov, Petr Lapukhov, Puneet Mehra, Kyle Nekritz, Deepak Ravikumar, Paul Saab, and Michael Shao for their work on this project.</span></i></p>

		
	</div></div>]]></content:encoded>
      <author>By Neel Goyal, Ajanthan Asogamoorthy, Mingtao Yang</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/07/encryption_hero.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Mon, 12 Jul 2021 16:00:56 +0000</pubDate>
    </item>
    <item>
      <title>Ribbon filter: Practically smaller than Bloom and Xor</title>
      <link>https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/</link>
      <description>&lt;p&gt;What the research is: The Ribbon filter is a new data structure that is more space-efficient than the popular Bloom filters that are widely used for optimizing data retrieval. One of the ways that Bloom, and now Ribbon, filters solve real engineering problems is by providing smooth configurability unmatched by other filters. Bloom filters work [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/07/09/data-infrastructure/ribbon-filter/&#34;&gt;Ribbon filter: Practically smaller than Bloom and Xor&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<h2><span>What the research is:</span></h2>
<p><span>The Ribbon filter is a new data structure that is more space-efficient than the popular Bloom filters that are widely used for optimizing data retrieval. One of the ways that Bloom, and now Ribbon, filters solve real engineering problems is by providing smooth configurability unmatched by other filters. Bloom filters work by overapproximating a set of keys associated with some data resource. With a Bloom filter, almost all negative queries to that resource can be skipped (filtered) because the Bloom filter rejects almost all query keys not associated with the resource.</span></p>
<p><span>With proper data layout and design, the Ribbon filter is the first Bloom alternative to match the near-continuous, hazard-free, accuracy-versus-space trade-off provided by Bloom filters.</span></p>
<p><span>Here, near-continuous means efficiently utilizing any amount of memory to represent any number of keys, so that wasted memory such as in internal fragmentation can be minimized to zero. The typical hazard to the accuracy-versus-space trade-off is bit alignment, where some data sizes (e.g., 4, 8, or 16 bits per key) are faster to access than others. Like Bloom, our data layout for Ribbon does not suffer this hazard in access times so it is more freely configurable. And Ribbon filters add new freedom of configurability to the space-versus-time trade-off.</span></p>
<p><span>Building on some prior lines of research, the Ribbon filter combines a simplified, faster, and more flexible construction algorithm; a data layout optimized for filter queries; and near-continuous configurability to make a practical alternative to static (immutable) Bloom filters.</span></p>
<p><span>While well-engineered Bloom filters are extremely fast, they use roughly 50 percent more space (overhead) than the information-theoretic lower bound for filters on arbitrary keys. When Bloom filters cannot meet an application’s space efficiency targets, Ribbon filter variants dominate in space-versus-time trade-offs with near continuous configurability and space overhead as low as 1 percent or less. Ribbon filters have O(1) query times and save roughly 1/3 of memory compared with Bloom filters.</span></p>
<h2><span>How it works: </span></h2>
<p><span>Like some related immutable structures used for perfect hashing and maps, Ribbon filters are constructed by solving a linear system given by hash functions applied to a set of keys. Each row in the linear system expresses that querying as some key, which involves XOR-ing the values at some set of array indices, must yield a prescribed value to indicate it is “in” the set of keys. </span></p>
<p><span>Despite using Boolean — GF(2) — arithmetic, the approach to solving this logical system of equations is to use Gaussian elimination, which fundamentally means subtracting equations from one another until you can isolate variables (unknowns). If a solution exists, this approach will find it.</span></p>
<p><span>The name Ribbon has dual meanings. First, we use a linear system from Dietzfelbinger and Walzer whose sorted coefficient matrix resembles a physical ribbon, or a wavy approximation of a band matrix. Gaussian elimination is fundamentally more efficient on this system because it is already close to a reduced form.</span></p>
<p><span>Ribbon also stands for Rapid Incremental Boolean Banding ON the fly, which is the name of our fast and flexible new Gaussian solver. Through an approach resembling insertion into a linear-probed hash table, Ribbon does Gaussian elimination on the fly. This saves time and space in construction because row reductions can be done in registers rather than in memory, and because the reduced form of the ribbon coefficient matrix — a band matrix — is more space-efficient than explicitly representing the ribbon form. On-the-fly construction also unlocks a solution to the core challenge of the Ribbon approach: scaling its space efficiency to very large numbers of keys.  </span></p>
<h2><span>Why it matters: </span></h2>
<p><span>At Facebook’s scale, we expect Ribbon filters to save several percent of RAM resources, with a tiny increase in CPU usage for some major storage systems. However, we do not implement efficiency gains at all engineering costs, so it’s also important to have a user-friendly data structure. This issue stalled implementation of other Bloom alternatives offering some space savings. </span></p>
<p><span>The Ribbon filter opens these new trade-offs without introducing notable discontinuities or hazards in the configuration space. In other words, there is some complexity to make Ribbon filters general and highly configurable, but these details can be hidden behind a relatively simple API. You have essentially free choice over any three of the four core performance dimensions — number of keys added to the set, memory usage, CPU efficiency, and accuracy — and the accuracy is automatically well optimized.</span></p>
<h2><span>Read the full paper: </span></h2>
<p><a href="https://arxiv.org/abs/2103.02515"><span>Ribbon filter: Practically smaller than Bloom and Xor</span></a></p>

		
	</div></div>]]></content:encoded>
      <author>By Peter Dillinger</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg" length="0" type="image/jpeg"></enclosure>
      <pubDate>Fri, 09 Jul 2021 16:00:16 +0000</pubDate>
    </item>
    <item>
      <title>Asicmon: A platform agnostic observability system for AI accelerators</title>
      <link>https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/</link>
      <description>&lt;p&gt;We will be hosting a talk about our work on, “A Platform Agnostic Observability System for AI Accelerators” during our virtual Systems @Scale event at 10:20 a.m. PT on Wednesday, June 30, followed by a live Q&amp;#38;A session. Please submit any questions to systemsatscale@fb.com before the event. Accelerators are special-purpose hardware devices optimized for specific [...]&lt;/p&gt;&#xA;&lt;p&gt;&lt;a class=&#34;btn btn-secondary understrap-read-more-link&#34; href=&#34;https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/&#34;&gt;Read More...&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The post &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com/2021/06/28/data-center-engineering/asicmon/&#34;&gt;Asicmon: A platform agnostic observability system for AI accelerators&lt;/a&gt; appeared first on &lt;a rel=&#34;nofollow&#34; href=&#34;https://engineering.fb.com&#34;&gt;Facebook Engineering&lt;/a&gt;.&lt;/p&gt;&#xA;</description>
      <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div>

		<p><b><i>We will be hosting a talk about our work on, “<a href="https://atscaleconference.com/events/systems-scale-summer-2021/">A Platform Agnostic Observability System for AI Accelerators</a>” during our virtual <a href="https://atscaleconference.com/events/systems-scale-summer-2021/">Systems @Scale</a> event at 10:20 a.m. PT on Wednesday, June 30, followed by a live Q&amp;A session. Please submit any questions to <a href="mailto:systemsatscale@fb.com">systemsatscale@fb.com</a> before the event.</i></b></p>
<p><span>Accelerators are special-purpose hardware devices optimized for specific applications, like AI prediction and video encoding. And Application-specific hardware platforms play an important role in meeting the growing latency and compute demands of workloads like deep learning, content understanding, and video encoding.</span></p>
<p><span>At Facebook, the inevitable rise in use of accelerators in our data centers has led to better performance and energy efficiency. However, it is challenging to operate these heterogeneous platforms efficiently at scale. To ensure that these complex accelerators operate smoothly, we need an excellent observability system with monitoring and tracing capabilities so we can understand the performance and interactions between CPUs and accelerators.</span></p>
<p><span>To meet these challenges, we’ve introduced three new tools:</span></p>
<ol>
<li><b>ASIC Monitoring (Asicmon)</b><span>, a scalable observability framework. Asicmon’s library abstracts an accelerator’s custom interfaces and provides a standard interface to our internal tools. Asicmon has facilitated load balancing, performance monitoring, and automated health checks for hundreds of thousands of accelerators running in our data centers.</span></li>
<li><b>Asimov</b><span>, a custom specification language that makes developing and rapid prototyping new accelerators easier. It has shrunk our development time for onboarding a new accelerator from a month to under a week.</span></li>
<li><b>Atrace</b><span>, an accelerator tracing solution that collects traces remotely on production servers. It allows us to inspect accelerator systems in detail and provides actionable trace summaries and analyses. An initial version of Atrace allowed us to close a 10 percent performance gap between </span><a href="https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/"><span>Caffe2 and PyTorch implementations</span></a><span> of a large AI model. </span></li>
</ol>
<h2><span>Background</span></h2>
<p><span>Facebook’s cloud infrastructure handles about 150 trillion AI predictions per day for tasks ranging from feed recommendations to combating harmful content. Running these AI models comes with heavy infrastructure demands. And as these models improve, so do their <a href="https://arxiv.org/pdf/2003.09518.pdf">computational requirements</a>.</span></p>
<p><span>The graph below of AI model adoption at Facebook illustrates this </span><a href="https://arxiv.org/pdf/2003.09518.pdf"><span>unmistakable pattern</span></a><span>.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?w=852" alt="" width="852" height="916" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png 988w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=852,916 852w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=768,826 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=953,1024 953w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=96,103 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Facebook-AI-model-adoption-graph.png?resize=192,206 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>The need for accelerators</span></h2>
<p><span>Good old general-purpose processors (CPUs) offer versatility and have grown exponentially faster over the decades. However, CPUs fail to meet the rising</span><a href="https://openai.com/blog/ai-and-compute/"> <span>computational demands of AI applications</span></a><span> today. They also tend to exhibit inefficiency in terms of energy used per AI prediction. As investigated by the OpenAI community, we’ve seen </span><a href="https://openai.com/blog/ai-and-compute/"><span>two distinct eras of compute in AI models</span></a><span>. </span><span>In recent times, model complexity and compute requirements for AI have grown by roughly a factor of 10 each year. </span><span>This far outpaces improvements in CPU performance.   </span></p>
<p><span>How do we remedy this? By designing hardware that is customized to accelerate AI operations via application-specific integrated circuits (ASICs).  </span></p>
<p><span>Since 2019, Facebook has invested </span><a href="https://engineering.fb.com/2019/03/14/data-center-engineering/accelerating-infrastructure/"><span>heavily in deploying accelerator-based servers</span></a><span> to provide higher performance and energy efficiency. Today, our first-generation systems are 10-30x more performant on our largest AI models. They also delivered a 3-10x performance-per-watt improvement over a CPU.</span></p>
<p><span>We also invested in specialized hardware for </span><a href="https://engineering.fb.com/2021/04/05/video-engineering/how-facebook-encodes-your-videos/"><span>video encoding</span></a><span> and decoding. This enables Facebook to process the nearly 250 million videos uploaded to our app each day. These videos are viewable on any device and with varying internet bandwidth. Our first-generation video accelerators delivered a 10x performance-per-watt improvement in processing 4K videos.</span></p>
<p><span>The figure below illustrates the design of our AI inference server. As you can see, it consists of two Twin Lake CPUs and multiple accelerators (M.2 modules) connected to them using a PCIE switch</span><span>.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?w=1024" alt="" width="1024" height="615" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png 2000w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=916,550 916w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=768,461 768w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=1024,615 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=1536,922 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=96,58 96w, https://engineering.fb.com/wp-content/uploads/2021/06/AI-inference-server-design.png?resize=192,115 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>The challenges of operating accelerators</span></h2>
<p><span>In your typical cloud server, the CPU represents the most complex component. We focus a lot on building software to efficiently operate the CPU and monitor its performance and availability. However, with an accelerator system, we can imagine the CPU now has a complicated and brawnier sibling! The accelerator, or ASIC, represents a complex hardware and software system in its own right.</span></p>
<p><span>To deliver an excellent user experience, the cloud infrastructure needs to keep hundreds of thousands of accelerators running reliably and efficiently. This is where observability systems come to our rescue. Observability allows us to understand what happens in the accelerator hardware and software when any issue arises. It is useful in multiple ways: </span></p>
<ol>
<li><b>Health monitoring:</b><span> Just like any other piece of hardware, accelerators can overheat or hit a faulty condition or a functional bug. We can track various health metrics for the ASICs and use them in automated systems. These systems can then (if needed) remediate the issue by rebooting the accelerator or moving it into a repair state.</span></li>
<li><b>Performance monitoring:</b><span> By monitoring the performance and system load on an accelerator, we can efficiently scale our AI jobs to meet variable demand throughout the day. It also enables us to detect regressions in performance with new software deployments.</span></li>
<li><b>Performance profiling:</b><span> When we encounter issues such as poor performance or time-outs, we need to look deeper into how the accelerator server is functioning. We also need to equip software developers with tools to understand the performance of their applications while they run on accelerators. </span></li>
</ol>
<h2><span>The accelerator zoo</span></h2>
<p><span>Specialization is both a boon and bane for accelerators. As a result, we end up running multiple types of accelerators in our data centers at any given point.</span></p>
<p><span>In 2020 we started</span><a href="https://engineering.fb.com/2019/03/14/data-center-engineering/accelerating-infrastructure/"> <span>deploying the first generation</span></a><span> of these accelerators. In the near future, we will be developing two to three new accelerators for the second generation. Each accelerator will have unique driver interfaces, making the task of operating them harder. But duplicating the observability software for each accelerator would not be feasible in the timeline we have set out. The observability framework must be easy to prototype and adapt to multiple types of accelerators in a short time. It also needs to be efficient to avoid interfering with the original application. </span></p>
<h2><span>How we developed Asicmon and Asimov</span></h2>
<p><span>Our first challenge involved finding a way to effectively monitor different types of accelerators without duplicating code (and developer time). As you may have guessed, we can leverage abstraction to achieve this. </span></p>
<p><span>For example, consider an abstract metric: </span><i><span>device_utilization</span></i><span> — the measure of how busy an accelerator is — which becomes useful for balancing load across accelerators. To compute this metric, we may need to understand the internal architecture of the accelerator. With an abstract counter, however, engineers working on load balancing can more easily use the metric without being aware of finer details.</span></p>
<p><i><span>device_utilization = max(compute_core_active_i) /  total_time </span></i></p>
<p><span>With the above in mind, we designed Asicmon with these design objectives: </span></p>
<ol>
<li><b>Abstraction:</b><span> We needed a simple and uniform interface for all of our internal monitoring and operational tools to use. This enables infrastructure engineers and hardware teams to effectively operate multiple accelerators in a common way.</span></li>
<li><b>Development velocity:</b><span> Accelerators are new. Interfaces can also change due to evolving requirements. The framework should be easy to learn and able to iterate quickly.</span></li>
<li><b>Performance:</b><span> Finally, any observability system should be lightweight in terms of resources. As a result, it diminishes interference with high-throughput video and AI applications.</span></li>
</ol>
<p><span>The diagram below illustrates the overall software stack for monitoring accelerators. Asicmon acts as a bridge between individual accelerator drivers and the rest of the internal monitoring software. The left top illustrates automated health check tools that spot bad health signals and</span><a href="https://engineering.fb.com/2020/12/09/data-center-engineering/how-facebook-keeps-its-large-scale-infrastructure-hardware-up-and-running/"> <span>automatically fix faulty ASICs</span></a><span>. On the right, a telemetry daemon periodically publishes performance metrics for engineers to inspect the accelerators. Furthermore, automated load balancing and auto-scaling systems like</span><a href="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/"> <span>Shard Manager</span></a><span> utilize these counters. </span><span> </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h3><span>How does Asicmon work?  </span></h3>
<p><span>Under the hood, Asicmon creates an instance of a monitoring module per accelerator device. It maintains a cache of statistics that it updates periodically by probing the accelerator driver and computing-derived metrics. Queries to Asicmon’s standard interface for counters get implemented as a lookup into this cache. This shields the system against accidental overload of counter requests.</span></p>
<h3><span>Enter Asimov</span></h3>
<p><span>All great so far! We used abstraction to address the scalability aspect of observability software layers above Asicmon. However, the problem of building the glue code between the accelerator driver and these standard metrics still eluded us. This has to be done separately for each of the accelerators that have aggressive and overlapping timelines. So, we needed a method to develop on Asicmon that was quick to iterate and easy to ramp up on, while also being efficient. That’s where Asimov comes in. </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png 1999w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=916,516 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Asimov-flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Asimov is an expressive Python-like custom language to instrument the accelerator driver. It essentially allows developers to focus on how to probe the accelerator interfaces and express derived metrics using them. The Asimov compiler generates an efficient C++ implementation of the monitoring module. It also handles details like caching the metrics, periodically reading them, and providing thread safety.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-06_Flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The code snippets below show examples of Asimov being used to read system metrics using interfaces ranging from Linux sysfs files (a) to custom library C functions (b).</span></p>
<p><span>Asimov incorporates the same standard interface as Asicmon in its internal representation (the stats data structure, left hand side in the code). We can also invoke C-library functions provided by the device driver and express equations/conditions for derived metrics like any regular language.  </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-Code.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Asimov is built with the</span><a href="https://www.antlr.org/"> <span>ANTLR</span></a><span> compiler framework under the hood to provide the lexer/parser logic for the language. We then emit C++ code using templates that manage all the essential parts, like initialization, thread safety, etc., so someone using Asimov doesn’t need to worry about it.</span></p>
<h2><span>Asicmon in action</span></h2>
<p><span>Let’s look at a few illustrative examples of how Asimov and Asicmon are beneficial for operating accelerators at scale.</span></p>
<p><span>For AI inference applications, we use a system called</span><a href="https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/"> <span>Shard Manager</span></a><span> to automatically scale the inference service instances. A shard is essentially a copy of the AI model that can serve inferences. Asicmon measures the load on the device using an abstract metric — accelerator</span> <span>device</span> <span>utilization. This helps Shard Manager effectively balance the load among servers and automatically scale up or down the number of shards. The diagram below explains how the number of shards gets scaled automatically during model update rollouts and increases in traffic.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-07_Chart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>The figure below illustrates the advantages of building observability early on in a project’s development cycle. In our test deployment for video accelerators, we detected a memory leak using an Asicmon counter for available device memory. It took multiple fixes to the driver to finally resolve the issue, well in time before its debut in production.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-08_Chart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>Finally, let’s take a look at the ease of prototyping with Asimov. While we certainly took longer to build the first version of Asimov alongside the first video accelerator, supporting the second one (the AI inference accelerator) went incredibly fast. Bootstrapping basic metrics for the AI inference accelerator took less than a week. Since implementing Asicmon we’ve been able to increase our AI accelerator metrics support from ~30 percent to ~75 percent</span></p>
<h2><span>Atrace: Accelerator tracing at scale</span></h2>
<h3><span>Why tracing?</span></h3>
<p><span>Now that we can monitor the performance of accelerators in our data centers, the next step involves addressing why performance metrics like the latency and throughput change over time. The tried-and-tested method for CPUs involves leveraging a stack-based profiler to sample the running function call stack at periodic intervals. However, for inference accelerators, tracing is the best form of profiling. Why? Because accelerators use special hardware units and thus do not have an equivalent notion of a function stack on a core. </span></p>
<p><span>As shown in the figure below, a trace essentially consists of a time series of events occurring on different parts in a system. Events in a trace can represent, among many things, functions, execution of AI operators, or data transfers. Traces offer deeper insights into the operation of the system, including understanding the latency and scheduling of operators and how the CPU and accelerator interact with each other. </span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-10_Flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h3><span>Designing the tracing system</span></h3>
<p><span>While AI inference accelerator vendors do provide tools and APIs to collect traces from the device. These tools are designed to work on a single server and are often hard to use. In order to profile production systems better, we set out building a layer on top of this native capability. This better scales out the collection, processing, and analysis of traces themselves. </span></p>
<p><span>We kept two target use cases in mind while developing Atrace: </span></p>
<ol>
<li><b>Model development:</b><span> Model developers would typically be attempting to target their AI models to new inference hardware. They can run the tracing tool locally. But by integrating it with internal visualization and summarization tools, we can provide quicker feedback to engineers to iteratively tune their model.</span></li>
<li><b>Production:</b><span> Debugging performance issues in production is an important use case for tracing. For instance, say a continuous integration (CI) test detects a regression in performance. By collecting traces remotely and on the fly, production engineers can quickly diagnose the problem.</span></li>
</ol>
<p><span>To develop a scalable and ubiquitous tracing solution, we built a set of components that remotely trigger and collect traces. We save each trace to a shared storage and post process and summarize it. The diagram below outlines this, starting on the left with the trace being triggered, to the trace collection and post processing on the right.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-11_Flowchart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<h2><span>Insights from accelerator traces</span></h2>
<h3><span>Trace profiles and summaries</span></h3>
<p><span>Traces themselves can be enormous and overwhelming to dive into directly. However, we can learn a great deal about an AI program by summarizing the trace at a high level. To achieve this, we built a summary of trace statistics grouped by various AI operator types, as shown below.</span></p>
<p><img loading="lazy" src="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?w=1024" alt="" width="1024" height="576" srcset="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png 8000w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=580,326 580w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=916,515 916w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=768,432 768w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=1024,576 1024w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=1536,864 1536w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=2048,1152 2048w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=96,54 96w, https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-12_PieChart.png?resize=192,108 192w" sizes="(max-width: 992px) 100vw, 62vw"/></p>
<p><span>This operator breakdown shows our engineers which operators consume the most execution time and merit optimization. It also allows for comparisons and debugging of performance regressions between two software versions.</span></p>
<h3><span>Trace critical path analysis</span></h3>
<p><span>For advanced users, who might want to delve deeper into the traces, we added visualization support for both the open source</span><a href="https://www.chromium.org/developers/how-tos/trace-event-profiling-tool"> <span>Chrome trace viewer</span></a><span> and an internal trace visualization tool from Facebook. It works all from a single click. We can also run automated analysis on the trace to infer the critical path of operators. This uses the dependency graph of the AI model and trace statistics.</span></p>
<p><span>This analysis lets us optimize the latency of the AI prediction. It can also highlight issues like an imbalance in operators. Doing so closed a 10 percent latency gap between the Caffe2 and PyTorch versions of one of our AI models.</span></p>
<h3><span>Trace correlation</span></h3>
<p><span>Lastly, it is also noteworthy that several software layers exist to handle the processing of an inference request. These include the application layer, PyTorch framework, and</span><a href="https://engineering.fb.com/2018/09/13/ml-applications/glow-a-community-driven-approach-to-ai-infrastructure/"> <span>Glow</span></a><span>, an open source graph lowering compiler for accelerators.</span></p>
<p><span>For more complex models involving video understanding or natural language processing, we learned that the model may be run partially on a CPU and partially on an accelerator. Thus, tracing the operations across multiple layers on the CPU and correlating them with the accelerator becomes a necessity.</span></p>
<p><span>We developed a</span><a href="https://github.com/pytorch/glow/pull/5568"> <span>prototype of trace correlation</span></a><span> into Glow and PyTorch. This allowed us to connect operations on the CPU in the Glow runtime, to the accelerator. Trace correlation is important for examining the complex software stack used for AI inference.</span></p>
<h2><span>Next steps</span></h2>
<p><span>In addition to continuing to support next-generation AI and video accelerators using Asimov and the Asicmon we are also exploring:</span></p>
<ol>
<li><b>Open source specifications:</b><span> There are multitudes of companies building accelerator chips today. But the monitoring interfaces for accelerators lack standardization. We are collaborating with the</span><a href="https://www.opencompute.org/wiki/Server/ODSA"> <span>Open Domain-Specific Accelerators (ODSA)</span></a><span> project so the industry as whole can benefit from a common specification.</span></li>
<li><b>Trace visualization and analysis:</b><span> We are investigating ways to automatically generate optimization recommendations from the trace and support better visualizations, such as integrating with TensorBoard.</span></li>
<li><b>Distributed tracing:</b><span> Since microservices do not run in isolation, we plan on exploring how to correlate distributed traces collected by the Canopy distributed tracing tool with system-level accelerator traces. This would allow us to debug the end-to-end latency of microservices that use AI accelerators.</span></li>
</ol>
<h2><span>Thanks</span></h2>
<p><em><span>We would like to thank our many collaborators at Facebook, including Jerry Liu, Thiara Ortiz, Jeremy Yang, Ashwin Poojary, Deng Pan, Craig Ross, Ashwin Narasimha, Gisle Dankel, Michael Anderson, Allan Di Wu, Yinghai Lu, Satish Nadathur, Garret Catron, and Jack Montgomery for supporting us in creating this framework.</span></em></p>

		
	</div></div>]]></content:encoded>
      <author>By Brian Coutinho, Hao Wang, David Carrillo-Cisneros, Cynthia Liu, Parth Malani</author>
      <enclosure url="https://engineering.fb.com/wp-content/uploads/2021/06/Asicmon-04_Flowchart.png" length="0" type="image/png"></enclosure>
      <pubDate>Mon, 28 Jun 2021 16:00:24 +0000</pubDate>
    </item>
  </channel>
</rss>