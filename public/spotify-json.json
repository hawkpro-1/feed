{
  "version": "https://jsonfeed.org/version/1",
  "title": "Spotify",
  "home_page_url": "https://engineering.atspotify.com/",
  "items": [
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/03/incident-report-spotify-outage-on-march-8/",
      "title": "\n                                            Incident Report: Spotify Outage on March 8\n                                        ",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-5183\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 11, 2022\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2022/03/incident-report-spotify-outage-on-march-8/\" title=\"Incident Report: Spotify Outage on March 8\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header-700x344.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header-120x59.png 120w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eOn March 8, we experienced a global outage triggered by issues in a cloud-hosted service discovery system used at Spotify. We were made aware of issues with login at 18:12 UTC / 13:12 ET and started implementing fixes to critical systems at 18:39 UTC / 13:39 ET. This outage affected our users and we apologize for the inconvenience it may have caused. Our service has now fully recovered.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat happened?\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Spotify backend consists of multiple microservices that communicate with each other. For microservices to be able to find each other, we utilize multiple service discovery technologies. Most of our services are using a DNS based service discovery system; however, some of our services use an xDS based traffic control plane and discovery system called \u003ca href=\"https://cloud.google.com/traffic-director\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eTraffic Director\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eOn March 8, \u003ca href=\"https://status.cloud.google.com/incidents/LuGcJVjNTeC5Sb9pSJ9o\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle Cloud Traffic Director experienced an outage\u003c/a\u003e. This in coordination with \u003ca href=\"https://github.com/grpc/grpc-java/issues/8950\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ea bug in a client (gRPC) library\u003c/a\u003e caused the Spotify outage that affected many of our users: if you were logged out of a Spotify app, you were unable to log back in.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs soon as the problem was discovered, we rolled out configuration changes to revert our affected systems back to use our DNS-based service discovery and saw it recover gradually. See the timeline below for more details.\u003c/p\u003e\n\n\n\n\u003ch3\u003eTimeline\u003c/h3\u003e\n\n\n\n\u003cp\u003e18:12 UTC / 13:12 ET  – Reports of users being logged out of the different client apps start to surface.\u003c/p\u003e\n\n\n\n\u003cp\u003e18:39 UTC / 13:39 ET – Remediations were being put in place to restore the affected systems\u003c/p\u003e\n\n\n\n\u003cp\u003e20:35 UTC / 15:35 ET – Incident fully mitigated at Spotify\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhere do we go from here?\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the short term:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe are working with Google Cloud to better understand how issues with Traffic Director resulted in a large outage affecting Spotify’s users.\u003c/li\u003e\u003cli\u003eWe will add additional monitoring and alerting to ensure that we would catch similar service discovery related problems earlier.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eWe will continue to invest in resiliency by identifying and implementing additional safety nets in terms of monitoring, automatic error detection, and self-recovery.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "On March 8, we experienced a global outage triggered by issues in a cloud-hosted service discovery system used at Spotify. We were made aware of issues with login at 18:12 UTC / 13:12 ET and started implementing fixes to critical systems at 18:39 UTC / 13:39 ET. This outage affected our users and we",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Incident-Report_Header.png",
      "date_published": "2022-03-11T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/03/chantal-delfeld-engineer/",
      "title": "\n                                            Chantal Delfeld: Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-5158\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld-700x438.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld-768x480.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld-120x75.png 120w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eChantal is a Software Engineer and part of the Spotify team in New York. But she lives thousands of miles away in Austin, Texas – with her husband, mother and two young children. \u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e7:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy early mornings are spent running about after my children – getting them dressed, giving them breakfast and putting on their sunscreen, so they’re all ready for the day ahead. At 8am, I drive my daughter to preschool, while my mom looks after my one-year-old son – she’s been living with us since the start of the pandemic and has been a huge help when it comes to family life and childcare.  \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e8:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBack home after drop-off, I make myself a nice coffee, head to the home office we’ve built in our backyard and get started on my working day. I’m a full-stack engineer, focussing mostly on the backend. But I’m becoming ‘T-shaped’ – which means gaining a broad knowledge of certain areas and doing some frontend work too – and I really like the variety that brings.  \u003c/p\u003e\n\n\n\n\u003cp\u003eI’ve been at Spotify six months now, working as part of a big team that deals with commerce-related projects. Our main product is a wallet that allows creators to add credit cards as payment. And right now, we’re improving our operations ahead of opening up in new markets – getting ourselves ready for the increased traffic and making sure we’ve got all the right learnings and monitoring in place for this exciting new phase. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe’re also in the process of developing a middle layer between our frontend and backend services – a sort of gateway service that means frontend won’t ever have to deal with backend directly. We have a lot of dependencies with other services and this new gateway will help us integrate with them more easily. It’ll also make things more secure, so it’s a really great project to be part of. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI take a half-hour break around noon and grab a few minutes with my mom and son while heating up some food for lunch. I usually eat at my desk though – I’m new to the world of commerce and take every chance to do additional reading or watch something that’s related to the field. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:30pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eIn the afternoon, I tend to have a few more meetings, or I often pair with someone in our team and work through a certain task or issue together. It’s nice to have someone to communicate with in this way, since I work remotely and haven’t actually met any of my team in person! \u003c/p\u003e\n\n\n\n\u003cp\u003eI can’t see myself moving to New York anytime soon, but what’s really great is that Spotify is running lots of workshops to help with remote working – I’m doing one called ‘Rethinking Communication in a Distributed World’ and another all about asynchronous communication. I’m really keen to make sure that my team know I’m there for them whenever they need me – despite the distance and the time difference. That’s super-important to me and I’m doing everything I can to make sure things run smoothly.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e3:30pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTime to pick up my daughter from preschool and have a quick play with both children, before wrapping up a few last bits and bobs for work. When that’s all done, I make dinner for the kids and go through the usual bedtime madness – then, I sit down with my husband for dinner, get a little grown-up time and chat about the ups and downs of our days.\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"595\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-700x595.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-700x595.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-250x213.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-768x653.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-1536x1306.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown-120x102.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Chantal-Delfeld_Weekly-Breakdown.png 1584w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eYou can hear more about Chantal’s life as a Spotify engineer on our new podcast \u003ca href=\"https://open.spotify.com/episode/43cbJh4ccRD7lzM2730YK3\"\u003eNerdOut@Spotify\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Chantal is a Software Engineer and part of the Spotify team in New York. But she lives thousands of miles away in Austin, Texas – with her husband, mother and two young children.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/Header-Image_Chantal-Delfeld.png",
      "date_published": "2022-03-09T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/03/jordan-loeser-web-engineer/",
      "title": "\n                                            Jordan Loeser: Web Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-5150\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan.png 1000w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan-120x59.png 120w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eJordan works at Spotify in New York and has been part of Spotify Wrapped for the past two years…\u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cp\u003e\u003cstrong\u003eTell us more about working on Spotify Wrapped… \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eMy main focus on Wrapped was the social media share cards – the static images that summarize the information from someone’s data stories and can be shared on platforms like Instagram, TikTok and Snapchat. Since these cards must accommodate a variety of languages and dynamic data within a fixed space, we needed to work really closely with our designers and localization team right from the start and there were plenty of interesting challenges to work through together. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWhat were the biggest challenges? \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBy far the most challenging – and exciting – share card to implement was the \u003ca href=\"https://engineering.atspotify.com/2021/12/the-audio-aura-story-mystical-to-mathematical/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAudio Aura\u003c/a\u003e story. This was our first time leveraging this kind of advanced web technology to generate graphics in real-time, so there was lots of trial and error. But after experimenting and collaborating closely with our designers, we were able to get things just right. And with nearly 100 different aura combinations presented in 30 different languages, this share card really got people talking. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWhat was the most inspiring moment? \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eOne moment that stands out to me vividly is the first time we were able to run through the full, end-to-end Wrapped experience using our own listening data. That test session was the first time we could see all the brand kit, data stories and share cards come together and really enjoy the fruits of our collaboration in a tangible way. Seeing our own favorite artists and hearing our own top tracks was a great reminder of why the experience resonates with listeners and what makes Wrapped so magical in the first place.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWhat made it a fun project to be part of? \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIt was a joy to collaborate with such an incredible team of bright, talented colleagues. And I love working on something so visual and user-facing – it’s amazing to release Wrapped into the wild and see how users engage and make it their own after launch. Working on a project that has such a presence on social media and generates so many memes and comments never fails to blow me away – it’s the part of the campaign that I truly cherish every year.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eAny special shout-outs? \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIt really does take a village! Share cards wouldn’t have been possible without the help of so many brilliant Spotifiers: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eSamantha Whitt, my collaborator and share card builder extraordinaire\u003c/li\u003e\u003cli\u003eMelia Wagner, expert in all things Localization\u003c/li\u003e\u003cli\u003eOur exceptional designers, Angeline Toh, Cait Charniga and Sona Dolasia\u003c/li\u003e\u003cli\u003eMy backend heroes, Mike Smith and Patty Santa Cruz\u003c/li\u003e\u003cli\u003eGenius problem-solver, Itay Yahimovitz\u003c/li\u003e\u003cli\u003eOur fearless tech lead and my mentor, Zela Taino\u003c/li\u003e\u003cli\u003eThe glue of it all, our road manager, Udaya Uma Pillalamarri\u003c/li\u003e\u003cli\u003eLast but not least, my incredible manager and advocate, Ashley Casey.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/web/\" rel=\"tag\"\u003eweb\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/wrapped21/\" rel=\"tag\"\u003eWrapped21\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Tell us more about working on Spotify Wrapped… My main focus on Wrapped was the social media share cards – the static images that summarize the information from someone’s data stories and can be shared on platforms like Instagram, TikTok and Snapchat. Since these cards must accommodate a variety",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/0132-Wrapped-MyBeat-takeover_Jordan.png",
      "date_published": "2022-03-04T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/03/introducing-nerdout-at-spotify-tech-podcast/",
      "title": "\n                                            Introducing NerdOut@Spotify: A New Podcast for Developers\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-5125\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 1, 2022\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2022/03/introducing-nerdout-at-spotify-tech-podcast/\" title=\"Introducing NerdOut@Spotify: A New Podcast for Developers\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify.png 1000w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify-120x59.png 120w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e For years, Spotify’s official engineering blog has been giving you a peek behind the curtain at Spotify R\u0026amp;D. Today, we’re announcing \u003ca href=\"https://open.spotify.com/show/5eXZwvvxt3K2dxha3BSaAe?si=a6db69f76efe4459\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNerdOut@Spotify\u003c/a\u003e, our new R\u0026amp;D podcast that gives you another view into our tech world. In each episode I’ll talk with Spotify developers about challenging tech problems and give you a firsthand look into what we’re doing, what we’re building, and what we’re nerding out about at Spotify every day.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs a Spotify engineer, I’ve seen this company transition from a music-only service that was fighting to keep up with the speed of our user growth to becoming an audio streaming platform operating at global scale. I’m excited to see what happens next in our tech journey, and this podcast gives us a way to share that journey with you, while introducing you to the amazing people who work here. \u003c/p\u003e\n\n\n\n\u003ch2\u003eCome learn with us\u003c/h2\u003e\n\n\n\n\u003cp\u003eSoftware development has always been a very rapidly changing field, and every company does it a little bit differently. Both writing software and owning and operating high-scale systems have drastically changed over the years, and will continue to rapidly change every day. This brings new challenges, new technologies, new languages, but also bigger opportunities and all new ways to make an impact. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs developers in this rapidly changing industry, we’re constantly learning; finding new things that we didn’t know about every day. If you’d like to join us on our journey of constantly experimenting, inventing, building, testing, and debugging, and just dig much deeper into all of the nerdy things that we’re up to at Spotify, NerdOut@Spotify is the podcast for you.\u003c/p\u003e\n\n\n\n\u003ch2\u003eTackling tech issues from the inside out\u003c/h2\u003e\n\n\n\n\u003cp\u003eJoin us on a behind-the-scenes tour of how we innovate at Spotify. We’re tackling interesting challenges and exploring the possibilities that new technology and trends bring to our products. Find out what we’re currently learning about — new cloud services, open source software like our own Backstage, infrastructure trends like Kubernetes, applying new innovations in machine learning, and our constantly increasing scale and the challenges that that brings. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe believe that these are the same problems that many of our peer companies are facing. So, we are bringing these conversations from the hallways of conferences to your podcast player. We’ll bring these topics in a way that only Spotify can, and highlight the ways in which we are approaching these challenges that are unique to our aligned and autonomous squad culture. Of course, we’ll also touch on the playful sides of our culture and show you some of the nerdy ways in which we have fun.\u003c/p\u003e\n\n\n\n\u003ch2\u003eMeet the people behind the tech\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn every episode, you’ll hear from nerds just like you digging into topics that they love. We’ll introduce you to people from all over Spotify and all around the world — developers, engineering managers, data scientists, designers, PMs, and just about any other R\u0026amp;D role — from all experience levels and backgrounds. Hear their individual stories and how they got to where they are now. Share in their passions, and learn from their failures and successes. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe’ll cover plenty of big topics, but won’t be afraid to go a little too deep into some small obsessions, covering everything from the world of DevOps and some language wars, to Lego, gymnastics, and Kubernetes clusters running the lights in our homes.\u003c/p\u003e\n\n\n\n\u003ch2\u003eListen and subscribe now\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur first few episodes are live now and dive into the deep end: We explore open source, the problem of developer experience, and \u003ca href=\"https://backstage.io/docs/overview/what-is-backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, the open platform for building developer portals that was created at Spotify, donated to the \u003ca href=\"https://www.cncf.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCloud Native Computing Foundation (CNCF)\u003c/a\u003e, and is now maintained by a worldwide community of contributors. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"343\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-700x343.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage-120x59.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut_Ep01_Backstage.png 1200w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eCome and \u003ca href=\"https://open.spotify.com/show/5eXZwvvxt3K2dxha3BSaAe?si=a6db69f76efe4459\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elisten on Spotify\u003c/a\u003e or wherever you get your podcasts, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003ca href=\"https://podcasts.apple.com/us/podcast/nerdout-spotify/id1610028432\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eApple Podcasts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://podcasts.google.com/feed/aHR0cHM6Ly9hbmNob3IuZm0vcy84MDU2ODJiMC9wb2RjYXN0L3Jzcw\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle Podcasts\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://overcast.fm/itunes1610028432/nerdout-spotify\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOvercast\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://pca.st/1t2eonyr\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePocket Casts\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eBe sure to follow \u003ca href=\"https://open.spotify.com/show/5eXZwvvxt3K2dxha3BSaAe?si=a6db69f76efe4459\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe show\u003c/a\u003e so you’re in the loop when new episodes drop. I hope that you’ll listen and enjoy the show.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eNerdOut@Spotify is produced by Spotify’s Ted Vergakis and our friends at \u003c/em\u003e\u003ca href=\"https://seaplanearmada.com/\"\u003e\u003cem\u003eSeaplane Armada\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-culture/\" rel=\"tag\"\u003eengineering culture\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "TL;DR For years, Spotify’s official engineering blog has been giving you a peek behind the curtain at Spotify R\u0026D. Today, we’re announcing NerdOut@Spotify, our new R\u0026D podcast that gives you another view into our tech world. In each episode I’ll talk with Spotify developers about challenging",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/03/NerdOut@Spotify.png",
      "date_published": "2022-03-01T00:00:00Z",
      "author": {
        "name": "Published by Dave Zolotusky, Principal Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/02/search-journey-towards-better-experimentation-practices/",
      "title": "\n                                            Search Journey Towards Better Experimentation Practices \n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eFebruary 28, 2022\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2022/02/search-journey-towards-better-experimentation-practices/\" title=\"Search Journey Towards Better Experimentation Practices \"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header.png 2106w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header-120x60.png 120w\" sizes=\"(max-width: 2106px) 100vw, 2106px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, we aim to build and improve our product in a data-informed way. To do that, teams are encouraged to generate and test hypotheses by running experiments and gathering evidence for what works and what doesn’t. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn the Search team, in our journey towards this goal, we have learned that, besides having the ambition, we need at least two more things:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAn experimentation platform that allows us to run experiments at scale and generate accurate results\u003c/li\u003e\u003cli\u003eA product development culture with evidence-based hypothesis testing at its core\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBackground\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eOver the last two years, Spotify has invested in building a new Experimentation Platform (see \u003ca href=\"https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s New Experimentation Platform (Part 1)\u003c/a\u003e and \u003ca href=\"https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s New Experimentation Platform (Part 2)\u003c/a\u003e) which, to a large extent, solves the first point. The experimentation platform is already mature and offers most of the functionality and flexibility required — and the pace at which the experimentation platform improves is often faster than the rate of adopting new functionality by individual teams. In addition, a lot of documentation and best practices have been established for the platform’s functionalities, which help with the process of setting up tests. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the effort to adopt a truly data-informed product development process doesn’t stop there — it requires continuous effort and push. \u003c/p\u003e\n\n\n\n\u003cp\u003eExperimentation practices and data-driven product development is often considered a data science topic — and that is true to some extent. Data scientists usually have a deeper understanding of statistics and experimentation, so they tend to be the most excited about adopting a data-driven product development culture. The Search team at Spotify consists of several engineering teams, an array of product managers, a product insights team with a number of data scientists, and a product area leads group. You’d probably guess that engineering teams have to be pretty independent from data scientists to avoid creating bottlenecks, and you’d be right. \u003c/p\u003e\n\n\n\n\u003cp\u003eDefining best practices for experimentation is made relatively easily thanks to hundreds of articles that explain how to do things like perform a power analysis, choose metrics, interpret a p-value, etc. It is, however, less easy to integrate those practices day to day and turn them into habits. This requires creating an environment where adapting ways of working to improve our experimentation culture feels empowering, rather than uncomfortable, in a diverse team with different disciplines and varying levels of knowledge. \u003c/p\u003e\n\n\n\n\u003cp\u003eRead on to find out more about the two key drivers of our success and the concrete actions we took over the last year.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eKey driver 1 – Roadmap\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen it comes to adopting a truly data-informed product development process, the road to the end goal can feel overwhelming both for the engineering team and for data scientists. It is crucial that the plan towards that goal takes into account the maturity, ability, and bandwidth of the teams. In large organizations, these aspects often vary a lot from team to team and across parts of the organization. Too much too soon will lead to people feeling overwhelmed and giving up; too little too seldom and the momentum and excitement will be lost. To make this balancing easier, we created a roadmap with three simple steps:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eIndividual experiment quality — Adopt quality standards for each experiment to meet\u003c/li\u003e\u003cli\u003eCross-experiments quality — Coordinate individual experiments\u003c/li\u003e\u003cli\u003eMeasure total business impact — Estimate the cumulative impact of all the features released in a quarter\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch3\u003eStep 1 — Individual experiment quality \u003c/h3\u003e\n\n\n\n\u003cp\u003eFor an experiment to bring the promised gold-standard value to a product decision, many things have to be in place: proper hypothesis, well-defined metrics to evaluate the hypothesis, decision rules set before the experiment starts, etc. These individual practices can be introduced to the teams gradually, making each one easy to integrate into the workflow. Combined, they raise each experiment to the gold-standard level. The main goal of Step 1 is to ensure that each experiment leads to a highly reliable data-informed product decision. Once the quality, and thereby the value, of each experiment starts to improve, the number of experiments will often also increase. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt Spotify Search, we continue to scale the speed of iteration in each team using experiments, and thus need to make each decision transparent and self-explanatory for anyone, e.g., new team members or those external to the team. For this reason, it is important to guide individual teams during Step 1 into using shared practices. Examples of such shared practices are using common conventions for naming experiments, writing hypotheses, and logging decisions after the experiment ends. We have applied this by having our Search data science team provide and advocate for simple templates for test specifications and experiment setups. These templates help remind each team member of important steps and considerations for each test while also streamlining the practices across teams. \u003c/p\u003e\n\n\n\n\u003ch3\u003eStep 2 — Cross-experiments quality\u003c/h3\u003e\n\n\n\n\u003cp\u003eAnother important aspect of experimentation at Spotify is coordination. Most of the time, teams have to run several experiments on top of each other, and some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time.  The Search team is no exception and coordinating experiments is a big part of our effort.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s experimentation platform\u003c/a\u003e has flexible coordination capabilities, so coordinating experiments at scale is possible, but it requires cross-team collaboration and alignment. The Search data science team helps experimenting teams find the right setup for the right experiment, and provides guidelines to avoid running out of users that are available to try our new search features. This effort is again helped by the Step 1 alignment, where template experiments have pre-selected coordination behaviors that nudge team members to select the appropriate setup.\u003c/p\u003e\n\n\n\n\u003ch3\u003eStep 3 — Measure total business impact\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe final step of our plan to adopt a truly data-informed product development process is to measure the total impact of a product initiative over some relevant reporting time period, e.g., a quarter. The goal is to answer the question “What is the combined causal effect of all our product changes on our key metrics?” This can be done in several ways. A simple and naive total impact analysis is to sum the individual estimated causal effects from the experiments of all shipped product changes. At Spotify Search we use quarterly holdbacks to estimate the total impact of the product development program. This is achieved by holding a set of users back from all product changes during the quarter. At the end of the quarter, we run one experiment on the users in the holdback, where one group is given no product change (control group), and one group is given all the shipped product changes (treatment group). This yields an unbiased estimator of the total causal effect of all product changes and directly answers the question posed above. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-700x352.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter-120x60.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Before-the-quarter.png 1011w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: During the quarter\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"307\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-700x307.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-700x307.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-250x110.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-768x337.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter-120x53.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/After-the-quarter.png 1011w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 2: After the quarter\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eAn important aspect of these steps is that they grow from within one team and spread across teams and within each individual experiment to across several experiments within the same initiative.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt is not uncommon that experimentation initiatives in larger organizations are driven by the middle-level decision makers’ need for better total business impact estimates to guide future priorities. However, it is important to acknowledge that measuring the impact of a full product initiative with decent precision is not a trivial thing to achieve. This is especially challenging when product evaluation is not already a natural part of the product development culture. Our belief and experience is that the quickest way to achieve proper overall product evaluation is to build from the bottom up with solid product evaluation practices in each team. Moreover, we have found that each of these steps has brought a lot of value to our product development cycle beyond measuring impact, e.g., product quality awareness, better practices around product evaluation in general, knowledge sharing, and alignment across product teams. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eKey driver 2 – Constant injection of energy\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003ch3\u003e\u003cstrong\u003eStarting the fire\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eBuilding great experimentation practices is like starting a fire with wet wood. It won’t start just because you provide a spark, but if you give it care and provide it with the right resources, the fire will eventually start and burn by itself. It’s the same for experimentation practices. You will need to start with a source of energy, pulling and pushing in the right direction, and you will need to continuously inject energy, for a long time.\u003c/p\u003e\n\n\n\n\u003cp\u003eAdopting new practices that sometimes conflict with existing habits is a big challenge — it will take time and effort to convince teams of the value that experimentation brings to the product development process.\u003c/p\u003e\n\n\n\n\u003cp\u003eOur learnings reveal that at the beginning of the journey, a lot of things have to be boosted and done \u003cem\u003efor\u003c/em\u003e the engineering teams rather than \u003cem\u003eby\u003c/em\u003e them. By actually setting up and reviewing tests \u003cem\u003efor\u003c/em\u003e the teams, it allows them to see for themselves that the amount of work required on a day-to-day basis is manageable and, most importantly, the value those efforts bring to the team and product. Doing certain tasks for the teams will also deepen the trust between the experimentation advocate and the teams, reassuring engineers that they will get support along the way.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the early experimenting stage of any team, it is critical to accept that some tests will not be perfect and some product decisions, therefore, not fully data informed. However, it is of the utmost importance that the teams get sufficient support to succeed with an experiment by continuously injecting support and energy, starting the fire and keeping it burning. In our experience, the key to the success of starting experimentation is endurance and continuity. The smallest lighter can make any wet wood burn with sufficient time.\u003c/p\u003e\n\n\n\n\u003ch3\u003e\u003cstrong\u003eMaintaining the fire\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eJust as we had to scale our practices between teams, we had to scale the injection of energy. The solution to scale presented itself in the form of individuals across Search teams who developed an interest and expertise in experimentation and became new precious sources of energy. As we progressed as a team towards more data-driven product development, the expertise and support gradually transferred from the advocating team to individual team members who naturally started to inject energy. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn big and agile organizations, there are often organizational changes made to fit new needs, as well as new people joining, which makes it harder to keep practices aligned. So even if the fire burns by itself at some point, it is still crucial to have people dedicated to keep injecting energy, restarting and boosting the fire continuously. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eSpotify Search in 2021\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we have covered the major steps and principles that led Spotify Search on the journey to data-driven product development, we will look in more detail at the specific actions that were taken in Search last year.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"177\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-700x177.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-700x177.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-250x63.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-768x195.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline-120x30.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Timeline.png 1404w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eDecember 2020:\u003c/strong\u003e Unlock the use of all features from our experimentation platform (like sample size calculation or result page) by integrating our Search-specific metric into the system. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eJanuary 2021: \u003c/strong\u003eCreation of a dedicated weekly forum and communication channel to make information flow better between the search engineering teams and data scientists.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMarch 2021:\u003c/strong\u003e Standardization of template for the test setup and test specification with pre-filled parameters. That unlocked the possibility for anyone to create and launch an experiment with basic search-specific parameters without prior knowledge.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eJune 2021:\u003c/strong\u003e End-of-quarter cumulative experiment. At this point, most of the new features we developed for users were tested with an experiment, so we had all the ingredients to start measuring the cumulative impact of all the features we launch in Search over a quarter. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAugust 2021:\u003c/strong\u003e Experimentation Champion program. Scale knowledge-sharing when new members join by establishing one person from each team as a dedicated experimentation champion. The role of the champion is to help spread knowledge between the Search data science team and their own.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSeptember 2021:\u003c/strong\u003e Naming convention for experiments. As part of our effort to scale and make each test more transparent, we created a naming convention to be able to determine from the name of the experiment which quarter and initiative this experiment was part of.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSeptember 2021:\u003c/strong\u003e The team committed to shipping all product changes gradually using monitoring. This was a great milestone for us! It showed that the whole team was feeling confident enough in our practices to commit to this objective.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOctober 2021:\u003c/strong\u003e Decision-making process improvement. To ensure that we made good decisions for each experiment we ran, the experiment design needed to contain three elements: Hypothesis, Metrics, and Decision rules.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eThe key to the success of this roadmap was that the team only needed to focus on one small step at a time. Implementing these small steps over the course of just one year led to an impressive change in Search experimentation practices. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eIn 2022: keep the fire burning!\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn 2021 we made great progress improving each experiment’s quality, improving coordination between experiments, and measuring global impact. This year in Search, we will take one more step toward truly building and improving our product in a data-informed way by integrating experimentation into a more global evaluation process for each initiative of our product development. If you’re interested in joining the team, take a look at our \u003ca href=\"https://www.lifeatspotify.com/jobs/data-scientist-search-insights\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen jobs\u003c/a\u003e!\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eApproaching this problem, I thought that changing product development culture could only come from a change in expectations and incentives from leaders higher up in the organization. However, during the last year, I have seen many people from several disciplines stepping in and taking responsibility for the progress of our experimentation practices. It is now common to see engineers discussing if they should stop a product change from being launched to prevent a negative user experience. We have PMs on our team driving the education of PMs at Spotify globally on the topic of experimentation. \u003c/p\u003e\n\n\n\n\u003cp\u003eTo me this evolution of behavior is proof that it is possible to improve our product evaluation process substantially by taking three simple measures: providing a safe environment for teams to evolve their practices, continuously injecting energy and support, and ensuring that the teams can see, with their own eyes, the benefits of the changes they’ve made.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, we aim to build and improve our product in a data-informed way. To do that, teams are encouraged to generate and test hypotheses by running experiments and gathering evidence for what works and what doesn’t. In the Search team, in our journey towards this goal, we have learned that,",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Search-journey-towards-better-experimentation-practices_Header.png",
      "date_published": "2022-02-28T00:00:00Z",
      "author": {
        "name": "Published by Claire Detilleux, Sr. Data Scientist"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/02/mesfin-mekonnen-senior-engineer/",
      "title": "\n                                            Mesfin Mekonnen: Senior Engineer \n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-5103\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin.png 1000w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin-700x343.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin-768x376.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin-120x59.png 120w\" sizes=\"(max-width: 1000px) 100vw, 1000px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eMesfin is part of our Spotify New York team and jumped at the chance to work as an embed on 2021 Wrapped. \u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cp\u003e\u003cstrong\u003eTell us more about working on Spotify Wrapped… \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eI specialize in iOS Engineering and was one of a few iOS embeds working on 2021 Wrapped. We divided up the various Wrapped stories amongst ourselves — my focus was on Top Five Artists, Top Five Songs, Top Five Podcasts, Top Genres and the \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/12/17/the-audio-aura-story-mystical-to-mathematical/\" target=\"_blank\"\u003eAudio Aura\u003c/a\u003e, which was brand new for this year and really fun to be part of. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWhat were the biggest challenges? \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe all thought the Audio Aura was going to be the toughest story to build — it certainly took the most time to implement, because it had a lot of different components and we were starting from scratch, rather than building on executions from previous years. \u003c/p\u003e\n\n\n\n\u003cp\u003eHowever, I actually found the Top Genres story to be the most technically challenging — we needed to use a special type of software for the bar charts, programmatically drawing the text into a canvas and rendering the context as an image. I’d never done that before and learned a lot from the deeper knowledge of the other engineers. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWhat made it a fun project to be part of? \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe best part of working on Wrapped was that it was a truly collaborative effort. We had a tight schedule and weekly deliverables for all the stories, which could be stressful at times. But whenever blockers came up, we could always rely on our teammates to pitch in and help out — it was a really safe, supportive atmosphere and we were constantly learning from one another. \u003c/p\u003e\n\n\n\n\u003cp\u003eAll the stories had iOS and Android counterparts, so we communicated a lot with our mobile teammates to share our approaches and flag up any issues we encountered along the way. Basically, it was just a really fun team to be part of — there were so many genuinely great people involved. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eAny special shout-outs? \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eSo many! The whole core team was amazing – they gave me and the other embeds such a warm welcome and I really enjoyed the opportunity to work with them. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn particular, I’d like to shout out: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eCait Charniga and Sona Dolasia for their incredibly detailed designs and awesome motion specs\u003c/li\u003e\u003cli\u003eUdaya Pillalamarri, the engineering manager, for caring so much about our wellbeing and fostering a working environment where we could be our true selves — it showed that she cared about us personally beyond delivering the project\u003c/li\u003e\u003cli\u003eZela Taino, the tech lead, for her impressive project and time management skills — managing the Wrapped experience, as well as implementing some of the Wrapped stories\u003c/li\u003e\u003cli\u003eKylan McBride for his deep technical iOS expertise and being someone I could always turn to with my toughest challenges.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eWhat were the most inspiring moments? \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eHaving always admired the Wrapped experience from afar, I knew it would take a lot to deliver such a personalized project, with such massive scale and reach. However, I didn’t truly appreciate the amount of people and work involved — it requires literally hundreds of Spotifiers. \u003c/p\u003e\n\n\n\n\u003cp\u003eThroughout the project, I was very inspired by the attention to detail shown by our localization teams — we had weekly testing sessions, where the localization manager would identify any issues related to translations and ensure we had a quality experience in all languages. Their input pushed us to work harder on fixing issues that came up with non-Latin characters. And this kind of dedication — along with a real focus on accessibility — is something I’m taking back to my home squad and my day-to-day work at Spotify.\u003c/p\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Mesfin is part of our Spotify New York team and jumped at the chance to work as an embed on 2021 Wrapped.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/0132-DDS_1-Wrapped-MyBeat-takeover_Mesfin.png",
      "date_published": "2022-02-22T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/02/fred-wang-senior-backend-engineer/",
      "title": "\n                                            Fred Wang: Senior Backend Engineer￼\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-5047\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred.png 600w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred-250x168.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred-120x81.png 120w\" sizes=\"(max-width: 600px) 100vw, 600px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003eAs a Senior Backend Engineer at Spotify New York, Fred’s role on 2021 Wrapped involved serving data stories content for iOS and Android. Here, he shares some of the most memorable moments…\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\n\n\n\n\u003ch3 id=\"tell-us-more-about-working-on-spotify-wrapped\"\u003e\u003cstrong\u003eTell us more about working on Spotify Wrapped… \u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eI’m part of the Mambas team at Spotify, which is responsible for all the animation, presentation and personalized data in the Wrapped interactive mobile app. My job is to take the raw data from the backend and enrich it, translate it and format it for different regions before it goes to the frontend mobile engineers. That includes deciding what music to play during data stories and what images and localised text to display – all the things we serve from the backend to help iOS and Android show the data stories correctly. \u003c/p\u003e\n\n\n\n\u003ch3 id=\"what-were-the-biggest-challenges\"\u003e\u003cstrong\u003eWhat were the biggest challenges? \u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eLoad-testing was definitely the most challenging aspect of working on this project. In the first few hours after launch, Wrapped gets a high volume of unique visitors – all with their own unique data stories, all served to different types of mobile devices. \u003c/p\u003e\n\n\n\n\u003cp\u003eSo we can’t just roll it out and hope for the best – we have to simulate that load and make sure we can withstand all the requests we’re going to get at peak usage. \u003c/p\u003e\n\n\n\n\u003cp\u003eDuring load test sessions, we process a high volume of requests and try to uncover as many issues across our dependencies as possible. It’s a big job, but really helps discover all weak links in the chain\u003c/p\u003e\n\n\n\n\u003ch3 id=\"what-were-the-best-moments\"\u003e\u003cstrong\u003eWhat were the best moments? \u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eIn the first two hours after the launch of Spotify Wrapped, we saw unprecedented traffic from around the world – yet we scaled without any major issues. It was nerve-wracking, but also extremely exciting to see all our hard work pay off like that.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"what-made-it-a-fun-project-to-be-part-of\"\u003e\u003cstrong\u003eWhat made it a fun project to be part of? \u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eI really enjoyed collaborating with all the great engineers and localization teams to make the data stories work cleanly across every region, language and format. And I loved the moment when the data pipelines started working – we all turned on live data for our endpoints and got to see our own Wrapped stories for the very first time. We were all so giddy and excited – all we could do was play around and compare notes for a good few hours afterwards! \u003c/p\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/wrapped21/\" rel=\"tag\"\u003eWrapped21\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "As a Senior Backend Engineer at Spotify New York, Fred’s role on 2021 Wrapped involved serving data stories content for iOS and Android. Here, he shares some of the most memorable moments…",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/0132-Wrapped-MyBeat-takeover_Fred.png",
      "date_published": "2022-02-16T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/02/introducing-ruler-our-tool-for-measuring-android-app-size/",
      "title": "\n                                            Introducing Ruler: Our Tool for Measuring Android App Size\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eFebruary 14, 2022\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2022/02/introducing-ruler-our-tool-for-measuring-android-app-size/\" title=\"Introducing Ruler: Our Tool for Measuring Android App Size\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1.png 1011w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1-120x60.png 120w\" sizes=\"(max-width: 1011px) 100vw, 1011px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, we strive to make our apps available to as many people as possible. As mobile developers, that means we want everybody to be able to download our app without hiccups or constraints.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne important metric related to this goal is the size of the Spotify app — if it’s too big, users with poor network connectivity or little device storage might not be able to download it. And since shrinking download size \u003ca href=\"https://medium.com/googleplaydev/shrinking-apks-growing-installs-5d3fcba23ce2\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehas been shown\u003c/a\u003e to improve install conversion rate, we aim to keep the app as lean as possible.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut working with app size is not always easy, particularly for large applications with numerous contributors adding cool new features. That’s why we built and open sourced \u003ca href=\"https://github.com/spotify/ruler\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eRuler\u003c/a\u003e — a tool to measure and analyze the size of your Android apps, built with automation in mind.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"where-we-started\"\u003eWhere we started\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe set out to see how we could decrease app size, and started our investigations by using existing tools like \u003ca href=\"https://github.com/jakewharton/diffuse\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDiffuse\u003c/a\u003e and Android Studio. Those work great if you want to get a high-level overview of your app, but when we wanted to dig deeper, we quickly arrived at another question — how much are certain modules and dependencies contributing to the overall app size?\u003c/p\u003e\n\n\n\n\u003cp\u003eThe codebase of the main Android Spotify app consists of over 1,000 Gradle modules and hundreds of third-party dependencies. All of these modules and dependencies are merged and packaged into a single app, without a clear way to determine where things came from. This can make it hard to analyze app size and determine where optimization opportunities lie.\u003c/p\u003e\n\n\n\n\u003cp\u003eRuler is a Gradle plugin that solves this exact problem. It allows you to analyze your app and gives you detailed insights into the origin and size of certain files, modules, and third-party dependencies.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"how-does-ruler-work\"\u003eHow does Ruler work?\u003c/h2\u003e\n\n\n\n\u003cp\u003eAndroid apps are typically packaged and uploaded to the Play Store as \u003ca href=\"https://developer.android.com/guide/app-bundle\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eApp Bundles\u003c/a\u003e. The Play Store uses these bundles to generate an optimized \u003ca href=\"https://en.wikipedia.org/wiki/Android_application_package\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAndroid application package (APK)\u003c/a\u003e for every device. Ruler replicates this mechanism (using \u003ca href=\"https://github.com/google/bundletool\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle’s Bundletool\u003c/a\u003e) to generate an APK for a given device configuration. We do that to accurately measure what ends up on the devices of our users.\u003c/p\u003e\n\n\n\n\u003cp\u003eNext, we analyze this APK to see which files actually end up in the app and how much space those files take up, leveraging \u003ca href=\"https://developer.android.com/studio/command-line/apkanalyzer\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eapkanalyzer\u003c/a\u003e to ensure the numbers reported by Ruler stay consistent with those analyzed by Android Studio and guaranteeing we measure app size after any optimizations done by another process (e.g. R8). \u003c/p\u003e\n\n\n\n\u003cp\u003eFor each file, Ruler captures two measurements:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eDownload size: \u003c/strong\u003eBytes transferred over the network when a user downloads the app\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eInstall size:\u003c/strong\u003e Bytes a file takes up on the device once the app has been installed\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch3 id=\"determining-the-origin-of-files\"\u003eDetermining the origin of files\u003c/h3\u003e\n\n\n\n\u003cp\u003eAfter analyzing the APK, we end up with a list of all files and their respective sizes. But how do we determine where they came from? To do that, Ruler scans through all Gradle modules and dependencies included in the build and analyzes which files they contain. The result of this is a second list of all components and their contents. Based on this second list, we can now group all files of the app by their source and therefore determine how much each module and dependency contributes to the overall app size.\u003c/p\u003e\n\n\n\n\u003cp\u003eRuler adds a simple analyzeReleaseBundle task to your project, which you can use to execute all this logic. This task will generate two outputs — a JSON report you can use for further processing and an HTML report you can use to analyze and dig into the data yourself.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003ch3 id=\"class-size\"\u003eClass size\u003c/h3\u003e\n\n\n\n\u003cp\u003eOn Android, all classes are compressed into one or more DEX files. Because we want to be able to see the impact of individual classes, Ruler parses those DEX files and treats every class like a separate file.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen classes are compressed, some information is shared between class entries inside the DEX file. Because of that, it’s not possible to 100% accurately measure how much a single class contributes to the overall app size. Ruler solves this problem by approximating the size of each class by setting the raw class size in proportion to the size of the DEX file.\u003c/p\u003e\n\n\n\n\u003ch3 id=\"ownership\"\u003eOwnership\u003c/h3\u003e\n\n\n\n\u003cp\u003eKnowing which components contribute to the size of an app is great, but knowing who owns these components is even better. Because of this, Ruler supports gathering and analyzing app size ownership data.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you provide a list of component owners to Ruler, it can analyze the contributions of certain teams to the size of the overall app. This can be helpful to determine who to talk to if questions about certain parts of the app arise.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"569\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-700x569.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-700x569.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-250x203.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-768x625.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-1536x1249.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview-120x98.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Example_Ownership-Overview.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigures above are for illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2 id=\"ruler-at-spotify\"\u003eRuler at Spotify\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe’ve been using Ruler at Spotify for over half a year now and have seen great success. It has allowed us to identify many improvement opportunities and we have been able to reduce our app size by a little over 9% so far.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe export the app size data once a day, using the latest main build. This data is used to track historical trends, both of the app as a whole and of individual modules and third-party dependencies. Additionally, we analyze the app size impact of every pull request, so we can give early feedback to developers and prevent regressions from being merged in the first place.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"start-using-ruler-today\"\u003eStart using Ruler today\u003c/h2\u003e\n\n\n\n\u003cp\u003eIf you are curious about Ruler, you can try it out today. All you need to do is apply the plugin to your Android project and run a single Gradle task. Check out \u003ca href=\"https://github.com/spotify/ruler\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGitHub\u003c/a\u003e for an always up-to-date guide on how you can integrate Ruler.\u003c/p\u003e\n\n\n\n\u003ch2 id=\"contributing-to-ruler\"\u003eContributing to Ruler\u003c/h2\u003e\n\n\n\n\u003cp\u003eRuler is fully written in Kotlin and leverages exciting technologies like Kotlin Multiplatform. Ruler continues to be actively developed, and we already have many exciting ideas about our new tool. At Spotify, we benefit immensely from open source software, so we decided to open source Ruler and give back to the community.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe’re always looking for input — both raising issues and opening pull requests are very welcome and appreciated. We believe that, together, we can move this project further and help make Android apps accessible to more people.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd if you want to work full time on tools like Ruler, please check out our \u003ca href=\"https://www.lifeatspotify.com/jobs?c=mobile\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen job positions\u003c/a\u003e — we’re always looking for great minds to join the band.\u003c/p\u003e\n\n\n\n\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, we strive to make our apps available to as many people as possible. As mobile developers, that means we want everybody to be able to download our app without hiccups or constraints. One important metric related to this goal is the size of the Spotify app — if it’s too big, users with",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/02/Spotify_RnD-Blog_Header-Ruler_1.png",
      "date_published": "2022-02-14T00:00:00Z",
      "author": {
        "name": "Published by Simon Schiller, Android Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/01/19/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/",
      "title": "\n                                            Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eJanuary 19, 2022\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2022/01/19/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/\" title=\"Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003ch2\u003eIntroduction \u003c/h2\u003e\n\n\n\n\u003cp\u003eBuilding platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built \u003cstrong\u003eML Home\u003c/strong\u003e, the internal user interface for Spotify’s Machine Learning Platform, and the product lessons we learned along the way in our quest to entrench it in Spotify’s ML ecosystem.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s a massive understatement to say that machine learning is at the heart of Spotify’s success story. Spotify has delivered beloved audio experiences such as Discover Weekly, Daily Mix, and \u003ca href=\"https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWrapped\u003c/a\u003e on the strength of ML-powered personalized recommendations. Today, almost every part of Spotify has some applied ML systems, and a significant and growing portion of our R\u0026amp;D teams consist of ML engineers and data scientists. In order to support ML systems at the scale and speed that our business requires, and to apply ML responsibly for our listeners, we have platformized sizable parts of the most common ML infrastructure within our Machine Learning Platform. \u003c/p\u003e\n\n\n\n\u003ch2\u003eOverview of Spotify’s ML Platform\u003cbr/\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSince the beginning, our ambition for \u003ca href=\"https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s ML Platform\u003c/a\u003e was to connect the end-to-end user journey for ML practitioners. We subscribe to the “walking skeleton” model of product development, focusing from the start on end-to-end workflow and subsequently fleshing out functionality once we’ve proven value.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"154\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-250x55.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-768x169.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow.png 1448w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn late 2019 / early 2020, our ML Platform consisted of a few components that covered the (supervised) machine learning workflow experience for Spotify’s ML practitioners: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eSpotify Kubeflow\u003cstrong\u003e, \u003c/strong\u003ewhich is our version of the open source Kubeflow Pipelines platform that helped us standardize ML workflows on the TensorFlow Extended (TFX) ecosystem  \u003c/li\u003e\u003cli\u003eJukebox,\u003cstrong\u003e \u003c/strong\u003ewhich is based on TensorFlow Transform and powers our feature engineering and management workflows \u003c/li\u003e\u003cli\u003eSalem, which is based on TensorFlow Serving and helps us standardize model serving and production workflows, and \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eKlio\u003c/a\u003e, which is our open source solution for audio processing with Apache Beam and Dataflow\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"153\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-250x54.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-768x167.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-1536x335.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1.png 1606w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eThe Product Opportunity\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we started to onboard more ML teams onto our platform, we identified two important gaps in our end-to-end support for ML workflows: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eA \u003cstrong\u003ecentralized metadata layer\u003c/strong\u003e,\u003cstrong\u003e \u003c/strong\u003ewhere we could define our platform entities / entity relationships (e.g., models, evaluations, training sets)\u003c/li\u003e\u003cli\u003eA \u003cstrong\u003emetadata presentation layer\u003c/strong\u003e, where users could store, track, and manage the metadata generated from their ML workflows, which is the focus of this blogpost\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eAs the ML Platform team, we knew we wanted a tool where ML engineers could store ML project information and access metadata related to the ML application lifecycle, but weren’t entirely sure what that product would be. As we began exploring, we found that teams were using spreadsheets to track ML metadata and gave us hyper-specific feature requests for their individual problems. We also came away with broader unmet needs such as discovery of ML projects\u003cstrong\u003e,\u003c/strong\u003e support for ML team collaboration\u003cstrong\u003e,\u003c/strong\u003e and important product\u003cstrong\u003e \u003c/strong\u003egaps within our own ML Platform tooling. These learnings informed the initial scope of our MVP (minimally viable product) and taught us our first product lesson. \u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 1: Balancing Product Vision and Product Strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eFor the first iteration of the product, we took a \u003ca href=\"https://jchyip.medium.com/why-t-shaped-people-e8706198e437\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eT-shaped\u003c/a\u003e approach. We focused on building horizontal solutions\u003cstrong\u003e \u003c/strong\u003efor needs we heard most commonly across all ML practitioner roles, such as being able to collaborate more effectively as an ML team. We also built a vertical solution\u003cstrong\u003e \u003c/strong\u003ethat mapped to a specific platform tooling gap: better evaluation tooling for offline model training for ML engineers. We launched our MVP with an aspirational name and product vision: ML Home, one-stop shop for machine learning at Spotify.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"282\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-250x101.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-768x309.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-120x48.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021.png 1504w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eThe initial feedback we received on our MVP fell on two ends of the spectrum. Individual contributions struggled with the broad idea of a “one-stop shop” and wanted to know what concrete problems the product could solve for them today. Leadership wanted to know how many users we would serve over the long run and how big our impact could be.\u003c/p\u003e\u003cp\u003eThroughout the process of selling our MVP, we learned how difficult it is to balance product vision and product strategy without compromising one for the other. Had we scratched our broader vision based on the initial feedback and focused exclusively on the concrete needs (e.g., I want to see all my training pipelines in one view), we would have risked delivering a narrow point solution. On the other hand, if we over-indexed our roadmap on the broad, ambiguous needs (e.g. I want to collaborate more effectively with my team), we would have delivered a nice-to-have but not a must-have product. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eBy intentionally keeping our \u003cstrong\u003eProduct Vision broad\u003c/strong\u003e \u003cstrong\u003eand future-looking \u003c/strong\u003e(one-stop shop)\u003cstrong\u003e, \u003c/strong\u003ewe gave ourselves the runway to think bigger about our solution space and our potential impact down the line. And by keeping \u003cstrong\u003eProduct Strategy concrete\u003c/strong\u003e \u003cstrong\u003eand iterative\u003c/strong\u003e (offline evaluation tool), we were able to ensure that we solved concrete problems that over time helped us ingrain the product into our user’s daily workflows. \u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 2: The Limits of MVPs \u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"355\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home.png 1080w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eML Home\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eAs we wrapped up the feedback and adoption drive for the MVP, we learned our second product lesson. It is no secret that driving product adoption is hard, especially for products that are trying to replace existing solutions or market incumbents. We hit the ceiling of our MVP’s potential fairly quickly. We did not see a surge of adoption beyond the handful of users who were involved in the very early ideation process. Most users understood the value proposition of what we were building, but did not see enough depth to switch over from their existing tooling. In retrospect, our expectations of what we delivered and how valuable it would be did not match the depth of our users’ needs. It would have been easy at that stage to dismiss the product entirely, based on early adoption signals. That would have been a mistake. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs we plowed on with more detailed user feedback in our quest to drive more adoption, it became clear to us that we were misplacing our expectations on what role the MVP played in the product development process. \u003cstrong\u003eThe most valuable end goal of an MVP is to get enough of the vision and strategies out there to help validate or invalidate them. \u003c/strong\u003eOur initial MVP helped us test and de-risk our work because we were able to get detailed validation of the workflows from ML teams and lay out the technical foundations for the product. It did not matter how many daily active users we had at that stage, as long as we had enough users (which we did) who saw the value in what we were building. These users continued to attend our user feedback sessions and helped us get the product to a higher and more valuable place.\u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 3: Knowing the true Differentiators\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we moved beyond the MVP phase and started to map out our next steps (focusing on some aspects of the product over others), we learned our third and perhaps most important product lesson. We realized that in order to provide a really valuable product to our users, we needed to not only reach feature parity with existing solutions, but also double down on ML Home’s unique differentiators. In short, ML Home as a product needed to be more compelling than the competing solutions. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor a while, we probed, debated, and stack-ranked specific features and workflows that we felt would be game-changing for our users. Our theory was that if we built a “compelling feature,” it would be able to singularly pass a threshold for users to adopt. In the end, we realized that the unique differentiators for ML Home actually came in the form of our other ML Platform offerings, not any one individual feature. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile some aspects of ML Home could functionally serve as a stand-alone product, by enriching it with training, evaluation, and system metadata generated from the rest of our ML Platform, it became a much more compelling product. \u003cstrong\u003eML Home’s unique differentiator isn’t any one silver-bullet feature but rather the gateway value it provides as the sum of our ML Platform capabilities.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eMuch of the work we did in this phase was building out our metadata service to consolidate our overall entities and concepts across the platform, but we also spent significant time building flexibility into the product’s interface. For example, annotation capabilities such as tagging and notes became key features that enabled teams to customize and mirror their own workflows. That, paired with a faster, slicker product experience and information-rich model comparisons, tipped the balance in our favor. \u003c/p\u003e\n\n\n\n\u003cp\u003eBy the time we released the second version of ML Home, we had successfully onboarded more ML teams who were actively using the product in their daily workflows. \u003c/p\u003e\n\n\n\n\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"473\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-250x169.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-768x519.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-1536x1039.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-120x81.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eSample ML Project in ML Home. For illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eScaling the Product to Our Vision\u003c/h2\u003e\n\n\n\n\u003cp\u003eGetting closer to\u003cstrong\u003e \u003c/strong\u003eproduct-market fit taught us a lot about how to iterate moving forward. We knew that ML Home only served one typical ML workflow. However, in order to be an indispensable product for \u003cem\u003eall\u003c/em\u003e ML practitioners, it needed to cover more ground. We also knew that tightly coupling ML Home’s capabilities to our existing ML Platform products resulted in a much higher rate of adoption than stand-alone solutions. Armed with these takeaways, we wireframed a broader vision for the product.\u003c/p\u003e\n\n\n\n\u003cp\u003eToday, ML Home provides Spotify’s ML practitioners with artifacts and workflow metadata of all models passing through individual components of our ML Platform. It includes capabilities such as the ability to track and evaluate offline experiments, visualize results, track and monitor deployed models, explore features, certify models for production readiness, and much more.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThrough intuitive workflows and simplified information architecture, users are able to quickly spin up a project space to collaborate with their team and discover the 220+ ML projects across Spotify currently listed in ML Home. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe have seen a 200% growth in daily active users since we began our scaling efforts a year ago, and ML Home is now solidly entrenched in the daily workflows of some of the most important ML teams at Spotify. Despite its short tenure in Spotify’s Infrastructure landscape, ML Home is well on its way to becoming the one-stop shop for all things ML at Spotify. \u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"476\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-768x523.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home.png 1340w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eConceptual rendering of ML Home. For illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eThree Key Lessons\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe saying goes that hindsight is 20/20, and it’s true. Looking back, these are the lessons that stick out the most from our product development process: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eProduct vision vs product strategy. \u003c/strong\u003eIt is difficult to strike the right balance between an inspiring vision that can support future solutions and a responsive product strategy that addresses today’s problems. But it is crucially important to not conflate the two in the early stages of product development. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLimits of MVPs.\u003c/strong\u003e MVPs provide the most value as a validation and de-risking tool for product strategy and overall direction. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eKnow the true differentiators.\u003c/strong\u003e It’s worth paying attention to what the real differentiators are for a product. It does not have to be a “compelling” feature; it can simply be opportunities found in the ecosystem that turn the tide for a product’s success. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eLooking Ahead \u003c/h2\u003e\n\n\n\n\u003cp\u003eML Home is not done — not even close. We know this because, in the last year, Spotify’s ML community has proposed new and inventive ways to evolve the product. For exampleML engineers saw the potential to build on top of ML Home and proposed we build production readiness certification of ML models in the interface. In addition, we are exploring aspects such as explainability to advance model interpretability and observability to better understand model health. Then there are the ever-inspiring hack week projects that tell us that our product has taken root at Spotify. We are excited for what’s next! \u003c/p\u003e\n\n\n\n\u003cp\u003eIf you are interested in building cutting edge machine learning infrastructure at Spotify, we are \u003ca href=\"https://www.lifeatspotify.com/jobs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehiring\u003c/a\u003e for engineering and product roles across the board. \u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eML Home would not exist without the brilliant work of the ML UX team, our teammates from ML Platform, and the generous guidance from Spotify’s ML community. Since the list of individuals to thank would far exceed the words in this post, I will instead mention the individuals whose work made ML Home possible: Johan Bååth, Joshua Baer, Hayden Betts, Martin Bomio, Matt Brown, Keshi Dai, Omar Delarosa, Funmilayo Doro, Gandalf Hernandez, Adam Laiacano, Brian Martin, Daniel Norberg, James O’Dwyer, Ed Samour, Wesley Yee, and Qi Zheng.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003c/p\u003e\n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "Introduction Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built ML Home, the internal user interface for Spotify’s Ma",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png",
      "date_published": "2022-01-19T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2022/01/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/",
      "title": "\n                                            Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eJanuary 19, 2022\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2022/01/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/\" title=\"Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003ch2\u003eIntroduction \u003c/h2\u003e\n\n\n\n\u003cp\u003eBuilding platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built \u003cstrong\u003eML Home\u003c/strong\u003e, the internal user interface for Spotify’s Machine Learning Platform, and the product lessons we learned along the way in our quest to entrench it in Spotify’s ML ecosystem.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s a massive understatement to say that machine learning is at the heart of Spotify’s success story. Spotify has delivered beloved audio experiences such as Discover Weekly, Daily Mix, and \u003ca href=\"https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWrapped\u003c/a\u003e on the strength of ML-powered personalized recommendations. Today, almost every part of Spotify has some applied ML systems, and a significant and growing portion of our R\u0026amp;D teams consist of ML engineers and data scientists. In order to support ML systems at the scale and speed that our business requires, and to apply ML responsibly for our listeners, we have platformized sizable parts of the most common ML infrastructure within our Machine Learning Platform. \u003c/p\u003e\n\n\n\n\u003ch2\u003eOverview of Spotify’s ML Platform\u003cbr/\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSince the beginning, our ambition for \u003ca href=\"https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s ML Platform\u003c/a\u003e was to connect the end-to-end user journey for ML practitioners. We subscribe to the “walking skeleton” model of product development, focusing from the start on end-to-end workflow and subsequently fleshing out functionality once we’ve proven value.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"154\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-250x55.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-768x169.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow.png 1448w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn late 2019 / early 2020, our ML Platform consisted of a few components that covered the (supervised) machine learning workflow experience for Spotify’s ML practitioners: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eSpotify Kubeflow\u003cstrong\u003e, \u003c/strong\u003ewhich is our version of the open source Kubeflow Pipelines platform that helped us standardize ML workflows on the TensorFlow Extended (TFX) ecosystem  \u003c/li\u003e\u003cli\u003eJukebox,\u003cstrong\u003e \u003c/strong\u003ewhich is based on TensorFlow Transform and powers our feature engineering and management workflows \u003c/li\u003e\u003cli\u003eSalem, which is based on TensorFlow Serving and helps us standardize model serving and production workflows, and \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eKlio\u003c/a\u003e, which is our open source solution for audio processing with Apache Beam and Dataflow\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"153\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-250x54.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-768x167.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-1536x335.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1.png 1606w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eThe Product Opportunity\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we started to onboard more ML teams onto our platform, we identified two important gaps in our end-to-end support for ML workflows: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eA \u003cstrong\u003ecentralized metadata layer\u003c/strong\u003e,\u003cstrong\u003e \u003c/strong\u003ewhere we could define our platform entities / entity relationships (e.g., models, evaluations, training sets)\u003c/li\u003e\u003cli\u003eA \u003cstrong\u003emetadata presentation layer\u003c/strong\u003e, where users could store, track, and manage the metadata generated from their ML workflows, which is the focus of this blogpost\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eAs the ML Platform team, we knew we wanted a tool where ML engineers could store ML project information and access metadata related to the ML application lifecycle, but weren’t entirely sure what that product would be. As we began exploring, we found that teams were using spreadsheets to track ML metadata and gave us hyper-specific feature requests for their individual problems. We also came away with broader unmet needs such as discovery of ML projects\u003cstrong\u003e,\u003c/strong\u003e support for ML team collaboration\u003cstrong\u003e,\u003c/strong\u003e and important product\u003cstrong\u003e \u003c/strong\u003egaps within our own ML Platform tooling. These learnings informed the initial scope of our MVP (minimally viable product) and taught us our first product lesson. \u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 1: Balancing Product Vision and Product Strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eFor the first iteration of the product, we took a \u003ca href=\"https://jchyip.medium.com/why-t-shaped-people-e8706198e437\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eT-shaped\u003c/a\u003e approach. We focused on building horizontal solutions\u003cstrong\u003e \u003c/strong\u003efor needs we heard most commonly across all ML practitioner roles, such as being able to collaborate more effectively as an ML team. We also built a vertical solution\u003cstrong\u003e \u003c/strong\u003ethat mapped to a specific platform tooling gap: better evaluation tooling for offline model training for ML engineers. We launched our MVP with an aspirational name and product vision: ML Home, one-stop shop for machine learning at Spotify.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"282\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-250x101.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-768x309.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-120x48.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021.png 1504w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eThe initial feedback we received on our MVP fell on two ends of the spectrum. Individual contributions struggled with the broad idea of a “one-stop shop” and wanted to know what concrete problems the product could solve for them today. Leadership wanted to know how many users we would serve over the long run and how big our impact could be.\u003c/p\u003e\u003cp\u003eThroughout the process of selling our MVP, we learned how difficult it is to balance product vision and product strategy without compromising one for the other. Had we scratched our broader vision based on the initial feedback and focused exclusively on the concrete needs (e.g., I want to see all my training pipelines in one view), we would have risked delivering a narrow point solution. On the other hand, if we over-indexed our roadmap on the broad, ambiguous needs (e.g. I want to collaborate more effectively with my team), we would have delivered a nice-to-have but not a must-have product. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eBy intentionally keeping our \u003cstrong\u003eProduct Vision broad\u003c/strong\u003e \u003cstrong\u003eand future-looking \u003c/strong\u003e(one-stop shop)\u003cstrong\u003e, \u003c/strong\u003ewe gave ourselves the runway to think bigger about our solution space and our potential impact down the line. And by keeping \u003cstrong\u003eProduct Strategy concrete\u003c/strong\u003e \u003cstrong\u003eand iterative\u003c/strong\u003e (offline evaluation tool), we were able to ensure that we solved concrete problems that over time helped us ingrain the product into our user’s daily workflows. \u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 2: The Limits of MVPs \u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"355\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home.png 1080w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eML Home\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eAs we wrapped up the feedback and adoption drive for the MVP, we learned our second product lesson. It is no secret that driving product adoption is hard, especially for products that are trying to replace existing solutions or market incumbents. We hit the ceiling of our MVP’s potential fairly quickly. We did not see a surge of adoption beyond the handful of users who were involved in the very early ideation process. Most users understood the value proposition of what we were building, but did not see enough depth to switch over from their existing tooling. In retrospect, our expectations of what we delivered and how valuable it would be did not match the depth of our users’ needs. It would have been easy at that stage to dismiss the product entirely, based on early adoption signals. That would have been a mistake. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs we plowed on with more detailed user feedback in our quest to drive more adoption, it became clear to us that we were misplacing our expectations on what role the MVP played in the product development process. \u003cstrong\u003eThe most valuable end goal of an MVP is to get enough of the vision and strategies out there to help validate or invalidate them. \u003c/strong\u003eOur initial MVP helped us test and de-risk our work because we were able to get detailed validation of the workflows from ML teams and lay out the technical foundations for the product. It did not matter how many daily active users we had at that stage, as long as we had enough users (which we did) who saw the value in what we were building. These users continued to attend our user feedback sessions and helped us get the product to a higher and more valuable place.\u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 3: Knowing the true Differentiators\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we moved beyond the MVP phase and started to map out our next steps (focusing on some aspects of the product over others), we learned our third and perhaps most important product lesson. We realized that in order to provide a really valuable product to our users, we needed to not only reach feature parity with existing solutions, but also double down on ML Home’s unique differentiators. In short, ML Home as a product needed to be more compelling than the competing solutions. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor a while, we probed, debated, and stack-ranked specific features and workflows that we felt would be game-changing for our users. Our theory was that if we built a “compelling feature,” it would be able to singularly pass a threshold for users to adopt. In the end, we realized that the unique differentiators for ML Home actually came in the form of our other ML Platform offerings, not any one individual feature. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile some aspects of ML Home could functionally serve as a stand-alone product, by enriching it with training, evaluation, and system metadata generated from the rest of our ML Platform, it became a much more compelling product. \u003cstrong\u003eML Home’s unique differentiator isn’t any one silver-bullet feature but rather the gateway value it provides as the sum of our ML Platform capabilities.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eMuch of the work we did in this phase was building out our metadata service to consolidate our overall entities and concepts across the platform, but we also spent significant time building flexibility into the product’s interface. For example, annotation capabilities such as tagging and notes became key features that enabled teams to customize and mirror their own workflows. That, paired with a faster, slicker product experience and information-rich model comparisons, tipped the balance in our favor. \u003c/p\u003e\n\n\n\n\u003cp\u003eBy the time we released the second version of ML Home, we had successfully onboarded more ML teams who were actively using the product in their daily workflows. \u003c/p\u003e\n\n\n\n\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"473\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-250x169.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-768x519.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-1536x1039.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-120x81.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eSample ML Project in ML Home. For illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eScaling the Product to Our Vision\u003c/h2\u003e\n\n\n\n\u003cp\u003eGetting closer to\u003cstrong\u003e \u003c/strong\u003eproduct-market fit taught us a lot about how to iterate moving forward. We knew that ML Home only served one typical ML workflow. However, in order to be an indispensable product for \u003cem\u003eall\u003c/em\u003e ML practitioners, it needed to cover more ground. We also knew that tightly coupling ML Home’s capabilities to our existing ML Platform products resulted in a much higher rate of adoption than stand-alone solutions. Armed with these takeaways, we wireframed a broader vision for the product.\u003c/p\u003e\n\n\n\n\u003cp\u003eToday, ML Home provides Spotify’s ML practitioners with artifacts and workflow metadata of all models passing through individual components of our ML Platform. It includes capabilities such as the ability to track and evaluate offline experiments, visualize results, track and monitor deployed models, explore features, certify models for production readiness, and much more.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThrough intuitive workflows and simplified information architecture, users are able to quickly spin up a project space to collaborate with their team and discover the 220+ ML projects across Spotify currently listed in ML Home. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe have seen a 200% growth in daily active users since we began our scaling efforts a year ago, and ML Home is now solidly entrenched in the daily workflows of some of the most important ML teams at Spotify. Despite its short tenure in Spotify’s Infrastructure landscape, ML Home is well on its way to becoming the one-stop shop for all things ML at Spotify. \u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"476\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-768x523.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home.png 1340w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eConceptual rendering of ML Home. For illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eThree Key Lessons\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe saying goes that hindsight is 20/20, and it’s true. Looking back, these are the lessons that stick out the most from our product development process: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eProduct vision vs product strategy. \u003c/strong\u003eIt is difficult to strike the right balance between an inspiring vision that can support future solutions and a responsive product strategy that addresses today’s problems. But it is crucially important to not conflate the two in the early stages of product development. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLimits of MVPs.\u003c/strong\u003e MVPs provide the most value as a validation and de-risking tool for product strategy and overall direction. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eKnow the true differentiators.\u003c/strong\u003e It’s worth paying attention to what the real differentiators are for a product. It does not have to be a “compelling” feature; it can simply be opportunities found in the ecosystem that turn the tide for a product’s success. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eLooking Ahead \u003c/h2\u003e\n\n\n\n\u003cp\u003eML Home is not done — not even close. We know this because, in the last year, Spotify’s ML community has proposed new and inventive ways to evolve the product. For exampleML engineers saw the potential to build on top of ML Home and proposed we build production readiness certification of ML models in the interface. In addition, we are exploring aspects such as explainability to advance model interpretability and observability to better understand model health. Then there are the ever-inspiring hack week projects that tell us that our product has taken root at Spotify. We are excited for what’s next! \u003c/p\u003e\n\n\n\n\u003cp\u003eIf you are interested in building cutting edge machine learning infrastructure at Spotify, we are \u003ca href=\"https://www.lifeatspotify.com/jobs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehiring\u003c/a\u003e for engineering and product roles across the board. \u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eML Home would not exist without the brilliant work of the ML UX team, our teammates from ML Platform, and the generous guidance from Spotify’s ML community. Since the list of individuals to thank would far exceed the words in this post, I will instead mention the individuals whose work made ML Home possible: Johan Bååth, Joshua Baer, Hayden Betts, Martin Bomio, Matt Brown, Keshi Dai, Omar Delarosa, Funmilayo Doro, Gandalf Hernandez, Adam Laiacano, Brian Martin, Daniel Norberg, James O’Dwyer, Ed Samour, Wesley Yee, and Qi Zheng.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Introduction Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built ML Home, the internal user interface for Spotify’s Ma",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png",
      "date_published": "2022-01-19T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/",
      "title": "\n                                            Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eJanuary 19, 2022\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/\" title=\"Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003ch2\u003eIntroduction \u003c/h2\u003e\n\n\n\n\u003cp\u003eBuilding platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built \u003cstrong\u003eML Home\u003c/strong\u003e, the internal user interface for Spotify’s Machine Learning Platform, and the product lessons we learned along the way in our quest to entrench it in Spotify’s ML ecosystem.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s a massive understatement to say that machine learning is at the heart of Spotify’s success story. Spotify has delivered beloved audio experiences such as Discover Weekly, Daily Mix, and \u003ca href=\"https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWrapped\u003c/a\u003e on the strength of ML-powered personalized recommendations. Today, almost every part of Spotify has some applied ML systems, and a significant and growing portion of our R\u0026amp;D teams consist of ML engineers and data scientists. In order to support ML systems at the scale and speed that our business requires, and to apply ML responsibly for our listeners, we have platformized sizable parts of the most common ML infrastructure within our Machine Learning Platform. \u003c/p\u003e\n\n\n\n\u003ch2\u003eOverview of Spotify’s ML Platform\u003cbr/\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSince the beginning, our ambition for \u003ca href=\"https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s ML Platform\u003c/a\u003e was to connect the end-to-end user journey for ML practitioners. We subscribe to the “walking skeleton” model of product development, focusing from the start on end-to-end workflow and subsequently fleshing out functionality once we’ve proven value.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"154\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-250x55.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-768x169.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow.png 1448w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn late 2019 / early 2020, our ML Platform consisted of a few components that covered the (supervised) machine learning workflow experience for Spotify’s ML practitioners: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eSpotify Kubeflow\u003cstrong\u003e, \u003c/strong\u003ewhich is our version of the open source Kubeflow Pipelines platform that helped us standardize ML workflows on the TensorFlow Extended (TFX) ecosystem  \u003c/li\u003e\u003cli\u003eJukebox,\u003cstrong\u003e \u003c/strong\u003ewhich is based on TensorFlow Transform and powers our feature engineering and management workflows \u003c/li\u003e\u003cli\u003eSalem, which is based on TensorFlow Serving and helps us standardize model serving and production workflows, and \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eKlio\u003c/a\u003e, which is our open source solution for audio processing with Apache Beam and Dataflow\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"153\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-250x54.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-768x167.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-1536x335.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1.png 1606w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eThe Product Opportunity\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we started to onboard more ML teams onto our platform, we identified two important gaps in our end-to-end support for ML workflows: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eA \u003cstrong\u003ecentralized metadata layer\u003c/strong\u003e,\u003cstrong\u003e \u003c/strong\u003ewhere we could define our platform entities / entity relationships (e.g., models, evaluations, training sets)\u003c/li\u003e\u003cli\u003eA \u003cstrong\u003emetadata presentation layer\u003c/strong\u003e, where users could store, track, and manage the metadata generated from their ML workflows, which is the focus of this blogpost\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eAs the ML Platform team, we knew we wanted a tool where ML engineers could store ML project information and access metadata related to the ML application lifecycle, but weren’t entirely sure what that product would be. As we began exploring, we found that teams were using spreadsheets to track ML metadata and gave us hyper-specific feature requests for their individual problems. We also came away with broader unmet needs such as discovery of ML projects\u003cstrong\u003e,\u003c/strong\u003e support for ML team collaboration\u003cstrong\u003e,\u003c/strong\u003e and important product\u003cstrong\u003e \u003c/strong\u003egaps within our own ML Platform tooling. These learnings informed the initial scope of our MVP (minimally viable product) and taught us our first product lesson. \u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 1: Balancing Product Vision and Product Strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eFor the first iteration of the product, we took a \u003ca href=\"https://jchyip.medium.com/why-t-shaped-people-e8706198e437\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eT-shaped\u003c/a\u003e approach. We focused on building horizontal solutions\u003cstrong\u003e \u003c/strong\u003efor needs we heard most commonly across all ML practitioner roles, such as being able to collaborate more effectively as an ML team. We also built a vertical solution\u003cstrong\u003e \u003c/strong\u003ethat mapped to a specific platform tooling gap: better evaluation tooling for offline model training for ML engineers. We launched our MVP with an aspirational name and product vision: ML Home, one-stop shop for machine learning at Spotify.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"282\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-250x101.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-768x309.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-120x48.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021.png 1504w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eThe initial feedback we received on our MVP fell on two ends of the spectrum. Individual contributions struggled with the broad idea of a “one-stop shop” and wanted to know what concrete problems the product could solve for them today. Leadership wanted to know how many users we would serve over the long run and how big our impact could be.\u003c/p\u003e\u003cp\u003eThroughout the process of selling our MVP, we learned how difficult it is to balance product vision and product strategy without compromising one for the other. Had we scratched our broader vision based on the initial feedback and focused exclusively on the concrete needs (e.g., I want to see all my training pipelines in one view), we would have risked delivering a narrow point solution. On the other hand, if we over-indexed our roadmap on the broad, ambiguous needs (e.g. I want to collaborate more effectively with my team), we would have delivered a nice-to-have but not a must-have product. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eBy intentionally keeping our \u003cstrong\u003eProduct Vision broad\u003c/strong\u003e \u003cstrong\u003eand future-looking \u003c/strong\u003e(one-stop shop)\u003cstrong\u003e, \u003c/strong\u003ewe gave ourselves the runway to think bigger about our solution space and our potential impact down the line. And by keeping \u003cstrong\u003eProduct Strategy concrete\u003c/strong\u003e \u003cstrong\u003eand iterative\u003c/strong\u003e (offline evaluation tool), we were able to ensure that we solved concrete problems that over time helped us ingrain the product into our user’s daily workflows. \u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 2: The Limits of MVPs \u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"355\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home.png 1080w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eML Home\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eAs we wrapped up the feedback and adoption drive for the MVP, we learned our second product lesson. It is no secret that driving product adoption is hard, especially for products that are trying to replace existing solutions or market incumbents. We hit the ceiling of our MVP’s potential fairly quickly. We did not see a surge of adoption beyond the handful of users who were involved in the very early ideation process. Most users understood the value proposition of what we were building, but did not see enough depth to switch over from their existing tooling. In retrospect, our expectations of what we delivered and how valuable it would be did not match the depth of our users’ needs. It would have been easy at that stage to dismiss the product entirely, based on early adoption signals. That would have been a mistake. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs we plowed on with more detailed user feedback in our quest to drive more adoption, it became clear to us that we were misplacing our expectations on what role the MVP played in the product development process. \u003cstrong\u003eThe most valuable end goal of an MVP is to get enough of the vision and strategies out there to help validate or invalidate them. \u003c/strong\u003eOur initial MVP helped us test and de-risk our work because we were able to get detailed validation of the workflows from ML teams and lay out the technical foundations for the product. It did not matter how many daily active users we had at that stage, as long as we had enough users (which we did) who saw the value in what we were building. These users continued to attend our user feedback sessions and helped us get the product to a higher and more valuable place.\u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 3: Knowing the true Differentiators\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we moved beyond the MVP phase and started to map out our next steps (focusing on some aspects of the product over others), we learned our third and perhaps most important product lesson. We realized that in order to provide a really valuable product to our users, we needed to not only reach feature parity with existing solutions, but also double down on ML Home’s unique differentiators. In short, ML Home as a product needed to be more compelling than the competing solutions. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor a while, we probed, debated, and stack-ranked specific features and workflows that we felt would be game-changing for our users. Our theory was that if we built a “compelling feature,” it would be able to singularly pass a threshold for users to adopt. In the end, we realized that the unique differentiators for ML Home actually came in the form of our other ML Platform offerings, not any one individual feature. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile some aspects of ML Home could functionally serve as a stand-alone product, by enriching it with training, evaluation, and system metadata generated from the rest of our ML Platform, it became a much more compelling product. \u003cstrong\u003eML Home’s unique differentiator isn’t any one silver-bullet feature but rather the gateway value it provides as the sum of our ML Platform capabilities.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eMuch of the work we did in this phase was building out our metadata service to consolidate our overall entities and concepts across the platform, but we also spent significant time building flexibility into the product’s interface. For example, annotation capabilities such as tagging and notes became key features that enabled teams to customize and mirror their own workflows. That, paired with a faster, slicker product experience and information-rich model comparisons, tipped the balance in our favor. \u003c/p\u003e\n\n\n\n\u003cp\u003eBy the time we released the second version of ML Home, we had successfully onboarded more ML teams who were actively using the product in their daily workflows. \u003c/p\u003e\n\n\n\n\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"473\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-250x169.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-768x519.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-1536x1039.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-120x81.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eSample ML Project in ML Home. For illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eScaling the Product to Our Vision\u003c/h2\u003e\n\n\n\n\u003cp\u003eGetting closer to\u003cstrong\u003e \u003c/strong\u003eproduct-market fit taught us a lot about how to iterate moving forward. We knew that ML Home only served one typical ML workflow. However, in order to be an indispensable product for \u003cem\u003eall\u003c/em\u003e ML practitioners, it needed to cover more ground. We also knew that tightly coupling ML Home’s capabilities to our existing ML Platform products resulted in a much higher rate of adoption than stand-alone solutions. Armed with these takeaways, we wireframed a broader vision for the product.\u003c/p\u003e\n\n\n\n\u003cp\u003eToday, ML Home provides Spotify’s ML practitioners with artifacts and workflow metadata of all models passing through individual components of our ML Platform. It includes capabilities such as the ability to track and evaluate offline experiments, visualize results, track and monitor deployed models, explore features, certify models for production readiness, and much more.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThrough intuitive workflows and simplified information architecture, users are able to quickly spin up a project space to collaborate with their team and discover the 220+ ML projects across Spotify currently listed in ML Home. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe have seen a 200% growth in daily active users since we began our scaling efforts a year ago, and ML Home is now solidly entrenched in the daily workflows of some of the most important ML teams at Spotify. Despite its short tenure in Spotify’s Infrastructure landscape, ML Home is well on its way to becoming the one-stop shop for all things ML at Spotify. \u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"476\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-768x523.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home.png 1340w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eConceptual rendering of ML Home. For illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eThree Key Lessons\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe saying goes that hindsight is 20/20, and it’s true. Looking back, these are the lessons that stick out the most from our product development process: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eProduct vision vs product strategy. \u003c/strong\u003eIt is difficult to strike the right balance between an inspiring vision that can support future solutions and a responsive product strategy that addresses today’s problems. But it is crucially important to not conflate the two in the early stages of product development. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLimits of MVPs.\u003c/strong\u003e MVPs provide the most value as a validation and de-risking tool for product strategy and overall direction. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eKnow the true differentiators.\u003c/strong\u003e It’s worth paying attention to what the real differentiators are for a product. It does not have to be a “compelling” feature; it can simply be opportunities found in the ecosystem that turn the tide for a product’s success. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eLooking Ahead \u003c/h2\u003e\n\n\n\n\u003cp\u003eML Home is not done — not even close. We know this because, in the last year, Spotify’s ML community has proposed new and inventive ways to evolve the product. For exampleML engineers saw the potential to build on top of ML Home and proposed we build production readiness certification of ML models in the interface. In addition, we are exploring aspects such as explainability to advance model interpretability and observability to better understand model health. Then there are the ever-inspiring hack week projects that tell us that our product has taken root at Spotify. We are excited for what’s next! \u003c/p\u003e\n\n\n\n\u003cp\u003eIf you are interested in building cutting edge machine learning infrastructure at Spotify, we are \u003ca href=\"https://www.lifeatspotify.com/jobs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehiring\u003c/a\u003e for engineering and product roles across the board. \u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eML Home would not exist without the brilliant work of the ML UX team, our teammates from ML Platform, and the generous guidance from Spotify’s ML community. Since the list of individuals to thank would far exceed the words in this post, I will instead mention the individuals whose work made ML Home possible: Johan Bååth, Joshua Baer, Hayden Betts, Martin Bomio, Matt Brown, Keshi Dai, Omar Delarosa, Funmilayo Doro, Gandalf Hernandez, Adam Laiacano, Brian Martin, Daniel Norberg, James O’Dwyer, Ed Samour, Wesley Yee, and Qi Zheng.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Introduction Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built ML Home, the internal user interface for Spotify’s Ma",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png",
      "date_published": "2022-01-19T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/",
      "title": "\n                                            Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eJanuary 19, 2022\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/\" title=\"Product Lessons from ML Home: Spotify’s One-Stop Shop for Machine Learning\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003ch2\u003eIntroduction \u003c/h2\u003e\n\n\n\n\u003cp\u003eBuilding platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built \u003cstrong\u003eML Home\u003c/strong\u003e, the internal user interface for Spotify’s Machine Learning Platform, and the product lessons we learned along the way in our quest to entrench it in Spotify’s ML ecosystem.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s a massive understatement to say that machine learning is at the heart of Spotify’s success story. Spotify has delivered beloved audio experiences such as Discover Weekly, Daily Mix, and \u003ca href=\"https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWrapped\u003c/a\u003e on the strength of ML-powered personalized recommendations. Today, almost every part of Spotify has some applied ML systems, and a significant and growing portion of our R\u0026amp;D teams consist of ML engineers and data scientists. In order to support ML systems at the scale and speed that our business requires, and to apply ML responsibly for our listeners, we have platformized sizable parts of the most common ML infrastructure within our Machine Learning Platform. \u003c/p\u003e\n\n\n\n\u003ch2\u003eOverview of Spotify’s ML Platform\u003cbr/\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSince the beginning, our ambition for \u003ca href=\"https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s ML Platform\u003c/a\u003e was to connect the end-to-end user journey for ML practitioners. We subscribe to the “walking skeleton” model of product development, focusing from the start on end-to-end workflow and subsequently fleshing out functionality once we’ve proven value.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"154\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-700x154.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-250x55.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-768x169.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-User-Workflow.png 1448w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn late 2019 / early 2020, our ML Platform consisted of a few components that covered the (supervised) machine learning workflow experience for Spotify’s ML practitioners: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eSpotify Kubeflow\u003cstrong\u003e, \u003c/strong\u003ewhich is our version of the open source Kubeflow Pipelines platform that helped us standardize ML workflows on the TensorFlow Extended (TFX) ecosystem  \u003c/li\u003e\u003cli\u003eJukebox,\u003cstrong\u003e \u003c/strong\u003ewhich is based on TensorFlow Transform and powers our feature engineering and management workflows \u003c/li\u003e\u003cli\u003eSalem, which is based on TensorFlow Serving and helps us standardize model serving and production workflows, and \u003c/li\u003e\u003cli\u003e\u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eKlio\u003c/a\u003e, which is our open source solution for audio processing with Apache Beam and Dataflow\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"153\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-700x153.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-250x54.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-768x167.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-1536x335.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1-120x26.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2019-1.png 1606w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eThe Product Opportunity\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we started to onboard more ML teams onto our platform, we identified two important gaps in our end-to-end support for ML workflows: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eA \u003cstrong\u003ecentralized metadata layer\u003c/strong\u003e,\u003cstrong\u003e \u003c/strong\u003ewhere we could define our platform entities / entity relationships (e.g., models, evaluations, training sets)\u003c/li\u003e\u003cli\u003eA \u003cstrong\u003emetadata presentation layer\u003c/strong\u003e, where users could store, track, and manage the metadata generated from their ML workflows, which is the focus of this blogpost\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eAs the ML Platform team, we knew we wanted a tool where ML engineers could store ML project information and access metadata related to the ML application lifecycle, but weren’t entirely sure what that product would be. As we began exploring, we found that teams were using spreadsheets to track ML metadata and gave us hyper-specific feature requests for their individual problems. We also came away with broader unmet needs such as discovery of ML projects\u003cstrong\u003e,\u003c/strong\u003e support for ML team collaboration\u003cstrong\u003e,\u003c/strong\u003e and important product\u003cstrong\u003e \u003c/strong\u003egaps within our own ML Platform tooling. These learnings informed the initial scope of our MVP (minimally viable product) and taught us our first product lesson. \u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 1: Balancing Product Vision and Product Strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eFor the first iteration of the product, we took a \u003ca href=\"https://jchyip.medium.com/why-t-shaped-people-e8706198e437\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eT-shaped\u003c/a\u003e approach. We focused on building horizontal solutions\u003cstrong\u003e \u003c/strong\u003efor needs we heard most commonly across all ML practitioner roles, such as being able to collaborate more effectively as an ML team. We also built a vertical solution\u003cstrong\u003e \u003c/strong\u003ethat mapped to a specific platform tooling gap: better evaluation tooling for offline model training for ML engineers. We launched our MVP with an aspirational name and product vision: ML Home, one-stop shop for machine learning at Spotify.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"282\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-700x282.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-250x101.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-768x309.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021-120x48.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Platform_2021.png 1504w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eThe initial feedback we received on our MVP fell on two ends of the spectrum. Individual contributions struggled with the broad idea of a “one-stop shop” and wanted to know what concrete problems the product could solve for them today. Leadership wanted to know how many users we would serve over the long run and how big our impact could be.\u003c/p\u003e\u003cp\u003eThroughout the process of selling our MVP, we learned how difficult it is to balance product vision and product strategy without compromising one for the other. Had we scratched our broader vision based on the initial feedback and focused exclusively on the concrete needs (e.g., I want to see all my training pipelines in one view), we would have risked delivering a narrow point solution. On the other hand, if we over-indexed our roadmap on the broad, ambiguous needs (e.g. I want to collaborate more effectively with my team), we would have delivered a nice-to-have but not a must-have product. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eBy intentionally keeping our \u003cstrong\u003eProduct Vision broad\u003c/strong\u003e \u003cstrong\u003eand future-looking \u003c/strong\u003e(one-stop shop)\u003cstrong\u003e, \u003c/strong\u003ewe gave ourselves the runway to think bigger about our solution space and our potential impact down the line. And by keeping \u003cstrong\u003eProduct Strategy concrete\u003c/strong\u003e \u003cstrong\u003eand iterative\u003c/strong\u003e (offline evaluation tool), we were able to ensure that we solved concrete problems that over time helped us ingrain the product into our user’s daily workflows. \u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 2: The Limits of MVPs \u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"355\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Spotify-ML-Home.png 1080w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eML Home\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eAs we wrapped up the feedback and adoption drive for the MVP, we learned our second product lesson. It is no secret that driving product adoption is hard, especially for products that are trying to replace existing solutions or market incumbents. We hit the ceiling of our MVP’s potential fairly quickly. We did not see a surge of adoption beyond the handful of users who were involved in the very early ideation process. Most users understood the value proposition of what we were building, but did not see enough depth to switch over from their existing tooling. In retrospect, our expectations of what we delivered and how valuable it would be did not match the depth of our users’ needs. It would have been easy at that stage to dismiss the product entirely, based on early adoption signals. That would have been a mistake. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs we plowed on with more detailed user feedback in our quest to drive more adoption, it became clear to us that we were misplacing our expectations on what role the MVP played in the product development process. \u003cstrong\u003eThe most valuable end goal of an MVP is to get enough of the vision and strategies out there to help validate or invalidate them. \u003c/strong\u003eOur initial MVP helped us test and de-risk our work because we were able to get detailed validation of the workflows from ML teams and lay out the technical foundations for the product. It did not matter how many daily active users we had at that stage, as long as we had enough users (which we did) who saw the value in what we were building. These users continued to attend our user feedback sessions and helped us get the product to a higher and more valuable place.\u003c/p\u003e\n\n\n\n\u003ch2\u003eProduct Lesson 3: Knowing the true Differentiators\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we moved beyond the MVP phase and started to map out our next steps (focusing on some aspects of the product over others), we learned our third and perhaps most important product lesson. We realized that in order to provide a really valuable product to our users, we needed to not only reach feature parity with existing solutions, but also double down on ML Home’s unique differentiators. In short, ML Home as a product needed to be more compelling than the competing solutions. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor a while, we probed, debated, and stack-ranked specific features and workflows that we felt would be game-changing for our users. Our theory was that if we built a “compelling feature,” it would be able to singularly pass a threshold for users to adopt. In the end, we realized that the unique differentiators for ML Home actually came in the form of our other ML Platform offerings, not any one individual feature. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile some aspects of ML Home could functionally serve as a stand-alone product, by enriching it with training, evaluation, and system metadata generated from the rest of our ML Platform, it became a much more compelling product. \u003cstrong\u003eML Home’s unique differentiator isn’t any one silver-bullet feature but rather the gateway value it provides as the sum of our ML Platform capabilities.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eMuch of the work we did in this phase was building out our metadata service to consolidate our overall entities and concepts across the platform, but we also spent significant time building flexibility into the product’s interface. For example, annotation capabilities such as tagging and notes became key features that enabled teams to customize and mirror their own workflows. That, paired with a faster, slicker product experience and information-rich model comparisons, tipped the balance in our favor. \u003c/p\u003e\n\n\n\n\u003cp\u003eBy the time we released the second version of ML Home, we had successfully onboarded more ML teams who were actively using the product in their daily workflows. \u003c/p\u003e\n\n\n\n\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"473\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-700x473.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-250x169.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-768x519.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-1536x1039.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project-120x81.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Sample-NLP-Project.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eSample ML Project in ML Home. For illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eScaling the Product to Our Vision\u003c/h2\u003e\n\n\n\n\u003cp\u003eGetting closer to\u003cstrong\u003e \u003c/strong\u003eproduct-market fit taught us a lot about how to iterate moving forward. We knew that ML Home only served one typical ML workflow. However, in order to be an indispensable product for \u003cem\u003eall\u003c/em\u003e ML practitioners, it needed to cover more ground. We also knew that tightly coupling ML Home’s capabilities to our existing ML Platform products resulted in a much higher rate of adoption than stand-alone solutions. Armed with these takeaways, we wireframed a broader vision for the product.\u003c/p\u003e\n\n\n\n\u003cp\u003eToday, ML Home provides Spotify’s ML practitioners with artifacts and workflow metadata of all models passing through individual components of our ML Platform. It includes capabilities such as the ability to track and evaluate offline experiments, visualize results, track and monitor deployed models, explore features, certify models for production readiness, and much more.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThrough intuitive workflows and simplified information architecture, users are able to quickly spin up a project space to collaborate with their team and discover the 220+ ML projects across Spotify currently listed in ML Home. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe have seen a 200% growth in daily active users since we began our scaling efforts a year ago, and ML Home is now solidly entrenched in the daily workflows of some of the most important ML teams at Spotify. Despite its short tenure in Spotify’s Infrastructure landscape, ML Home is well on its way to becoming the one-stop shop for all things ML at Spotify. \u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"476\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-700x476.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-768x523.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home-120x82.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/Conceptual-Rendering-of-ML-Home.png 1340w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eConceptual rendering of ML Home. For illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eThree Key Lessons\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe saying goes that hindsight is 20/20, and it’s true. Looking back, these are the lessons that stick out the most from our product development process: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eProduct vision vs product strategy. \u003c/strong\u003eIt is difficult to strike the right balance between an inspiring vision that can support future solutions and a responsive product strategy that addresses today’s problems. But it is crucially important to not conflate the two in the early stages of product development. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLimits of MVPs.\u003c/strong\u003e MVPs provide the most value as a validation and de-risking tool for product strategy and overall direction. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eKnow the true differentiators.\u003c/strong\u003e It’s worth paying attention to what the real differentiators are for a product. It does not have to be a “compelling” feature; it can simply be opportunities found in the ecosystem that turn the tide for a product’s success. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eLooking Ahead \u003c/h2\u003e\n\n\n\n\u003cp\u003eML Home is not done — not even close. We know this because, in the last year, Spotify’s ML community has proposed new and inventive ways to evolve the product. For exampleML engineers saw the potential to build on top of ML Home and proposed we build production readiness certification of ML models in the interface. In addition, we are exploring aspects such as explainability to advance model interpretability and observability to better understand model health. Then there are the ever-inspiring hack week projects that tell us that our product has taken root at Spotify. We are excited for what’s next! \u003c/p\u003e\n\n\n\n\u003cp\u003eIf you are interested in building cutting edge machine learning infrastructure at Spotify, we are \u003ca href=\"https://www.lifeatspotify.com/jobs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehiring\u003c/a\u003e for engineering and product roles across the board. \u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eML Home would not exist without the brilliant work of the ML UX team, our teammates from ML Platform, and the generous guidance from Spotify’s ML community. Since the list of individuals to thank would far exceed the words in this post, I will instead mention the individuals whose work made ML Home possible: Johan Bååth, Joshua Baer, Hayden Betts, Martin Bomio, Matt Brown, Keshi Dai, Omar Delarosa, Funmilayo Doro, Gandalf Hernandez, Adam Laiacano, Brian Martin, Daniel Norberg, James O’Dwyer, Ed Samour, Wesley Yee, and Qi Zheng.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Introduction Building platforms is a hard business. Building platforms for discerning machine learning (ML) practitioners with bespoke needs and a do-it-yourself ethos is even harder. In today’s post, we will give you a peek into how we built ML Home, the internal user interface for Spotify’s Ma",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2022/01/ML-Home_Header.png",
      "date_published": "2022-01-19T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/the-audio-aura-story-mystical-to-mathematical/",
      "title": "\n                                            The Audio Aura Story: Mystical to Mathematical\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 17, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/the-audio-aura-story-mystical-to-mathematical/\" title=\"The Audio Aura Story: Mystical to Mathematical\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-768x380.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eFor 2021 Wrapped, we were challenged to visually express a user’s \u003ca href=\"https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAudio Aura\u003c/a\u003e based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like? That’s the 2021 Wrapped Audio Aura story. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eA new Wrapped experience\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpotify Wrapped is a company-wide effort with 300+ Spotifiers across 20+ teams. My team in particular is a design/engineering team and our main focus is to design, build, and launch the Wrapped personalized user experience to millions of Spotify users around the world. These personalized stories bring users’ annual listening habits to life through creative storytelling. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe personalized experience shows listeners’ data stories such as their top songs, top artists, and top genres of the year. In addition to building these stories, our Brand and Creative team thinks outside the box and ideates new ways to illuminate insights into a user’s audio streams. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eImagine.\u003c/em\u003e A meeting room full of professionals — a virtual meeting room, of course. Brows furrowed, pens clicking, and fingers scratching heads. We were brainstorming the answer to the question: how do we \u003cem\u003edefine\u003c/em\u003e a Spotify user’s Audio Aura? To help with this question and guarantee the auras we create are valid, we consulted an aura reader, \u003ca href=\"https://www.mysticmichaela.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMystic Michaela\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Mystic Michaela, auras are “your personal energy signature. Everyone has one, and aura readers see them as a combination of colors, each representative of the traits that make you, you” (check out Michaela’s full take on auras \u003ca href=\"https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e). For the purposes of our Audio Aura story, we can extend the definition to say that a 2021 Spotify Wrapped Audio Aura is a colorful energy made up of two colors that help our listeners understand the vibe or mood of the music they streamed this year.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eLet’s get to the whiteboard \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003ch5\u003e\u003cstrong\u003eChallenge #1\u003c/strong\u003e \u003c/h5\u003e\n\n\n\n\u003cp\u003eReturn the top two mood categories and the descriptor associated with their 2021 listening history for each user.\u003c/p\u003e\n\n\n\n\u003ch5\u003e\u003cstrong\u003eChallenge #2\u003c/strong\u003e\u003c/h5\u003e\n\n\n\n\u003cp\u003eIdentify how much of a user’s music is represented by each mood aka the mood weight.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo address the above challenges, we utilized a track mood descriptor dataset, aggregated each user’s listening history, and created six broad mood categories (i.e. “Happy”, “Calm”, “Hopeful”) that would then be narrowed down to two to create the audio aura.\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eCount the number of streams for a given track and retrieve its top mood descriptor as determined by the mood descriptor dataset.\u003c/li\u003e\u003cli\u003eBucket those moods into one of the six mood categories.\u003c/li\u003e\u003cli\u003ePerform an aggregation to find the total number of streams for each mood category and take the mood descriptor with the highest number of streams.\u003c/li\u003e\u003cli\u003eCarry out one last calculation to find the percentage of streams of a mood over the total number of streams.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"505\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-250x180.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-768x554.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-1536x1107.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-120x87.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn the example above, tracks that are “happy” or “calm” are the most streamed, making them the top two moods found in a specific user’s music. “happy” makes up 47% of the user’s listening with the mood descriptor being “blissful”. “calm” makes up 29% of the user’s listening with the granular descriptor being “chill”.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eNow, let’s turn these moods into colors\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eDry-erase pen caps click… we’ve identified a user’s top two Audio Aura moods and their mood weights. Now let’s turn this data into colors. Mystic Michaela stepped in to lend her expertise on auras and their colors. With her guidance, we assigned six core colors to the six mood descriptor categories mentioned above (which include “happy”, “calm”, and “hopeful”). To provide secondary colors to the aura visual, our designer represents mood weights through varying levels of contrast. The more a mood is present in a user’s listening, the darker the color. The less a mood is present in a user’s listening, the lighter the color. In the graphic above, the user has a mood weight of 0.47; therefore, the secondary color would be found in the row labeled Encore 100. Their second mood weight was 0.29; therefore, the secondary color would be found in the row labeled Encore 180.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eCreativity and embracing constraints\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we have a user’s Audio Aura colors, all that’s left is for design and engineering to consider the last question: how can we engineer an ethereal Aura visual?\u003c/p\u003e\n\n\n\n\u003cp\u003eOur deadline was two weeks away. In our milestones planning, we allocated a few days of engineering per story since we committed to a number of data stories and several new features. We had a few days to explore an elegant aura visual that was feasible on both iOS and Android. Part of the challenge for design is knowing what’s possible for engineering to execute in a dedicated amount of time. And part of the challenge for engineering is accurately estimating the time it will take to build a given design. We had to come up with a solution that made good use of our time and garnered a confident engineering sign-off. Situations like this are very unique to the Wrapped design/engineering experience and through such situations, I’ve learned that creativity comes from embracing constraints and making use of what we have in novel ways. Our designer made use of three elements: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAn animated gradient built for last year’s 2020 Wrapped campaign to create a “lava lamp” effect.\u003c/li\u003e\u003cli\u003eRibbons built for 2021 Wrapped to create organic and unique shapes.\u003c/li\u003e\u003cli\u003eA blur effect to blend everything together in a dreamlike fashion.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSince we had already built the animated gradient and ribbons, we were able to build the aura visual with high confidence, on time, and to accurately depict an ethereal aura!\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"250\" height=\"501\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-700x1403.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-768x1540.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-766x1536.png 766w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-120x241.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura.png 858w\" sizes=\"(max-width: 250px) 100vw, 250px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eTo wrap it up\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe 2021 Wrapped Audio Aura story is a peek into the unique experience of engineering and design. We took an esoteric concept and broke it down into its basic elements to build an exciting feature for the Wrapped experience. Our ability to work together with teams across Spotify allows us to find creative solutions, enabling us to create stories that, hopefully, delight our users. If you’re interested in joining our efforts to bring Spotify listeners new experiences, check out our \u003ca href=\"https://www.lifeatspotify.com/jobs?c=engineering\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen roles\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR For 2021 Wrapped, we were challenged to visually express a user’s Audio Aura based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like?",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png",
      "date_published": "2021-12-17T00:00:00Z",
      "author": {
        "name": "Published by Zela Taino, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/the-audio-aura-story-mystical-to-mathematical/",
      "title": "\n                                            The Audio Aura Story: Mystical to Mathematical\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 17, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/the-audio-aura-story-mystical-to-mathematical/\" title=\"The Audio Aura Story: Mystical to Mathematical\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-768x380.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eFor 2021 Wrapped, we were challenged to visually express a user’s \u003ca href=\"https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAudio Aura\u003c/a\u003e based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like? That’s the 2021 Wrapped Audio Aura story. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eA new Wrapped experience\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpotify Wrapped is a company-wide effort with 300+ Spotifiers across 20+ teams. My team in particular is a design/engineering team and our main focus is to design, build, and launch the Wrapped personalized user experience to millions of Spotify users around the world. These personalized stories bring users’ annual listening habits to life through creative storytelling. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe personalized experience shows listeners’ data stories such as their top songs, top artists, and top genres of the year. In addition to building these stories, our Brand and Creative team thinks outside the box and ideates new ways to illuminate insights into a user’s audio streams. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eImagine.\u003c/em\u003e A meeting room full of professionals — a virtual meeting room, of course. Brows furrowed, pens clicking, and fingers scratching heads. We were brainstorming the answer to the question: how do we \u003cem\u003edefine\u003c/em\u003e a Spotify user’s Audio Aura? To help with this question and guarantee the auras we create are valid, we consulted an aura reader, \u003ca href=\"https://www.mysticmichaela.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMystic Michaela\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Mystic Michaela, auras are “your personal energy signature. Everyone has one, and aura readers see them as a combination of colors, each representative of the traits that make you, you” (check out Michaela’s full take on auras \u003ca href=\"https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e). For the purposes of our Audio Aura story, we can extend the definition to say that a 2021 Spotify Wrapped Audio Aura is a colorful energy made up of two colors that help our listeners understand the vibe or mood of the music they streamed this year.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eLet’s get to the whiteboard \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003ch5\u003e\u003cstrong\u003eChallenge #1\u003c/strong\u003e \u003c/h5\u003e\n\n\n\n\u003cp\u003eReturn the top two mood categories and the descriptor associated with their 2021 listening history for each user.\u003c/p\u003e\n\n\n\n\u003ch5\u003e\u003cstrong\u003eChallenge #2\u003c/strong\u003e\u003c/h5\u003e\n\n\n\n\u003cp\u003eIdentify how much of a user’s music is represented by each mood aka the mood weight.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo address the above challenges, we utilized a track mood descriptor dataset, aggregated each user’s listening history, and created six broad mood categories (i.e. “Happy”, “Calm”, “Hopeful”) that would then be narrowed down to two to create the audio aura.\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eCount the number of streams for a given track and retrieve its top mood descriptor as determined by the mood descriptor dataset.\u003c/li\u003e\u003cli\u003eBucket those moods into one of the six mood categories.\u003c/li\u003e\u003cli\u003ePerform an aggregation to find the total number of streams for each mood category and take the mood descriptor with the highest number of streams.\u003c/li\u003e\u003cli\u003eCarry out one last calculation to find the percentage of streams of a mood over the total number of streams.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"505\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-250x180.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-768x554.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-1536x1107.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-120x87.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn the example above, tracks that are “happy” or “calm” are the most streamed, making them the top two moods found in a specific user’s music. “happy” makes up 47% of the user’s listening with the mood descriptor being “blissful”. “calm” makes up 29% of the user’s listening with the granular descriptor being “chill”.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eNow, let’s turn these moods into colors\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eDry-erase pen caps click… we’ve identified a user’s top two Audio Aura moods and their mood weights. Now let’s turn this data into colors. Mystic Michaela stepped in to lend her expertise on auras and their colors. With her guidance, we assigned six core colors to the six mood descriptor categories mentioned above (which include “happy”, “calm”, and “hopeful”). To provide secondary colors to the aura visual, our designer represents mood weights through varying levels of contrast. The more a mood is present in a user’s listening, the darker the color. The less a mood is present in a user’s listening, the lighter the color. In the graphic above, the user has a mood weight of 0.47; therefore, the secondary color would be found in the row labeled Encore 100. Their second mood weight was 0.29; therefore, the secondary color would be found in the row labeled Encore 180.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eCreativity and embracing constraints\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we have a user’s Audio Aura colors, all that’s left is for design and engineering to consider the last question: how can we engineer an ethereal Aura visual?\u003c/p\u003e\n\n\n\n\u003cp\u003eOur deadline was two weeks away. In our milestones planning, we allocated a few days of engineering per story since we committed to a number of data stories and several new features. We had a few days to explore an elegant aura visual that was feasible on both iOS and Android. Part of the challenge for design is knowing what’s possible for engineering to execute in a dedicated amount of time. And part of the challenge for engineering is accurately estimating the time it will take to build a given design. We had to come up with a solution that made good use of our time and garnered a confident engineering sign-off. Situations like this are very unique to the Wrapped design/engineering experience and through such situations, I’ve learned that creativity comes from embracing constraints and making use of what we have in novel ways. Our designer made use of three elements: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAn animated gradient built for last year’s 2020 Wrapped campaign to create a “lava lamp” effect.\u003c/li\u003e\u003cli\u003eRibbons built for 2021 Wrapped to create organic and unique shapes.\u003c/li\u003e\u003cli\u003eA blur effect to blend everything together in a dreamlike fashion.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSince we had already built the animated gradient and ribbons, we were able to build the aura visual with high confidence, on time, and to accurately depict an ethereal aura!\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"250\" height=\"501\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-700x1403.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-768x1540.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-766x1536.png 766w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-120x241.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura.png 858w\" sizes=\"(max-width: 250px) 100vw, 250px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eTo wrap it up\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe 2021 Wrapped Audio Aura story is a peek into the unique experience of engineering and design. We took an esoteric concept and broke it down into its basic elements to build an exciting feature for the Wrapped experience. Our ability to work together with teams across Spotify allows us to find creative solutions, enabling us to create stories that, hopefully, delight our users. If you’re interested in joining our efforts to bring Spotify listeners new experiences, check out our \u003ca href=\"https://www.lifeatspotify.com/jobs?c=engineering\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen roles\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR For 2021 Wrapped, we were challenged to visually express a user’s Audio Aura based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like?",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png",
      "date_published": "2021-12-17T00:00:00Z",
      "author": {
        "name": "Published by Zela Taino, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/12/17/the-audio-aura-story-mystical-to-mathematical/",
      "title": "\n                                            The Audio Aura Story: Mystical to Mathematical\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 17, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/12/17/the-audio-aura-story-mystical-to-mathematical/\" title=\"The Audio Aura Story: Mystical to Mathematical\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-768x380.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eFor 2021 Wrapped, we were challenged to visually express a user’s \u003ca href=\"https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAudio Aura\u003c/a\u003e based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like? That’s the 2021 Wrapped Audio Aura story. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eA new Wrapped experience\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpotify Wrapped is a company-wide effort with 300+ Spotifiers across 20+ teams. My team in particular is a design/engineering team and our main focus is to design, build, and launch the Wrapped personalized user experience to millions of Spotify users around the world. These personalized stories bring users’ annual listening habits to life through creative storytelling. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe personalized experience shows listeners’ data stories such as their top songs, top artists, and top genres of the year. In addition to building these stories, our Brand and Creative team thinks outside the box and ideates new ways to illuminate insights into a user’s audio streams. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eImagine.\u003c/em\u003e A meeting room full of professionals — a virtual meeting room, of course. Brows furrowed, pens clicking, and fingers scratching heads. We were brainstorming the answer to the question: how do we \u003cem\u003edefine\u003c/em\u003e a Spotify user’s Audio Aura? To help with this question and guarantee the auras we create are valid, we consulted an aura reader, \u003ca href=\"https://www.mysticmichaela.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMystic Michaela\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Mystic Michaela, auras are “your personal energy signature. Everyone has one, and aura readers see them as a combination of colors, each representative of the traits that make you, you” (check out Michaela’s full take on auras \u003ca href=\"https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e). For the purposes of our Audio Aura story, we can extend the definition to say that a 2021 Spotify Wrapped Audio Aura is a colorful energy made up of two colors that help our listeners understand the vibe or mood of the music they streamed this year.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eLet’s get to the whiteboard \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003ch5\u003e\u003cstrong\u003eChallenge #1\u003c/strong\u003e \u003c/h5\u003e\n\n\n\n\u003cp\u003eReturn the top two mood categories and the descriptor associated with their 2021 listening history for each user.\u003c/p\u003e\n\n\n\n\u003ch5\u003e\u003cstrong\u003eChallenge #2\u003c/strong\u003e\u003c/h5\u003e\n\n\n\n\u003cp\u003eIdentify how much of a user’s music is represented by each mood aka the mood weight.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo address the above challenges, we utilized a track mood descriptor dataset, aggregated each user’s listening history, and created six broad mood categories (i.e. “Happy”, “Calm”, “Hopeful”) that would then be narrowed down to two to create the audio aura.\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eCount the number of streams for a given track and retrieve its top mood descriptor as determined by the mood descriptor dataset.\u003c/li\u003e\u003cli\u003eBucket those moods into one of the six mood categories.\u003c/li\u003e\u003cli\u003ePerform an aggregation to find the total number of streams for each mood category and take the mood descriptor with the highest number of streams.\u003c/li\u003e\u003cli\u003eCarry out one last calculation to find the percentage of streams of a mood over the total number of streams.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"505\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-250x180.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-768x554.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-1536x1107.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-120x87.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn the example above, tracks that are “happy” or “calm” are the most streamed, making them the top two moods found in a specific user’s music. “happy” makes up 47% of the user’s listening with the mood descriptor being “blissful”. “calm” makes up 29% of the user’s listening with the granular descriptor being “chill”.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eNow, let’s turn these moods into colors\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eDry-erase pen caps click… we’ve identified a user’s top two Audio Aura moods and their mood weights. Now let’s turn this data into colors. Mystic Michaela stepped in to lend her expertise on auras and their colors. With her guidance, we assigned six core colors to the six mood descriptor categories mentioned above (which include “happy”, “calm”, and “hopeful”). To provide secondary colors to the aura visual, our designer represents mood weights through varying levels of contrast. The more a mood is present in a user’s listening, the darker the color. The less a mood is present in a user’s listening, the lighter the color. In the graphic above, the user has a mood weight of 0.47; therefore, the secondary color would be found in the row labeled Encore 100. Their second mood weight was 0.29; therefore, the secondary color would be found in the row labeled Encore 180.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eCreativity and embracing constraints\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we have a user’s Audio Aura colors, all that’s left is for design and engineering to consider the last question: how can we engineer an ethereal Aura visual?\u003c/p\u003e\n\n\n\n\u003cp\u003eOur deadline was two weeks away. In our milestones planning, we allocated a few days of engineering per story since we committed to a number of data stories and several new features. We had a few days to explore an elegant aura visual that was feasible on both iOS and Android. Part of the challenge for design is knowing what’s possible for engineering to execute in a dedicated amount of time. And part of the challenge for engineering is accurately estimating the time it will take to build a given design. We had to come up with a solution that made good use of our time and garnered a confident engineering sign-off. Situations like this are very unique to the Wrapped design/engineering experience and through such situations, I’ve learned that creativity comes from embracing constraints and making use of what we have in novel ways. Our designer made use of three elements: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAn animated gradient built for last year’s 2020 Wrapped campaign to create a “lava lamp” effect.\u003c/li\u003e\u003cli\u003eRibbons built for 2021 Wrapped to create organic and unique shapes.\u003c/li\u003e\u003cli\u003eA blur effect to blend everything together in a dreamlike fashion.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSince we had already built the animated gradient and ribbons, we were able to build the aura visual with high confidence, on time, and to accurately depict an ethereal aura!\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"250\" height=\"501\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-700x1403.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-768x1540.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-766x1536.png 766w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-120x241.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura.png 858w\" sizes=\"(max-width: 250px) 100vw, 250px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eTo wrap it up\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe 2021 Wrapped Audio Aura story is a peek into the unique experience of engineering and design. We took an esoteric concept and broke it down into its basic elements to build an exciting feature for the Wrapped experience. Our ability to work together with teams across Spotify allows us to find creative solutions, enabling us to create stories that, hopefully, delight our users. If you’re interested in joining our efforts to bring Spotify listeners new experiences, check out our \u003ca href=\"https://www.lifeatspotify.com/jobs?c=engineering\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen roles\u003c/a\u003e!\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR For 2021 Wrapped, we were challenged to visually express a user’s Audio Aura based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like?",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png",
      "date_published": "2021-12-17T00:00:00Z",
      "author": {
        "name": "Published by Zela Taino, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/12/the-audio-aura-story-mystical-to-mathematical/",
      "title": "\n                                            The Audio Aura Story: Mystical to Mathematical\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 17, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/12/the-audio-aura-story-mystical-to-mathematical/\" title=\"The Audio Aura Story: Mystical to Mathematical\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-768x380.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eFor 2021 Wrapped, we were challenged to visually express a user’s \u003ca href=\"https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAudio Aura\u003c/a\u003e based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like? That’s the 2021 Wrapped Audio Aura story. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eA new Wrapped experience\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpotify Wrapped is a company-wide effort with 300+ Spotifiers across 20+ teams. My team in particular is a design/engineering team and our main focus is to design, build, and launch the Wrapped personalized user experience to millions of Spotify users around the world. These personalized stories bring users’ annual listening habits to life through creative storytelling. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe personalized experience shows listeners’ data stories such as their top songs, top artists, and top genres of the year. In addition to building these stories, our Brand and Creative team thinks outside the box and ideates new ways to illuminate insights into a user’s audio streams. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eImagine.\u003c/em\u003e A meeting room full of professionals — a virtual meeting room, of course. Brows furrowed, pens clicking, and fingers scratching heads. We were brainstorming the answer to the question: how do we \u003cem\u003edefine\u003c/em\u003e a Spotify user’s Audio Aura? To help with this question and guarantee the auras we create are valid, we consulted an aura reader, \u003ca href=\"https://www.mysticmichaela.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMystic Michaela\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Mystic Michaela, auras are “your personal energy signature. Everyone has one, and aura readers see them as a combination of colors, each representative of the traits that make you, you” (check out Michaela’s full take on auras \u003ca href=\"https://newsroom.spotify.com/2021-12-01/learn-more-about-the-audio-aura-in-your-spotify-2021-wrapped-with-aura-reader-mystic-michaela/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e). For the purposes of our Audio Aura story, we can extend the definition to say that a 2021 Spotify Wrapped Audio Aura is a colorful energy made up of two colors that help our listeners understand the vibe or mood of the music they streamed this year.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eLet’s get to the whiteboard \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003ch5\u003e\u003cstrong\u003eChallenge #1\u003c/strong\u003e \u003c/h5\u003e\n\n\n\n\u003cp\u003eReturn the top two mood categories and the descriptor associated with their 2021 listening history for each user.\u003c/p\u003e\n\n\n\n\u003ch5\u003e\u003cstrong\u003eChallenge #2\u003c/strong\u003e\u003c/h5\u003e\n\n\n\n\u003cp\u003eIdentify how much of a user’s music is represented by each mood aka the mood weight.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo address the above challenges, we utilized a track mood descriptor dataset, aggregated each user’s listening history, and created six broad mood categories (i.e. “Happy”, “Calm”, “Hopeful”) that would then be narrowed down to two to create the audio aura.\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eCount the number of streams for a given track and retrieve its top mood descriptor as determined by the mood descriptor dataset.\u003c/li\u003e\u003cli\u003eBucket those moods into one of the six mood categories.\u003c/li\u003e\u003cli\u003ePerform an aggregation to find the total number of streams for each mood category and take the mood descriptor with the highest number of streams.\u003c/li\u003e\u003cli\u003eCarry out one last calculation to find the percentage of streams of a mood over the total number of streams.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"505\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-700x505.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-250x180.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-768x554.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-1536x1107.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight-120x87.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weight.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn the example above, tracks that are “happy” or “calm” are the most streamed, making them the top two moods found in a specific user’s music. “happy” makes up 47% of the user’s listening with the mood descriptor being “blissful”. “calm” makes up 29% of the user’s listening with the granular descriptor being “chill”.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Mood-Weightdescriptor.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eNow, let’s turn these moods into colors\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eDry-erase pen caps click… we’ve identified a user’s top two Audio Aura moods and their mood weights. Now let’s turn this data into colors. Mystic Michaela stepped in to lend her expertise on auras and their colors. With her guidance, we assigned six core colors to the six mood descriptor categories mentioned above (which include “happy”, “calm”, and “hopeful”). To provide secondary colors to the aura visual, our designer represents mood weights through varying levels of contrast. The more a mood is present in a user’s listening, the darker the color. The less a mood is present in a user’s listening, the lighter the color. In the graphic above, the user has a mood weight of 0.47; therefore, the secondary color would be found in the row labeled Encore 100. Their second mood weight was 0.29; therefore, the secondary color would be found in the row labeled Encore 180.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Core-Colors.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eCreativity and embracing constraints\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we have a user’s Audio Aura colors, all that’s left is for design and engineering to consider the last question: how can we engineer an ethereal Aura visual?\u003c/p\u003e\n\n\n\n\u003cp\u003eOur deadline was two weeks away. In our milestones planning, we allocated a few days of engineering per story since we committed to a number of data stories and several new features. We had a few days to explore an elegant aura visual that was feasible on both iOS and Android. Part of the challenge for design is knowing what’s possible for engineering to execute in a dedicated amount of time. And part of the challenge for engineering is accurately estimating the time it will take to build a given design. We had to come up with a solution that made good use of our time and garnered a confident engineering sign-off. Situations like this are very unique to the Wrapped design/engineering experience and through such situations, I’ve learned that creativity comes from embracing constraints and making use of what we have in novel ways. Our designer made use of three elements: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAn animated gradient built for last year’s 2020 Wrapped campaign to create a “lava lamp” effect.\u003c/li\u003e\u003cli\u003eRibbons built for 2021 Wrapped to create organic and unique shapes.\u003c/li\u003e\u003cli\u003eA blur effect to blend everything together in a dreamlike fashion.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Gradient-ribbons-blur.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSince we had already built the animated gradient and ribbons, we were able to build the aura visual with high confidence, on time, and to accurately depict an ethereal aura!\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"250\" height=\"501\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-250x501.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-700x1403.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-768x1540.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-766x1536.png 766w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura-120x241.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Audio-Aura.png 858w\" sizes=\"(max-width: 250px) 100vw, 250px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eTo wrap it up\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe 2021 Wrapped Audio Aura story is a peek into the unique experience of engineering and design. We took an esoteric concept and broke it down into its basic elements to build an exciting feature for the Wrapped experience. Our ability to work together with teams across Spotify allows us to find creative solutions, enabling us to create stories that, hopefully, delight our users. If you’re interested in joining our efforts to bring Spotify listeners new experiences, check out our \u003ca href=\"https://www.lifeatspotify.com/jobs?c=engineering\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eopen roles\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR For 2021 Wrapped, we were challenged to visually express a user’s Audio Aura based on how they listened this year. I like to think of it like this: if your music listening data became a person and walked down the street to the neighborhood aura reader, what would that person’s aura look like?",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/image3.png",
      "date_published": "2021-12-17T00:00:00Z",
      "author": {
        "name": "Published by Zela Taino, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/12/a-look-behind-blend-the-personalized-playlist-for-youand-you/",
      "title": "\n                                            A Look Behind Blend: The Personalized Playlist for You…and You\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/12/a-look-behind-blend-the-personalized-playlist-for-youand-you/\" title=\"A Look Behind Blend: The Personalized Playlist for You…and You\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png 2097w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-2048x1010.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-120x59.png 120w\" sizes=\"(max-width: 2097px) 100vw, 2097px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe recently launched a new playlist initiative, \u003ca href=\"https://newsroom.spotify.com/2021-08-31/how-spotifys-newest-personalized-experience-blend-creates-a-playlist-for-you-and-your-bestie/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBlend\u003c/a\u003e, where any user can invite any user to generate a playlist wherein the two users’ tastes are combined into one shared playlist. Prior to Blend, the team worked on similar products, Family Mix and Duo Mix. These products create shared playlists for users on the same Family or Duo plan. The products were well received, so we decided to expand this product line, creating a version of opt-in, automatic, shared, personalized playlists that could work for any two users. \u003c/p\u003e\n\n\n\n\u003cp\u003eAnytime we want to make a new playlist at Spotify, we’re aiming to do something different that we haven’t been able to accomplish before. This means we can’t always lean on our past experiences, and often encounter new challenges that require new solutions. With Blend in particular, we were taking concepts from Family Mix and Duo Mix, and expanding them to a much larger user group. A major complication we saw here was the increase of scale in the number of users we had to deal with. We dealt with unique challenges both in the content creation process, and in the invitation flow, to create a Blend.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost playlists are composed of a number of attributes and characteristics. For example, with Discover Weekly, our main attribute is discovery. For Daily Mix, our attributes are familiarity and coherency. When we are working with multiple users, however, we have the challenge of taking more attributes into account. Is the playlist:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRelevant:\u003c/strong\u003e Does the track we’re selecting for that user reflect their taste? Or is it just a song they accidentally listened to once?\u003cul\u003e\u003cli\u003eThis is especially important for track attribution — if we put a user’s profile image next to a song, we need to make sure that this specific user would agree the song listed is representative of their taste.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCoherent: \u003c/strong\u003eDoes the playlist have flow, or do the tracks feel completely random and unrelated to each other?\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEqual:\u003c/strong\u003e Are both users in the Blend represented equally?\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDemocratic:\u003c/strong\u003e Does music that both users like rise to the top?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eOne of the core decisions we made for this product was whether it was better to “minimize the misery” or “maximize the joy”. In other words, is it better to pick everyone’s favorite tracks, even if other people in the group wouldn’t like them, or is it better to pick the tracks that everyone is likely to like, even if their favorite songs never get selected? “Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"477\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-1536x1047.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-2048x1397.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-120x82.png 120w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIt’s a bit simpler to create a Blend with users with similar taste since they listen to a lot of the same music. However, if we have two users with no common music listening history, it’s significantly more difficult to create a perfect Blend. We needed an approach that worked for both types of pairs, while also taking into consideration how any changes to the Blend algorithm impacts all combinations of users.\u003c/p\u003e\n\n\n\n\u003cp\u003eBetween fetching data for both users in the Blend, and trying to come up with the ideal sequence balancing for all of our attributes, creating a Blend is a pretty heavy process. When we tried to come up with the best algorithm, we weren’t so concerned about our latency. Once we were happy with Blend quality, and started to think about scaling the service, we realized how bad our latency had gotten while iterating on the algorithm. We spent a lot of time trying to make the service as fast as possible. What we learned is that our code base had some hot spots in it: some sections of the code were run over 50 times per Blend generation, while other sections of the code were only run once. If we tried to optimize sections of the code that weren’t run many times, we didn’t make much of an impact in our latency. However, when we made improvements to our hot spots, we were able to make a huge difference. The biggest example here was swapping the order of two function calls within an if statement, taking advantage of Java’s short circuiting. This simple code change reduced our latency to 1/10 of its original time.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were able to make content quality improvements by using both qualitative and quantitative methods. While we normally rely on testing our own playlists when we make changes, we also needed to make sure that we checked several different types of Blends (for example: test a high taste overlap Blend and a low taste overlap Blend). We created some offline metrics to measure how our attributes performed. We also work closely with a Data Curation team, often referred to as the “humans in the loop”. The Data Curation team evaluates and ensures content quality for recommendation systems. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, when the team wanted to make a change to make the playlist more coherent, we:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eTested our own playlists — the team had implemented the change for about a month before we evaluated it. During this time, we were able to get a good feel for whether we preferred the change or not.\u003c/li\u003e\u003cli\u003ePerformed a heuristic review, where our Data Curation team reviewed a number of Blends with a variety of taste overlap scores. \u003cul\u003e\u003cli\u003eThis process helps identify issues with usability and comprehensibility associated most closely with content quality and with the user experience.\u003c/li\u003e\u003cli\u003eUtilize a tool called a “Content Recommendation Scorecard”.\u003cul\u003e\u003cli\u003eScore each track over a number of attributes such as relevance and coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eFrom the Content Recommendation Scorecard, we were able to see that the new approach more strongly met our criteria in terms of the attributes we wanted to optimize for.\u003c/li\u003e\u003cli\u003eThe review built enough confidence for the team to roll out the new approach to all users.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eCreating a social playlist presented a new set of challenges in creating a new playlist algorithm. We had to try to optimize for many attributes: relevance, coherence, equality, and democratic decisions. We also had to consider both high taste overlap users and users who don’t have much taste overlap. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile building out the Blend product, we wanted a way to communicate information to the users about what similarities and differences they have in their music taste. This led to us building out Blend Data Stories, where we can show the users’ information like the artist that brings them together and their taste match score. This year, during the Wrapped Campaign, we gave users a \u003ca href=\"https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWrapped Blend\u003c/a\u003e experience. We modified the Blend Data Stories to use data from Wrapped, to show users information like their top mutual artists and top mutual genre of the year.\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe’re still working hard to improve Blend, and build a product that allows our users to feel closer through music, while thinking of more fun ways to grow the social experience in Spotify. If this type of work sounds interesting, our Personalization team is \u003ca href=\"https://www.lifeatspotify.com/jobs?q=personalization\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehiring\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/data-modeling/\" rel=\"tag\"\u003edata modeling\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. We recently launched a",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png",
      "date_published": "2021-12-07T00:00:00Z",
      "author": {
        "name": "Published by Jen Lamere, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/12/07/a-look-behind-blend-the-personalized-playlist-for-youand-you/",
      "title": "\n                                            A Look Behind Blend: The Personalized Playlist for You…and You\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/12/07/a-look-behind-blend-the-personalized-playlist-for-youand-you/\" title=\"A Look Behind Blend: The Personalized Playlist for You…and You\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png 2097w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-2048x1010.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-120x59.png 120w\" sizes=\"(max-width: 2097px) 100vw, 2097px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe recently launched a new playlist initiative, \u003ca href=\"https://newsroom.spotify.com/2021-08-31/how-spotifys-newest-personalized-experience-blend-creates-a-playlist-for-you-and-your-bestie/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBlend\u003c/a\u003e, where any user can invite any user to generate a playlist wherein the two users’ tastes are combined into one shared playlist. Prior to Blend, the team worked on similar products, Family Mix and Duo Mix. These products create shared playlists for users on the same Family or Duo plan. The products were well received, so we decided to expand this product line, creating a version of opt-in, automatic, shared, personalized playlists that could work for any two users. \u003c/p\u003e\n\n\n\n\u003cp\u003eAnytime we want to make a new playlist at Spotify, we’re aiming to do something different that we haven’t been able to accomplish before. This means we can’t always lean on our past experiences, and often encounter new challenges that require new solutions. With Blend in particular, we were taking concepts from Family Mix and Duo Mix, and expanding them to a much larger user group. A major complication we saw here was the increase of scale in the number of users we had to deal with. We dealt with unique challenges both in the content creation process, and in the invitation flow, to create a Blend.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost playlists are composed of a number of attributes and characteristics. For example, with Discover Weekly, our main attribute is discovery. For Daily Mix, our attributes are familiarity and coherency. When we are working with multiple users, however, we have the challenge of taking more attributes into account. Is the playlist:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRelevant:\u003c/strong\u003e Does the track we’re selecting for that user reflect their taste? Or is it just a song they accidentally listened to once?\u003cul\u003e\u003cli\u003eThis is especially important for track attribution — if we put a user’s profile image next to a song, we need to make sure that this specific user would agree the song listed is representative of their taste.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCoherent: \u003c/strong\u003eDoes the playlist have flow, or do the tracks feel completely random and unrelated to each other?\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEqual:\u003c/strong\u003e Are both users in the Blend represented equally?\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDemocratic:\u003c/strong\u003e Does music that both users like rise to the top?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eOne of the core decisions we made for this product was whether it was better to “minimize the misery” or “maximize the joy”. In other words, is it better to pick everyone’s favorite tracks, even if other people in the group wouldn’t like them, or is it better to pick the tracks that everyone is likely to like, even if their favorite songs never get selected? “Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"477\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-1536x1047.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-2048x1397.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-120x82.png 120w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIt’s a bit simpler to create a Blend with users with similar taste since they listen to a lot of the same music. However, if we have two users with no common music listening history, it’s significantly more difficult to create a perfect Blend. We needed an approach that worked for both types of pairs, while also taking into consideration how any changes to the Blend algorithm impacts all combinations of users.\u003c/p\u003e\n\n\n\n\u003cp\u003eBetween fetching data for both users in the Blend, and trying to come up with the ideal sequence balancing for all of our attributes, creating a Blend is a pretty heavy process. When we tried to come up with the best algorithm, we weren’t so concerned about our latency. Once we were happy with Blend quality, and started to think about scaling the service, we realized how bad our latency had gotten while iterating on the algorithm. We spent a lot of time trying to make the service as fast as possible. What we learned is that our code base had some hot spots in it: some sections of the code were run over 50 times per Blend generation, while other sections of the code were only run once. If we tried to optimize sections of the code that weren’t run many times, we didn’t make much of an impact in our latency. However, when we made improvements to our hot spots, we were able to make a huge difference. The biggest example here was swapping the order of two function calls within an if statement, taking advantage of Java’s short circuiting. This simple code change reduced our latency to 1/10 of its original time.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were able to make content quality improvements by using both qualitative and quantitative methods. While we normally rely on testing our own playlists when we make changes, we also needed to make sure that we checked several different types of Blends (for example: test a high taste overlap Blend and a low taste overlap Blend). We created some offline metrics to measure how our attributes performed. We also work closely with a Data Curation team, often referred to as the “humans in the loop”. The Data Curation team evaluates and ensures content quality for recommendation systems. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, when the team wanted to make a change to make the playlist more coherent, we:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eTested our own playlists — the team had implemented the change for about a month before we evaluated it. During this time, we were able to get a good feel for whether we preferred the change or not.\u003c/li\u003e\u003cli\u003ePerformed a heuristic review, where our Data Curation team reviewed a number of Blends with a variety of taste overlap scores. \u003cul\u003e\u003cli\u003eThis process helps identify issues with usability and comprehensibility associated most closely with content quality and with the user experience.\u003c/li\u003e\u003cli\u003eUtilize a tool called a “Content Recommendation Scorecard”.\u003cul\u003e\u003cli\u003eScore each track over a number of attributes such as relevance and coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eFrom the Content Recommendation Scorecard, we were able to see that the new approach more strongly met our criteria in terms of the attributes we wanted to optimize for.\u003c/li\u003e\u003cli\u003eThe review built enough confidence for the team to roll out the new approach to all users.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eCreating a social playlist presented a new set of challenges in creating a new playlist algorithm. We had to try to optimize for many attributes: relevance, coherence, equality, and democratic decisions. We also had to consider both high taste overlap users and users who don’t have much taste overlap. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile building out the Blend product, we wanted a way to communicate information to the users about what similarities and differences they have in their music taste. This led to us building out Blend Data Stories, where we can show the users’ information like the artist that brings them together and their taste match score. This year, during the Wrapped Campaign, we gave users a \u003ca href=\"https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWrapped Blend\u003c/a\u003e experience. We modified the Blend Data Stories to use data from Wrapped, to show users information like their top mutual artists and top mutual genre of the year.\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe’re still working hard to improve Blend, and build a product that allows our users to feel closer through music, while thinking of more fun ways to grow the social experience in Spotify. If this type of work sounds interesting, our Personalization team is \u003ca href=\"https://www.lifeatspotify.com/jobs?q=personalization\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehiring\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/data-modeling/\" rel=\"tag\"\u003edata modeling\u003c/a\u003e\u003c/p\u003e\n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. We recently launched a",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png",
      "date_published": "2021-12-07T00:00:00Z",
      "author": {
        "name": "Published by Jen Lamere, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/a-look-behind-blend-the-personalized-playlist-for-youand-you/",
      "title": "\n                                            A Look Behind Blend: The Personalized Playlist for You…and You\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/a-look-behind-blend-the-personalized-playlist-for-youand-you/\" title=\"A Look Behind Blend: The Personalized Playlist for You…and You\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png 2097w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-2048x1010.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-120x59.png 120w\" sizes=\"(max-width: 2097px) 100vw, 2097px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe recently launched a new playlist initiative, \u003ca href=\"https://newsroom.spotify.com/2021-08-31/how-spotifys-newest-personalized-experience-blend-creates-a-playlist-for-you-and-your-bestie/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBlend\u003c/a\u003e, where any user can invite any user to generate a playlist wherein the two users’ tastes are combined into one shared playlist. Prior to Blend, the team worked on similar products, Family Mix and Duo Mix. These products create shared playlists for users on the same Family or Duo plan. The products were well received, so we decided to expand this product line, creating a version of opt-in, automatic, shared, personalized playlists that could work for any two users. \u003c/p\u003e\n\n\n\n\u003cp\u003eAnytime we want to make a new playlist at Spotify, we’re aiming to do something different that we haven’t been able to accomplish before. This means we can’t always lean on our past experiences, and often encounter new challenges that require new solutions. With Blend in particular, we were taking concepts from Family Mix and Duo Mix, and expanding them to a much larger user group. A major complication we saw here was the increase of scale in the number of users we had to deal with. We dealt with unique challenges both in the content creation process, and in the invitation flow, to create a Blend.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost playlists are composed of a number of attributes and characteristics. For example, with Discover Weekly, our main attribute is discovery. For Daily Mix, our attributes are familiarity and coherency. When we are working with multiple users, however, we have the challenge of taking more attributes into account. Is the playlist:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRelevant:\u003c/strong\u003e Does the track we’re selecting for that user reflect their taste? Or is it just a song they accidentally listened to once?\u003cul\u003e\u003cli\u003eThis is especially important for track attribution — if we put a user’s profile image next to a song, we need to make sure that this specific user would agree the song listed is representative of their taste.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCoherent: \u003c/strong\u003eDoes the playlist have flow, or do the tracks feel completely random and unrelated to each other?\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEqual:\u003c/strong\u003e Are both users in the Blend represented equally?\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDemocratic:\u003c/strong\u003e Does music that both users like rise to the top?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eOne of the core decisions we made for this product was whether it was better to “minimize the misery” or “maximize the joy”. In other words, is it better to pick everyone’s favorite tracks, even if other people in the group wouldn’t like them, or is it better to pick the tracks that everyone is likely to like, even if their favorite songs never get selected? “Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"477\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-1536x1047.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-2048x1397.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-120x82.png 120w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIt’s a bit simpler to create a Blend with users with similar taste since they listen to a lot of the same music. However, if we have two users with no common music listening history, it’s significantly more difficult to create a perfect Blend. We needed an approach that worked for both types of pairs, while also taking into consideration how any changes to the Blend algorithm impacts all combinations of users.\u003c/p\u003e\n\n\n\n\u003cp\u003eBetween fetching data for both users in the Blend, and trying to come up with the ideal sequence balancing for all of our attributes, creating a Blend is a pretty heavy process. When we tried to come up with the best algorithm, we weren’t so concerned about our latency. Once we were happy with Blend quality, and started to think about scaling the service, we realized how bad our latency had gotten while iterating on the algorithm. We spent a lot of time trying to make the service as fast as possible. What we learned is that our code base had some hot spots in it: some sections of the code were run over 50 times per Blend generation, while other sections of the code were only run once. If we tried to optimize sections of the code that weren’t run many times, we didn’t make much of an impact in our latency. However, when we made improvements to our hot spots, we were able to make a huge difference. The biggest example here was swapping the order of two function calls within an if statement, taking advantage of Java’s short circuiting. This simple code change reduced our latency to 1/10 of its original time.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were able to make content quality improvements by using both qualitative and quantitative methods. While we normally rely on testing our own playlists when we make changes, we also needed to make sure that we checked several different types of Blends (for example: test a high taste overlap Blend and a low taste overlap Blend). We created some offline metrics to measure how our attributes performed. We also work closely with a Data Curation team, often referred to as the “humans in the loop”. The Data Curation team evaluates and ensures content quality for recommendation systems. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, when the team wanted to make a change to make the playlist more coherent, we:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eTested our own playlists — the team had implemented the change for about a month before we evaluated it. During this time, we were able to get a good feel for whether we preferred the change or not.\u003c/li\u003e\u003cli\u003ePerformed a heuristic review, where our Data Curation team reviewed a number of Blends with a variety of taste overlap scores. \u003cul\u003e\u003cli\u003eThis process helps identify issues with usability and comprehensibility associated most closely with content quality and with the user experience.\u003c/li\u003e\u003cli\u003eUtilize a tool called a “Content Recommendation Scorecard”.\u003cul\u003e\u003cli\u003eScore each track over a number of attributes such as relevance and coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eFrom the Content Recommendation Scorecard, we were able to see that the new approach more strongly met our criteria in terms of the attributes we wanted to optimize for.\u003c/li\u003e\u003cli\u003eThe review built enough confidence for the team to roll out the new approach to all users.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eCreating a social playlist presented a new set of challenges in creating a new playlist algorithm. We had to try to optimize for many attributes: relevance, coherence, equality, and democratic decisions. We also had to consider both high taste overlap users and users who don’t have much taste overlap. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile building out the Blend product, we wanted a way to communicate information to the users about what similarities and differences they have in their music taste. This led to us building out Blend Data Stories, where we can show the users’ information like the artist that brings them together and their taste match score. This year, during the Wrapped Campaign, we gave users a \u003ca href=\"https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWrapped Blend\u003c/a\u003e experience. We modified the Blend Data Stories to use data from Wrapped, to show users information like their top mutual artists and top mutual genre of the year.\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe’re still working hard to improve Blend, and build a product that allows our users to feel closer through music, while thinking of more fun ways to grow the social experience in Spotify. If this type of work sounds interesting, our Personalization team is \u003ca href=\"https://www.lifeatspotify.com/jobs?q=personalization\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehiring\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/data-modeling/\" rel=\"tag\"\u003edata modeling\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. We recently launched a",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png",
      "date_published": "2021-12-07T00:00:00Z",
      "author": {
        "name": "Published by Jen Lamere, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/a-look-behind-blend-the-personalized-playlist-for-youand-you/",
      "title": "\n                                            A Look Behind Blend: The Personalized Playlist for You…and You\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/a-look-behind-blend-the-personalized-playlist-for-youand-you/\" title=\"A Look Behind Blend: The Personalized Playlist for You…and You\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png 2097w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-2048x1010.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header-120x59.png 120w\" sizes=\"(max-width: 2097px) 100vw, 2097px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe recently launched a new playlist initiative, \u003ca href=\"https://newsroom.spotify.com/2021-08-31/how-spotifys-newest-personalized-experience-blend-creates-a-playlist-for-you-and-your-bestie/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBlend\u003c/a\u003e, where any user can invite any user to generate a playlist wherein the two users’ tastes are combined into one shared playlist. Prior to Blend, the team worked on similar products, Family Mix and Duo Mix. These products create shared playlists for users on the same Family or Duo plan. The products were well received, so we decided to expand this product line, creating a version of opt-in, automatic, shared, personalized playlists that could work for any two users. \u003c/p\u003e\n\n\n\n\u003cp\u003eAnytime we want to make a new playlist at Spotify, we’re aiming to do something different that we haven’t been able to accomplish before. This means we can’t always lean on our past experiences, and often encounter new challenges that require new solutions. With Blend in particular, we were taking concepts from Family Mix and Duo Mix, and expanding them to a much larger user group. A major complication we saw here was the increase of scale in the number of users we had to deal with. We dealt with unique challenges both in the content creation process, and in the invitation flow, to create a Blend.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost playlists are composed of a number of attributes and characteristics. For example, with Discover Weekly, our main attribute is discovery. For Daily Mix, our attributes are familiarity and coherency. When we are working with multiple users, however, we have the challenge of taking more attributes into account. Is the playlist:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRelevant:\u003c/strong\u003e Does the track we’re selecting for that user reflect their taste? Or is it just a song they accidentally listened to once?\u003cul\u003e\u003cli\u003eThis is especially important for track attribution — if we put a user’s profile image next to a song, we need to make sure that this specific user would agree the song listed is representative of their taste.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCoherent: \u003c/strong\u003eDoes the playlist have flow, or do the tracks feel completely random and unrelated to each other?\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEqual:\u003c/strong\u003e Are both users in the Blend represented equally?\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDemocratic:\u003c/strong\u003e Does music that both users like rise to the top?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eOne of the core decisions we made for this product was whether it was better to “minimize the misery” or “maximize the joy”. In other words, is it better to pick everyone’s favorite tracks, even if other people in the group wouldn’t like them, or is it better to pick the tracks that everyone is likely to like, even if their favorite songs never get selected? “Minimize the misery” is valuing democratic and coherent attributes over relevance. “Maximize the joy” values relevance over democratic and coherent attributes. Our solution is more about maximizing the joy, where we try to select the songs that are most personally relevant to a user. This decision was made based on feedback from employees and our data curation team.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"477\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-700x477.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-250x170.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-768x524.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-1536x1047.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-2048x1397.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Mix-120x82.png 120w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIt’s a bit simpler to create a Blend with users with similar taste since they listen to a lot of the same music. However, if we have two users with no common music listening history, it’s significantly more difficult to create a perfect Blend. We needed an approach that worked for both types of pairs, while also taking into consideration how any changes to the Blend algorithm impacts all combinations of users.\u003c/p\u003e\n\n\n\n\u003cp\u003eBetween fetching data for both users in the Blend, and trying to come up with the ideal sequence balancing for all of our attributes, creating a Blend is a pretty heavy process. When we tried to come up with the best algorithm, we weren’t so concerned about our latency. Once we were happy with Blend quality, and started to think about scaling the service, we realized how bad our latency had gotten while iterating on the algorithm. We spent a lot of time trying to make the service as fast as possible. What we learned is that our code base had some hot spots in it: some sections of the code were run over 50 times per Blend generation, while other sections of the code were only run once. If we tried to optimize sections of the code that weren’t run many times, we didn’t make much of an impact in our latency. However, when we made improvements to our hot spots, we were able to make a huge difference. The biggest example here was swapping the order of two function calls within an if statement, taking advantage of Java’s short circuiting. This simple code change reduced our latency to 1/10 of its original time.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were able to make content quality improvements by using both qualitative and quantitative methods. While we normally rely on testing our own playlists when we make changes, we also needed to make sure that we checked several different types of Blends (for example: test a high taste overlap Blend and a low taste overlap Blend). We created some offline metrics to measure how our attributes performed. We also work closely with a Data Curation team, often referred to as the “humans in the loop”. The Data Curation team evaluates and ensures content quality for recommendation systems. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, when the team wanted to make a change to make the playlist more coherent, we:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eTested our own playlists — the team had implemented the change for about a month before we evaluated it. During this time, we were able to get a good feel for whether we preferred the change or not.\u003c/li\u003e\u003cli\u003ePerformed a heuristic review, where our Data Curation team reviewed a number of Blends with a variety of taste overlap scores. \u003cul\u003e\u003cli\u003eThis process helps identify issues with usability and comprehensibility associated most closely with content quality and with the user experience.\u003c/li\u003e\u003cli\u003eUtilize a tool called a “Content Recommendation Scorecard”.\u003cul\u003e\u003cli\u003eScore each track over a number of attributes such as relevance and coherence.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eFrom the Content Recommendation Scorecard, we were able to see that the new approach more strongly met our criteria in terms of the attributes we wanted to optimize for.\u003c/li\u003e\u003cli\u003eThe review built enough confidence for the team to roll out the new approach to all users.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eCreating a social playlist presented a new set of challenges in creating a new playlist algorithm. We had to try to optimize for many attributes: relevance, coherence, equality, and democratic decisions. We also had to consider both high taste overlap users and users who don’t have much taste overlap. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile building out the Blend product, we wanted a way to communicate information to the users about what similarities and differences they have in their music taste. This led to us building out Blend Data Stories, where we can show the users’ information like the artist that brings them together and their taste match score. This year, during the Wrapped Campaign, we gave users a \u003ca href=\"https://newsroom.spotify.com/2021-12-01/the-wait-is-over-your-spotify-2021-wrapped-is-here/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWrapped Blend\u003c/a\u003e experience. We modified the Blend Data Stories to use data from Wrapped, to show users information like their top mutual artists and top mutual genre of the year.\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe’re still working hard to improve Blend, and build a product that allows our users to feel closer through music, while thinking of more fun ways to grow the social experience in Spotify. If this type of work sounds interesting, our Personalization team is \u003ca href=\"https://www.lifeatspotify.com/jobs?q=personalization\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehiring\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/data-modeling/\" rel=\"tag\"\u003edata modeling\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "What does it take to go from an idea for a new playlist, to shipping that playlist to Spotify users all around the world? From inception, to prototyping, to QAing, and finally shipping, releasing a new playlist at Spotify is a long process full of new learnings every time. We recently launched a",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Blend_Header.png",
      "date_published": "2021-12-07T00:00:00Z",
      "author": {
        "name": "Published by Jen Lamere, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/how-spotify-uses-ml-to-create-the-future-of-personalization/",
      "title": "\n                                            How Spotify Uses ML to Create the Future of Personalization\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4904\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 2, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/how-spotify-uses-ml-to-create-the-future-of-personalization/\" title=\"How Spotify Uses ML to Create the Future of Personalization\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png 1920w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-120x68.png 120w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eMachine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But with a library of over 70 million tracks to thumb through, how do our ML models actually go about making these decisions?\u003c/p\u003e\n\n\n\n\u003cp\u003eWell, Spotify’s VP of Personalization Oskar Stål recently gave a talk at TransformX, a summit for leaders in ML and AI, to discuss just that. Read on to get a glimpse of how ML and reinforcement learning help inform our music and podcast recommendations, and don’t forget to check out Oskar’s presentation \u003ca href=\"https://www.youtube.com/watch?v=n16LOyba-SE\u0026amp;list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e (or below!) to hear even more about the future of ML at Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow do we use ML? \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIt all starts with data. At the most fundamental level, all sorts of user info — playlists, listening history, interactions with Spotify’s UI, etc. — are fed into our ML models, while keeping trust and responsibility top of mind. Every day, nearly half a trillion events are processed, and the more info our models gather, the smarter they become about making associations between different artists, songs, podcasts, and playlists.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut our ML models even go beyond this, incorporating other factors in their decision-making processes. What time of day is it? Is this playlist for working out or chilling out? Are you on mobile or desktop? By incorporating several of these ML models throughout Spotify’s infrastructure, we’re able to offer increasingly intelligent, specialized recommendations that can, as Oskar puts it, “serve even the narrowest of tastes”.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe aren’t just looking for our users’ instant gratification, though. We want to provide listeners with a lifetime of great audio experiences and be with them on every step of that journey. And that brings us to what we’re working on now.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe future is reinforcement learning\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eReinforcement learning, or RL, is a type of ML model that responds to its current environment in an effort to maximize the ultimate, long-term reward, whatever that may be. In our case, that reward is our users’ long-term satisfaction with Spotify. RL isn’t about short-term solutions. It’s always playing the long game.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a general sense, our RL model tries to predict how satisfied our users are with their current experience, and attempts to nudge them toward consuming more fulfilling content in their audio diet to make them happier with the service. In other words, rather than handing users the “empty calories” of a content diet that will only satisfy them in the moment, RL aims to push them to a more sustainable, diverse, and fulfilling content diet that will last a lifetime. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis could mean playing a new dance track we think might fit a user’s current mood, or it could mean suggesting a calming, ambient piece to help them study. Predicting what a user will want 10 minutes from now, a day from now, a week from now, means creating a ton of simulations and running the RL model against those simulations to make it smarter, like a computer playing against itself in chess to get better at the game.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith ML and RL, we’re trying to create a more holistic audio experience, focused on recommendations that ensure long-term satisfaction and enjoyment. Our approach to personalization doesn’t just benefit listeners: better and more satisfying recommendations help out artists, exposing their work to a larger audience more likely to enjoy it. After all, there’s a reason there are 16 billion artist discoveries every month on our platform. And the best is yet to come.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe loading=\"lazy\" title=\"VP of Personalization Oskar Stål Talks the Future of ML at TransformX\" width=\"900\" height=\"506\" src=\"https://www.youtube.com/embed/n16LOyba-SE?list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png",
      "date_published": "2021-12-02T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/12/how-spotify-uses-ml-to-create-the-future-of-personalization/",
      "title": "\n                                            How Spotify Uses ML to Create the Future of Personalization\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4904\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 2, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/12/how-spotify-uses-ml-to-create-the-future-of-personalization/\" title=\"How Spotify Uses ML to Create the Future of Personalization\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png 1920w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-120x68.png 120w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eMachine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But with a library of over 70 million tracks to thumb through, how do our ML models actually go about making these decisions?\u003c/p\u003e\n\n\n\n\u003cp\u003eWell, Spotify’s VP of Personalization Oskar Stål recently gave a talk at TransformX, a summit for leaders in ML and AI, to discuss just that. Read on to get a glimpse of how ML and reinforcement learning help inform our music and podcast recommendations, and don’t forget to check out Oskar’s presentation \u003ca href=\"https://www.youtube.com/watch?v=n16LOyba-SE\u0026amp;list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e (or below!) to hear even more about the future of ML at Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow do we use ML? \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIt all starts with data. At the most fundamental level, all sorts of user info — playlists, listening history, interactions with Spotify’s UI, etc. — are fed into our ML models, while keeping trust and responsibility top of mind. Every day, nearly half a trillion events are processed, and the more info our models gather, the smarter they become about making associations between different artists, songs, podcasts, and playlists.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut our ML models even go beyond this, incorporating other factors in their decision-making processes. What time of day is it? Is this playlist for working out or chilling out? Are you on mobile or desktop? By incorporating several of these ML models throughout Spotify’s infrastructure, we’re able to offer increasingly intelligent, specialized recommendations that can, as Oskar puts it, “serve even the narrowest of tastes”.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe aren’t just looking for our users’ instant gratification, though. We want to provide listeners with a lifetime of great audio experiences and be with them on every step of that journey. And that brings us to what we’re working on now.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe future is reinforcement learning\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eReinforcement learning, or RL, is a type of ML model that responds to its current environment in an effort to maximize the ultimate, long-term reward, whatever that may be. In our case, that reward is our users’ long-term satisfaction with Spotify. RL isn’t about short-term solutions. It’s always playing the long game.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a general sense, our RL model tries to predict how satisfied our users are with their current experience, and attempts to nudge them toward consuming more fulfilling content in their audio diet to make them happier with the service. In other words, rather than handing users the “empty calories” of a content diet that will only satisfy them in the moment, RL aims to push them to a more sustainable, diverse, and fulfilling content diet that will last a lifetime. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis could mean playing a new dance track we think might fit a user’s current mood, or it could mean suggesting a calming, ambient piece to help them study. Predicting what a user will want 10 minutes from now, a day from now, a week from now, means creating a ton of simulations and running the RL model against those simulations to make it smarter, like a computer playing against itself in chess to get better at the game.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith ML and RL, we’re trying to create a more holistic audio experience, focused on recommendations that ensure long-term satisfaction and enjoyment. Our approach to personalization doesn’t just benefit listeners: better and more satisfying recommendations help out artists, exposing their work to a larger audience more likely to enjoy it. After all, there’s a reason there are 16 billion artist discoveries every month on our platform. And the best is yet to come.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe loading=\"lazy\" title=\"VP of Personalization Oskar Stål Talks the Future of ML at TransformX\" width=\"900\" height=\"506\" src=\"https://www.youtube.com/embed/n16LOyba-SE?list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png",
      "date_published": "2021-12-02T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/12/02/how-spotify-uses-ml-to-create-the-future-of-personalization/",
      "title": "\n                                            How Spotify Uses ML to Create the Future of Personalization\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4904\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 2, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/12/02/how-spotify-uses-ml-to-create-the-future-of-personalization/\" title=\"How Spotify Uses ML to Create the Future of Personalization\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png 1920w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-120x68.png 120w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eMachine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But with a library of over 70 million tracks to thumb through, how do our ML models actually go about making these decisions?\u003c/p\u003e\n\n\n\n\u003cp\u003eWell, Spotify’s VP of Personalization Oskar Stål recently gave a talk at TransformX, a summit for leaders in ML and AI, to discuss just that. Read on to get a glimpse of how ML and reinforcement learning help inform our music and podcast recommendations, and don’t forget to check out Oskar’s presentation \u003ca href=\"https://www.youtube.com/watch?v=n16LOyba-SE\u0026amp;list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e (or below!) to hear even more about the future of ML at Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow do we use ML? \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIt all starts with data. At the most fundamental level, all sorts of user info — playlists, listening history, interactions with Spotify’s UI, etc. — are fed into our ML models, while keeping trust and responsibility top of mind. Every day, nearly half a trillion events are processed, and the more info our models gather, the smarter they become about making associations between different artists, songs, podcasts, and playlists.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut our ML models even go beyond this, incorporating other factors in their decision-making processes. What time of day is it? Is this playlist for working out or chilling out? Are you on mobile or desktop? By incorporating several of these ML models throughout Spotify’s infrastructure, we’re able to offer increasingly intelligent, specialized recommendations that can, as Oskar puts it, “serve even the narrowest of tastes”.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe aren’t just looking for our users’ instant gratification, though. We want to provide listeners with a lifetime of great audio experiences and be with them on every step of that journey. And that brings us to what we’re working on now.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe future is reinforcement learning\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eReinforcement learning, or RL, is a type of ML model that responds to its current environment in an effort to maximize the ultimate, long-term reward, whatever that may be. In our case, that reward is our users’ long-term satisfaction with Spotify. RL isn’t about short-term solutions. It’s always playing the long game.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a general sense, our RL model tries to predict how satisfied our users are with their current experience, and attempts to nudge them toward consuming more fulfilling content in their audio diet to make them happier with the service. In other words, rather than handing users the “empty calories” of a content diet that will only satisfy them in the moment, RL aims to push them to a more sustainable, diverse, and fulfilling content diet that will last a lifetime. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis could mean playing a new dance track we think might fit a user’s current mood, or it could mean suggesting a calming, ambient piece to help them study. Predicting what a user will want 10 minutes from now, a day from now, a week from now, means creating a ton of simulations and running the RL model against those simulations to make it smarter, like a computer playing against itself in chess to get better at the game.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith ML and RL, we’re trying to create a more holistic audio experience, focused on recommendations that ensure long-term satisfaction and enjoyment. Our approach to personalization doesn’t just benefit listeners: better and more satisfying recommendations help out artists, exposing their work to a larger audience more likely to enjoy it. After all, there’s a reason there are 16 billion artist discoveries every month on our platform. And the best is yet to come.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe loading=\"lazy\" title=\"VP of Personalization Oskar Stål Talks the Future of ML at TransformX\" width=\"900\" height=\"506\" src=\"https://www.youtube.com/embed/n16LOyba-SE?list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003c/p\u003e\n\n        \n\n            \u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png",
      "date_published": "2021-12-02T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/how-spotify-uses-ml-to-create-the-future-of-personalization/",
      "title": "\n                                            How Spotify Uses ML to Create the Future of Personalization\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4904\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 2, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/how-spotify-uses-ml-to-create-the-future-of-personalization/\" title=\"How Spotify Uses ML to Create the Future of Personalization\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png 1920w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-768x432.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-1536x864.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header-120x68.png 120w\" sizes=\"(max-width: 1920px) 100vw, 1920px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eMachine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But with a library of over 70 million tracks to thumb through, how do our ML models actually go about making these decisions?\u003c/p\u003e\n\n\n\n\u003cp\u003eWell, Spotify’s VP of Personalization Oskar Stål recently gave a talk at TransformX, a summit for leaders in ML and AI, to discuss just that. Read on to get a glimpse of how ML and reinforcement learning help inform our music and podcast recommendations, and don’t forget to check out Oskar’s presentation \u003ca href=\"https://www.youtube.com/watch?v=n16LOyba-SE\u0026amp;list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e (or below!) to hear even more about the future of ML at Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow do we use ML? \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIt all starts with data. At the most fundamental level, all sorts of user info — playlists, listening history, interactions with Spotify’s UI, etc. — are fed into our ML models, while keeping trust and responsibility top of mind. Every day, nearly half a trillion events are processed, and the more info our models gather, the smarter they become about making associations between different artists, songs, podcasts, and playlists.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut our ML models even go beyond this, incorporating other factors in their decision-making processes. What time of day is it? Is this playlist for working out or chilling out? Are you on mobile or desktop? By incorporating several of these ML models throughout Spotify’s infrastructure, we’re able to offer increasingly intelligent, specialized recommendations that can, as Oskar puts it, “serve even the narrowest of tastes”.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe aren’t just looking for our users’ instant gratification, though. We want to provide listeners with a lifetime of great audio experiences and be with them on every step of that journey. And that brings us to what we’re working on now.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe future is reinforcement learning\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eReinforcement learning, or RL, is a type of ML model that responds to its current environment in an effort to maximize the ultimate, long-term reward, whatever that may be. In our case, that reward is our users’ long-term satisfaction with Spotify. RL isn’t about short-term solutions. It’s always playing the long game.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a general sense, our RL model tries to predict how satisfied our users are with their current experience, and attempts to nudge them toward consuming more fulfilling content in their audio diet to make them happier with the service. In other words, rather than handing users the “empty calories” of a content diet that will only satisfy them in the moment, RL aims to push them to a more sustainable, diverse, and fulfilling content diet that will last a lifetime. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis could mean playing a new dance track we think might fit a user’s current mood, or it could mean suggesting a calming, ambient piece to help them study. Predicting what a user will want 10 minutes from now, a day from now, a week from now, means creating a ton of simulations and running the RL model against those simulations to make it smarter, like a computer playing against itself in chess to get better at the game.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith ML and RL, we’re trying to create a more holistic audio experience, focused on recommendations that ensure long-term satisfaction and enjoyment. Our approach to personalization doesn’t just benefit listeners: better and more satisfying recommendations help out artists, exposing their work to a larger audience more likely to enjoy it. After all, there’s a reason there are 16 billion artist discoveries every month on our platform. And the best is yet to come.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe loading=\"lazy\" title=\"VP of Personalization Oskar Stål Talks the Future of ML at TransformX\" width=\"900\" height=\"506\" src=\"https://www.youtube.com/embed/n16LOyba-SE?list=PLf1KFlSkDLIBNfiMCsXfj_pegmiyRwrSc\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Machine learning is what drives personalization on Spotify. We may have a single platform with 381 million different users, but it may actually be more accurate to say there are 381 million individual versions of Spotify, each one filled with different Home pages, playlists, and recommendations. But",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/12/Oskar-at-TransformX_Header.png",
      "date_published": "2021-12-02T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/11/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/",
      "title": "\n                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 18, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/11/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/\" title=\"The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png 2098w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-2048x1009.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-120x59.png 120w\" sizes=\"(max-width: 2098px) 100vw, 2098px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eIn \u003ca href=\"https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThe Podcast Model:\u003c/strong\u003e Predicts podcasts a listener is likely to listen to in the \u003cem\u003eShows you might like\u003c/em\u003e shelf. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Shortcuts Model: \u003c/strong\u003ePredicts the listener’s next familiar listen in the Shortcuts feature. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Playlists Model: \u003c/strong\u003ePredicts the playlists a new listener is likely to listen to in the \u003cem\u003eTry something else\u003c/em\u003e shelf.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eIn this part of the series, we’ll highlight how and why we evaluate our models with different tools, and the hurdles to maintaining these models in production. \u003c/p\u003e\n\n\n\n\u003ch2\u003eTrust but verify your recommendations… with dashboards\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo let’s talk about what we do with that data — specifically, how we run experiments, and maybe more importantly, how we evaluate our models’ performance.\u003c/p\u003e\n\n\n\n\u003ch3\u003eMaking experimentation simpler\u003c/h3\u003e\n\n\n\n\u003ch4\u003eHow it started: experimenting on a siloed platform\u003c/h4\u003e\n\n\n\n\u003cp\u003eNot that long ago, after transforming our training data, we would run experiments on a siloed platform specifically geared towards model experimentation, and that was only really used within our team — we did this for both the initial Podcast Model as well as for the Shortcuts Model. This platform could easily launch hundreds of experiments by using a configuration file to specify hyperparameters (it also supported a grid search on specified hyperparameters). And since everything that was submitted to run an experiment was a script, it supported custom evaluation metrics — something that has always been important in our team. While it provided these necessary features, it wasn’t scalable, wasn’t maintained, and had an incomplete UI. Sometimes the compute instances would lose connection with the API (via a periodic ping) and would end up being ghost workers — still running, but not connected to anything.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eHow it’s going: integration with Spotify ML ecosystem\u003c/h4\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s managed Kubeflow clusters\u003c/a\u003e provide a more scalable approach, modular components, and are compatible with other parts of the Spotify ML infrastructure, so it was an obvious choice to move our experimentation to this platform. Training our models using Kubeflow pipelines is easy and efficient, but running the evaluation we needed and tracking those results were our biggest pain points for two reasons: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAs Spotify’s SDK for Kubeflow uses Tensorflow Model Analysis (TFMA), comparing the performance of a non-ML heuristic algorithm to that of a trained model is challenging to set up and requires extra infrastructure. \u003c/li\u003e\u003cli\u003eWe often have custom evaluation metrics that are specific to the model’s task, but they are infinitely more difficult to implement in TFMA than in vanilla Python.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch3\u003eEvaluating models against simpler (non-ML) solutions\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs I alluded to in an earlier paragraph, we don’t typically start solutions to a problem with an ML solution. We first identify a heuristic, or rule-based, solution, and the most appropriate way to evaluate it.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe first step — having a baseline for comparison\u003c/h4\u003e\n\n\n\n\u003cp\u003eWe are often tasked with creating better recommendations for content X, but what are “better recommendations,” and what are they better than? Having a baseline helps answer these questions, giving us something to compare our models against. And a good baseline — usually a heuristic/rule-based solution — is a quick, efficient, but maybe not the most optimal, solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eTake the Shortcuts Model as an example. We created an initial heuristic that recommended, simply, the most frequently played items from a listener’s short-term listening history. We improved the heuristic over many iterations, then compared it to the performance of the models we trained. Being able to compare these heuristics to the models gave us confidence to say that having a model was an improvement over the heuristics and was worth the extra effort of maintaining, deploying, and monitoring these models.\u003c/p\u003e\n\n\n\n\u003ch4\u003eComparing model performance to baseline performance is difficult\u003c/h4\u003e\n\n\n\n\u003cp\u003eAfter establishing our baseline and training our model(s), the difficulty lies in how we compare them evenly. In a perfect world we would run infinite A/B tests with hundreds of test cells to compare the performance of all our solutions in the real world, on real listeners. Since it’s not a perfect world, we need reliable offline metrics that act as a proxy for the online metrics we can’t get in those A/B tests.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWhen evaluating our recommendations models, we typically use normalized discounted cumulative gain (NDCG@k) as our metric, which can be implemented using Spotify’s Python SDK for Kubeflow pipelines. The question then becomes: how do we do the same for our heuristic? As we’ve mentioned before, transformation logic consistency is paramount, and so is evaluation logic — ideally, we’d have the same evaluation logic and the same evaluation test set of data. Unfortunately, our heuristics are generally written in a Java service and are tested with unit tests (not for performance).\u003c/p\u003e\n\n\n\n\u003cp\u003eFor fairly simple heuristics, we found a way to “train a model” so that its output is the heuristic rule’s output. This allowed us to use the same evaluation and evaluation test set as the models we were comparing against. We took this same approach when coming up with a solution to recommendations in the \u003cem\u003eTry something else\u003c/em\u003e shelf for new users on Home. We computed a popularity heuristic based on a listener’s demographics in Tensorflow Transform (TFT) and used the model as a lookup utility (with a fake loss).  \u003c/p\u003e\n\n\n\n\u003cp\u003eWe can’t always fit our problem into such a simple heuristic, as was the case for Shortcuts. The logic used in most of the Shortcuts heuristics was too complex to write in Tensorflow, so we implemented a completely separate offline evaluation pipeline that would gather recommendations made by models and heuristics, and apply custom evaluation functions for comparison.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eAdding freedom and flexibility to our evaluation tools\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs mentioned earlier, there’s a second pain point we run into often: using custom evaluation metrics in TFMA.\u003c/p\u003e\n\n\n\n\u003ch4\u003eTFMA is sometimes too rigid\u003c/h4\u003e\n\n\n\n\u003cp\u003eSpotify’s SDK for Kubeflow only supports evaluations using TFMA, which provides fairly basic metrics out of the box — think: precision, recall, accuracy. The most common metric we typically use is NDCG@k — TFMA provides NDCG, but not NDCG@k. Implementing metrics in TFMA is notoriously difficult; it takes ~120 lines of code to implement NDCG@k in TFMA, but only a single line of code using scikit-learn in Python.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost recently, we were experimenting with a model that predicts the next playlist that a new user will listen to, and as we have very little information about new users, we wanted to ensure that the model was not just predicting the most popular content. To do so, we were going to evaluate the model with a diversity metric that measured the difference between specific characteristics of items in each playlist. This was nearly impossible to implement in TFMA, so our team contributed to the Python SDK for Kubeflow to support any custom Python evaluation. We have been using this and running our experiments via Kubeflow pipelines since October 2020. \u003c/p\u003e\n\n\n\n\u003ch4\u003eCompare and track experiment results\u003c/h4\u003e\n\n\n\n\u003cp\u003eIn the pre-Kubeflow world, our experimentation platform allowed for a way to track and compare models — now, we are using Spotify’s internal UI for machine learning, as it easily integrates with our Kubeflow runs. We can view and compare the evaluation scores of our experiments — both NDCG and custom metrics — in the UI. We’ve been using this for a number of our models, and it allows us to track our model deployments as well.\u003c/p\u003e\n\n\n\n\u003ch3\u003eLooking at more than just the numbers for evaluating recommendations\u003c/h3\u003e\n\n\n\n\u003cp\u003eI’ve mostly mentioned what metrics we use and why they are important, but there is another incredibly useful way we evaluate our models — sometimes more useful than what a metric can reveal.\u003c/p\u003e\n\n\n\n\u003ch4\u003eWe build custom dashboards to manually evaluate the recs\u003c/h4\u003e\n\n\n\n\u003cp\u003eBased on past issues, we know that evaluation metrics don’t show the whole picture of how well a model is recommending content. Sometimes, the best way to evaluate a model is by seeing what content it recommends given a specific set of features about a listener. And for this reason, our team built a dashboard that does exactly that. It loads models simply by supplying the storage location of the model, and supports comparison of multiple models given a set of features. We often test and evaluate the recommendations that a new model will provide before deploying it to production by making predictions with different sets of feature values; this gives us an intuition behind what content will be recommended to different users that have these feature values. This has helped us find glaring issues; for example, when developing and testing a new model, we found that it would recommend the same popular playlist to listeners in all European countries. Having this knowledge allowed us to fix and improve the model before deploying it to production.  \u003c/p\u003e\n\n\n\n\u003cp\u003eMost recently, we have been working on a new model to recommend albums a listener might like based on their locality and what they like to listen to. We have been running experiments comparing evaluation metric values, but we have also been looking at the recommendations on our dashboard. This dashboard gives you the ability to try different features and compare the recommendations across different models — all before the models are used to recommend content to our listeners. At the beginning stages of experimentation and modeling for this project, we noticed that the same album was recommended as the first item no matter what input features (such as user’s country, followed artists, etc.) were used for testing, meaning this album would have been recommended to everyone as the first recommendation. Without this dashboard as a tool, it would have been more challenging to identify this issue and remediate it before the model went live.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile our offline metrics might indicate poor performance, they don’t tell us anything about what the reason might be, whereas this dashboard can show the quality of our recommendations and is extremely useful in finding issues like this.\u003c/p\u003e\n\n\n\n\u003cp\u003eThrough the use of task-specific custom evaluations and dashboards to show evaluation metrics and recommendations per feature set, we have been able to gain deep insight into how our models are behaving, and make our models a little less of a black box. \u003c/p\u003e\n\n\n\n\u003ch2\u003eThe struggles of automated model retraining and deployment\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s dive into our last topic, which is all about maintaining models in production: retraining and automatic deployment.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBut do we actually need to retrain our models?\u003c/h3\u003e\n\n\n\n\u003cp\u003eIt would be really nice if we could train a model once, deploy it, and then not have to do anything except monitor its online performance. Sadly, we’ve never seen this in reality.\u003c/p\u003e\n\n\n\n\u003ch4\u003eSometimes the model’s task requires frequent retraining\u003c/h4\u003e\n\n\n\n\u003cp\u003eSince we first deployed the Podcast Model in Home, we have always had retraining set up for it — and that’s because it only recommends podcast shows that it has seen in training data. So if we didn’t retrain it, it wouldn’t recommend any newly published shows.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe rest of the time, it just becomes a tech debt monster\u003c/h4\u003e\n\n\n\n\u003cp\u003eBut in some cases, retraining isn’t necessarily required to capture the full set of possible candidates. For the Shortcuts Model, we didn’t have retraining set up because it only recommends content that the listener has previously listened to (which is always in the serving features). But while retraining wasn’t needed for the Shortcuts Model to operate, the lack of it became one of the biggest sources of ML tech debt. We did not implement retraining for Shortcuts because it wasn’t needed for launching the feature, but have seen that it would have saved us time and effort in the long run had we invested some time in the short term. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt wasn’t until many months after the launch of the model that we saw issues with the quality of recommendations in Shortcuts due to no retraining — some of the features for this model describe the type of content that a listener has listened to, like whether it’s a personalized playlist or an album, etc., and there was a recent addition of a new type of content that was introduced after the model was last trained. As a result, the model didn’t recommend this piece of content in Shortcuts. While this starts to look like the same scenario as the Podcast Model described above, we also saw issues with migrating to different tools and platforms because the model was trained using older versions of libraries.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eImplement for the short term while waiting for the long-term solution\u003c/h3\u003e\n\n\n\n\u003cp\u003eOnce upon a time, we only had that singular Podcast Model, which was used to generate batch predictions, not real-time predictions. We had a Scio pipeline that used \u003ca href=\"https://spotify.github.io/zoltar/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eZoltar\u003c/a\u003e to predict podcast recommendations for all listeners, and we stored these predictions in our Bigtable instance that holds all of our content recommendations. This was a great start, but fairly inflexible when it came to when and how often we could make predictions for a given listener — and this is important because the listeners’ features could change if they listen to new content or follow new artists, which could provide better information to the model.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eBuilding a recommender service for the short term\u003c/h4\u003e\n\n\n\n\u003cp\u003eConsequently, we built a new service to serve this model and enable online predictions. We could get fresh recommendations for a listener almost instantly, and we could get these recommendations at useful times, such as when a listener follows a new artist. While this was a great improvement to move from offline predictions to online predictions, and an important step in making a better product, we knew we were only going to be in this state for the short term. Spotify’s online serving platform was on the horizon, but not yet ready; the benefits to building a short-term less-than-optimal solution outweighed the benefits of waiting to serve online models until Spotify’s serving solution was production ready.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWith that said, let’s talk about some challenges we faced in building this recommender service, such as how to refresh the local version of a deployed model. Our solution was to poll our internal storage directory every 10 minutes to check if there was a new revision of the model; if so, the service would pull the model down from where it was stored and start using that model to make predictions. Nevermind that we only retrained weekly or that there would be some state at which some machines would have the new revision of a model and others would have the older revision (although this was not something we worried about in our specific use case).  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe pain of manually deploying models\u003c/h4\u003e\n\n\n\n\u003cp\u003eThis was really a solution to serving models online, and less of a solution to a better process of serving models. Each time we wanted to deploy a model we had to: 1) copy the model to a specific storage location, 2) manually generate a pointer in our internal storage directory for that location, and 3) add this pointer to our recommender service along with the logic to fetch and transform features for the model. If we were to retrain the model, we would have to repeat each of those steps.  \u003c/p\u003e\n\n\n\n\u003cp\u003eObviously, this was a cumbersome process, but because we had this short-term solution, we were able to deploy four models to production and tested many others in A/B tests.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCI/CD — but make it for model training and deployment\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhile this recommender service lived a long life of about 10 months, the next obvious step was to migrate to Spotify’s model serving platform, which enabled us to automate retraining and deployment of retrained models.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eAutomating feature transformations without Tensorflow Transform\u003c/h4\u003e\n\n\n\n\u003cp\u003eThe first step in automating retraining is automating train dataset and test dataset curation, fetching the correct features and performing the necessary feature transformations. While feature transformations are generally handled automatically via TFT in a Kubeflow pipeline, we don’t perform our feature transformations in TFT (and therefore not in our experiment pipeline) because many the transformations we perform on the data are fairly complex and would be unnecessarily difficult to do in Tensorflow.  \u003c/p\u003e\n\n\n\n\u003cp\u003eBut because the serving platform provides feature logging, we enabled logging of \u003cem\u003ealready transformed\u003c/em\u003e features, to which we then apply the correct labels, and separate into train and test sets. These actions are all performed in scheduled pipelines that run weekly and produce weekly datasets for our models to use.\u003c/p\u003e\n\n\n\n\u003ch4\u003eMigrating from our short-term solution to a long-term solution\u003c/h4\u003e\n\n\n\n\u003cp\u003eIn order to enable feature logging, we had to migrate to the new online model serving platform from our recommender service using Zoltar. It was a matter of dark loading all prediction traffic to the new deployment and then running a simple rollout to start directing traffic to our new deployment instead of using Zoltar to make predictions in our own service. This was an easy migration and provided the benefits that the online serving platform offers — feature logging, faster predictions / lower latencies, less code managed by our team — and it also supports pushing a new model version (from a Kubeflow pipeline), as opposed to constantly polling for a new model version. \u003c/p\u003e\n\n\n\n\u003ch4\u003eContinuous retraining and automatic deployment\u003c/h4\u003e\n\n\n\n\u003cp\u003eNow that our models are all deployed via the Spotify serving platform, it enables us to employ CI/CD. We can schedule our models to be retrained via a Kubeflow pipeline, and as part of the Kubeflow pipeline we can ensure that a “bad” model is not accidentally automatically deployed by specifying that it should: 1) check that the evaluation score is greater than our configured threshold, and 2) automatically push it to our serving infrastructure if it is greater than the threshold. This automates a lot of the processes that we had to perform manually not long ago.  \u003c/p\u003e\n\n\n\n\u003cp\u003eEnabling CI/CD for retraining and model deployment is hard, but it’s becoming easier with the new tools available and makes the quality and reliability of our models better. And at first glance, you might not think you need retraining for a model because of the task it performs, but without it, your model could make predictions in unpredictable ways and increase your tech debt.   \u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur ML stack has come a long way in recent years, but it’s not perfect by any means. There are still a number of challenges we are tackling — data versioning, model versioning, moving feature transformations to Tensorflow Transform — and better ways to compare offline metrics across both ML and non-ML solutions. But it has decreased the time it takes for us to iterate, experiment, and deploy quality models.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe have adopted and/or built the components we need to successfully and efficiently manage our data, experiment with different models, and support continuous integration and development throughout the deployment and retraining processes. Our ML stack has enabled us to launch numerous models that serve millions of listeners on Home every day.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you are interested in joining us and helping improve how we recommend content on Home, we are \u003ca href=\"https://www.lifeatspotify.com/jobs\"\u003ehiring\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "In Part I of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including: The Podcast Model: Predicts podcasts a listener is likely to listen to in the Shows you might like shelf. The Shortcuts Model: Predicts the listener’s next fa",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png",
      "date_published": "2021-11-18T00:00:00Z",
      "author": {
        "name": "Published by Annie Edmundson, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/",
      "title": "\n                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 18, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/\" title=\"The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png 2098w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-2048x1009.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-120x59.png 120w\" sizes=\"(max-width: 2098px) 100vw, 2098px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eIn \u003ca href=\"https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThe Podcast Model:\u003c/strong\u003e Predicts podcasts a listener is likely to listen to in the \u003cem\u003eShows you might like\u003c/em\u003e shelf. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Shortcuts Model: \u003c/strong\u003ePredicts the listener’s next familiar listen in the Shortcuts feature. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Playlists Model: \u003c/strong\u003ePredicts the playlists a new listener is likely to listen to in the \u003cem\u003eTry something else\u003c/em\u003e shelf.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eIn this part of the series, we’ll highlight how and why we evaluate our models with different tools, and the hurdles to maintaining these models in production. \u003c/p\u003e\n\n\n\n\u003ch2\u003eTrust but verify your recommendations… with dashboards\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo let’s talk about what we do with that data — specifically, how we run experiments, and maybe more importantly, how we evaluate our models’ performance.\u003c/p\u003e\n\n\n\n\u003ch3\u003eMaking experimentation simpler\u003c/h3\u003e\n\n\n\n\u003ch4\u003eHow it started: experimenting on a siloed platform\u003c/h4\u003e\n\n\n\n\u003cp\u003eNot that long ago, after transforming our training data, we would run experiments on a siloed platform specifically geared towards model experimentation, and that was only really used within our team — we did this for both the initial Podcast Model as well as for the Shortcuts Model. This platform could easily launch hundreds of experiments by using a configuration file to specify hyperparameters (it also supported a grid search on specified hyperparameters). And since everything that was submitted to run an experiment was a script, it supported custom evaluation metrics — something that has always been important in our team. While it provided these necessary features, it wasn’t scalable, wasn’t maintained, and had an incomplete UI. Sometimes the compute instances would lose connection with the API (via a periodic ping) and would end up being ghost workers — still running, but not connected to anything.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eHow it’s going: integration with Spotify ML ecosystem\u003c/h4\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s managed Kubeflow clusters\u003c/a\u003e provide a more scalable approach, modular components, and are compatible with other parts of the Spotify ML infrastructure, so it was an obvious choice to move our experimentation to this platform. Training our models using Kubeflow pipelines is easy and efficient, but running the evaluation we needed and tracking those results were our biggest pain points for two reasons: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAs Spotify’s SDK for Kubeflow uses Tensorflow Model Analysis (TFMA), comparing the performance of a non-ML heuristic algorithm to that of a trained model is challenging to set up and requires extra infrastructure. \u003c/li\u003e\u003cli\u003eWe often have custom evaluation metrics that are specific to the model’s task, but they are infinitely more difficult to implement in TFMA than in vanilla Python.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch3\u003eEvaluating models against simpler (non-ML) solutions\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs I alluded to in an earlier paragraph, we don’t typically start solutions to a problem with an ML solution. We first identify a heuristic, or rule-based, solution, and the most appropriate way to evaluate it.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe first step — having a baseline for comparison\u003c/h4\u003e\n\n\n\n\u003cp\u003eWe are often tasked with creating better recommendations for content X, but what are “better recommendations,” and what are they better than? Having a baseline helps answer these questions, giving us something to compare our models against. And a good baseline — usually a heuristic/rule-based solution — is a quick, efficient, but maybe not the most optimal, solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eTake the Shortcuts Model as an example. We created an initial heuristic that recommended, simply, the most frequently played items from a listener’s short-term listening history. We improved the heuristic over many iterations, then compared it to the performance of the models we trained. Being able to compare these heuristics to the models gave us confidence to say that having a model was an improvement over the heuristics and was worth the extra effort of maintaining, deploying, and monitoring these models.\u003c/p\u003e\n\n\n\n\u003ch4\u003eComparing model performance to baseline performance is difficult\u003c/h4\u003e\n\n\n\n\u003cp\u003eAfter establishing our baseline and training our model(s), the difficulty lies in how we compare them evenly. In a perfect world we would run infinite A/B tests with hundreds of test cells to compare the performance of all our solutions in the real world, on real listeners. Since it’s not a perfect world, we need reliable offline metrics that act as a proxy for the online metrics we can’t get in those A/B tests.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWhen evaluating our recommendations models, we typically use normalized discounted cumulative gain (NDCG@k) as our metric, which can be implemented using Spotify’s Python SDK for Kubeflow pipelines. The question then becomes: how do we do the same for our heuristic? As we’ve mentioned before, transformation logic consistency is paramount, and so is evaluation logic — ideally, we’d have the same evaluation logic and the same evaluation test set of data. Unfortunately, our heuristics are generally written in a Java service and are tested with unit tests (not for performance).\u003c/p\u003e\n\n\n\n\u003cp\u003eFor fairly simple heuristics, we found a way to “train a model” so that its output is the heuristic rule’s output. This allowed us to use the same evaluation and evaluation test set as the models we were comparing against. We took this same approach when coming up with a solution to recommendations in the \u003cem\u003eTry something else\u003c/em\u003e shelf for new users on Home. We computed a popularity heuristic based on a listener’s demographics in Tensorflow Transform (TFT) and used the model as a lookup utility (with a fake loss).  \u003c/p\u003e\n\n\n\n\u003cp\u003eWe can’t always fit our problem into such a simple heuristic, as was the case for Shortcuts. The logic used in most of the Shortcuts heuristics was too complex to write in Tensorflow, so we implemented a completely separate offline evaluation pipeline that would gather recommendations made by models and heuristics, and apply custom evaluation functions for comparison.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eAdding freedom and flexibility to our evaluation tools\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs mentioned earlier, there’s a second pain point we run into often: using custom evaluation metrics in TFMA.\u003c/p\u003e\n\n\n\n\u003ch4\u003eTFMA is sometimes too rigid\u003c/h4\u003e\n\n\n\n\u003cp\u003eSpotify’s SDK for Kubeflow only supports evaluations using TFMA, which provides fairly basic metrics out of the box — think: precision, recall, accuracy. The most common metric we typically use is NDCG@k — TFMA provides NDCG, but not NDCG@k. Implementing metrics in TFMA is notoriously difficult; it takes ~120 lines of code to implement NDCG@k in TFMA, but only a single line of code using scikit-learn in Python.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost recently, we were experimenting with a model that predicts the next playlist that a new user will listen to, and as we have very little information about new users, we wanted to ensure that the model was not just predicting the most popular content. To do so, we were going to evaluate the model with a diversity metric that measured the difference between specific characteristics of items in each playlist. This was nearly impossible to implement in TFMA, so our team contributed to the Python SDK for Kubeflow to support any custom Python evaluation. We have been using this and running our experiments via Kubeflow pipelines since October 2020. \u003c/p\u003e\n\n\n\n\u003ch4\u003eCompare and track experiment results\u003c/h4\u003e\n\n\n\n\u003cp\u003eIn the pre-Kubeflow world, our experimentation platform allowed for a way to track and compare models — now, we are using Spotify’s internal UI for machine learning, as it easily integrates with our Kubeflow runs. We can view and compare the evaluation scores of our experiments — both NDCG and custom metrics — in the UI. We’ve been using this for a number of our models, and it allows us to track our model deployments as well.\u003c/p\u003e\n\n\n\n\u003ch3\u003eLooking at more than just the numbers for evaluating recommendations\u003c/h3\u003e\n\n\n\n\u003cp\u003eI’ve mostly mentioned what metrics we use and why they are important, but there is another incredibly useful way we evaluate our models — sometimes more useful than what a metric can reveal.\u003c/p\u003e\n\n\n\n\u003ch4\u003eWe build custom dashboards to manually evaluate the recs\u003c/h4\u003e\n\n\n\n\u003cp\u003eBased on past issues, we know that evaluation metrics don’t show the whole picture of how well a model is recommending content. Sometimes, the best way to evaluate a model is by seeing what content it recommends given a specific set of features about a listener. And for this reason, our team built a dashboard that does exactly that. It loads models simply by supplying the storage location of the model, and supports comparison of multiple models given a set of features. We often test and evaluate the recommendations that a new model will provide before deploying it to production by making predictions with different sets of feature values; this gives us an intuition behind what content will be recommended to different users that have these feature values. This has helped us find glaring issues; for example, when developing and testing a new model, we found that it would recommend the same popular playlist to listeners in all European countries. Having this knowledge allowed us to fix and improve the model before deploying it to production.  \u003c/p\u003e\n\n\n\n\u003cp\u003eMost recently, we have been working on a new model to recommend albums a listener might like based on their locality and what they like to listen to. We have been running experiments comparing evaluation metric values, but we have also been looking at the recommendations on our dashboard. This dashboard gives you the ability to try different features and compare the recommendations across different models — all before the models are used to recommend content to our listeners. At the beginning stages of experimentation and modeling for this project, we noticed that the same album was recommended as the first item no matter what input features (such as user’s country, followed artists, etc.) were used for testing, meaning this album would have been recommended to everyone as the first recommendation. Without this dashboard as a tool, it would have been more challenging to identify this issue and remediate it before the model went live.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile our offline metrics might indicate poor performance, they don’t tell us anything about what the reason might be, whereas this dashboard can show the quality of our recommendations and is extremely useful in finding issues like this.\u003c/p\u003e\n\n\n\n\u003cp\u003eThrough the use of task-specific custom evaluations and dashboards to show evaluation metrics and recommendations per feature set, we have been able to gain deep insight into how our models are behaving, and make our models a little less of a black box. \u003c/p\u003e\n\n\n\n\u003ch2\u003eThe struggles of automated model retraining and deployment\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s dive into our last topic, which is all about maintaining models in production: retraining and automatic deployment.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBut do we actually need to retrain our models?\u003c/h3\u003e\n\n\n\n\u003cp\u003eIt would be really nice if we could train a model once, deploy it, and then not have to do anything except monitor its online performance. Sadly, we’ve never seen this in reality.\u003c/p\u003e\n\n\n\n\u003ch4\u003eSometimes the model’s task requires frequent retraining\u003c/h4\u003e\n\n\n\n\u003cp\u003eSince we first deployed the Podcast Model in Home, we have always had retraining set up for it — and that’s because it only recommends podcast shows that it has seen in training data. So if we didn’t retrain it, it wouldn’t recommend any newly published shows.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe rest of the time, it just becomes a tech debt monster\u003c/h4\u003e\n\n\n\n\u003cp\u003eBut in some cases, retraining isn’t necessarily required to capture the full set of possible candidates. For the Shortcuts Model, we didn’t have retraining set up because it only recommends content that the listener has previously listened to (which is always in the serving features). But while retraining wasn’t needed for the Shortcuts Model to operate, the lack of it became one of the biggest sources of ML tech debt. We did not implement retraining for Shortcuts because it wasn’t needed for launching the feature, but have seen that it would have saved us time and effort in the long run had we invested some time in the short term. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt wasn’t until many months after the launch of the model that we saw issues with the quality of recommendations in Shortcuts due to no retraining — some of the features for this model describe the type of content that a listener has listened to, like whether it’s a personalized playlist or an album, etc., and there was a recent addition of a new type of content that was introduced after the model was last trained. As a result, the model didn’t recommend this piece of content in Shortcuts. While this starts to look like the same scenario as the Podcast Model described above, we also saw issues with migrating to different tools and platforms because the model was trained using older versions of libraries.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eImplement for the short term while waiting for the long-term solution\u003c/h3\u003e\n\n\n\n\u003cp\u003eOnce upon a time, we only had that singular Podcast Model, which was used to generate batch predictions, not real-time predictions. We had a Scio pipeline that used \u003ca href=\"https://spotify.github.io/zoltar/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eZoltar\u003c/a\u003e to predict podcast recommendations for all listeners, and we stored these predictions in our Bigtable instance that holds all of our content recommendations. This was a great start, but fairly inflexible when it came to when and how often we could make predictions for a given listener — and this is important because the listeners’ features could change if they listen to new content or follow new artists, which could provide better information to the model.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eBuilding a recommender service for the short term\u003c/h4\u003e\n\n\n\n\u003cp\u003eConsequently, we built a new service to serve this model and enable online predictions. We could get fresh recommendations for a listener almost instantly, and we could get these recommendations at useful times, such as when a listener follows a new artist. While this was a great improvement to move from offline predictions to online predictions, and an important step in making a better product, we knew we were only going to be in this state for the short term. Spotify’s online serving platform was on the horizon, but not yet ready; the benefits to building a short-term less-than-optimal solution outweighed the benefits of waiting to serve online models until Spotify’s serving solution was production ready.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWith that said, let’s talk about some challenges we faced in building this recommender service, such as how to refresh the local version of a deployed model. Our solution was to poll our internal storage directory every 10 minutes to check if there was a new revision of the model; if so, the service would pull the model down from where it was stored and start using that model to make predictions. Nevermind that we only retrained weekly or that there would be some state at which some machines would have the new revision of a model and others would have the older revision (although this was not something we worried about in our specific use case).  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe pain of manually deploying models\u003c/h4\u003e\n\n\n\n\u003cp\u003eThis was really a solution to serving models online, and less of a solution to a better process of serving models. Each time we wanted to deploy a model we had to: 1) copy the model to a specific storage location, 2) manually generate a pointer in our internal storage directory for that location, and 3) add this pointer to our recommender service along with the logic to fetch and transform features for the model. If we were to retrain the model, we would have to repeat each of those steps.  \u003c/p\u003e\n\n\n\n\u003cp\u003eObviously, this was a cumbersome process, but because we had this short-term solution, we were able to deploy four models to production and tested many others in A/B tests.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCI/CD — but make it for model training and deployment\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhile this recommender service lived a long life of about 10 months, the next obvious step was to migrate to Spotify’s model serving platform, which enabled us to automate retraining and deployment of retrained models.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eAutomating feature transformations without Tensorflow Transform\u003c/h4\u003e\n\n\n\n\u003cp\u003eThe first step in automating retraining is automating train dataset and test dataset curation, fetching the correct features and performing the necessary feature transformations. While feature transformations are generally handled automatically via TFT in a Kubeflow pipeline, we don’t perform our feature transformations in TFT (and therefore not in our experiment pipeline) because many the transformations we perform on the data are fairly complex and would be unnecessarily difficult to do in Tensorflow.  \u003c/p\u003e\n\n\n\n\u003cp\u003eBut because the serving platform provides feature logging, we enabled logging of \u003cem\u003ealready transformed\u003c/em\u003e features, to which we then apply the correct labels, and separate into train and test sets. These actions are all performed in scheduled pipelines that run weekly and produce weekly datasets for our models to use.\u003c/p\u003e\n\n\n\n\u003ch4\u003eMigrating from our short-term solution to a long-term solution\u003c/h4\u003e\n\n\n\n\u003cp\u003eIn order to enable feature logging, we had to migrate to the new online model serving platform from our recommender service using Zoltar. It was a matter of dark loading all prediction traffic to the new deployment and then running a simple rollout to start directing traffic to our new deployment instead of using Zoltar to make predictions in our own service. This was an easy migration and provided the benefits that the online serving platform offers — feature logging, faster predictions / lower latencies, less code managed by our team — and it also supports pushing a new model version (from a Kubeflow pipeline), as opposed to constantly polling for a new model version. \u003c/p\u003e\n\n\n\n\u003ch4\u003eContinuous retraining and automatic deployment\u003c/h4\u003e\n\n\n\n\u003cp\u003eNow that our models are all deployed via the Spotify serving platform, it enables us to employ CI/CD. We can schedule our models to be retrained via a Kubeflow pipeline, and as part of the Kubeflow pipeline we can ensure that a “bad” model is not accidentally automatically deployed by specifying that it should: 1) check that the evaluation score is greater than our configured threshold, and 2) automatically push it to our serving infrastructure if it is greater than the threshold. This automates a lot of the processes that we had to perform manually not long ago.  \u003c/p\u003e\n\n\n\n\u003cp\u003eEnabling CI/CD for retraining and model deployment is hard, but it’s becoming easier with the new tools available and makes the quality and reliability of our models better. And at first glance, you might not think you need retraining for a model because of the task it performs, but without it, your model could make predictions in unpredictable ways and increase your tech debt.   \u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur ML stack has come a long way in recent years, but it’s not perfect by any means. There are still a number of challenges we are tackling — data versioning, model versioning, moving feature transformations to Tensorflow Transform — and better ways to compare offline metrics across both ML and non-ML solutions. But it has decreased the time it takes for us to iterate, experiment, and deploy quality models.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe have adopted and/or built the components we need to successfully and efficiently manage our data, experiment with different models, and support continuous integration and development throughout the deployment and retraining processes. Our ML stack has enabled us to launch numerous models that serve millions of listeners on Home every day.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you are interested in joining us and helping improve how we recommend content on Home, we are \u003ca href=\"https://www.lifeatspotify.com/jobs\"\u003ehiring\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "In Part I of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including: The Podcast Model: Predicts podcasts a listener is likely to listen to in the Shows you might like shelf. The Shortcuts Model: Predicts the listener’s next fa",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png",
      "date_published": "2021-11-18T00:00:00Z",
      "author": {
        "name": "Published by Annie Edmundson, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/",
      "title": "\n                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 18, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/\" title=\"The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png 2098w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-2048x1009.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-120x59.png 120w\" sizes=\"(max-width: 2098px) 100vw, 2098px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eIn \u003ca href=\"https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThe Podcast Model:\u003c/strong\u003e Predicts podcasts a listener is likely to listen to in the \u003cem\u003eShows you might like\u003c/em\u003e shelf. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Shortcuts Model: \u003c/strong\u003ePredicts the listener’s next familiar listen in the Shortcuts feature. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Playlists Model: \u003c/strong\u003ePredicts the playlists a new listener is likely to listen to in the \u003cem\u003eTry something else\u003c/em\u003e shelf.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eIn this part of the series, we’ll highlight how and why we evaluate our models with different tools, and the hurdles to maintaining these models in production. \u003c/p\u003e\n\n\n\n\u003ch2\u003eTrust but verify your recommendations… with dashboards\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo let’s talk about what we do with that data — specifically, how we run experiments, and maybe more importantly, how we evaluate our models’ performance.\u003c/p\u003e\n\n\n\n\u003ch3\u003eMaking experimentation simpler\u003c/h3\u003e\n\n\n\n\u003ch4\u003eHow it started: experimenting on a siloed platform\u003c/h4\u003e\n\n\n\n\u003cp\u003eNot that long ago, after transforming our training data, we would run experiments on a siloed platform specifically geared towards model experimentation, and that was only really used within our team — we did this for both the initial Podcast Model as well as for the Shortcuts Model. This platform could easily launch hundreds of experiments by using a configuration file to specify hyperparameters (it also supported a grid search on specified hyperparameters). And since everything that was submitted to run an experiment was a script, it supported custom evaluation metrics — something that has always been important in our team. While it provided these necessary features, it wasn’t scalable, wasn’t maintained, and had an incomplete UI. Sometimes the compute instances would lose connection with the API (via a periodic ping) and would end up being ghost workers — still running, but not connected to anything.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eHow it’s going: integration with Spotify ML ecosystem\u003c/h4\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s managed Kubeflow clusters\u003c/a\u003e provide a more scalable approach, modular components, and are compatible with other parts of the Spotify ML infrastructure, so it was an obvious choice to move our experimentation to this platform. Training our models using Kubeflow pipelines is easy and efficient, but running the evaluation we needed and tracking those results were our biggest pain points for two reasons: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAs Spotify’s SDK for Kubeflow uses Tensorflow Model Analysis (TFMA), comparing the performance of a non-ML heuristic algorithm to that of a trained model is challenging to set up and requires extra infrastructure. \u003c/li\u003e\u003cli\u003eWe often have custom evaluation metrics that are specific to the model’s task, but they are infinitely more difficult to implement in TFMA than in vanilla Python.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch3\u003eEvaluating models against simpler (non-ML) solutions\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs I alluded to in an earlier paragraph, we don’t typically start solutions to a problem with an ML solution. We first identify a heuristic, or rule-based, solution, and the most appropriate way to evaluate it.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe first step — having a baseline for comparison\u003c/h4\u003e\n\n\n\n\u003cp\u003eWe are often tasked with creating better recommendations for content X, but what are “better recommendations,” and what are they better than? Having a baseline helps answer these questions, giving us something to compare our models against. And a good baseline — usually a heuristic/rule-based solution — is a quick, efficient, but maybe not the most optimal, solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eTake the Shortcuts Model as an example. We created an initial heuristic that recommended, simply, the most frequently played items from a listener’s short-term listening history. We improved the heuristic over many iterations, then compared it to the performance of the models we trained. Being able to compare these heuristics to the models gave us confidence to say that having a model was an improvement over the heuristics and was worth the extra effort of maintaining, deploying, and monitoring these models.\u003c/p\u003e\n\n\n\n\u003ch4\u003eComparing model performance to baseline performance is difficult\u003c/h4\u003e\n\n\n\n\u003cp\u003eAfter establishing our baseline and training our model(s), the difficulty lies in how we compare them evenly. In a perfect world we would run infinite A/B tests with hundreds of test cells to compare the performance of all our solutions in the real world, on real listeners. Since it’s not a perfect world, we need reliable offline metrics that act as a proxy for the online metrics we can’t get in those A/B tests.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWhen evaluating our recommendations models, we typically use normalized discounted cumulative gain (NDCG@k) as our metric, which can be implemented using Spotify’s Python SDK for Kubeflow pipelines. The question then becomes: how do we do the same for our heuristic? As we’ve mentioned before, transformation logic consistency is paramount, and so is evaluation logic — ideally, we’d have the same evaluation logic and the same evaluation test set of data. Unfortunately, our heuristics are generally written in a Java service and are tested with unit tests (not for performance).\u003c/p\u003e\n\n\n\n\u003cp\u003eFor fairly simple heuristics, we found a way to “train a model” so that its output is the heuristic rule’s output. This allowed us to use the same evaluation and evaluation test set as the models we were comparing against. We took this same approach when coming up with a solution to recommendations in the \u003cem\u003eTry something else\u003c/em\u003e shelf for new users on Home. We computed a popularity heuristic based on a listener’s demographics in Tensorflow Transform (TFT) and used the model as a lookup utility (with a fake loss).  \u003c/p\u003e\n\n\n\n\u003cp\u003eWe can’t always fit our problem into such a simple heuristic, as was the case for Shortcuts. The logic used in most of the Shortcuts heuristics was too complex to write in Tensorflow, so we implemented a completely separate offline evaluation pipeline that would gather recommendations made by models and heuristics, and apply custom evaluation functions for comparison.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eAdding freedom and flexibility to our evaluation tools\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs mentioned earlier, there’s a second pain point we run into often: using custom evaluation metrics in TFMA.\u003c/p\u003e\n\n\n\n\u003ch4\u003eTFMA is sometimes too rigid\u003c/h4\u003e\n\n\n\n\u003cp\u003eSpotify’s SDK for Kubeflow only supports evaluations using TFMA, which provides fairly basic metrics out of the box — think: precision, recall, accuracy. The most common metric we typically use is NDCG@k — TFMA provides NDCG, but not NDCG@k. Implementing metrics in TFMA is notoriously difficult; it takes ~120 lines of code to implement NDCG@k in TFMA, but only a single line of code using scikit-learn in Python.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost recently, we were experimenting with a model that predicts the next playlist that a new user will listen to, and as we have very little information about new users, we wanted to ensure that the model was not just predicting the most popular content. To do so, we were going to evaluate the model with a diversity metric that measured the difference between specific characteristics of items in each playlist. This was nearly impossible to implement in TFMA, so our team contributed to the Python SDK for Kubeflow to support any custom Python evaluation. We have been using this and running our experiments via Kubeflow pipelines since October 2020. \u003c/p\u003e\n\n\n\n\u003ch4\u003eCompare and track experiment results\u003c/h4\u003e\n\n\n\n\u003cp\u003eIn the pre-Kubeflow world, our experimentation platform allowed for a way to track and compare models — now, we are using Spotify’s internal UI for machine learning, as it easily integrates with our Kubeflow runs. We can view and compare the evaluation scores of our experiments — both NDCG and custom metrics — in the UI. We’ve been using this for a number of our models, and it allows us to track our model deployments as well.\u003c/p\u003e\n\n\n\n\u003ch3\u003eLooking at more than just the numbers for evaluating recommendations\u003c/h3\u003e\n\n\n\n\u003cp\u003eI’ve mostly mentioned what metrics we use and why they are important, but there is another incredibly useful way we evaluate our models — sometimes more useful than what a metric can reveal.\u003c/p\u003e\n\n\n\n\u003ch4\u003eWe build custom dashboards to manually evaluate the recs\u003c/h4\u003e\n\n\n\n\u003cp\u003eBased on past issues, we know that evaluation metrics don’t show the whole picture of how well a model is recommending content. Sometimes, the best way to evaluate a model is by seeing what content it recommends given a specific set of features about a listener. And for this reason, our team built a dashboard that does exactly that. It loads models simply by supplying the storage location of the model, and supports comparison of multiple models given a set of features. We often test and evaluate the recommendations that a new model will provide before deploying it to production by making predictions with different sets of feature values; this gives us an intuition behind what content will be recommended to different users that have these feature values. This has helped us find glaring issues; for example, when developing and testing a new model, we found that it would recommend the same popular playlist to listeners in all European countries. Having this knowledge allowed us to fix and improve the model before deploying it to production.  \u003c/p\u003e\n\n\n\n\u003cp\u003eMost recently, we have been working on a new model to recommend albums a listener might like based on their locality and what they like to listen to. We have been running experiments comparing evaluation metric values, but we have also been looking at the recommendations on our dashboard. This dashboard gives you the ability to try different features and compare the recommendations across different models — all before the models are used to recommend content to our listeners. At the beginning stages of experimentation and modeling for this project, we noticed that the same album was recommended as the first item no matter what input features (such as user’s country, followed artists, etc.) were used for testing, meaning this album would have been recommended to everyone as the first recommendation. Without this dashboard as a tool, it would have been more challenging to identify this issue and remediate it before the model went live.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile our offline metrics might indicate poor performance, they don’t tell us anything about what the reason might be, whereas this dashboard can show the quality of our recommendations and is extremely useful in finding issues like this.\u003c/p\u003e\n\n\n\n\u003cp\u003eThrough the use of task-specific custom evaluations and dashboards to show evaluation metrics and recommendations per feature set, we have been able to gain deep insight into how our models are behaving, and make our models a little less of a black box. \u003c/p\u003e\n\n\n\n\u003ch2\u003eThe struggles of automated model retraining and deployment\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s dive into our last topic, which is all about maintaining models in production: retraining and automatic deployment.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBut do we actually need to retrain our models?\u003c/h3\u003e\n\n\n\n\u003cp\u003eIt would be really nice if we could train a model once, deploy it, and then not have to do anything except monitor its online performance. Sadly, we’ve never seen this in reality.\u003c/p\u003e\n\n\n\n\u003ch4\u003eSometimes the model’s task requires frequent retraining\u003c/h4\u003e\n\n\n\n\u003cp\u003eSince we first deployed the Podcast Model in Home, we have always had retraining set up for it — and that’s because it only recommends podcast shows that it has seen in training data. So if we didn’t retrain it, it wouldn’t recommend any newly published shows.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe rest of the time, it just becomes a tech debt monster\u003c/h4\u003e\n\n\n\n\u003cp\u003eBut in some cases, retraining isn’t necessarily required to capture the full set of possible candidates. For the Shortcuts Model, we didn’t have retraining set up because it only recommends content that the listener has previously listened to (which is always in the serving features). But while retraining wasn’t needed for the Shortcuts Model to operate, the lack of it became one of the biggest sources of ML tech debt. We did not implement retraining for Shortcuts because it wasn’t needed for launching the feature, but have seen that it would have saved us time and effort in the long run had we invested some time in the short term. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt wasn’t until many months after the launch of the model that we saw issues with the quality of recommendations in Shortcuts due to no retraining — some of the features for this model describe the type of content that a listener has listened to, like whether it’s a personalized playlist or an album, etc., and there was a recent addition of a new type of content that was introduced after the model was last trained. As a result, the model didn’t recommend this piece of content in Shortcuts. While this starts to look like the same scenario as the Podcast Model described above, we also saw issues with migrating to different tools and platforms because the model was trained using older versions of libraries.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eImplement for the short term while waiting for the long-term solution\u003c/h3\u003e\n\n\n\n\u003cp\u003eOnce upon a time, we only had that singular Podcast Model, which was used to generate batch predictions, not real-time predictions. We had a Scio pipeline that used \u003ca href=\"https://spotify.github.io/zoltar/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eZoltar\u003c/a\u003e to predict podcast recommendations for all listeners, and we stored these predictions in our Bigtable instance that holds all of our content recommendations. This was a great start, but fairly inflexible when it came to when and how often we could make predictions for a given listener — and this is important because the listeners’ features could change if they listen to new content or follow new artists, which could provide better information to the model.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eBuilding a recommender service for the short term\u003c/h4\u003e\n\n\n\n\u003cp\u003eConsequently, we built a new service to serve this model and enable online predictions. We could get fresh recommendations for a listener almost instantly, and we could get these recommendations at useful times, such as when a listener follows a new artist. While this was a great improvement to move from offline predictions to online predictions, and an important step in making a better product, we knew we were only going to be in this state for the short term. Spotify’s online serving platform was on the horizon, but not yet ready; the benefits to building a short-term less-than-optimal solution outweighed the benefits of waiting to serve online models until Spotify’s serving solution was production ready.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWith that said, let’s talk about some challenges we faced in building this recommender service, such as how to refresh the local version of a deployed model. Our solution was to poll our internal storage directory every 10 minutes to check if there was a new revision of the model; if so, the service would pull the model down from where it was stored and start using that model to make predictions. Nevermind that we only retrained weekly or that there would be some state at which some machines would have the new revision of a model and others would have the older revision (although this was not something we worried about in our specific use case).  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe pain of manually deploying models\u003c/h4\u003e\n\n\n\n\u003cp\u003eThis was really a solution to serving models online, and less of a solution to a better process of serving models. Each time we wanted to deploy a model we had to: 1) copy the model to a specific storage location, 2) manually generate a pointer in our internal storage directory for that location, and 3) add this pointer to our recommender service along with the logic to fetch and transform features for the model. If we were to retrain the model, we would have to repeat each of those steps.  \u003c/p\u003e\n\n\n\n\u003cp\u003eObviously, this was a cumbersome process, but because we had this short-term solution, we were able to deploy four models to production and tested many others in A/B tests.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCI/CD — but make it for model training and deployment\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhile this recommender service lived a long life of about 10 months, the next obvious step was to migrate to Spotify’s model serving platform, which enabled us to automate retraining and deployment of retrained models.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eAutomating feature transformations without Tensorflow Transform\u003c/h4\u003e\n\n\n\n\u003cp\u003eThe first step in automating retraining is automating train dataset and test dataset curation, fetching the correct features and performing the necessary feature transformations. While feature transformations are generally handled automatically via TFT in a Kubeflow pipeline, we don’t perform our feature transformations in TFT (and therefore not in our experiment pipeline) because many the transformations we perform on the data are fairly complex and would be unnecessarily difficult to do in Tensorflow.  \u003c/p\u003e\n\n\n\n\u003cp\u003eBut because the serving platform provides feature logging, we enabled logging of \u003cem\u003ealready transformed\u003c/em\u003e features, to which we then apply the correct labels, and separate into train and test sets. These actions are all performed in scheduled pipelines that run weekly and produce weekly datasets for our models to use.\u003c/p\u003e\n\n\n\n\u003ch4\u003eMigrating from our short-term solution to a long-term solution\u003c/h4\u003e\n\n\n\n\u003cp\u003eIn order to enable feature logging, we had to migrate to the new online model serving platform from our recommender service using Zoltar. It was a matter of dark loading all prediction traffic to the new deployment and then running a simple rollout to start directing traffic to our new deployment instead of using Zoltar to make predictions in our own service. This was an easy migration and provided the benefits that the online serving platform offers — feature logging, faster predictions / lower latencies, less code managed by our team — and it also supports pushing a new model version (from a Kubeflow pipeline), as opposed to constantly polling for a new model version. \u003c/p\u003e\n\n\n\n\u003ch4\u003eContinuous retraining and automatic deployment\u003c/h4\u003e\n\n\n\n\u003cp\u003eNow that our models are all deployed via the Spotify serving platform, it enables us to employ CI/CD. We can schedule our models to be retrained via a Kubeflow pipeline, and as part of the Kubeflow pipeline we can ensure that a “bad” model is not accidentally automatically deployed by specifying that it should: 1) check that the evaluation score is greater than our configured threshold, and 2) automatically push it to our serving infrastructure if it is greater than the threshold. This automates a lot of the processes that we had to perform manually not long ago.  \u003c/p\u003e\n\n\n\n\u003cp\u003eEnabling CI/CD for retraining and model deployment is hard, but it’s becoming easier with the new tools available and makes the quality and reliability of our models better. And at first glance, you might not think you need retraining for a model because of the task it performs, but without it, your model could make predictions in unpredictable ways and increase your tech debt.   \u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur ML stack has come a long way in recent years, but it’s not perfect by any means. There are still a number of challenges we are tackling — data versioning, model versioning, moving feature transformations to Tensorflow Transform — and better ways to compare offline metrics across both ML and non-ML solutions. But it has decreased the time it takes for us to iterate, experiment, and deploy quality models.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe have adopted and/or built the components we need to successfully and efficiently manage our data, experiment with different models, and support continuous integration and development throughout the deployment and retraining processes. Our ML stack has enabled us to launch numerous models that serve millions of listeners on Home every day.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you are interested in joining us and helping improve how we recommend content on Home, we are \u003ca href=\"https://www.lifeatspotify.com/jobs\"\u003ehiring\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "In Part I of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including: The Podcast Model: Predicts podcasts a listener is likely to listen to in the Shows you might like shelf. The Shortcuts Model: Predicts the listener’s next fa",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png",
      "date_published": "2021-11-18T00:00:00Z",
      "author": {
        "name": "Published by Annie Edmundson, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/11/18/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/",
      "title": "\n                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 18, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/11/18/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/\" title=\"The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part II)\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png 2098w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-768x379.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-2048x1009.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A-120x59.png 120w\" sizes=\"(max-width: 2098px) 100vw, 2098px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eIn \u003ca href=\"https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThe Podcast Model:\u003c/strong\u003e Predicts podcasts a listener is likely to listen to in the \u003cem\u003eShows you might like\u003c/em\u003e shelf. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Shortcuts Model: \u003c/strong\u003ePredicts the listener’s next familiar listen in the Shortcuts feature. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Playlists Model: \u003c/strong\u003ePredicts the playlists a new listener is likely to listen to in the \u003cem\u003eTry something else\u003c/em\u003e shelf.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eIn this part of the series, we’ll highlight how and why we evaluate our models with different tools, and the hurdles to maintaining these models in production. \u003c/p\u003e\n\n\n\n\u003ch2\u003eTrust but verify your recommendations… with dashboards\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo let’s talk about what we do with that data — specifically, how we run experiments, and maybe more importantly, how we evaluate our models’ performance.\u003c/p\u003e\n\n\n\n\u003ch3\u003eMaking experimentation simpler\u003c/h3\u003e\n\n\n\n\u003ch4\u003eHow it started: experimenting on a siloed platform\u003c/h4\u003e\n\n\n\n\u003cp\u003eNot that long ago, after transforming our training data, we would run experiments on a siloed platform specifically geared towards model experimentation, and that was only really used within our team — we did this for both the initial Podcast Model as well as for the Shortcuts Model. This platform could easily launch hundreds of experiments by using a configuration file to specify hyperparameters (it also supported a grid search on specified hyperparameters). And since everything that was submitted to run an experiment was a script, it supported custom evaluation metrics — something that has always been important in our team. While it provided these necessary features, it wasn’t scalable, wasn’t maintained, and had an incomplete UI. Sometimes the compute instances would lose connection with the API (via a periodic ping) and would end up being ghost workers — still running, but not connected to anything.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eHow it’s going: integration with Spotify ML ecosystem\u003c/h4\u003e\n\n\n\n\u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2019/12/13/the-winding-road-to-better-machine-learning-infrastructure-through-tensorflow-extended-and-kubeflow/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s managed Kubeflow clusters\u003c/a\u003e provide a more scalable approach, modular components, and are compatible with other parts of the Spotify ML infrastructure, so it was an obvious choice to move our experimentation to this platform. Training our models using Kubeflow pipelines is easy and efficient, but running the evaluation we needed and tracking those results were our biggest pain points for two reasons: \u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eAs Spotify’s SDK for Kubeflow uses Tensorflow Model Analysis (TFMA), comparing the performance of a non-ML heuristic algorithm to that of a trained model is challenging to set up and requires extra infrastructure. \u003c/li\u003e\u003cli\u003eWe often have custom evaluation metrics that are specific to the model’s task, but they are infinitely more difficult to implement in TFMA than in vanilla Python.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch3\u003eEvaluating models against simpler (non-ML) solutions\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs I alluded to in an earlier paragraph, we don’t typically start solutions to a problem with an ML solution. We first identify a heuristic, or rule-based, solution, and the most appropriate way to evaluate it.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe first step — having a baseline for comparison\u003c/h4\u003e\n\n\n\n\u003cp\u003eWe are often tasked with creating better recommendations for content X, but what are “better recommendations,” and what are they better than? Having a baseline helps answer these questions, giving us something to compare our models against. And a good baseline — usually a heuristic/rule-based solution — is a quick, efficient, but maybe not the most optimal, solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eTake the Shortcuts Model as an example. We created an initial heuristic that recommended, simply, the most frequently played items from a listener’s short-term listening history. We improved the heuristic over many iterations, then compared it to the performance of the models we trained. Being able to compare these heuristics to the models gave us confidence to say that having a model was an improvement over the heuristics and was worth the extra effort of maintaining, deploying, and monitoring these models.\u003c/p\u003e\n\n\n\n\u003ch4\u003eComparing model performance to baseline performance is difficult\u003c/h4\u003e\n\n\n\n\u003cp\u003eAfter establishing our baseline and training our model(s), the difficulty lies in how we compare them evenly. In a perfect world we would run infinite A/B tests with hundreds of test cells to compare the performance of all our solutions in the real world, on real listeners. Since it’s not a perfect world, we need reliable offline metrics that act as a proxy for the online metrics we can’t get in those A/B tests.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWhen evaluating our recommendations models, we typically use normalized discounted cumulative gain (NDCG@k) as our metric, which can be implemented using Spotify’s Python SDK for Kubeflow pipelines. The question then becomes: how do we do the same for our heuristic? As we’ve mentioned before, transformation logic consistency is paramount, and so is evaluation logic — ideally, we’d have the same evaluation logic and the same evaluation test set of data. Unfortunately, our heuristics are generally written in a Java service and are tested with unit tests (not for performance).\u003c/p\u003e\n\n\n\n\u003cp\u003eFor fairly simple heuristics, we found a way to “train a model” so that its output is the heuristic rule’s output. This allowed us to use the same evaluation and evaluation test set as the models we were comparing against. We took this same approach when coming up with a solution to recommendations in the \u003cem\u003eTry something else\u003c/em\u003e shelf for new users on Home. We computed a popularity heuristic based on a listener’s demographics in Tensorflow Transform (TFT) and used the model as a lookup utility (with a fake loss).  \u003c/p\u003e\n\n\n\n\u003cp\u003eWe can’t always fit our problem into such a simple heuristic, as was the case for Shortcuts. The logic used in most of the Shortcuts heuristics was too complex to write in Tensorflow, so we implemented a completely separate offline evaluation pipeline that would gather recommendations made by models and heuristics, and apply custom evaluation functions for comparison.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eAdding freedom and flexibility to our evaluation tools\u003c/h3\u003e\n\n\n\n\u003cp\u003eAs mentioned earlier, there’s a second pain point we run into often: using custom evaluation metrics in TFMA.\u003c/p\u003e\n\n\n\n\u003ch4\u003eTFMA is sometimes too rigid\u003c/h4\u003e\n\n\n\n\u003cp\u003eSpotify’s SDK for Kubeflow only supports evaluations using TFMA, which provides fairly basic metrics out of the box — think: precision, recall, accuracy. The most common metric we typically use is NDCG@k — TFMA provides NDCG, but not NDCG@k. Implementing metrics in TFMA is notoriously difficult; it takes ~120 lines of code to implement NDCG@k in TFMA, but only a single line of code using scikit-learn in Python.\u003c/p\u003e\n\n\n\n\u003cp\u003eMost recently, we were experimenting with a model that predicts the next playlist that a new user will listen to, and as we have very little information about new users, we wanted to ensure that the model was not just predicting the most popular content. To do so, we were going to evaluate the model with a diversity metric that measured the difference between specific characteristics of items in each playlist. This was nearly impossible to implement in TFMA, so our team contributed to the Python SDK for Kubeflow to support any custom Python evaluation. We have been using this and running our experiments via Kubeflow pipelines since October 2020. \u003c/p\u003e\n\n\n\n\u003ch4\u003eCompare and track experiment results\u003c/h4\u003e\n\n\n\n\u003cp\u003eIn the pre-Kubeflow world, our experimentation platform allowed for a way to track and compare models — now, we are using Spotify’s internal UI for machine learning, as it easily integrates with our Kubeflow runs. We can view and compare the evaluation scores of our experiments — both NDCG and custom metrics — in the UI. We’ve been using this for a number of our models, and it allows us to track our model deployments as well.\u003c/p\u003e\n\n\n\n\u003ch3\u003eLooking at more than just the numbers for evaluating recommendations\u003c/h3\u003e\n\n\n\n\u003cp\u003eI’ve mostly mentioned what metrics we use and why they are important, but there is another incredibly useful way we evaluate our models — sometimes more useful than what a metric can reveal.\u003c/p\u003e\n\n\n\n\u003ch4\u003eWe build custom dashboards to manually evaluate the recs\u003c/h4\u003e\n\n\n\n\u003cp\u003eBased on past issues, we know that evaluation metrics don’t show the whole picture of how well a model is recommending content. Sometimes, the best way to evaluate a model is by seeing what content it recommends given a specific set of features about a listener. And for this reason, our team built a dashboard that does exactly that. It loads models simply by supplying the storage location of the model, and supports comparison of multiple models given a set of features. We often test and evaluate the recommendations that a new model will provide before deploying it to production by making predictions with different sets of feature values; this gives us an intuition behind what content will be recommended to different users that have these feature values. This has helped us find glaring issues; for example, when developing and testing a new model, we found that it would recommend the same popular playlist to listeners in all European countries. Having this knowledge allowed us to fix and improve the model before deploying it to production.  \u003c/p\u003e\n\n\n\n\u003cp\u003eMost recently, we have been working on a new model to recommend albums a listener might like based on their locality and what they like to listen to. We have been running experiments comparing evaluation metric values, but we have also been looking at the recommendations on our dashboard. This dashboard gives you the ability to try different features and compare the recommendations across different models — all before the models are used to recommend content to our listeners. At the beginning stages of experimentation and modeling for this project, we noticed that the same album was recommended as the first item no matter what input features (such as user’s country, followed artists, etc.) were used for testing, meaning this album would have been recommended to everyone as the first recommendation. Without this dashboard as a tool, it would have been more challenging to identify this issue and remediate it before the model went live.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile our offline metrics might indicate poor performance, they don’t tell us anything about what the reason might be, whereas this dashboard can show the quality of our recommendations and is extremely useful in finding issues like this.\u003c/p\u003e\n\n\n\n\u003cp\u003eThrough the use of task-specific custom evaluations and dashboards to show evaluation metrics and recommendations per feature set, we have been able to gain deep insight into how our models are behaving, and make our models a little less of a black box. \u003c/p\u003e\n\n\n\n\u003ch2\u003eThe struggles of automated model retraining and deployment\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s dive into our last topic, which is all about maintaining models in production: retraining and automatic deployment.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBut do we actually need to retrain our models?\u003c/h3\u003e\n\n\n\n\u003cp\u003eIt would be really nice if we could train a model once, deploy it, and then not have to do anything except monitor its online performance. Sadly, we’ve never seen this in reality.\u003c/p\u003e\n\n\n\n\u003ch4\u003eSometimes the model’s task requires frequent retraining\u003c/h4\u003e\n\n\n\n\u003cp\u003eSince we first deployed the Podcast Model in Home, we have always had retraining set up for it — and that’s because it only recommends podcast shows that it has seen in training data. So if we didn’t retrain it, it wouldn’t recommend any newly published shows.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe rest of the time, it just becomes a tech debt monster\u003c/h4\u003e\n\n\n\n\u003cp\u003eBut in some cases, retraining isn’t necessarily required to capture the full set of possible candidates. For the Shortcuts Model, we didn’t have retraining set up because it only recommends content that the listener has previously listened to (which is always in the serving features). But while retraining wasn’t needed for the Shortcuts Model to operate, the lack of it became one of the biggest sources of ML tech debt. We did not implement retraining for Shortcuts because it wasn’t needed for launching the feature, but have seen that it would have saved us time and effort in the long run had we invested some time in the short term. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt wasn’t until many months after the launch of the model that we saw issues with the quality of recommendations in Shortcuts due to no retraining — some of the features for this model describe the type of content that a listener has listened to, like whether it’s a personalized playlist or an album, etc., and there was a recent addition of a new type of content that was introduced after the model was last trained. As a result, the model didn’t recommend this piece of content in Shortcuts. While this starts to look like the same scenario as the Podcast Model described above, we also saw issues with migrating to different tools and platforms because the model was trained using older versions of libraries.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eImplement for the short term while waiting for the long-term solution\u003c/h3\u003e\n\n\n\n\u003cp\u003eOnce upon a time, we only had that singular Podcast Model, which was used to generate batch predictions, not real-time predictions. We had a Scio pipeline that used \u003ca href=\"https://spotify.github.io/zoltar/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eZoltar\u003c/a\u003e to predict podcast recommendations for all listeners, and we stored these predictions in our Bigtable instance that holds all of our content recommendations. This was a great start, but fairly inflexible when it came to when and how often we could make predictions for a given listener — and this is important because the listeners’ features could change if they listen to new content or follow new artists, which could provide better information to the model.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eBuilding a recommender service for the short term\u003c/h4\u003e\n\n\n\n\u003cp\u003eConsequently, we built a new service to serve this model and enable online predictions. We could get fresh recommendations for a listener almost instantly, and we could get these recommendations at useful times, such as when a listener follows a new artist. While this was a great improvement to move from offline predictions to online predictions, and an important step in making a better product, we knew we were only going to be in this state for the short term. Spotify’s online serving platform was on the horizon, but not yet ready; the benefits to building a short-term less-than-optimal solution outweighed the benefits of waiting to serve online models until Spotify’s serving solution was production ready.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWith that said, let’s talk about some challenges we faced in building this recommender service, such as how to refresh the local version of a deployed model. Our solution was to poll our internal storage directory every 10 minutes to check if there was a new revision of the model; if so, the service would pull the model down from where it was stored and start using that model to make predictions. Nevermind that we only retrained weekly or that there would be some state at which some machines would have the new revision of a model and others would have the older revision (although this was not something we worried about in our specific use case).  \u003c/p\u003e\n\n\n\n\u003ch4\u003eThe pain of manually deploying models\u003c/h4\u003e\n\n\n\n\u003cp\u003eThis was really a solution to serving models online, and less of a solution to a better process of serving models. Each time we wanted to deploy a model we had to: 1) copy the model to a specific storage location, 2) manually generate a pointer in our internal storage directory for that location, and 3) add this pointer to our recommender service along with the logic to fetch and transform features for the model. If we were to retrain the model, we would have to repeat each of those steps.  \u003c/p\u003e\n\n\n\n\u003cp\u003eObviously, this was a cumbersome process, but because we had this short-term solution, we were able to deploy four models to production and tested many others in A/B tests.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCI/CD — but make it for model training and deployment\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhile this recommender service lived a long life of about 10 months, the next obvious step was to migrate to Spotify’s model serving platform, which enabled us to automate retraining and deployment of retrained models.  \u003c/p\u003e\n\n\n\n\u003ch4\u003eAutomating feature transformations without Tensorflow Transform\u003c/h4\u003e\n\n\n\n\u003cp\u003eThe first step in automating retraining is automating train dataset and test dataset curation, fetching the correct features and performing the necessary feature transformations. While feature transformations are generally handled automatically via TFT in a Kubeflow pipeline, we don’t perform our feature transformations in TFT (and therefore not in our experiment pipeline) because many the transformations we perform on the data are fairly complex and would be unnecessarily difficult to do in Tensorflow.  \u003c/p\u003e\n\n\n\n\u003cp\u003eBut because the serving platform provides feature logging, we enabled logging of \u003cem\u003ealready transformed\u003c/em\u003e features, to which we then apply the correct labels, and separate into train and test sets. These actions are all performed in scheduled pipelines that run weekly and produce weekly datasets for our models to use.\u003c/p\u003e\n\n\n\n\u003ch4\u003eMigrating from our short-term solution to a long-term solution\u003c/h4\u003e\n\n\n\n\u003cp\u003eIn order to enable feature logging, we had to migrate to the new online model serving platform from our recommender service using Zoltar. It was a matter of dark loading all prediction traffic to the new deployment and then running a simple rollout to start directing traffic to our new deployment instead of using Zoltar to make predictions in our own service. This was an easy migration and provided the benefits that the online serving platform offers — feature logging, faster predictions / lower latencies, less code managed by our team — and it also supports pushing a new model version (from a Kubeflow pipeline), as opposed to constantly polling for a new model version. \u003c/p\u003e\n\n\n\n\u003ch4\u003eContinuous retraining and automatic deployment\u003c/h4\u003e\n\n\n\n\u003cp\u003eNow that our models are all deployed via the Spotify serving platform, it enables us to employ CI/CD. We can schedule our models to be retrained via a Kubeflow pipeline, and as part of the Kubeflow pipeline we can ensure that a “bad” model is not accidentally automatically deployed by specifying that it should: 1) check that the evaluation score is greater than our configured threshold, and 2) automatically push it to our serving infrastructure if it is greater than the threshold. This automates a lot of the processes that we had to perform manually not long ago.  \u003c/p\u003e\n\n\n\n\u003cp\u003eEnabling CI/CD for retraining and model deployment is hard, but it’s becoming easier with the new tools available and makes the quality and reliability of our models better. And at first glance, you might not think you need retraining for a model because of the task it performs, but without it, your model could make predictions in unpredictable ways and increase your tech debt.   \u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur ML stack has come a long way in recent years, but it’s not perfect by any means. There are still a number of challenges we are tackling — data versioning, model versioning, moving feature transformations to Tensorflow Transform — and better ways to compare offline metrics across both ML and non-ML solutions. But it has decreased the time it takes for us to iterate, experiment, and deploy quality models.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe have adopted and/or built the components we need to successfully and efficiently manage our data, experiment with different models, and support continuous integration and development throughout the deployment and retraining processes. Our ML stack has enabled us to launch numerous models that serve millions of listeners on Home every day.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you are interested in joining us and helping improve how we recommend content on Home, we are \u003ca href=\"https://www.lifeatspotify.com/jobs\"\u003ehiring\u003c/a\u003e!\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003c/p\u003e\n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "In Part I of this two-part series, we talked about the challenges we faced with the models we use to recommend content on Home, including: The Podcast Model: Predicts podcasts a listener is likely to listen to in the Shows you might like shelf. The Shortcuts Model: Predicts the listener’s next fa",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/147_Part-02A.png",
      "date_published": "2021-11-18T00:00:00Z",
      "author": {
        "name": "Published by Annie Edmundson, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/11/16/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/",
      "title": "\n                                            Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 16, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/11/16/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/\" title=\"Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are excited to be open sourcing \u003ca href=\"https://github.com/spotify/XCRemoteCache\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCRemoteCache\u003c/a\u003e, the library we created to mitigate long local builds. As the name suggests, this library is a remote caching implementation for iOS projects with an aim to reuse Xcode target artifacts generated on Continuous Integration (CI) machines. It supports Objective-C, Swift, and ObjC+Swift targets and can be easily integrated with existing Xcode projects, including ones managed by CocoaPods or Carthage.\u003c/p\u003e\n\n\n\n\u003cp\u003eBest of all, XCRemoteCache resulted in a \u003cstrong\u003e70% decrease in clean build times\u003c/strong\u003e (we classify a build as \u003cem\u003eclean\u003c/em\u003e when at least 50% of all targets compile at least one file).\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBackground\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUsing our Xcode build metrics (for more details on how this works, take a look at our open source \u003ca href=\"https://xcmetrics.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCMetrics project\u003c/a\u003e), we found out that it often takes our developers more than 10 minutes to build the main Spotify iOS application. Even though the number of these builds is relatively small (less than 3% of all builds), they take more than 50% of global building times (Figure 1). \u003c/p\u003e\n\n\n\n\u003cp\u003eAfter some investigation, it was revealed that long-lasting builds usually happen after rebasing or merging remote branches. Implementing a remote cache solution was the perfect fit.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"354\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-768x389.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times.png 1205w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: Distribution of local machines’ build times and their total build times.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eRemote cache principle\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eA remote cache is a popular technique to speed up builds of big applications by applying the “compile once, use everywhere” approach. As long as all input files and compilation parameters are the same, instead of building a target locally, one can download artifacts that were built and shared from some other machine. A key success factor for remote caching is finding an optimal caching level. Caching units that are too granular, where every single piece of the compilation step is cacheable, may lead to extensive network traffic overhead, which can offset CPU savings. On the other hand, putting the entire codebase into a single cacheable unit may significantly degrade the caching hit rate; every single local change invalidates remotely available cache artifacts, triggering a full build, locally.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe main Spotify iOS application is highly modularized and contains more than 400 independent modules configured as separate Xcode targets. Applying target-level caching was natural, and as we found out later, the right decision.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eDesigning the remote cache solution \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the design phase, our aim was to come up with a solution generic enough that it could be applied to a broad range of iOS applications with minimal or no project changes. That was an ambitious goal given how the Xcode build system works. Before going straight to the applied solution, let’s consider how an Xcode build actually works. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow does XCRemoteCache work?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn general, all caching mechanisms use a fingerprint of input files to recognize if build products can be reused. However, finding a precise set of these files is a nontrivial task. The Xcode build system is very liberal when it comes to dependency attribution. It tries to optimistically find a definition of the dependency (header files, \u003ccode\u003e.swiftmodule\u003c/code\u003e, etc.) in all available search paths — provided either by the developer in the Xcode project settings or in the current build product directory. As a result, developers don’t have to explicitly specify all dependencies a target uses, but just have to make sure that those dependencies will be placed in a correct location \u003cem\u003ebefore\u003c/em\u003e Xcode actually needs them. The fact that compilers are able to implicitly find required dependencies, hinders the simple fingerprint generation by hashing all available files in header and framework search paths — a list of files to consider in the fingerprinting would often be too broad.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the other hand, we observed that Xcode works quite well for local incremental builds and executes only a narrow subset of steps that were affected since the last build. In other words, the Xcode build system knows which files are the actual input files for the compilation, but that list is generated as compiler’s output (.d files) and is not available ahead of a compilation.\u003c/p\u003e\n\n\n\n\u003cp\u003eXCRemoteCache applies a unique approach to automatically identify all input files of the compilation based on Git history and dependency lists provided as a compilation output. The generation side, called \u003cem\u003eproducer mode\u003c/em\u003e, along with the compilation product, also uploads a meta file. That file contains a list of all compilation files the compiler used and the full SHA-1 (Secure Hash Algorithm 1) commit identifier it was built against. The producer mode should run on CI for each primary branch (like \u003ccode\u003emaster\u003c/code\u003e or \u003ccode\u003edevelop\u003c/code\u003e) commit, as a part of the post-merge phase.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the consumer side (aka \u003cem\u003econsumer mode\u003c/em\u003e), XCRemoteCache finds the most recent history commit for which the remote server contains build artifacts and builds a fingerprint based on input files provided in the meta file. Let’s imagine two developers, \u003ccode\u003eA\u003c/code\u003e and \u003ccode\u003eB\u003c/code\u003e, working on their local branches \u003ccode\u003efeatureA \u003c/code\u003eand \u003ccode\u003efeatureB\u003c/code\u003e, branching out from \u003ccode\u003emaster \u003c/code\u003eon \u003ccode\u003eCommit1\u003c/code\u003e and \u003ccode\u003eCommit3\u003c/code\u003e, respectively (Figure 2). A CI job that produces and uploads cache artifacts to the central server finished its work only for commits 1, 2, and 4. For some reason, \u003ccode\u003eCommit3\u003c/code\u003e artifacts are not ready — either the build is in progress or it has failed. Developer A’s machine will reuse the artifacts generated for \u003ccode\u003eCommit1\u003c/code\u003e, while Developer B’s takes them from \u003ccode\u003eCommit2\u003c/code\u003e — it tried with \u003ccode\u003eCommit3\u003c/code\u003e, but they are not ready yet.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"440\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-250x157.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-768x483.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts.png 1205w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 2: Finding a commit with artifacts to reuse.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eWith that procedure, XCRemoteCache gets a strict list of input files almost for free.\u003c/p\u003e\u003cp\u003eAssuming we have an app split into several independent targets and local branches that don’t divert much from a primary branch, the caching hit rate will be high, minus only these targets that contain changes comparing the commit of which remote artifacts are used. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch3\u003eBuild artifacts portability\u003c/h3\u003e\n\n\n\n\u003cp\u003eAnother problem to consider is “build artifacts portability” between multiple machines. Several types of compilation output files include absolute paths, so for full compatibility, some kind of normalization is required. For iOS projects, such a step is required for \u003ccode\u003e.swiftmodule\u003c/code\u003e files and debug symbols. \u003c/p\u003e\n\n\n\n\u003cp\u003eProjects cloned to \u003ccode\u003e/dir1\u003c/code\u003e and \u003ccode\u003e/dir2\u003c/code\u003e generate \u003ccode\u003e.swiftmodule \u003c/code\u003efiles that do not match on a byte level. Swiftmodule represents an inter-module API (Swift counterparts to .h) that can be included in the list of fingerprinting files. To overcome falsy cache misses if two machines don’t have the same absolute source root paths, XCRemoteCache carries an extra fingerprint file (called a fingerprint override) next to the \u003ccode\u003e.swiftmodule \u003c/code\u003ethat includes a fingerprint of all the files used in the compilation step. A fingerprint override is path agnostic, so contrary to the \u003ccode\u003e.swiftmodule\u003c/code\u003e file, it can be used as a byte-level stable fingerprint of a Swift target. \u003c/p\u003e\n\n\n\n\u003cp\u003eDebug symbols, other path-sensitive files, are appended to the binary package to associate the machine code with the corresponding source location when debugging. The XCRemoteCache leverages support for LLDB runtime path rewrites using \u003ccode\u003esettings set target.source-map\u003c/code\u003e. Both producer and consumer pass \u003ccode\u003edebug-prefix-map\u003c/code\u003e parameters to the Swift and Clang compilers to align the source root of all debug symbols making the LLDB source mapping a simple, single-line command.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe Spotify story\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eAt Spotify, we have fast, well-optimized CI jobs that no longer slow down our development feedback loop (please head to \u003ca href=\"https://engineering.atspotify.com/2020/05/01/how-we-gave-superpowers-to-our-macos-ci/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHow We Gave Superpowers to Our macOS CI\u003c/a\u003e to read more about that), so we focused on applying XCRemoteCache on local machines. Keep in mind that the tool is able to work on CI machines and accelerate PR jobs, too.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn controlled conditions, XCRemoteCache was able to cut the very first iOS Spotify application build time by 85%. This was a great achievement, but in practice, developers introduce changes locally, and some targets have to be compiled locally. Our goal was to evaluate real-world scenarios to understand their true impact. To estimate that, we rolled out the remote cache to 50% of our developers for a week and compared all build metrics. \u003c/p\u003e\n\n\n\n\u003cp\u003eResults exceeded our expectations — we observed a huge improvement of the local build times: \u003cstrong\u003emedian clean build and incremental build times decreased by 70% and 6%, respectively. \u003c/strong\u003eWe classify builds as \u003cem\u003eclean\u003c/em\u003e when at least 50% of all targets compile at least one file.  Other builds that compile at least one file are \u003cem\u003eincremental\u003c/em\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, we enabled XCRemoteCache into our main application more than a year ago, and since then, it has worked flawlessly. We get very positive feedback from our developers and, as a side effect, we’ve seen the portion of clean builds almost double compared to the pre-remote cache times. Developers nowadays are twice more likely to rebase their working branches, eventually leading to fewer conflicts when creating a pull request. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow to integrate XCRemoteCache into your existing project\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that XCRemoteCache is open source, you can apply it to your own project with minimum effort. It supports multiple project setups, including the ones managed by CocoaPods, Carthage, or any other custom dependency management. Keep in mind, though, that for best results, your project should be split into several targets or you risk frequent cache invalidations, as described above.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eFor seamless integration, we are open sourcing a \u003ca href=\"https://github.com/spotify/XCRemoteCache/tree/master/cocoapods-plugin\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCocoaPods plugin\u003c/a\u003e and provide an automated script to modify the existing .xcodeproj project.\u003c/p\u003e\u003cp\u003eXCRemoteCache works with any HTTP server that supports PUT, HEAD, and GET requests. You are free to pick a server that works best for you, including the two popular storage options provided by Amazon S3 and Google’s Google Cloud Storage. We also provide a simple docker image that hosts a local server, perfect for the development phase.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWith that in place, you should be able to try out XCRemoteCache within minutes. For a list of integration steps, head to the \u003ca href=\"https://github.com/spotify/XCRemoteCache#how-to-integrate-xcremotecache-with-your-xcode-project\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eHow-to\u003c/em\u003e\u003c/a\u003e\u003cem\u003e \u003c/em\u003esection in the GitHub repo.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eContributing to XCRemoteCache\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe XCRemoteCache tool is written fully in Swift, so iOS developers can easily familiarize themselves with the codebase and potentially contribute to it. We tried to cover most of the common scenarios but, keeping in mind that Xcode projects may have very custom setups, some of them may not be compatible right now. Therefore, any inputs from the community, both raising issues or pull requests, are very welcome. We believe that, together, we will be able to move the project even further and support a wider audience of iOS developers. \u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to contribute to the codebase, make sure to check out our \u003ca href=\"https://github.com/spotify/XCRemoteCache/blob/master/docs/Development.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edevelopment\u003c/a\u003e guide. And if you want to work full-time on tools like that, please check out our \u003ca rel=\"noreferrer noopener\" href=\"https://www.lifeatspotify.com/jobs\" target=\"_blank\"\u003eopen job positions\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eI want to personally thank Erick Camacho for his help in preparing this blog post.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eXcode is a trademark of Apple Inc., registered in the U.S. and other countries.\u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003c/p\u003e\n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain. We are excited to be open sourcing X",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png",
      "date_published": "2021-11-16T00:00:00Z",
      "author": {
        "name": "Published by Bartosz Polaczyk, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/",
      "title": "\n                                            Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 16, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/\" title=\"Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are excited to be open sourcing \u003ca href=\"https://github.com/spotify/XCRemoteCache\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCRemoteCache\u003c/a\u003e, the library we created to mitigate long local builds. As the name suggests, this library is a remote caching implementation for iOS projects with an aim to reuse Xcode target artifacts generated on Continuous Integration (CI) machines. It supports Objective-C, Swift, and ObjC+Swift targets and can be easily integrated with existing Xcode projects, including ones managed by CocoaPods or Carthage.\u003c/p\u003e\n\n\n\n\u003cp\u003eBest of all, XCRemoteCache resulted in a \u003cstrong\u003e70% decrease in clean build times\u003c/strong\u003e (we classify a build as \u003cem\u003eclean\u003c/em\u003e when at least 50% of all targets compile at least one file).\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBackground\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUsing our Xcode build metrics (for more details on how this works, take a look at our open source \u003ca href=\"https://xcmetrics.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCMetrics project\u003c/a\u003e), we found out that it often takes our developers more than 10 minutes to build the main Spotify iOS application. Even though the number of these builds is relatively small (less than 3% of all builds), they take more than 50% of global building times (Figure 1). \u003c/p\u003e\n\n\n\n\u003cp\u003eAfter some investigation, it was revealed that long-lasting builds usually happen after rebasing or merging remote branches. Implementing a remote cache solution was the perfect fit.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"354\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-768x389.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times.png 1205w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: Distribution of local machines’ build times and their total build times.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eRemote cache principle\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eA remote cache is a popular technique to speed up builds of big applications by applying the “compile once, use everywhere” approach. As long as all input files and compilation parameters are the same, instead of building a target locally, one can download artifacts that were built and shared from some other machine. A key success factor for remote caching is finding an optimal caching level. Caching units that are too granular, where every single piece of the compilation step is cacheable, may lead to extensive network traffic overhead, which can offset CPU savings. On the other hand, putting the entire codebase into a single cacheable unit may significantly degrade the caching hit rate; every single local change invalidates remotely available cache artifacts, triggering a full build, locally.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe main Spotify iOS application is highly modularized and contains more than 400 independent modules configured as separate Xcode targets. Applying target-level caching was natural, and as we found out later, the right decision.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eDesigning the remote cache solution \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the design phase, our aim was to come up with a solution generic enough that it could be applied to a broad range of iOS applications with minimal or no project changes. That was an ambitious goal given how the Xcode build system works. Before going straight to the applied solution, let’s consider how an Xcode build actually works. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow does XCRemoteCache work?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn general, all caching mechanisms use a fingerprint of input files to recognize if build products can be reused. However, finding a precise set of these files is a nontrivial task. The Xcode build system is very liberal when it comes to dependency attribution. It tries to optimistically find a definition of the dependency (header files, \u003ccode\u003e.swiftmodule\u003c/code\u003e, etc.) in all available search paths — provided either by the developer in the Xcode project settings or in the current build product directory. As a result, developers don’t have to explicitly specify all dependencies a target uses, but just have to make sure that those dependencies will be placed in a correct location \u003cem\u003ebefore\u003c/em\u003e Xcode actually needs them. The fact that compilers are able to implicitly find required dependencies, hinders the simple fingerprint generation by hashing all available files in header and framework search paths — a list of files to consider in the fingerprinting would often be too broad.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the other hand, we observed that Xcode works quite well for local incremental builds and executes only a narrow subset of steps that were affected since the last build. In other words, the Xcode build system knows which files are the actual input files for the compilation, but that list is generated as compiler’s output (.d files) and is not available ahead of a compilation.\u003c/p\u003e\n\n\n\n\u003cp\u003eXCRemoteCache applies a unique approach to automatically identify all input files of the compilation based on Git history and dependency lists provided as a compilation output. The generation side, called \u003cem\u003eproducer mode\u003c/em\u003e, along with the compilation product, also uploads a meta file. That file contains a list of all compilation files the compiler used and the full SHA-1 (Secure Hash Algorithm 1) commit identifier it was built against. The producer mode should run on CI for each primary branch (like \u003ccode\u003emaster\u003c/code\u003e or \u003ccode\u003edevelop\u003c/code\u003e) commit, as a part of the post-merge phase.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the consumer side (aka \u003cem\u003econsumer mode\u003c/em\u003e), XCRemoteCache finds the most recent history commit for which the remote server contains build artifacts and builds a fingerprint based on input files provided in the meta file. Let’s imagine two developers, \u003ccode\u003eA\u003c/code\u003e and \u003ccode\u003eB\u003c/code\u003e, working on their local branches \u003ccode\u003efeatureA \u003c/code\u003eand \u003ccode\u003efeatureB\u003c/code\u003e, branching out from \u003ccode\u003emaster \u003c/code\u003eon \u003ccode\u003eCommit1\u003c/code\u003e and \u003ccode\u003eCommit3\u003c/code\u003e, respectively (Figure 2). A CI job that produces and uploads cache artifacts to the central server finished its work only for commits 1, 2, and 4. For some reason, \u003ccode\u003eCommit3\u003c/code\u003e artifacts are not ready — either the build is in progress or it has failed. Developer A’s machine will reuse the artifacts generated for \u003ccode\u003eCommit1\u003c/code\u003e, while Developer B’s takes them from \u003ccode\u003eCommit2\u003c/code\u003e — it tried with \u003ccode\u003eCommit3\u003c/code\u003e, but they are not ready yet.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"440\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-250x157.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-768x483.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts.png 1205w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 2: Finding a commit with artifacts to reuse.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eWith that procedure, XCRemoteCache gets a strict list of input files almost for free.\u003c/p\u003e\u003cp\u003eAssuming we have an app split into several independent targets and local branches that don’t divert much from a primary branch, the caching hit rate will be high, minus only these targets that contain changes comparing the commit of which remote artifacts are used. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch3\u003eBuild artifacts portability\u003c/h3\u003e\n\n\n\n\u003cp\u003eAnother problem to consider is “build artifacts portability” between multiple machines. Several types of compilation output files include absolute paths, so for full compatibility, some kind of normalization is required. For iOS projects, such a step is required for \u003ccode\u003e.swiftmodule\u003c/code\u003e files and debug symbols. \u003c/p\u003e\n\n\n\n\u003cp\u003eProjects cloned to \u003ccode\u003e/dir1\u003c/code\u003e and \u003ccode\u003e/dir2\u003c/code\u003e generate \u003ccode\u003e.swiftmodule \u003c/code\u003efiles that do not match on a byte level. Swiftmodule represents an inter-module API (Swift counterparts to .h) that can be included in the list of fingerprinting files. To overcome falsy cache misses if two machines don’t have the same absolute source root paths, XCRemoteCache carries an extra fingerprint file (called a fingerprint override) next to the \u003ccode\u003e.swiftmodule \u003c/code\u003ethat includes a fingerprint of all the files used in the compilation step. A fingerprint override is path agnostic, so contrary to the \u003ccode\u003e.swiftmodule\u003c/code\u003e file, it can be used as a byte-level stable fingerprint of a Swift target. \u003c/p\u003e\n\n\n\n\u003cp\u003eDebug symbols, other path-sensitive files, are appended to the binary package to associate the machine code with the corresponding source location when debugging. The XCRemoteCache leverages support for LLDB runtime path rewrites using \u003ccode\u003esettings set target.source-map\u003c/code\u003e. Both producer and consumer pass \u003ccode\u003edebug-prefix-map\u003c/code\u003e parameters to the Swift and Clang compilers to align the source root of all debug symbols making the LLDB source mapping a simple, single-line command.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe Spotify story\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eAt Spotify, we have fast, well-optimized CI jobs that no longer slow down our development feedback loop (please head to \u003ca href=\"https://engineering.atspotify.com/2020/05/01/how-we-gave-superpowers-to-our-macos-ci/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHow We Gave Superpowers to Our macOS CI\u003c/a\u003e to read more about that), so we focused on applying XCRemoteCache on local machines. Keep in mind that the tool is able to work on CI machines and accelerate PR jobs, too.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn controlled conditions, XCRemoteCache was able to cut the very first iOS Spotify application build time by 85%. This was a great achievement, but in practice, developers introduce changes locally, and some targets have to be compiled locally. Our goal was to evaluate real-world scenarios to understand their true impact. To estimate that, we rolled out the remote cache to 50% of our developers for a week and compared all build metrics. \u003c/p\u003e\n\n\n\n\u003cp\u003eResults exceeded our expectations — we observed a huge improvement of the local build times: \u003cstrong\u003emedian clean build and incremental build times decreased by 70% and 6%, respectively. \u003c/strong\u003eWe classify builds as \u003cem\u003eclean\u003c/em\u003e when at least 50% of all targets compile at least one file.  Other builds that compile at least one file are \u003cem\u003eincremental\u003c/em\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, we enabled XCRemoteCache into our main application more than a year ago, and since then, it has worked flawlessly. We get very positive feedback from our developers and, as a side effect, we’ve seen the portion of clean builds almost double compared to the pre-remote cache times. Developers nowadays are twice more likely to rebase their working branches, eventually leading to fewer conflicts when creating a pull request. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow to integrate XCRemoteCache into your existing project\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that XCRemoteCache is open source, you can apply it to your own project with minimum effort. It supports multiple project setups, including the ones managed by CocoaPods, Carthage, or any other custom dependency management. Keep in mind, though, that for best results, your project should be split into several targets or you risk frequent cache invalidations, as described above.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eFor seamless integration, we are open sourcing a \u003ca href=\"https://github.com/spotify/XCRemoteCache/tree/master/cocoapods-plugin\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCocoaPods plugin\u003c/a\u003e and provide an automated script to modify the existing .xcodeproj project.\u003c/p\u003e\u003cp\u003eXCRemoteCache works with any HTTP server that supports PUT, HEAD, and GET requests. You are free to pick a server that works best for you, including the two popular storage options provided by Amazon S3 and Google’s Google Cloud Storage. We also provide a simple docker image that hosts a local server, perfect for the development phase.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWith that in place, you should be able to try out XCRemoteCache within minutes. For a list of integration steps, head to the \u003ca href=\"https://github.com/spotify/XCRemoteCache#how-to-integrate-xcremotecache-with-your-xcode-project\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eHow-to\u003c/em\u003e\u003c/a\u003e\u003cem\u003e \u003c/em\u003esection in the GitHub repo.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eContributing to XCRemoteCache\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe XCRemoteCache tool is written fully in Swift, so iOS developers can easily familiarize themselves with the codebase and potentially contribute to it. We tried to cover most of the common scenarios but, keeping in mind that Xcode projects may have very custom setups, some of them may not be compatible right now. Therefore, any inputs from the community, both raising issues or pull requests, are very welcome. We believe that, together, we will be able to move the project even further and support a wider audience of iOS developers. \u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to contribute to the codebase, make sure to check out our \u003ca href=\"https://github.com/spotify/XCRemoteCache/blob/master/docs/Development.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edevelopment\u003c/a\u003e guide. And if you want to work full-time on tools like that, please check out our \u003ca rel=\"noreferrer noopener\" href=\"https://www.lifeatspotify.com/jobs\" target=\"_blank\"\u003eopen job positions\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eI want to personally thank Erick Camacho for his help in preparing this blog post.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eXcode is a trademark of Apple Inc., registered in the U.S. and other countries.\u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain. We are excited to be open sourcing X",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png",
      "date_published": "2021-11-16T00:00:00Z",
      "author": {
        "name": "Published by Bartosz Polaczyk, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/11/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/",
      "title": "\n                                            Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 16, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/11/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/\" title=\"Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are excited to be open sourcing \u003ca href=\"https://github.com/spotify/XCRemoteCache\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCRemoteCache\u003c/a\u003e, the library we created to mitigate long local builds. As the name suggests, this library is a remote caching implementation for iOS projects with an aim to reuse Xcode target artifacts generated on Continuous Integration (CI) machines. It supports Objective-C, Swift, and ObjC+Swift targets and can be easily integrated with existing Xcode projects, including ones managed by CocoaPods or Carthage.\u003c/p\u003e\n\n\n\n\u003cp\u003eBest of all, XCRemoteCache resulted in a \u003cstrong\u003e70% decrease in clean build times\u003c/strong\u003e (we classify a build as \u003cem\u003eclean\u003c/em\u003e when at least 50% of all targets compile at least one file).\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBackground\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUsing our Xcode build metrics (for more details on how this works, take a look at our open source \u003ca href=\"https://xcmetrics.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCMetrics project\u003c/a\u003e), we found out that it often takes our developers more than 10 minutes to build the main Spotify iOS application. Even though the number of these builds is relatively small (less than 3% of all builds), they take more than 50% of global building times (Figure 1). \u003c/p\u003e\n\n\n\n\u003cp\u003eAfter some investigation, it was revealed that long-lasting builds usually happen after rebasing or merging remote branches. Implementing a remote cache solution was the perfect fit.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"354\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-768x389.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times.png 1205w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: Distribution of local machines’ build times and their total build times.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eRemote cache principle\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eA remote cache is a popular technique to speed up builds of big applications by applying the “compile once, use everywhere” approach. As long as all input files and compilation parameters are the same, instead of building a target locally, one can download artifacts that were built and shared from some other machine. A key success factor for remote caching is finding an optimal caching level. Caching units that are too granular, where every single piece of the compilation step is cacheable, may lead to extensive network traffic overhead, which can offset CPU savings. On the other hand, putting the entire codebase into a single cacheable unit may significantly degrade the caching hit rate; every single local change invalidates remotely available cache artifacts, triggering a full build, locally.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe main Spotify iOS application is highly modularized and contains more than 400 independent modules configured as separate Xcode targets. Applying target-level caching was natural, and as we found out later, the right decision.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eDesigning the remote cache solution \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the design phase, our aim was to come up with a solution generic enough that it could be applied to a broad range of iOS applications with minimal or no project changes. That was an ambitious goal given how the Xcode build system works. Before going straight to the applied solution, let’s consider how an Xcode build actually works. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow does XCRemoteCache work?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn general, all caching mechanisms use a fingerprint of input files to recognize if build products can be reused. However, finding a precise set of these files is a nontrivial task. The Xcode build system is very liberal when it comes to dependency attribution. It tries to optimistically find a definition of the dependency (header files, \u003ccode\u003e.swiftmodule\u003c/code\u003e, etc.) in all available search paths — provided either by the developer in the Xcode project settings or in the current build product directory. As a result, developers don’t have to explicitly specify all dependencies a target uses, but just have to make sure that those dependencies will be placed in a correct location \u003cem\u003ebefore\u003c/em\u003e Xcode actually needs them. The fact that compilers are able to implicitly find required dependencies, hinders the simple fingerprint generation by hashing all available files in header and framework search paths — a list of files to consider in the fingerprinting would often be too broad.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the other hand, we observed that Xcode works quite well for local incremental builds and executes only a narrow subset of steps that were affected since the last build. In other words, the Xcode build system knows which files are the actual input files for the compilation, but that list is generated as compiler’s output (.d files) and is not available ahead of a compilation.\u003c/p\u003e\n\n\n\n\u003cp\u003eXCRemoteCache applies a unique approach to automatically identify all input files of the compilation based on Git history and dependency lists provided as a compilation output. The generation side, called \u003cem\u003eproducer mode\u003c/em\u003e, along with the compilation product, also uploads a meta file. That file contains a list of all compilation files the compiler used and the full SHA-1 (Secure Hash Algorithm 1) commit identifier it was built against. The producer mode should run on CI for each primary branch (like \u003ccode\u003emaster\u003c/code\u003e or \u003ccode\u003edevelop\u003c/code\u003e) commit, as a part of the post-merge phase.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the consumer side (aka \u003cem\u003econsumer mode\u003c/em\u003e), XCRemoteCache finds the most recent history commit for which the remote server contains build artifacts and builds a fingerprint based on input files provided in the meta file. Let’s imagine two developers, \u003ccode\u003eA\u003c/code\u003e and \u003ccode\u003eB\u003c/code\u003e, working on their local branches \u003ccode\u003efeatureA \u003c/code\u003eand \u003ccode\u003efeatureB\u003c/code\u003e, branching out from \u003ccode\u003emaster \u003c/code\u003eon \u003ccode\u003eCommit1\u003c/code\u003e and \u003ccode\u003eCommit3\u003c/code\u003e, respectively (Figure 2). A CI job that produces and uploads cache artifacts to the central server finished its work only for commits 1, 2, and 4. For some reason, \u003ccode\u003eCommit3\u003c/code\u003e artifacts are not ready — either the build is in progress or it has failed. Developer A’s machine will reuse the artifacts generated for \u003ccode\u003eCommit1\u003c/code\u003e, while Developer B’s takes them from \u003ccode\u003eCommit2\u003c/code\u003e — it tried with \u003ccode\u003eCommit3\u003c/code\u003e, but they are not ready yet.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"440\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-250x157.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-768x483.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts.png 1205w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 2: Finding a commit with artifacts to reuse.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eWith that procedure, XCRemoteCache gets a strict list of input files almost for free.\u003c/p\u003e\u003cp\u003eAssuming we have an app split into several independent targets and local branches that don’t divert much from a primary branch, the caching hit rate will be high, minus only these targets that contain changes comparing the commit of which remote artifacts are used. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch3\u003eBuild artifacts portability\u003c/h3\u003e\n\n\n\n\u003cp\u003eAnother problem to consider is “build artifacts portability” between multiple machines. Several types of compilation output files include absolute paths, so for full compatibility, some kind of normalization is required. For iOS projects, such a step is required for \u003ccode\u003e.swiftmodule\u003c/code\u003e files and debug symbols. \u003c/p\u003e\n\n\n\n\u003cp\u003eProjects cloned to \u003ccode\u003e/dir1\u003c/code\u003e and \u003ccode\u003e/dir2\u003c/code\u003e generate \u003ccode\u003e.swiftmodule \u003c/code\u003efiles that do not match on a byte level. Swiftmodule represents an inter-module API (Swift counterparts to .h) that can be included in the list of fingerprinting files. To overcome falsy cache misses if two machines don’t have the same absolute source root paths, XCRemoteCache carries an extra fingerprint file (called a fingerprint override) next to the \u003ccode\u003e.swiftmodule \u003c/code\u003ethat includes a fingerprint of all the files used in the compilation step. A fingerprint override is path agnostic, so contrary to the \u003ccode\u003e.swiftmodule\u003c/code\u003e file, it can be used as a byte-level stable fingerprint of a Swift target. \u003c/p\u003e\n\n\n\n\u003cp\u003eDebug symbols, other path-sensitive files, are appended to the binary package to associate the machine code with the corresponding source location when debugging. The XCRemoteCache leverages support for LLDB runtime path rewrites using \u003ccode\u003esettings set target.source-map\u003c/code\u003e. Both producer and consumer pass \u003ccode\u003edebug-prefix-map\u003c/code\u003e parameters to the Swift and Clang compilers to align the source root of all debug symbols making the LLDB source mapping a simple, single-line command.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe Spotify story\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eAt Spotify, we have fast, well-optimized CI jobs that no longer slow down our development feedback loop (please head to \u003ca href=\"https://engineering.atspotify.com/2020/05/01/how-we-gave-superpowers-to-our-macos-ci/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHow We Gave Superpowers to Our macOS CI\u003c/a\u003e to read more about that), so we focused on applying XCRemoteCache on local machines. Keep in mind that the tool is able to work on CI machines and accelerate PR jobs, too.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn controlled conditions, XCRemoteCache was able to cut the very first iOS Spotify application build time by 85%. This was a great achievement, but in practice, developers introduce changes locally, and some targets have to be compiled locally. Our goal was to evaluate real-world scenarios to understand their true impact. To estimate that, we rolled out the remote cache to 50% of our developers for a week and compared all build metrics. \u003c/p\u003e\n\n\n\n\u003cp\u003eResults exceeded our expectations — we observed a huge improvement of the local build times: \u003cstrong\u003emedian clean build and incremental build times decreased by 70% and 6%, respectively. \u003c/strong\u003eWe classify builds as \u003cem\u003eclean\u003c/em\u003e when at least 50% of all targets compile at least one file.  Other builds that compile at least one file are \u003cem\u003eincremental\u003c/em\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, we enabled XCRemoteCache into our main application more than a year ago, and since then, it has worked flawlessly. We get very positive feedback from our developers and, as a side effect, we’ve seen the portion of clean builds almost double compared to the pre-remote cache times. Developers nowadays are twice more likely to rebase their working branches, eventually leading to fewer conflicts when creating a pull request. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow to integrate XCRemoteCache into your existing project\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that XCRemoteCache is open source, you can apply it to your own project with minimum effort. It supports multiple project setups, including the ones managed by CocoaPods, Carthage, or any other custom dependency management. Keep in mind, though, that for best results, your project should be split into several targets or you risk frequent cache invalidations, as described above.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eFor seamless integration, we are open sourcing a \u003ca href=\"https://github.com/spotify/XCRemoteCache/tree/master/cocoapods-plugin\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCocoaPods plugin\u003c/a\u003e and provide an automated script to modify the existing .xcodeproj project.\u003c/p\u003e\u003cp\u003eXCRemoteCache works with any HTTP server that supports PUT, HEAD, and GET requests. You are free to pick a server that works best for you, including the two popular storage options provided by Amazon S3 and Google’s Google Cloud Storage. We also provide a simple docker image that hosts a local server, perfect for the development phase.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWith that in place, you should be able to try out XCRemoteCache within minutes. For a list of integration steps, head to the \u003ca href=\"https://github.com/spotify/XCRemoteCache#how-to-integrate-xcremotecache-with-your-xcode-project\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eHow-to\u003c/em\u003e\u003c/a\u003e\u003cem\u003e \u003c/em\u003esection in the GitHub repo.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eContributing to XCRemoteCache\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe XCRemoteCache tool is written fully in Swift, so iOS developers can easily familiarize themselves with the codebase and potentially contribute to it. We tried to cover most of the common scenarios but, keeping in mind that Xcode projects may have very custom setups, some of them may not be compatible right now. Therefore, any inputs from the community, both raising issues or pull requests, are very welcome. We believe that, together, we will be able to move the project even further and support a wider audience of iOS developers. \u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to contribute to the codebase, make sure to check out our \u003ca href=\"https://github.com/spotify/XCRemoteCache/blob/master/docs/Development.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edevelopment\u003c/a\u003e guide. And if you want to work full-time on tools like that, please check out our \u003ca rel=\"noreferrer noopener\" href=\"https://www.lifeatspotify.com/jobs\" target=\"_blank\"\u003eopen job positions\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eI want to personally thank Erick Camacho for his help in preparing this blog post.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eXcode is a trademark of Apple Inc., registered in the U.S. and other countries.\u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain. We are excited to be open sourcing X",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png",
      "date_published": "2021-11-16T00:00:00Z",
      "author": {
        "name": "Published by Bartosz Polaczyk, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/",
      "title": "\n                                            Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 16, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/introducing-xcremotecache-the-ios-remote-caching-tool-that-cut-our-clean-build-times-by-70/\" title=\"Introducing XCRemoteCache: The iOS Remote Caching Tool that Cut Our Clean Build Times by 70%\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are excited to be open sourcing \u003ca href=\"https://github.com/spotify/XCRemoteCache\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCRemoteCache\u003c/a\u003e, the library we created to mitigate long local builds. As the name suggests, this library is a remote caching implementation for iOS projects with an aim to reuse Xcode target artifacts generated on Continuous Integration (CI) machines. It supports Objective-C, Swift, and ObjC+Swift targets and can be easily integrated with existing Xcode projects, including ones managed by CocoaPods or Carthage.\u003c/p\u003e\n\n\n\n\u003cp\u003eBest of all, XCRemoteCache resulted in a \u003cstrong\u003e70% decrease in clean build times\u003c/strong\u003e (we classify a build as \u003cem\u003eclean\u003c/em\u003e when at least 50% of all targets compile at least one file).\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBackground\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUsing our Xcode build metrics (for more details on how this works, take a look at our open source \u003ca href=\"https://xcmetrics.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCMetrics project\u003c/a\u003e), we found out that it often takes our developers more than 10 minutes to build the main Spotify iOS application. Even though the number of these builds is relatively small (less than 3% of all builds), they take more than 50% of global building times (Figure 1). \u003c/p\u003e\n\n\n\n\u003cp\u003eAfter some investigation, it was revealed that long-lasting builds usually happen after rebasing or merging remote branches. Implementing a remote cache solution was the perfect fit.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"354\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-700x354.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-768x389.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/BuildsBuild-Times.png 1205w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: Distribution of local machines’ build times and their total build times.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eRemote cache principle\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eA remote cache is a popular technique to speed up builds of big applications by applying the “compile once, use everywhere” approach. As long as all input files and compilation parameters are the same, instead of building a target locally, one can download artifacts that were built and shared from some other machine. A key success factor for remote caching is finding an optimal caching level. Caching units that are too granular, where every single piece of the compilation step is cacheable, may lead to extensive network traffic overhead, which can offset CPU savings. On the other hand, putting the entire codebase into a single cacheable unit may significantly degrade the caching hit rate; every single local change invalidates remotely available cache artifacts, triggering a full build, locally.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe main Spotify iOS application is highly modularized and contains more than 400 independent modules configured as separate Xcode targets. Applying target-level caching was natural, and as we found out later, the right decision.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eDesigning the remote cache solution \u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the design phase, our aim was to come up with a solution generic enough that it could be applied to a broad range of iOS applications with minimal or no project changes. That was an ambitious goal given how the Xcode build system works. Before going straight to the applied solution, let’s consider how an Xcode build actually works. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow does XCRemoteCache work?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn general, all caching mechanisms use a fingerprint of input files to recognize if build products can be reused. However, finding a precise set of these files is a nontrivial task. The Xcode build system is very liberal when it comes to dependency attribution. It tries to optimistically find a definition of the dependency (header files, \u003ccode\u003e.swiftmodule\u003c/code\u003e, etc.) in all available search paths — provided either by the developer in the Xcode project settings or in the current build product directory. As a result, developers don’t have to explicitly specify all dependencies a target uses, but just have to make sure that those dependencies will be placed in a correct location \u003cem\u003ebefore\u003c/em\u003e Xcode actually needs them. The fact that compilers are able to implicitly find required dependencies, hinders the simple fingerprint generation by hashing all available files in header and framework search paths — a list of files to consider in the fingerprinting would often be too broad.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the other hand, we observed that Xcode works quite well for local incremental builds and executes only a narrow subset of steps that were affected since the last build. In other words, the Xcode build system knows which files are the actual input files for the compilation, but that list is generated as compiler’s output (.d files) and is not available ahead of a compilation.\u003c/p\u003e\n\n\n\n\u003cp\u003eXCRemoteCache applies a unique approach to automatically identify all input files of the compilation based on Git history and dependency lists provided as a compilation output. The generation side, called \u003cem\u003eproducer mode\u003c/em\u003e, along with the compilation product, also uploads a meta file. That file contains a list of all compilation files the compiler used and the full SHA-1 (Secure Hash Algorithm 1) commit identifier it was built against. The producer mode should run on CI for each primary branch (like \u003ccode\u003emaster\u003c/code\u003e or \u003ccode\u003edevelop\u003c/code\u003e) commit, as a part of the post-merge phase.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the consumer side (aka \u003cem\u003econsumer mode\u003c/em\u003e), XCRemoteCache finds the most recent history commit for which the remote server contains build artifacts and builds a fingerprint based on input files provided in the meta file. Let’s imagine two developers, \u003ccode\u003eA\u003c/code\u003e and \u003ccode\u003eB\u003c/code\u003e, working on their local branches \u003ccode\u003efeatureA \u003c/code\u003eand \u003ccode\u003efeatureB\u003c/code\u003e, branching out from \u003ccode\u003emaster \u003c/code\u003eon \u003ccode\u003eCommit1\u003c/code\u003e and \u003ccode\u003eCommit3\u003c/code\u003e, respectively (Figure 2). A CI job that produces and uploads cache artifacts to the central server finished its work only for commits 1, 2, and 4. For some reason, \u003ccode\u003eCommit3\u003c/code\u003e artifacts are not ready — either the build is in progress or it has failed. Developer A’s machine will reuse the artifacts generated for \u003ccode\u003eCommit1\u003c/code\u003e, while Developer B’s takes them from \u003ccode\u003eCommit2\u003c/code\u003e — it tried with \u003ccode\u003eCommit3\u003c/code\u003e, but they are not ready yet.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"440\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-700x440.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-250x157.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-768x483.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Commit_Artifacts.png 1205w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 2: Finding a commit with artifacts to reuse.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eWith that procedure, XCRemoteCache gets a strict list of input files almost for free.\u003c/p\u003e\u003cp\u003eAssuming we have an app split into several independent targets and local branches that don’t divert much from a primary branch, the caching hit rate will be high, minus only these targets that contain changes comparing the commit of which remote artifacts are used. \u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch3\u003eBuild artifacts portability\u003c/h3\u003e\n\n\n\n\u003cp\u003eAnother problem to consider is “build artifacts portability” between multiple machines. Several types of compilation output files include absolute paths, so for full compatibility, some kind of normalization is required. For iOS projects, such a step is required for \u003ccode\u003e.swiftmodule\u003c/code\u003e files and debug symbols. \u003c/p\u003e\n\n\n\n\u003cp\u003eProjects cloned to \u003ccode\u003e/dir1\u003c/code\u003e and \u003ccode\u003e/dir2\u003c/code\u003e generate \u003ccode\u003e.swiftmodule \u003c/code\u003efiles that do not match on a byte level. Swiftmodule represents an inter-module API (Swift counterparts to .h) that can be included in the list of fingerprinting files. To overcome falsy cache misses if two machines don’t have the same absolute source root paths, XCRemoteCache carries an extra fingerprint file (called a fingerprint override) next to the \u003ccode\u003e.swiftmodule \u003c/code\u003ethat includes a fingerprint of all the files used in the compilation step. A fingerprint override is path agnostic, so contrary to the \u003ccode\u003e.swiftmodule\u003c/code\u003e file, it can be used as a byte-level stable fingerprint of a Swift target. \u003c/p\u003e\n\n\n\n\u003cp\u003eDebug symbols, other path-sensitive files, are appended to the binary package to associate the machine code with the corresponding source location when debugging. The XCRemoteCache leverages support for LLDB runtime path rewrites using \u003ccode\u003esettings set target.source-map\u003c/code\u003e. Both producer and consumer pass \u003ccode\u003edebug-prefix-map\u003c/code\u003e parameters to the Swift and Clang compilers to align the source root of all debug symbols making the LLDB source mapping a simple, single-line command.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eThe Spotify story\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eAt Spotify, we have fast, well-optimized CI jobs that no longer slow down our development feedback loop (please head to \u003ca href=\"https://engineering.atspotify.com/2020/05/01/how-we-gave-superpowers-to-our-macos-ci/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eHow We Gave Superpowers to Our macOS CI\u003c/a\u003e to read more about that), so we focused on applying XCRemoteCache on local machines. Keep in mind that the tool is able to work on CI machines and accelerate PR jobs, too.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn controlled conditions, XCRemoteCache was able to cut the very first iOS Spotify application build time by 85%. This was a great achievement, but in practice, developers introduce changes locally, and some targets have to be compiled locally. Our goal was to evaluate real-world scenarios to understand their true impact. To estimate that, we rolled out the remote cache to 50% of our developers for a week and compared all build metrics. \u003c/p\u003e\n\n\n\n\u003cp\u003eResults exceeded our expectations — we observed a huge improvement of the local build times: \u003cstrong\u003emedian clean build and incremental build times decreased by 70% and 6%, respectively. \u003c/strong\u003eWe classify builds as \u003cem\u003eclean\u003c/em\u003e when at least 50% of all targets compile at least one file.  Other builds that compile at least one file are \u003cem\u003eincremental\u003c/em\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, we enabled XCRemoteCache into our main application more than a year ago, and since then, it has worked flawlessly. We get very positive feedback from our developers and, as a side effect, we’ve seen the portion of clean builds almost double compared to the pre-remote cache times. Developers nowadays are twice more likely to rebase their working branches, eventually leading to fewer conflicts when creating a pull request. \u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eHow to integrate XCRemoteCache into your existing project\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that XCRemoteCache is open source, you can apply it to your own project with minimum effort. It supports multiple project setups, including the ones managed by CocoaPods, Carthage, or any other custom dependency management. Keep in mind, though, that for best results, your project should be split into several targets or you risk frequent cache invalidations, as described above.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eFor seamless integration, we are open sourcing a \u003ca href=\"https://github.com/spotify/XCRemoteCache/tree/master/cocoapods-plugin\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCocoaPods plugin\u003c/a\u003e and provide an automated script to modify the existing .xcodeproj project.\u003c/p\u003e\u003cp\u003eXCRemoteCache works with any HTTP server that supports PUT, HEAD, and GET requests. You are free to pick a server that works best for you, including the two popular storage options provided by Amazon S3 and Google’s Google Cloud Storage. We also provide a simple docker image that hosts a local server, perfect for the development phase.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWith that in place, you should be able to try out XCRemoteCache within minutes. For a list of integration steps, head to the \u003ca href=\"https://github.com/spotify/XCRemoteCache#how-to-integrate-xcremotecache-with-your-xcode-project\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eHow-to\u003c/em\u003e\u003c/a\u003e\u003cem\u003e \u003c/em\u003esection in the GitHub repo.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eContributing to XCRemoteCache\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe XCRemoteCache tool is written fully in Swift, so iOS developers can easily familiarize themselves with the codebase and potentially contribute to it. We tried to cover most of the common scenarios but, keeping in mind that Xcode projects may have very custom setups, some of them may not be compatible right now. Therefore, any inputs from the community, both raising issues or pull requests, are very welcome. We believe that, together, we will be able to move the project even further and support a wider audience of iOS developers. \u003c/p\u003e\n\n\n\n\u003cp\u003eIf you want to contribute to the codebase, make sure to check out our \u003ca href=\"https://github.com/spotify/XCRemoteCache/blob/master/docs/Development.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edevelopment\u003c/a\u003e guide. And if you want to work full-time on tools like that, please check out our \u003ca rel=\"noreferrer noopener\" href=\"https://www.lifeatspotify.com/jobs\" target=\"_blank\"\u003eopen job positions\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eI want to personally thank Erick Camacho for his help in preparing this blog post.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eXcode is a trademark of Apple Inc., registered in the U.S. and other countries.\u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, we constantly work on creating the best developer experience possible for our iOS engineers. Improving build times is one of the most common requests for infrastructure teams and, as such, we constantly seek to improve our infrastructure toolchain. We are excited to be open sourcing X",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header.png",
      "date_published": "2021-11-16T00:00:00Z",
      "author": {
        "name": "Published by Bartosz Polaczyk, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/",
      "title": "\n                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 15, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\" title=\"The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the lessons we’ve learned from building the ML stack that serves these models.\u003c/p\u003e\n\n\n\n\u003cp\u003eMachine learning is central to how we personalize the Home page user experience and connect listeners to the creators that are most relevant to them. Like many recommendation systems, the \u003ca href=\"https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/\"\u003eSpotify Home page recommendations are powered\u003c/a\u003e by two stages: \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStage 1: Candidate generation:\u003c/strong\u003e The best albums, playlists, artists, and podcasts are selected for each listener.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStage 2: Ranking:\u003c/strong\u003e Candidates are ranked in the best order for each listener.  \u003c/p\u003e\n\n\n\n\u003cp\u003eIn Part I of this series, we’ll focus on the first stage — the machine learning solutions we’ve built to personalize the content for listeners’ Home pages and, specifically, the lessons we’ve learned in building, experimenting, and deploying these models. \u003c/p\u003e\n\n\n\n\u003ch2\u003eHome @ Spotify\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png\" alt=\"\" width=\"303\" height=\"334\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-250x276.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-768x847.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-1393x1536.png 1393w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-120x132.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home.png 1642w\" sizes=\"(max-width: 303px) 100vw, 303px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe Home page consists of cards — the square items that represent an album, playlist, etc. — and shelves — the horizontal rows that contain multiple cards. We generate personalized content for listeners’ Home pages, algorithmically curating the music and podcasts that are shown to listeners in the shelves on Home. Some content is generated via heuristics and rules and some content is manually curated by editors, while other content is generated via predictions using trained models. We currently have a number of models running in production, each one powering content curation for a different shelf, but we will be discussing three of those models in this post, including: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThe Podcast Model:\u003c/strong\u003e Predicts podcasts a listener is likely to listen to in the \u003cem\u003eShows you might like\u003c/em\u003e shelf. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Shortcuts Model: \u003c/strong\u003ePredicts the listener’s next familiar listen in the Shortcuts feature. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Playlists Model: \u003c/strong\u003ePredicts the playlists a new listener is likely to listen to in the \u003cem\u003eTry something else\u003c/em\u003e shelf.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png\" alt=\"\" width=\"411\" height=\"375\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-250x228.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-768x700.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-1536x1400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-120x109.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2.png 1999w\" sizes=\"(max-width: 411px) 100vw, 411px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSince we launched our first model to recommend content on Home, we have worked to improve our ML stack and processes in order to experiment and productionize models more quickly and reliably.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe road to simplicity and automation\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs anyone who may have contributed to operationalizing an ML model knows, moving a model from experimentation to production is no easy feat. There are numerous challenges in managing the data that goes into a model, running and tracking experiments, and monitoring and retraining models. While we have always tried to keep our ML infrastructure simple, and as close to the sources of features as possible, it has become drastically easier for our squads to deploy and maintain models now than when we started.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt a high level, an ML workflow can be broken down into three main phases: 1) data management, 2) experimentation, and 3) operationalization.  \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png\" alt=\"\" width=\"784\" height=\"164\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-250x52.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-768x160.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-120x25.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow.png 1351w\" sizes=\"(max-width: 784px) 100vw, 784px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIt’s common to iteratively work on the training and evaluation phase until a final model version is selected as the best. This model is then deployed to production systems and can start making predictions for listeners. Similar to most production systems, models (and the services/pipelines that serve them) should be monitored closely. To keep a model up to date (which is more important for some tasks than others; more to come on this), retraining and model versioning are the last steps in our workflow. This part of our stack and workflow has had significant changes since our first model — making batch predictions (offline) of content listeners are likely to stream — to now, where all our models are served in real time. The figure below shows where our machine learning stack started and where we are now:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"533\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going.png 1012w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eOur current ML stack automates a lot of the processes involved in maintaining models in production (with online serving): we have automated feature logging instrumented in our serving infrastructure, with both scheduled Scio pipelines to transform these features and Kubeflow pipelines to retrain weekly. We have also implemented data validation of our training and serving features (as well as validation between subsequent training datasets) to verify our features are consistent and follow the same distributions at training and inference times. In our Kubeflow pipelines, we have components that check the evaluation score and automatically push the model to production if the score is above our threshold. With this stack, we monitor and alert on the automatic data validation pipeline, as well as the online deployments of our models — allowing us to handle any issues as soon as they arise.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith a lot of effort and many lessons learned, our ML stack has evolved to make these processes automated and more reliable, enabling us to iterate faster to improve our models and increase our engineering productivity. \u003c/p\u003e\n\n\n\n\u003ch2\u003eHow we unified training and serving data\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we first start to think about a problem, we always dig into the data first — what data would be useful? What data is available? And then we take a really close look at the data that will be used for features, characterizing what is in the dataset and identifying the edge cases in the data. We feel fairly confident about the contents of the data used for our training features as well as what the transformed data looks like, but features fetched and transformed at \u003cem\u003eserving\u003c/em\u003e time are an entirely different story. \u003c/p\u003e\n\n\n\n\u003ch3\u003eBatch training data and batch predictions\u003c/h3\u003e\n\n\n\n\u003cp\u003eHistorically, we have had one set of infrastructure for fetching and transforming features during experimentation (training) and a different set of infrastructure for fetching and transforming features for making predictions (serving). \u003c/p\u003e\n\n\n\n\u003ch3\u003eThen we started to make online predictions (… with the wrong data)\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhen we changed the Podcast Model from making batch offline predictions to serving in real time, we set up a new service that could support this — this new service had to fetch and transform features, make the prediction, and respond to the request. The important part here is that the feature processing and transformation was now in a different place than where the corresponding training feature processing took place. And, unfortunately, models are like black boxes, so testing the output is difficult, if not impossible. A while ago, we discovered that we had been transforming one of the model’s features slightly differently at training time than at serving time, leading to potentially degraded recommendations — and there was no way to detect this, so it continued to happen for four months.Think about this for just a second. Such a simple part of our stack — at most, a few lines of code — was doing the wrong thing and impacted the recommendations produced by our model. Our short-term fix was to simply change the one line of code in our prediction service that was causing the issue, but we knew long term that we needed to either have a single source of data for both training and serving, or we needed to ensure that data was produced and transformed the same way in both stages.\u003c/p\u003e\n\n\n\n\u003ch3\u003eOne transformation implementation to rule them all\u003c/h3\u003e\n\n\n\n\u003cp\u003eOur first approach was to make any feature processing and transformation occur in the same code path, so that training and serving features would be processed identically. Taking the Shortcuts Model as an example again, our goal was to get rid of the Python service that transformed training features — this service was always running and constantly checking, on all days, to see if it was a Monday; if so, then it would request data from the necessary service (at a rate-limited 5 requests/second) and transform them into features; ideally, this would have been implemented as a pipeline, but we couldn’t schedule and orchestrate it because the process took more than 24 hours. There were many reasons we wanted to migrate away from this approach, but logging features when the only data source for features is a different service (owned by a different squad) proved difficult. Using our serving infrastructure’s feature logging capabilities, we could automatically log already transformed features, which could later be used for training. At this point, all of our features for training and serving were being transformed by code in the Java service. And we now use this feature logging for all of our models both to solve this problem, and also because it reduces the amount of additional infrastructure we need to support.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eBut wait, we can do more by validating our data\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe second approach we took to ensure our training and serving features did not differ was to use Tensorflow Data Validation (TFDV) to compare training and serving data schemas and feature distributions on a daily basis. The alerting we have added to our data validation pipeline allows us to detect significant differences in our feature sets — it uses the Chebyshev distance metric, which compares the distance between two vectors, and can help alert us to drift in training and serving features.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we knew that understanding what is in our data is crucial, we quickly learned that it’s easy to make mistakes when moving models to production because the data often uses a different processing library. We didn’t expect many data differences, but validating and alerting on issues lets us know if something changed, and how we should remediate the issue.\u003c/p\u003e\n\n\n\n\u003cp\u003eStay tuned for Part II as we take a closer look at how we evaluate our models using offline and online metrics, why it’s so important to actually look at the recommendations we are making, and the challenges we faced in our journey to CI/CD in model retraining. \u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the les",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png",
      "date_published": "2021-11-15T00:00:00Z",
      "author": {
        "name": "Published by Annie Edmundson, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/11/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/",
      "title": "\n                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 15, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/11/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\" title=\"The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the lessons we’ve learned from building the ML stack that serves these models.\u003c/p\u003e\n\n\n\n\u003cp\u003eMachine learning is central to how we personalize the Home page user experience and connect listeners to the creators that are most relevant to them. Like many recommendation systems, the \u003ca href=\"https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/\"\u003eSpotify Home page recommendations are powered\u003c/a\u003e by two stages: \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStage 1: Candidate generation:\u003c/strong\u003e The best albums, playlists, artists, and podcasts are selected for each listener.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStage 2: Ranking:\u003c/strong\u003e Candidates are ranked in the best order for each listener.  \u003c/p\u003e\n\n\n\n\u003cp\u003eIn Part I of this series, we’ll focus on the first stage — the machine learning solutions we’ve built to personalize the content for listeners’ Home pages and, specifically, the lessons we’ve learned in building, experimenting, and deploying these models. \u003c/p\u003e\n\n\n\n\u003ch2\u003eHome @ Spotify\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png\" alt=\"\" width=\"303\" height=\"334\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-250x276.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-768x847.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-1393x1536.png 1393w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-120x132.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home.png 1642w\" sizes=\"(max-width: 303px) 100vw, 303px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe Home page consists of cards — the square items that represent an album, playlist, etc. — and shelves — the horizontal rows that contain multiple cards. We generate personalized content for listeners’ Home pages, algorithmically curating the music and podcasts that are shown to listeners in the shelves on Home. Some content is generated via heuristics and rules and some content is manually curated by editors, while other content is generated via predictions using trained models. We currently have a number of models running in production, each one powering content curation for a different shelf, but we will be discussing three of those models in this post, including: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThe Podcast Model:\u003c/strong\u003e Predicts podcasts a listener is likely to listen to in the \u003cem\u003eShows you might like\u003c/em\u003e shelf. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Shortcuts Model: \u003c/strong\u003ePredicts the listener’s next familiar listen in the Shortcuts feature. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Playlists Model: \u003c/strong\u003ePredicts the playlists a new listener is likely to listen to in the \u003cem\u003eTry something else\u003c/em\u003e shelf.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png\" alt=\"\" width=\"411\" height=\"375\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-250x228.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-768x700.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-1536x1400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-120x109.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2.png 1999w\" sizes=\"(max-width: 411px) 100vw, 411px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSince we launched our first model to recommend content on Home, we have worked to improve our ML stack and processes in order to experiment and productionize models more quickly and reliably.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe road to simplicity and automation\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs anyone who may have contributed to operationalizing an ML model knows, moving a model from experimentation to production is no easy feat. There are numerous challenges in managing the data that goes into a model, running and tracking experiments, and monitoring and retraining models. While we have always tried to keep our ML infrastructure simple, and as close to the sources of features as possible, it has become drastically easier for our squads to deploy and maintain models now than when we started.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt a high level, an ML workflow can be broken down into three main phases: 1) data management, 2) experimentation, and 3) operationalization.  \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png\" alt=\"\" width=\"784\" height=\"164\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-250x52.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-768x160.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-120x25.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow.png 1351w\" sizes=\"(max-width: 784px) 100vw, 784px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIt’s common to iteratively work on the training and evaluation phase until a final model version is selected as the best. This model is then deployed to production systems and can start making predictions for listeners. Similar to most production systems, models (and the services/pipelines that serve them) should be monitored closely. To keep a model up to date (which is more important for some tasks than others; more to come on this), retraining and model versioning are the last steps in our workflow. This part of our stack and workflow has had significant changes since our first model — making batch predictions (offline) of content listeners are likely to stream — to now, where all our models are served in real time. The figure below shows where our machine learning stack started and where we are now:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"533\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going.png 1012w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eOur current ML stack automates a lot of the processes involved in maintaining models in production (with online serving): we have automated feature logging instrumented in our serving infrastructure, with both scheduled Scio pipelines to transform these features and Kubeflow pipelines to retrain weekly. We have also implemented data validation of our training and serving features (as well as validation between subsequent training datasets) to verify our features are consistent and follow the same distributions at training and inference times. In our Kubeflow pipelines, we have components that check the evaluation score and automatically push the model to production if the score is above our threshold. With this stack, we monitor and alert on the automatic data validation pipeline, as well as the online deployments of our models — allowing us to handle any issues as soon as they arise.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith a lot of effort and many lessons learned, our ML stack has evolved to make these processes automated and more reliable, enabling us to iterate faster to improve our models and increase our engineering productivity. \u003c/p\u003e\n\n\n\n\u003ch2\u003eHow we unified training and serving data\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we first start to think about a problem, we always dig into the data first — what data would be useful? What data is available? And then we take a really close look at the data that will be used for features, characterizing what is in the dataset and identifying the edge cases in the data. We feel fairly confident about the contents of the data used for our training features as well as what the transformed data looks like, but features fetched and transformed at \u003cem\u003eserving\u003c/em\u003e time are an entirely different story. \u003c/p\u003e\n\n\n\n\u003ch3\u003eBatch training data and batch predictions\u003c/h3\u003e\n\n\n\n\u003cp\u003eHistorically, we have had one set of infrastructure for fetching and transforming features during experimentation (training) and a different set of infrastructure for fetching and transforming features for making predictions (serving). \u003c/p\u003e\n\n\n\n\u003ch3\u003eThen we started to make online predictions (… with the wrong data)\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhen we changed the Podcast Model from making batch offline predictions to serving in real time, we set up a new service that could support this — this new service had to fetch and transform features, make the prediction, and respond to the request. The important part here is that the feature processing and transformation was now in a different place than where the corresponding training feature processing took place. And, unfortunately, models are like black boxes, so testing the output is difficult, if not impossible. A while ago, we discovered that we had been transforming one of the model’s features slightly differently at training time than at serving time, leading to potentially degraded recommendations — and there was no way to detect this, so it continued to happen for four months.Think about this for just a second. Such a simple part of our stack — at most, a few lines of code — was doing the wrong thing and impacted the recommendations produced by our model. Our short-term fix was to simply change the one line of code in our prediction service that was causing the issue, but we knew long term that we needed to either have a single source of data for both training and serving, or we needed to ensure that data was produced and transformed the same way in both stages.\u003c/p\u003e\n\n\n\n\u003ch3\u003eOne transformation implementation to rule them all\u003c/h3\u003e\n\n\n\n\u003cp\u003eOur first approach was to make any feature processing and transformation occur in the same code path, so that training and serving features would be processed identically. Taking the Shortcuts Model as an example again, our goal was to get rid of the Python service that transformed training features — this service was always running and constantly checking, on all days, to see if it was a Monday; if so, then it would request data from the necessary service (at a rate-limited 5 requests/second) and transform them into features; ideally, this would have been implemented as a pipeline, but we couldn’t schedule and orchestrate it because the process took more than 24 hours. There were many reasons we wanted to migrate away from this approach, but logging features when the only data source for features is a different service (owned by a different squad) proved difficult. Using our serving infrastructure’s feature logging capabilities, we could automatically log already transformed features, which could later be used for training. At this point, all of our features for training and serving were being transformed by code in the Java service. And we now use this feature logging for all of our models both to solve this problem, and also because it reduces the amount of additional infrastructure we need to support.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eBut wait, we can do more by validating our data\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe second approach we took to ensure our training and serving features did not differ was to use Tensorflow Data Validation (TFDV) to compare training and serving data schemas and feature distributions on a daily basis. The alerting we have added to our data validation pipeline allows us to detect significant differences in our feature sets — it uses the Chebyshev distance metric, which compares the distance between two vectors, and can help alert us to drift in training and serving features.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we knew that understanding what is in our data is crucial, we quickly learned that it’s easy to make mistakes when moving models to production because the data often uses a different processing library. We didn’t expect many data differences, but validating and alerting on issues lets us know if something changed, and how we should remediate the issue.\u003c/p\u003e\n\n\n\n\u003cp\u003eStay tuned for Part II as we take a closer look at how we evaluate our models using offline and online metrics, why it’s so important to actually look at the recommendations we are making, and the challenges we faced in our journey to CI/CD in model retraining. \u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the les",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png",
      "date_published": "2021-11-15T00:00:00Z",
      "author": {
        "name": "Published by Annie Edmundson, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/",
      "title": "\n                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 15, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\" title=\"The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the lessons we’ve learned from building the ML stack that serves these models.\u003c/p\u003e\n\n\n\n\u003cp\u003eMachine learning is central to how we personalize the Home page user experience and connect listeners to the creators that are most relevant to them. Like many recommendation systems, the \u003ca href=\"https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/\"\u003eSpotify Home page recommendations are powered\u003c/a\u003e by two stages: \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStage 1: Candidate generation:\u003c/strong\u003e The best albums, playlists, artists, and podcasts are selected for each listener.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStage 2: Ranking:\u003c/strong\u003e Candidates are ranked in the best order for each listener.  \u003c/p\u003e\n\n\n\n\u003cp\u003eIn Part I of this series, we’ll focus on the first stage — the machine learning solutions we’ve built to personalize the content for listeners’ Home pages and, specifically, the lessons we’ve learned in building, experimenting, and deploying these models. \u003c/p\u003e\n\n\n\n\u003ch2\u003eHome @ Spotify\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png\" alt=\"\" width=\"303\" height=\"334\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-250x276.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-768x847.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-1393x1536.png 1393w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-120x132.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home.png 1642w\" sizes=\"(max-width: 303px) 100vw, 303px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe Home page consists of cards — the square items that represent an album, playlist, etc. — and shelves — the horizontal rows that contain multiple cards. We generate personalized content for listeners’ Home pages, algorithmically curating the music and podcasts that are shown to listeners in the shelves on Home. Some content is generated via heuristics and rules and some content is manually curated by editors, while other content is generated via predictions using trained models. We currently have a number of models running in production, each one powering content curation for a different shelf, but we will be discussing three of those models in this post, including: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThe Podcast Model:\u003c/strong\u003e Predicts podcasts a listener is likely to listen to in the \u003cem\u003eShows you might like\u003c/em\u003e shelf. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Shortcuts Model: \u003c/strong\u003ePredicts the listener’s next familiar listen in the Shortcuts feature. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Playlists Model: \u003c/strong\u003ePredicts the playlists a new listener is likely to listen to in the \u003cem\u003eTry something else\u003c/em\u003e shelf.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png\" alt=\"\" width=\"411\" height=\"375\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-250x228.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-768x700.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-1536x1400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-120x109.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2.png 1999w\" sizes=\"(max-width: 411px) 100vw, 411px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSince we launched our first model to recommend content on Home, we have worked to improve our ML stack and processes in order to experiment and productionize models more quickly and reliably.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe road to simplicity and automation\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs anyone who may have contributed to operationalizing an ML model knows, moving a model from experimentation to production is no easy feat. There are numerous challenges in managing the data that goes into a model, running and tracking experiments, and monitoring and retraining models. While we have always tried to keep our ML infrastructure simple, and as close to the sources of features as possible, it has become drastically easier for our squads to deploy and maintain models now than when we started.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt a high level, an ML workflow can be broken down into three main phases: 1) data management, 2) experimentation, and 3) operationalization.  \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png\" alt=\"\" width=\"784\" height=\"164\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-250x52.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-768x160.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-120x25.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow.png 1351w\" sizes=\"(max-width: 784px) 100vw, 784px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIt’s common to iteratively work on the training and evaluation phase until a final model version is selected as the best. This model is then deployed to production systems and can start making predictions for listeners. Similar to most production systems, models (and the services/pipelines that serve them) should be monitored closely. To keep a model up to date (which is more important for some tasks than others; more to come on this), retraining and model versioning are the last steps in our workflow. This part of our stack and workflow has had significant changes since our first model — making batch predictions (offline) of content listeners are likely to stream — to now, where all our models are served in real time. The figure below shows where our machine learning stack started and where we are now:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"533\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going.png 1012w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eOur current ML stack automates a lot of the processes involved in maintaining models in production (with online serving): we have automated feature logging instrumented in our serving infrastructure, with both scheduled Scio pipelines to transform these features and Kubeflow pipelines to retrain weekly. We have also implemented data validation of our training and serving features (as well as validation between subsequent training datasets) to verify our features are consistent and follow the same distributions at training and inference times. In our Kubeflow pipelines, we have components that check the evaluation score and automatically push the model to production if the score is above our threshold. With this stack, we monitor and alert on the automatic data validation pipeline, as well as the online deployments of our models — allowing us to handle any issues as soon as they arise.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith a lot of effort and many lessons learned, our ML stack has evolved to make these processes automated and more reliable, enabling us to iterate faster to improve our models and increase our engineering productivity. \u003c/p\u003e\n\n\n\n\u003ch2\u003eHow we unified training and serving data\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we first start to think about a problem, we always dig into the data first — what data would be useful? What data is available? And then we take a really close look at the data that will be used for features, characterizing what is in the dataset and identifying the edge cases in the data. We feel fairly confident about the contents of the data used for our training features as well as what the transformed data looks like, but features fetched and transformed at \u003cem\u003eserving\u003c/em\u003e time are an entirely different story. \u003c/p\u003e\n\n\n\n\u003ch3\u003eBatch training data and batch predictions\u003c/h3\u003e\n\n\n\n\u003cp\u003eHistorically, we have had one set of infrastructure for fetching and transforming features during experimentation (training) and a different set of infrastructure for fetching and transforming features for making predictions (serving). \u003c/p\u003e\n\n\n\n\u003ch3\u003eThen we started to make online predictions (… with the wrong data)\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhen we changed the Podcast Model from making batch offline predictions to serving in real time, we set up a new service that could support this — this new service had to fetch and transform features, make the prediction, and respond to the request. The important part here is that the feature processing and transformation was now in a different place than where the corresponding training feature processing took place. And, unfortunately, models are like black boxes, so testing the output is difficult, if not impossible. A while ago, we discovered that we had been transforming one of the model’s features slightly differently at training time than at serving time, leading to potentially degraded recommendations — and there was no way to detect this, so it continued to happen for four months.Think about this for just a second. Such a simple part of our stack — at most, a few lines of code — was doing the wrong thing and impacted the recommendations produced by our model. Our short-term fix was to simply change the one line of code in our prediction service that was causing the issue, but we knew long term that we needed to either have a single source of data for both training and serving, or we needed to ensure that data was produced and transformed the same way in both stages.\u003c/p\u003e\n\n\n\n\u003ch3\u003eOne transformation implementation to rule them all\u003c/h3\u003e\n\n\n\n\u003cp\u003eOur first approach was to make any feature processing and transformation occur in the same code path, so that training and serving features would be processed identically. Taking the Shortcuts Model as an example again, our goal was to get rid of the Python service that transformed training features — this service was always running and constantly checking, on all days, to see if it was a Monday; if so, then it would request data from the necessary service (at a rate-limited 5 requests/second) and transform them into features; ideally, this would have been implemented as a pipeline, but we couldn’t schedule and orchestrate it because the process took more than 24 hours. There were many reasons we wanted to migrate away from this approach, but logging features when the only data source for features is a different service (owned by a different squad) proved difficult. Using our serving infrastructure’s feature logging capabilities, we could automatically log already transformed features, which could later be used for training. At this point, all of our features for training and serving were being transformed by code in the Java service. And we now use this feature logging for all of our models both to solve this problem, and also because it reduces the amount of additional infrastructure we need to support.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eBut wait, we can do more by validating our data\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe second approach we took to ensure our training and serving features did not differ was to use Tensorflow Data Validation (TFDV) to compare training and serving data schemas and feature distributions on a daily basis. The alerting we have added to our data validation pipeline allows us to detect significant differences in our feature sets — it uses the Chebyshev distance metric, which compares the distance between two vectors, and can help alert us to drift in training and serving features.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we knew that understanding what is in our data is crucial, we quickly learned that it’s easy to make mistakes when moving models to production because the data often uses a different processing library. We didn’t expect many data differences, but validating and alerting on issues lets us know if something changed, and how we should remediate the issue.\u003c/p\u003e\n\n\n\n\u003cp\u003eStay tuned for Part II as we take a closer look at how we evaluate our models using offline and online metrics, why it’s so important to actually look at the recommendations we are making, and the challenges we faced in our journey to CI/CD in model retraining. \u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the les",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png",
      "date_published": "2021-11-15T00:00:00Z",
      "author": {
        "name": "Published by Annie Edmundson, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/",
      "title": "\n                                            The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 15, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\" title=\"The Rise (and Lessons Learned) of ML Models to Personalize Content on Home (Part I)\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-250x123.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-700x345.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-768x378.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-1536x757.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I-120x59.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the lessons we’ve learned from building the ML stack that serves these models.\u003c/p\u003e\n\n\n\n\u003cp\u003eMachine learning is central to how we personalize the Home page user experience and connect listeners to the creators that are most relevant to them. Like many recommendation systems, the \u003ca href=\"https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/\"\u003eSpotify Home page recommendations are powered\u003c/a\u003e by two stages: \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStage 1: Candidate generation:\u003c/strong\u003e The best albums, playlists, artists, and podcasts are selected for each listener.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStage 2: Ranking:\u003c/strong\u003e Candidates are ranked in the best order for each listener.  \u003c/p\u003e\n\n\n\n\u003cp\u003eIn Part I of this series, we’ll focus on the first stage — the machine learning solutions we’ve built to personalize the content for listeners’ Home pages and, specifically, the lessons we’ve learned in building, experimenting, and deploying these models. \u003c/p\u003e\n\n\n\n\u003ch2\u003eHome @ Spotify\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png\" alt=\"\" width=\"303\" height=\"334\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-700x772.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-250x276.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-768x847.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-1393x1536.png 1393w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home-120x132.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Home.png 1642w\" sizes=\"(max-width: 303px) 100vw, 303px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe Home page consists of cards — the square items that represent an album, playlist, etc. — and shelves — the horizontal rows that contain multiple cards. We generate personalized content for listeners’ Home pages, algorithmically curating the music and podcasts that are shown to listeners in the shelves on Home. Some content is generated via heuristics and rules and some content is manually curated by editors, while other content is generated via predictions using trained models. We currently have a number of models running in production, each one powering content curation for a different shelf, but we will be discussing three of those models in this post, including: \u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eThe Podcast Model:\u003c/strong\u003e Predicts podcasts a listener is likely to listen to in the \u003cem\u003eShows you might like\u003c/em\u003e shelf. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Shortcuts Model: \u003c/strong\u003ePredicts the listener’s next familiar listen in the Shortcuts feature. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eThe Playlists Model: \u003c/strong\u003ePredicts the playlists a new listener is likely to listen to in the \u003cem\u003eTry something else\u003c/em\u003e shelf.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png\" alt=\"\" width=\"411\" height=\"375\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-700x638.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-250x228.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-768x700.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-1536x1400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2-120x109.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Powered-by-2.png 1999w\" sizes=\"(max-width: 411px) 100vw, 411px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSince we launched our first model to recommend content on Home, we have worked to improve our ML stack and processes in order to experiment and productionize models more quickly and reliably.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe road to simplicity and automation\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs anyone who may have contributed to operationalizing an ML model knows, moving a model from experimentation to production is no easy feat. There are numerous challenges in managing the data that goes into a model, running and tracking experiments, and monitoring and retraining models. While we have always tried to keep our ML infrastructure simple, and as close to the sources of features as possible, it has become drastically easier for our squads to deploy and maintain models now than when we started.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt a high level, an ML workflow can be broken down into three main phases: 1) data management, 2) experimentation, and 3) operationalization.  \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png\" alt=\"\" width=\"784\" height=\"164\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-700x146.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-250x52.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-768x160.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow-120x25.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Workflow.png 1351w\" sizes=\"(max-width: 784px) 100vw, 784px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIt’s common to iteratively work on the training and evaluation phase until a final model version is selected as the best. This model is then deployed to production systems and can start making predictions for listeners. Similar to most production systems, models (and the services/pipelines that serve them) should be monitored closely. To keep a model up to date (which is more important for some tasks than others; more to come on this), retraining and model versioning are the last steps in our workflow. This part of our stack and workflow has had significant changes since our first model — making batch predictions (offline) of content listeners are likely to stream — to now, where all our models are served in real time. The figure below shows where our machine learning stack started and where we are now:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"533\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-700x533.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-250x190.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-768x584.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going-120x91.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/How-it-started_how-its-going.png 1012w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eOur current ML stack automates a lot of the processes involved in maintaining models in production (with online serving): we have automated feature logging instrumented in our serving infrastructure, with both scheduled Scio pipelines to transform these features and Kubeflow pipelines to retrain weekly. We have also implemented data validation of our training and serving features (as well as validation between subsequent training datasets) to verify our features are consistent and follow the same distributions at training and inference times. In our Kubeflow pipelines, we have components that check the evaluation score and automatically push the model to production if the score is above our threshold. With this stack, we monitor and alert on the automatic data validation pipeline, as well as the online deployments of our models — allowing us to handle any issues as soon as they arise.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith a lot of effort and many lessons learned, our ML stack has evolved to make these processes automated and more reliable, enabling us to iterate faster to improve our models and increase our engineering productivity. \u003c/p\u003e\n\n\n\n\u003ch2\u003eHow we unified training and serving data\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we first start to think about a problem, we always dig into the data first — what data would be useful? What data is available? And then we take a really close look at the data that will be used for features, characterizing what is in the dataset and identifying the edge cases in the data. We feel fairly confident about the contents of the data used for our training features as well as what the transformed data looks like, but features fetched and transformed at \u003cem\u003eserving\u003c/em\u003e time are an entirely different story. \u003c/p\u003e\n\n\n\n\u003ch3\u003eBatch training data and batch predictions\u003c/h3\u003e\n\n\n\n\u003cp\u003eHistorically, we have had one set of infrastructure for fetching and transforming features during experimentation (training) and a different set of infrastructure for fetching and transforming features for making predictions (serving). \u003c/p\u003e\n\n\n\n\u003ch3\u003eThen we started to make online predictions (… with the wrong data)\u003c/h3\u003e\n\n\n\n\u003cp\u003eWhen we changed the Podcast Model from making batch offline predictions to serving in real time, we set up a new service that could support this — this new service had to fetch and transform features, make the prediction, and respond to the request. The important part here is that the feature processing and transformation was now in a different place than where the corresponding training feature processing took place. And, unfortunately, models are like black boxes, so testing the output is difficult, if not impossible. A while ago, we discovered that we had been transforming one of the model’s features slightly differently at training time than at serving time, leading to potentially degraded recommendations — and there was no way to detect this, so it continued to happen for four months.Think about this for just a second. Such a simple part of our stack — at most, a few lines of code — was doing the wrong thing and impacted the recommendations produced by our model. Our short-term fix was to simply change the one line of code in our prediction service that was causing the issue, but we knew long term that we needed to either have a single source of data for both training and serving, or we needed to ensure that data was produced and transformed the same way in both stages.\u003c/p\u003e\n\n\n\n\u003ch3\u003eOne transformation implementation to rule them all\u003c/h3\u003e\n\n\n\n\u003cp\u003eOur first approach was to make any feature processing and transformation occur in the same code path, so that training and serving features would be processed identically. Taking the Shortcuts Model as an example again, our goal was to get rid of the Python service that transformed training features — this service was always running and constantly checking, on all days, to see if it was a Monday; if so, then it would request data from the necessary service (at a rate-limited 5 requests/second) and transform them into features; ideally, this would have been implemented as a pipeline, but we couldn’t schedule and orchestrate it because the process took more than 24 hours. There were many reasons we wanted to migrate away from this approach, but logging features when the only data source for features is a different service (owned by a different squad) proved difficult. Using our serving infrastructure’s feature logging capabilities, we could automatically log already transformed features, which could later be used for training. At this point, all of our features for training and serving were being transformed by code in the Java service. And we now use this feature logging for all of our models both to solve this problem, and also because it reduces the amount of additional infrastructure we need to support.  \u003c/p\u003e\n\n\n\n\u003ch3\u003eBut wait, we can do more by validating our data\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe second approach we took to ensure our training and serving features did not differ was to use Tensorflow Data Validation (TFDV) to compare training and serving data schemas and feature distributions on a daily basis. The alerting we have added to our data validation pipeline allows us to detect significant differences in our feature sets — it uses the Chebyshev distance metric, which compares the distance between two vectors, and can help alert us to drift in training and serving features.  \u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we knew that understanding what is in our data is crucial, we quickly learned that it’s easy to make mistakes when moving models to production because the data often uses a different processing library. We didn’t expect many data differences, but validating and alerting on issues lets us know if something changed, and how we should remediate the issue.\u003c/p\u003e\n\n\n\n\u003cp\u003eStay tuned for Part II as we take a closer look at how we evaluate our models using offline and online metrics, why it’s so important to actually look at the recommendations we are making, and the challenges we faced in our journey to CI/CD in model retraining. \u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003c/p\u003e\n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, our goal is to connect listeners with creators, and one way we do that is by recommending quality music and podcasts on the Home page. In this two-part blog series, we will talk about the ML models we build and use to recommend diverse and fulfilling content to our listeners, and the les",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/11/Header_part-I.png",
      "date_published": "2021-11-15T00:00:00Z",
      "author": {
        "name": "Published by Annie Edmundson, Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/10/20/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/",
      "title": "\n                                            Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 20, 2021\u003c/span\u003e\n                \u003cspan\u003e\n                    Published by Flavio Santos (Data Infrastructure Engineer) and Robert Stephenson (Senior Product Manager)                \u003c/span\u003e\n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/10/20/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/\" title=\"Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-768x381.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-2048x1015.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-120x59.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infrastructure (EDI). Throughout this blog post we make a distinction between the internal users of the EDI, who are Spotify Engineers, Data Scientists, PMs and squads, and end users, who use Spotify as a service and audio platform.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn 2016, we redesigned the EDI in Google Cloud Platform (GCP) when Spotify migrated to the cloud, and we documented the journey in three blog posts (\u003ca href=\"https://engineering.atspotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart II\u003c/a\u003e, and \u003ca href=\"https://engineering.atspotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart III\u003c/a\u003e). Not everything went as planned, and we wrote about our learnings from operating our cloud-native EDI in \u003ca href=\"https://engineering.atspotify.com/2019/11/12/spotifys-event-delivery-life-in-the-cloud/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart IV\u003c/a\u003e. Our design was optimized to make it quick and easy for internal developers to instrument and log the data they needed. We then extended it to adapt to the General Data Protection Regulation (GDPR), we introduced streaming event delivery in addition to batch, and we brought BigQuery to our data community. We also improved operational stability and the quality of life of our on-call engineers. The peak traffic increased from 1.5M events per second to nearly 8M, and we were ready for that massive scale increase. This increased the total volume of data which we ingested daily to nearly 70TB! (Figure 1).\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"436\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-768x478.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1.png 1480w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: Average total volume (TB) of events stored daily by our \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\" target=\"_blank\"\u003eETL process\u003c/a\u003e (after compression).\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eHowever, with that high adoption and traffic increase we discovered some bottlenecks. Our internal users had feature requests and needed more from the system. Now our incomplete and low-quality data was degrading the productivity of the Spotify data community. Whoops!\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat was hurting us?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we designed and built the initial EDI, our team had the mission statement to “provide infrastructure for teams at Spotify to reliably collect data, and make it available, safely and efficiently.” The use cases we focused on were well supported, such as music streaming and application monitoring. As other use cases started to appear, the assumptions we made when building the system had to be revisited. During three years of operating and scaling the existing EDI, we gathered a lot of feedback from our internal users and learned a lot about our limitations.\u003c/p\u003e\n\n\n\n\u003ch3\u003eData loss\u003c/h3\u003e\n\n\n\n\u003cp\u003eMost events generated on mobile clients were sent in a fire-and-forget fashion. This might seem surprising, but because end users can enjoy Spotify while offline, there are some complications around deduplication of data that is re-sent. For example, if we detect that we are missing a data point, we don’t necessarily know if it is actually lost, or just has not arrived yet due to the user being offline, in a tunnel, or maybe having a flaky network connection. This leads to a small percentage of data loss for nearly all the data we collect, which is not acceptable for some types of data. Furthermore, this problem is compounded for datasets generated from a combination of multiple event types in order to “connect the dots” in user journeys where, for example, a single lost event can compromise the whole journey. While we had some specific client code and algorithms to reliably deliver business-critical data exactly once, it was not done in a way that we could extend to all 600+ event types that we had at that time.\u003c/p\u003e\n\n\n\n\u003ch3\u003eControl plane UX\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe workflow for a customer to progress from “instrumentation to insights” took far too long. Under normal circumstances it would take a customer a week to go through this workflow and get their data. One issue was that multiple components in the EDI had to be schema aware. For example, the receiver service, which is the entry point of the infrastructure, uses the schemas to validate that incoming data is well formed. Due to some tech debt, it took a few hours to propagate the schemas for this validation. This was an eternity in terms of iteration time. Since this process was so painful, some teams tried to instrument their features or services, but then gave up. Some other teams would shoehorn their data into existing data events. This led to gaps in what was instrumented, and a data-quality nightmare.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBackwards compatible? Or stuck in the past?\u003c/h3\u003e\n\n\n\n\u003cp\u003eFor strategic reasons, it was critical, in 2016, that we build the EDI in GCP and migrate over as quickly as possible. A key decision we took to make this happen was to stay backwards compatible to minimize the migration time. That meant we had to stick with some historical design choices that we would not have if we had built this EDI from scratch. For example:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eTab-separated values (TSV):\u003c/strong\u003e All data events were sent as TSV strings. The schemas were parsed and converted to Avro with a Python library created in 2007. The schema-aware tooling for parsing the TSV data was the main cause for the painful control plane UX mentioned earlier.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStateful services:\u003c/strong\u003e Data events were first stored on disk and then forwarded to the EDI. This made us resilient to crashes, but made us vulnerable to data loss if a machine was taken down. Furthermore, Spotify could not take advantage of auto-scaling mechanisms or Kubernetes (without difficult workarounds) because the EDI made our service ecosystem stateful.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eLegacy perimeter:\u003c/strong\u003e Since data events were forwarded from disk to our EDI, all events triggered by Spotify clients needed to be emitted from our perimeter servers. These servers had to keep events on disk and were tightly coupled to our legacy logging mechanism. This caused some pain to perimeter administrators and hindered architectural innovations. Besides the additional complexity in the perimeter, the shared ownership of different teams with different goals caused alignment problems.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe situation\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe had hundreds of services sending events through a legacy EDI by logging data to disk. After being ingested by the infrastructure, events were consumed by hundreds of downstream data pipelines to produce derivative datasets (Figure 2). Our goal was to build a platform that takes advantage of the modern landscape in the cloud while also enabling legacy event types to be migrated easily. The workflow to create new events should be frictionless, while still following our data governance principles and applicable privacy laws.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"591\" height=\"302\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png 591w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-250x128.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-120x61.png 120w\" sizes=\"(max-width: 591px) 100vw, 591px\"/\u003e\u003cfigcaption\u003eFigure 2. Events produced by our internal services go through the legacy EDI and are consumed by hundreds of data pipelines.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eTransitioning event logging to a new infrastructure would need to take into consideration the long tail that mobile app updates have. A new version of our mobile apps takes several months to gain adoption from a high percentage of Spotify end users. We knew that we would have traffic coming to both the old and new EDIs for quite some time. Moreover, events emitted from embedded devices, such as TVs and speakers, would need special treatment as some of these devices are unlikely to ever be upgraded. We call this challenge “The Long Tail Problem”.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe partially solved “The Long Tail Problem” by designing a data transformation pipeline that reads events from legacy clients, converts them, and feeds them from the legacy EDI into the new infrastructure (Figure 3). Since we were breaking backwards compatibility, we took the opportunity to update our data model. The transformation to the new data model would not have all the necessary information available, so missing or inaccurate fields were expected occasionally. But since this transformation only applied to legacy clients, it would decrease as end users upgraded to the latest version of Spotify. This traffic would become negligible, eventually.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"355\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3.png 1286w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 3: To handle clients which had not yet upgraded to the latest version, we implemented a job to export legacy data to the new EDI and transform it to our new data model.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWe gave data producers two options to adopt the new EDI: either redesign their instrumentation using the new data model, or stick with what they have and turn on exporting data from the legacy EDI to the new EDI. After producers onboarded, event consumers would migrate to read data from the new EDI. If producers and consumers agree to use the exporter, they would first need to update any downstream pipelines to read from the new infrastructure before making client-side changes.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGet in production with real use cases ASAP\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn order to validate our decisions, we had to find early adopters to start producing events with the new infrastructure. We presented the advantages and explained the limitations of our alpha product to potential interested teams. It was important to be able to experiment, break, and fix issues fast and safely without worrying about affecting critical production systems or data. Setting expectations with our internal users was important so we could make breaking changes when our assumptions were wrong.\u003c/p\u003e\n\n\n\n\u003cp\u003eNext, we found at least one real use case to migrate. We were looking for something specific, since different event types have different levels of importance, timeliness requirements, and downstream dependencies. We reached out to event owners to understand how their data was being used and how we could help them migrate.\u003c/p\u003e\n\n\n\n\u003cp\u003eGiven a set of eligible event types, we identified use cases that were satisfied by the limited features we had built so far. Learning which features our internal users were missing also helped prioritize our roadmap. The more features we added to the new EDI, the more event types we could onboard. We periodically revisited our design decisions and assumptions in order to identify potential problems in the new infrastructure as quickly as possible.\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce we had a prototype that was working with real production traffic, we solidified the interfaces and data model and helped the alpha internal users adapt to the changes (Figure 4). This enabled us to decouple the significant work of migrating the 600+ event types which were running on the legacy infrastructure, and actually building the new EDI behind the abstractions.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"187\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-250x67.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-768x206.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-1536x411.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-120x32.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 4: New interfaces for the prototype infrastructure, so we could concurrently migrate internal users to the new Event Delivery Infrastructure while building it.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eJust-in-time optimizations\u003c/h2\u003e\n\n\n\n\u003cp\u003ePrematurely optimizing is generally a bad idea without motivating metrics. We always want to be as efficient as possible, but we had to prioritize and make trade-offs. Part of the challenge was to find a good balance between the desired efficiency of our infrastructure and the features we absolutely needed to release in order to accomplish our goals.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe learned from the first EDI that we needed to design for our targeted service availability from the start. Since transactional data collection was not required, there was no need for 100% delivery. We instead had to determine what level of service availability was acceptable and understand the trade-offs associated with that.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe design changes and decisions\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have two main interfaces to the EDI. The control plane is the starting point where internal users declare their events, design their schemas, and bind them to specific SDKs. The data plane receives events sent by those SDKs, divides them by event type, and makes them available as batch datasets or data-streaming topics. The events go through several other components that deduplicate, translate to our well-designed data model, and pseudonymize personal information. The output data is reliably stored for Spotify’s data community to consume and build data pipelines. Due to the evolving needs of internal users, as well as operational overhead and scalability concerns, we needed to make changes between the old and new infrastructure.\u003c/p\u003e\n\n\n\n\u003ch3\u003eClient re-sends and new deduplication\u003c/h3\u003e\n\n\n\n\u003cp\u003eIn order to reduce event loss and improve reliability, we implemented client resends. Due to connection stability and offline mode, these resends may happen immediately, within a few minutes, or maybe several days later. They may never happen! It’s actually impossible to tell if an event has been lost in transport, or if a user has used Spotify and then dropped their phone in the ocean, causing data loss. The combination of resend strategies and flaky network connections complicates things and introduces duplicate events.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the old infrastructure, we only deduplicate events within a small window of hours. However, due to the significant increase of duplicates, we hit some bottlenecks and decided to redesign the job. The biggest changes in the new job are the introduction of event message identifiers, and the adoption of Google’s Dataflow processing service instead of Hadoop. The event message identifiers were used to generate lookup indices and remove duplicates. This new strategy allowed us to look back across multiple weeks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eReceiver service — offline to online\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe legacy EDI used files on disk to store events before they were sent to a receiver service.  Spotify’s access point or other backend services would have their own availability guarantees, and we would read the data from disk eventually. In the new EDI, our receiver service needs its own availability guarantees, which was a paradigm shift in our infrastructure and for our team as \u003ca href=\"https://sre.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSREs\u003c/a\u003e. Furthermore, those files on disk were a blocker for Spotify to leverage auto-scaling fleets.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the new EDI, we have the receiver service as a highly available API used by SDKs to send events. In case of a receiver service outage, events would be temporarily stored on clients and, eventually, re-sent according to a predefined retry policy.\u003c/p\u003e\n\n\n\n\u003ch3\u003eManaged Dataflow\u003c/h3\u003e\n\n\n\n\u003cp\u003eWe wanted the new EDI to leverage cloud-managed services as much as possible. By rebuilding the architecture to run in the cloud, we can offload management responsibilities to Google, and our team can focus on providing additional value.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen building the legacy EDI, we needed to migrate a heavy Hadoop job from our on-premise cluster to the cloud. The easiest way was to run the same job on Google’s managed Hadoop solution, Dataproc, so that’s what we did. In the new EDI, the new implementation of that job uses \u003ca href=\"https://spotify.github.io/scio/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eScio\u003c/a\u003e (Scala API for Apache Beam) and runs on Google’s Dataflow instead. We considered Spark or Flink, but those had to run over Hadoop, which goes against our strategy to save us operational burden and cost.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy using Dataflow, we no longer needed to keep long-lived Hadoop clusters to execute our jobs. These clusters had to be big enough to process the largest job without issues, and were overkill for almost everything else. Maintaining these clusters was incredibly expensive. Conversely, Dataflow recycles clusters for every job and supports auto-scaling, allowing us to use and pay only for the resources we need.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWrap-up\u003c/h2\u003e\n\n\n\n\u003cp\u003eOnce we decided to redesign our EDI, we evaluated new technologies and adopted new paradigms available to us in the cloud. We had been operating our old infrastructure for years, and that helped us to understand the main pain points and fragilities. We made decisions based on the technical direction of the company, the industry state of the art, and the known scalability issues with existing components. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe started by first designing new components for the new EDI, which we hacked together into a proof of concept and quickly evolved to a more robust prototype that could be used in production. Shipping as soon as possible was critical to validate the infrastructure end to end and catch issues fast. Having the internal users onboarded early was an important forcing function to keep quality and operational maturity high. Next, we solidified the interfaces to the prototype infrastructure and scaled up traffic by onboarding many noncritical event types. With the interfaces stable, we could improve or change out the internals without friction. This approach decoupled the mass migration from actually rebuilding the infrastructure and reduced wall-clock project time significantly.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs we neared the end of the migration, we had thrown out nearly all the old, obsolete infrastructure in favor of the state of the art. We successfully changed the wheels of the moving bus, and gave Spotify’s data community a smooth ride.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infra",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png",
      "date_published": "2021-10-20T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/",
      "title": "\n                                            Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 20, 2021\u003c/span\u003e\n                \u003cspan\u003e\n                    Published by Flavio Santos (Data Infrastructure Engineer) and Robert Stephenson (Senior Product Manager)                \u003c/span\u003e\n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/\" title=\"Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-768x381.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-2048x1015.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-120x59.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infrastructure (EDI). Throughout this blog post we make a distinction between the internal users of the EDI, who are Spotify Engineers, Data Scientists, PMs and squads, and end users, who use Spotify as a service and audio platform.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn 2016, we redesigned the EDI in Google Cloud Platform (GCP) when Spotify migrated to the cloud, and we documented the journey in three blog posts (\u003ca href=\"https://engineering.atspotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart II\u003c/a\u003e, and \u003ca href=\"https://engineering.atspotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart III\u003c/a\u003e). Not everything went as planned, and we wrote about our learnings from operating our cloud-native EDI in \u003ca href=\"https://engineering.atspotify.com/2019/11/12/spotifys-event-delivery-life-in-the-cloud/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart IV\u003c/a\u003e. Our design was optimized to make it quick and easy for internal developers to instrument and log the data they needed. We then extended it to adapt to the General Data Protection Regulation (GDPR), we introduced streaming event delivery in addition to batch, and we brought BigQuery to our data community. We also improved operational stability and the quality of life of our on-call engineers. The peak traffic increased from 1.5M events per second to nearly 8M, and we were ready for that massive scale increase. This increased the total volume of data which we ingested daily to nearly 70TB! (Figure 1).\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"436\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-768x478.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1.png 1480w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: Average total volume (TB) of events stored daily by our \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\" target=\"_blank\"\u003eETL process\u003c/a\u003e (after compression).\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eHowever, with that high adoption and traffic increase we discovered some bottlenecks. Our internal users had feature requests and needed more from the system. Now our incomplete and low-quality data was degrading the productivity of the Spotify data community. Whoops!\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat was hurting us?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we designed and built the initial EDI, our team had the mission statement to “provide infrastructure for teams at Spotify to reliably collect data, and make it available, safely and efficiently.” The use cases we focused on were well supported, such as music streaming and application monitoring. As other use cases started to appear, the assumptions we made when building the system had to be revisited. During three years of operating and scaling the existing EDI, we gathered a lot of feedback from our internal users and learned a lot about our limitations.\u003c/p\u003e\n\n\n\n\u003ch3\u003eData loss\u003c/h3\u003e\n\n\n\n\u003cp\u003eMost events generated on mobile clients were sent in a fire-and-forget fashion. This might seem surprising, but because end users can enjoy Spotify while offline, there are some complications around deduplication of data that is re-sent. For example, if we detect that we are missing a data point, we don’t necessarily know if it is actually lost, or just has not arrived yet due to the user being offline, in a tunnel, or maybe having a flaky network connection. This leads to a small percentage of data loss for nearly all the data we collect, which is not acceptable for some types of data. Furthermore, this problem is compounded for datasets generated from a combination of multiple event types in order to “connect the dots” in user journeys where, for example, a single lost event can compromise the whole journey. While we had some specific client code and algorithms to reliably deliver business-critical data exactly once, it was not done in a way that we could extend to all 600+ event types that we had at that time.\u003c/p\u003e\n\n\n\n\u003ch3\u003eControl plane UX\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe workflow for a customer to progress from “instrumentation to insights” took far too long. Under normal circumstances it would take a customer a week to go through this workflow and get their data. One issue was that multiple components in the EDI had to be schema aware. For example, the receiver service, which is the entry point of the infrastructure, uses the schemas to validate that incoming data is well formed. Due to some tech debt, it took a few hours to propagate the schemas for this validation. This was an eternity in terms of iteration time. Since this process was so painful, some teams tried to instrument their features or services, but then gave up. Some other teams would shoehorn their data into existing data events. This led to gaps in what was instrumented, and a data-quality nightmare.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBackwards compatible? Or stuck in the past?\u003c/h3\u003e\n\n\n\n\u003cp\u003eFor strategic reasons, it was critical, in 2016, that we build the EDI in GCP and migrate over as quickly as possible. A key decision we took to make this happen was to stay backwards compatible to minimize the migration time. That meant we had to stick with some historical design choices that we would not have if we had built this EDI from scratch. For example:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eTab-separated values (TSV):\u003c/strong\u003e All data events were sent as TSV strings. The schemas were parsed and converted to Avro with a Python library created in 2007. The schema-aware tooling for parsing the TSV data was the main cause for the painful control plane UX mentioned earlier.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStateful services:\u003c/strong\u003e Data events were first stored on disk and then forwarded to the EDI. This made us resilient to crashes, but made us vulnerable to data loss if a machine was taken down. Furthermore, Spotify could not take advantage of auto-scaling mechanisms or Kubernetes (without difficult workarounds) because the EDI made our service ecosystem stateful.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eLegacy perimeter:\u003c/strong\u003e Since data events were forwarded from disk to our EDI, all events triggered by Spotify clients needed to be emitted from our perimeter servers. These servers had to keep events on disk and were tightly coupled to our legacy logging mechanism. This caused some pain to perimeter administrators and hindered architectural innovations. Besides the additional complexity in the perimeter, the shared ownership of different teams with different goals caused alignment problems.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe situation\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe had hundreds of services sending events through a legacy EDI by logging data to disk. After being ingested by the infrastructure, events were consumed by hundreds of downstream data pipelines to produce derivative datasets (Figure 2). Our goal was to build a platform that takes advantage of the modern landscape in the cloud while also enabling legacy event types to be migrated easily. The workflow to create new events should be frictionless, while still following our data governance principles and applicable privacy laws.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"591\" height=\"302\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png 591w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-250x128.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-120x61.png 120w\" sizes=\"(max-width: 591px) 100vw, 591px\"/\u003e\u003cfigcaption\u003eFigure 2: Events produced by our internal services go through the legacy EDI and are consumed by hundreds of data pipelines.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eTransitioning event logging to a new infrastructure would need to take into consideration the long tail that mobile app updates have. A new version of our mobile apps takes several months to gain adoption from a high percentage of Spotify end users. We knew that we would have traffic coming to both the old and new EDIs for quite some time. Moreover, events emitted from embedded devices, such as TVs and speakers, would need special treatment as some of these devices are unlikely to ever be upgraded. We call this challenge “The Long Tail Problem”.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe partially solved “The Long Tail Problem” by designing a data transformation pipeline that reads events from legacy clients, converts them, and feeds them from the legacy EDI into the new infrastructure (Figure 3). Since we were breaking backwards compatibility, we took the opportunity to update our data model. The transformation to the new data model would not have all the necessary information available, so missing or inaccurate fields were expected occasionally. But since this transformation only applied to legacy clients, it would decrease as end users upgraded to the latest version of Spotify. This traffic would become negligible, eventually.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"355\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3.png 1286w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 3: To handle clients which had not yet upgraded to the latest version, we implemented a job to export legacy data to the new EDI and transform it to our new data model.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWe gave data producers two options to adopt the new EDI: either redesign their instrumentation using the new data model, or stick with what they have and turn on exporting data from the legacy EDI to the new EDI. After producers onboarded, event consumers would migrate to read data from the new EDI. If producers and consumers agree to use the exporter, they would first need to update any downstream pipelines to read from the new infrastructure before making client-side changes.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGet in production with real use cases ASAP\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn order to validate our decisions, we had to find early adopters to start producing events with the new infrastructure. We presented the advantages and explained the limitations of our alpha product to potential interested teams. It was important to be able to experiment, break, and fix issues fast and safely without worrying about affecting critical production systems or data. Setting expectations with our internal users was important so we could make breaking changes when our assumptions were wrong.\u003c/p\u003e\n\n\n\n\u003cp\u003eNext, we found at least one real use case to migrate. We were looking for something specific, since different event types have different levels of importance, timeliness requirements, and downstream dependencies. We reached out to event owners to understand how their data was being used and how we could help them migrate.\u003c/p\u003e\n\n\n\n\u003cp\u003eGiven a set of eligible event types, we identified use cases that were satisfied by the limited features we had built so far. Learning which features our internal users were missing also helped prioritize our roadmap. The more features we added to the new EDI, the more event types we could onboard. We periodically revisited our design decisions and assumptions in order to identify potential problems in the new infrastructure as quickly as possible.\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce we had a prototype that was working with real production traffic, we solidified the interfaces and data model and helped the alpha internal users adapt to the changes (Figure 4). This enabled us to decouple the significant work of migrating the 600+ event types which were running on the legacy infrastructure, and actually building the new EDI behind the abstractions.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"187\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-250x67.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-768x206.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-1536x411.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-120x32.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 4: New interfaces for the prototype infrastructure, so we could concurrently migrate internal users to the new Event Delivery Infrastructure while building it.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eJust-in-time optimizations\u003c/h2\u003e\n\n\n\n\u003cp\u003ePrematurely optimizing is generally a bad idea without motivating metrics. We always want to be as efficient as possible, but we had to prioritize and make trade-offs. Part of the challenge was to find a good balance between the desired efficiency of our infrastructure and the features we absolutely needed to release in order to accomplish our goals.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe learned from the first EDI that we needed to design for our targeted service availability from the start. Since transactional data collection was not required, there was no need for 100% delivery. We instead had to determine what level of service availability was acceptable and understand the trade-offs associated with that.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe design changes and decisions\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have two main interfaces to the EDI. The control plane is the starting point where internal users declare their events, design their schemas, and bind them to specific SDKs. The data plane receives events sent by those SDKs, divides them by event type, and makes them available as batch datasets or data-streaming topics. The events go through several other components that deduplicate, translate to our well-designed data model, and pseudonymize personal information. The output data is reliably stored for Spotify’s data community to consume and build data pipelines. Due to the evolving needs of internal users, as well as operational overhead and scalability concerns, we needed to make changes between the old and new infrastructure.\u003c/p\u003e\n\n\n\n\u003ch3\u003eClient re-sends and new deduplication\u003c/h3\u003e\n\n\n\n\u003cp\u003eIn order to reduce event loss and improve reliability, we implemented client resends. Due to connection stability and offline mode, these resends may happen immediately, within a few minutes, or maybe several days later. They may never happen! It’s actually impossible to tell if an event has been lost in transport, or if a user has used Spotify and then dropped their phone in the ocean, causing data loss. The combination of resend strategies and flaky network connections complicates things and introduces duplicate events.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the old infrastructure, we only deduplicate events within a small window of hours. However, due to the significant increase of duplicates, we hit some bottlenecks and decided to redesign the job. The biggest changes in the new job are the introduction of event message identifiers, and the adoption of Google’s Dataflow processing service instead of Hadoop. The event message identifiers were used to generate lookup indices and remove duplicates. This new strategy allowed us to look back across multiple weeks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eReceiver service — offline to online\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe legacy EDI used files on disk to store events before they were sent to a receiver service.  Spotify’s access point or other backend services would have their own availability guarantees, and we would read the data from disk eventually. In the new EDI, our receiver service needs its own availability guarantees, which was a paradigm shift in our infrastructure and for our team as \u003ca href=\"https://sre.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSREs\u003c/a\u003e. Furthermore, those files on disk were a blocker for Spotify to leverage auto-scaling fleets.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the new EDI, we have the receiver service as a highly available API used by SDKs to send events. In case of a receiver service outage, events would be temporarily stored on clients and, eventually, re-sent according to a predefined retry policy.\u003c/p\u003e\n\n\n\n\u003ch3\u003eManaged Dataflow\u003c/h3\u003e\n\n\n\n\u003cp\u003eWe wanted the new EDI to leverage cloud-managed services as much as possible. By rebuilding the architecture to run in the cloud, we can offload management responsibilities to Google, and our team can focus on providing additional value.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen building the legacy EDI, we needed to migrate a heavy Hadoop job from our on-premise cluster to the cloud. The easiest way was to run the same job on Google’s managed Hadoop solution, Dataproc, so that’s what we did. In the new EDI, the new implementation of that job uses \u003ca href=\"https://spotify.github.io/scio/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eScio\u003c/a\u003e (Scala API for Apache Beam) and runs on Google’s Dataflow instead. We considered Spark or Flink, but those had to run over Hadoop, which goes against our strategy to save us operational burden and cost.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy using Dataflow, we no longer needed to keep long-lived Hadoop clusters to execute our jobs. These clusters had to be big enough to process the largest job without issues, and were overkill for almost everything else. Maintaining these clusters was incredibly expensive. Conversely, Dataflow recycles clusters for every job and supports auto-scaling, allowing us to use and pay only for the resources we need.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWrap-up\u003c/h2\u003e\n\n\n\n\u003cp\u003eOnce we decided to redesign our EDI, we evaluated new technologies and adopted new paradigms available to us in the cloud. We had been operating our old infrastructure for years, and that helped us to understand the main pain points and fragilities. We made decisions based on the technical direction of the company, the industry state of the art, and the known scalability issues with existing components. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe started by first designing new components for the new EDI, which we hacked together into a proof of concept and quickly evolved to a more robust prototype that could be used in production. Shipping as soon as possible was critical to validate the infrastructure end to end and catch issues fast. Having the internal users onboarded early was an important forcing function to keep quality and operational maturity high. Next, we solidified the interfaces to the prototype infrastructure and scaled up traffic by onboarding many noncritical event types. With the interfaces stable, we could improve or change out the internals without friction. This approach decoupled the mass migration from actually rebuilding the infrastructure and reduced wall-clock project time significantly.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs we neared the end of the migration, we had thrown out nearly all the old, obsolete infrastructure in favor of the state of the art. We successfully changed the wheels of the moving bus, and gave Spotify’s data community a smooth ride.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infra",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png",
      "date_published": "2021-10-20T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/10/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/",
      "title": "\n                                            Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 20, 2021\u003c/span\u003e\n                \u003cspan\u003e\n                    Published by Flavio Santos (Data Infrastructure Engineer) and Robert Stephenson (Senior Product Manager)                \u003c/span\u003e\n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/10/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/\" title=\"Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-768x381.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-2048x1015.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-120x59.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infrastructure (EDI). Throughout this blog post we make a distinction between the internal users of the EDI, who are Spotify Engineers, Data Scientists, PMs and squads, and end users, who use Spotify as a service and audio platform.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn 2016, we redesigned the EDI in Google Cloud Platform (GCP) when Spotify migrated to the cloud, and we documented the journey in three blog posts (\u003ca href=\"https://engineering.atspotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart II\u003c/a\u003e, and \u003ca href=\"https://engineering.atspotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart III\u003c/a\u003e). Not everything went as planned, and we wrote about our learnings from operating our cloud-native EDI in \u003ca href=\"https://engineering.atspotify.com/2019/11/12/spotifys-event-delivery-life-in-the-cloud/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart IV\u003c/a\u003e. Our design was optimized to make it quick and easy for internal developers to instrument and log the data they needed. We then extended it to adapt to the General Data Protection Regulation (GDPR), we introduced streaming event delivery in addition to batch, and we brought BigQuery to our data community. We also improved operational stability and the quality of life of our on-call engineers. The peak traffic increased from 1.5M events per second to nearly 8M, and we were ready for that massive scale increase. This increased the total volume of data which we ingested daily to nearly 70TB! (Figure 1).\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"436\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-768x478.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1.png 1480w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: Average total volume (TB) of events stored daily by our \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\" target=\"_blank\"\u003eETL process\u003c/a\u003e (after compression).\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eHowever, with that high adoption and traffic increase we discovered some bottlenecks. Our internal users had feature requests and needed more from the system. Now our incomplete and low-quality data was degrading the productivity of the Spotify data community. Whoops!\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat was hurting us?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we designed and built the initial EDI, our team had the mission statement to “provide infrastructure for teams at Spotify to reliably collect data, and make it available, safely and efficiently.” The use cases we focused on were well supported, such as music streaming and application monitoring. As other use cases started to appear, the assumptions we made when building the system had to be revisited. During three years of operating and scaling the existing EDI, we gathered a lot of feedback from our internal users and learned a lot about our limitations.\u003c/p\u003e\n\n\n\n\u003ch3\u003eData loss\u003c/h3\u003e\n\n\n\n\u003cp\u003eMost events generated on mobile clients were sent in a fire-and-forget fashion. This might seem surprising, but because end users can enjoy Spotify while offline, there are some complications around deduplication of data that is re-sent. For example, if we detect that we are missing a data point, we don’t necessarily know if it is actually lost, or just has not arrived yet due to the user being offline, in a tunnel, or maybe having a flaky network connection. This leads to a small percentage of data loss for nearly all the data we collect, which is not acceptable for some types of data. Furthermore, this problem is compounded for datasets generated from a combination of multiple event types in order to “connect the dots” in user journeys where, for example, a single lost event can compromise the whole journey. While we had some specific client code and algorithms to reliably deliver business-critical data exactly once, it was not done in a way that we could extend to all 600+ event types that we had at that time.\u003c/p\u003e\n\n\n\n\u003ch3\u003eControl plane UX\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe workflow for a customer to progress from “instrumentation to insights” took far too long. Under normal circumstances it would take a customer a week to go through this workflow and get their data. One issue was that multiple components in the EDI had to be schema aware. For example, the receiver service, which is the entry point of the infrastructure, uses the schemas to validate that incoming data is well formed. Due to some tech debt, it took a few hours to propagate the schemas for this validation. This was an eternity in terms of iteration time. Since this process was so painful, some teams tried to instrument their features or services, but then gave up. Some other teams would shoehorn their data into existing data events. This led to gaps in what was instrumented, and a data-quality nightmare.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBackwards compatible? Or stuck in the past?\u003c/h3\u003e\n\n\n\n\u003cp\u003eFor strategic reasons, it was critical, in 2016, that we build the EDI in GCP and migrate over as quickly as possible. A key decision we took to make this happen was to stay backwards compatible to minimize the migration time. That meant we had to stick with some historical design choices that we would not have if we had built this EDI from scratch. For example:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eTab-separated values (TSV):\u003c/strong\u003e All data events were sent as TSV strings. The schemas were parsed and converted to Avro with a Python library created in 2007. The schema-aware tooling for parsing the TSV data was the main cause for the painful control plane UX mentioned earlier.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStateful services:\u003c/strong\u003e Data events were first stored on disk and then forwarded to the EDI. This made us resilient to crashes, but made us vulnerable to data loss if a machine was taken down. Furthermore, Spotify could not take advantage of auto-scaling mechanisms or Kubernetes (without difficult workarounds) because the EDI made our service ecosystem stateful.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eLegacy perimeter:\u003c/strong\u003e Since data events were forwarded from disk to our EDI, all events triggered by Spotify clients needed to be emitted from our perimeter servers. These servers had to keep events on disk and were tightly coupled to our legacy logging mechanism. This caused some pain to perimeter administrators and hindered architectural innovations. Besides the additional complexity in the perimeter, the shared ownership of different teams with different goals caused alignment problems.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe situation\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe had hundreds of services sending events through a legacy EDI by logging data to disk. After being ingested by the infrastructure, events were consumed by hundreds of downstream data pipelines to produce derivative datasets (Figure 2). Our goal was to build a platform that takes advantage of the modern landscape in the cloud while also enabling legacy event types to be migrated easily. The workflow to create new events should be frictionless, while still following our data governance principles and applicable privacy laws.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"591\" height=\"302\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png 591w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-250x128.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-120x61.png 120w\" sizes=\"(max-width: 591px) 100vw, 591px\"/\u003e\u003cfigcaption\u003eFigure 2: Events produced by our internal services go through the legacy EDI and are consumed by hundreds of data pipelines.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eTransitioning event logging to a new infrastructure would need to take into consideration the long tail that mobile app updates have. A new version of our mobile apps takes several months to gain adoption from a high percentage of Spotify end users. We knew that we would have traffic coming to both the old and new EDIs for quite some time. Moreover, events emitted from embedded devices, such as TVs and speakers, would need special treatment as some of these devices are unlikely to ever be upgraded. We call this challenge “The Long Tail Problem”.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe partially solved “The Long Tail Problem” by designing a data transformation pipeline that reads events from legacy clients, converts them, and feeds them from the legacy EDI into the new infrastructure (Figure 3). Since we were breaking backwards compatibility, we took the opportunity to update our data model. The transformation to the new data model would not have all the necessary information available, so missing or inaccurate fields were expected occasionally. But since this transformation only applied to legacy clients, it would decrease as end users upgraded to the latest version of Spotify. This traffic would become negligible, eventually.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"355\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3.png 1286w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 3: To handle clients which had not yet upgraded to the latest version, we implemented a job to export legacy data to the new EDI and transform it to our new data model.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWe gave data producers two options to adopt the new EDI: either redesign their instrumentation using the new data model, or stick with what they have and turn on exporting data from the legacy EDI to the new EDI. After producers onboarded, event consumers would migrate to read data from the new EDI. If producers and consumers agree to use the exporter, they would first need to update any downstream pipelines to read from the new infrastructure before making client-side changes.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGet in production with real use cases ASAP\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn order to validate our decisions, we had to find early adopters to start producing events with the new infrastructure. We presented the advantages and explained the limitations of our alpha product to potential interested teams. It was important to be able to experiment, break, and fix issues fast and safely without worrying about affecting critical production systems or data. Setting expectations with our internal users was important so we could make breaking changes when our assumptions were wrong.\u003c/p\u003e\n\n\n\n\u003cp\u003eNext, we found at least one real use case to migrate. We were looking for something specific, since different event types have different levels of importance, timeliness requirements, and downstream dependencies. We reached out to event owners to understand how their data was being used and how we could help them migrate.\u003c/p\u003e\n\n\n\n\u003cp\u003eGiven a set of eligible event types, we identified use cases that were satisfied by the limited features we had built so far. Learning which features our internal users were missing also helped prioritize our roadmap. The more features we added to the new EDI, the more event types we could onboard. We periodically revisited our design decisions and assumptions in order to identify potential problems in the new infrastructure as quickly as possible.\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce we had a prototype that was working with real production traffic, we solidified the interfaces and data model and helped the alpha internal users adapt to the changes (Figure 4). This enabled us to decouple the significant work of migrating the 600+ event types which were running on the legacy infrastructure, and actually building the new EDI behind the abstractions.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"187\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-250x67.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-768x206.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-1536x411.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-120x32.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 4: New interfaces for the prototype infrastructure, so we could concurrently migrate internal users to the new Event Delivery Infrastructure while building it.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eJust-in-time optimizations\u003c/h2\u003e\n\n\n\n\u003cp\u003ePrematurely optimizing is generally a bad idea without motivating metrics. We always want to be as efficient as possible, but we had to prioritize and make trade-offs. Part of the challenge was to find a good balance between the desired efficiency of our infrastructure and the features we absolutely needed to release in order to accomplish our goals.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe learned from the first EDI that we needed to design for our targeted service availability from the start. Since transactional data collection was not required, there was no need for 100% delivery. We instead had to determine what level of service availability was acceptable and understand the trade-offs associated with that.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe design changes and decisions\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have two main interfaces to the EDI. The control plane is the starting point where internal users declare their events, design their schemas, and bind them to specific SDKs. The data plane receives events sent by those SDKs, divides them by event type, and makes them available as batch datasets or data-streaming topics. The events go through several other components that deduplicate, translate to our well-designed data model, and pseudonymize personal information. The output data is reliably stored for Spotify’s data community to consume and build data pipelines. Due to the evolving needs of internal users, as well as operational overhead and scalability concerns, we needed to make changes between the old and new infrastructure.\u003c/p\u003e\n\n\n\n\u003ch3\u003eClient re-sends and new deduplication\u003c/h3\u003e\n\n\n\n\u003cp\u003eIn order to reduce event loss and improve reliability, we implemented client resends. Due to connection stability and offline mode, these resends may happen immediately, within a few minutes, or maybe several days later. They may never happen! It’s actually impossible to tell if an event has been lost in transport, or if a user has used Spotify and then dropped their phone in the ocean, causing data loss. The combination of resend strategies and flaky network connections complicates things and introduces duplicate events.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the old infrastructure, we only deduplicate events within a small window of hours. However, due to the significant increase of duplicates, we hit some bottlenecks and decided to redesign the job. The biggest changes in the new job are the introduction of event message identifiers, and the adoption of Google’s Dataflow processing service instead of Hadoop. The event message identifiers were used to generate lookup indices and remove duplicates. This new strategy allowed us to look back across multiple weeks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eReceiver service — offline to online\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe legacy EDI used files on disk to store events before they were sent to a receiver service.  Spotify’s access point or other backend services would have their own availability guarantees, and we would read the data from disk eventually. In the new EDI, our receiver service needs its own availability guarantees, which was a paradigm shift in our infrastructure and for our team as \u003ca href=\"https://sre.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSREs\u003c/a\u003e. Furthermore, those files on disk were a blocker for Spotify to leverage auto-scaling fleets.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the new EDI, we have the receiver service as a highly available API used by SDKs to send events. In case of a receiver service outage, events would be temporarily stored on clients and, eventually, re-sent according to a predefined retry policy.\u003c/p\u003e\n\n\n\n\u003ch3\u003eManaged Dataflow\u003c/h3\u003e\n\n\n\n\u003cp\u003eWe wanted the new EDI to leverage cloud-managed services as much as possible. By rebuilding the architecture to run in the cloud, we can offload management responsibilities to Google, and our team can focus on providing additional value.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen building the legacy EDI, we needed to migrate a heavy Hadoop job from our on-premise cluster to the cloud. The easiest way was to run the same job on Google’s managed Hadoop solution, Dataproc, so that’s what we did. In the new EDI, the new implementation of that job uses \u003ca href=\"https://spotify.github.io/scio/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eScio\u003c/a\u003e (Scala API for Apache Beam) and runs on Google’s Dataflow instead. We considered Spark or Flink, but those had to run over Hadoop, which goes against our strategy to save us operational burden and cost.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy using Dataflow, we no longer needed to keep long-lived Hadoop clusters to execute our jobs. These clusters had to be big enough to process the largest job without issues, and were overkill for almost everything else. Maintaining these clusters was incredibly expensive. Conversely, Dataflow recycles clusters for every job and supports auto-scaling, allowing us to use and pay only for the resources we need.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWrap-up\u003c/h2\u003e\n\n\n\n\u003cp\u003eOnce we decided to redesign our EDI, we evaluated new technologies and adopted new paradigms available to us in the cloud. We had been operating our old infrastructure for years, and that helped us to understand the main pain points and fragilities. We made decisions based on the technical direction of the company, the industry state of the art, and the known scalability issues with existing components. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe started by first designing new components for the new EDI, which we hacked together into a proof of concept and quickly evolved to a more robust prototype that could be used in production. Shipping as soon as possible was critical to validate the infrastructure end to end and catch issues fast. Having the internal users onboarded early was an important forcing function to keep quality and operational maturity high. Next, we solidified the interfaces to the prototype infrastructure and scaled up traffic by onboarding many noncritical event types. With the interfaces stable, we could improve or change out the internals without friction. This approach decoupled the mass migration from actually rebuilding the infrastructure and reduced wall-clock project time significantly.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs we neared the end of the migration, we had thrown out nearly all the old, obsolete infrastructure in favor of the state of the art. We successfully changed the wheels of the moving bus, and gave Spotify’s data community a smooth ride.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infra",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png",
      "date_published": "2021-10-20T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/",
      "title": "\n                                            Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 20, 2021\u003c/span\u003e\n                \u003cspan\u003e\n                    Published by Flavio Santos (Data Infrastructure Engineer) and Robert Stephenson (Senior Product Manager)                \u003c/span\u003e\n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/changing-the-wheels-on-a-moving-bus-spotify-event-delivery-migration/\" title=\"Changing the Wheels on a Moving Bus — Spotify’s Event Delivery Migration\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-250x124.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-700x347.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-768x381.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-1536x761.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-2048x1015.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage-120x59.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infrastructure (EDI). Throughout this blog post we make a distinction between the internal users of the EDI, who are Spotify Engineers, Data Scientists, PMs and squads, and end users, who use Spotify as a service and audio platform.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn 2016, we redesigned the EDI in Google Cloud Platform (GCP) when Spotify migrated to the cloud, and we documented the journey in three blog posts (\u003ca href=\"https://engineering.atspotify.com/2016/02/25/spotifys-event-delivery-the-road-to-the-cloud-part-i/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/2016/03/03/spotifys-event-delivery-the-road-to-the-cloud-part-ii/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart II\u003c/a\u003e, and \u003ca href=\"https://engineering.atspotify.com/2016/03/10/spotifys-event-delivery-the-road-to-the-cloud-part-iii/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart III\u003c/a\u003e). Not everything went as planned, and we wrote about our learnings from operating our cloud-native EDI in \u003ca href=\"https://engineering.atspotify.com/2019/11/12/spotifys-event-delivery-life-in-the-cloud/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart IV\u003c/a\u003e. Our design was optimized to make it quick and easy for internal developers to instrument and log the data they needed. We then extended it to adapt to the General Data Protection Regulation (GDPR), we introduced streaming event delivery in addition to batch, and we brought BigQuery to our data community. We also improved operational stability and the quality of life of our on-call engineers. The peak traffic increased from 1.5M events per second to nearly 8M, and we were ready for that massive scale increase. This increased the total volume of data which we ingested daily to nearly 70TB! (Figure 1).\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"436\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-700x436.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-250x156.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-768x478.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1-120x75.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig1.png 1480w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: Average total volume (TB) of events stored daily by our \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Extract,_transform,_load\" target=\"_blank\"\u003eETL process\u003c/a\u003e (after compression).\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eHowever, with that high adoption and traffic increase we discovered some bottlenecks. Our internal users had feature requests and needed more from the system. Now our incomplete and low-quality data was degrading the productivity of the Spotify data community. Whoops!\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat was hurting us?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we designed and built the initial EDI, our team had the mission statement to “provide infrastructure for teams at Spotify to reliably collect data, and make it available, safely and efficiently.” The use cases we focused on were well supported, such as music streaming and application monitoring. As other use cases started to appear, the assumptions we made when building the system had to be revisited. During three years of operating and scaling the existing EDI, we gathered a lot of feedback from our internal users and learned a lot about our limitations.\u003c/p\u003e\n\n\n\n\u003ch3\u003eData loss\u003c/h3\u003e\n\n\n\n\u003cp\u003eMost events generated on mobile clients were sent in a fire-and-forget fashion. This might seem surprising, but because end users can enjoy Spotify while offline, there are some complications around deduplication of data that is re-sent. For example, if we detect that we are missing a data point, we don’t necessarily know if it is actually lost, or just has not arrived yet due to the user being offline, in a tunnel, or maybe having a flaky network connection. This leads to a small percentage of data loss for nearly all the data we collect, which is not acceptable for some types of data. Furthermore, this problem is compounded for datasets generated from a combination of multiple event types in order to “connect the dots” in user journeys where, for example, a single lost event can compromise the whole journey. While we had some specific client code and algorithms to reliably deliver business-critical data exactly once, it was not done in a way that we could extend to all 600+ event types that we had at that time.\u003c/p\u003e\n\n\n\n\u003ch3\u003eControl plane UX\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe workflow for a customer to progress from “instrumentation to insights” took far too long. Under normal circumstances it would take a customer a week to go through this workflow and get their data. One issue was that multiple components in the EDI had to be schema aware. For example, the receiver service, which is the entry point of the infrastructure, uses the schemas to validate that incoming data is well formed. Due to some tech debt, it took a few hours to propagate the schemas for this validation. This was an eternity in terms of iteration time. Since this process was so painful, some teams tried to instrument their features or services, but then gave up. Some other teams would shoehorn their data into existing data events. This led to gaps in what was instrumented, and a data-quality nightmare.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBackwards compatible? Or stuck in the past?\u003c/h3\u003e\n\n\n\n\u003cp\u003eFor strategic reasons, it was critical, in 2016, that we build the EDI in GCP and migrate over as quickly as possible. A key decision we took to make this happen was to stay backwards compatible to minimize the migration time. That meant we had to stick with some historical design choices that we would not have if we had built this EDI from scratch. For example:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eTab-separated values (TSV):\u003c/strong\u003e All data events were sent as TSV strings. The schemas were parsed and converted to Avro with a Python library created in 2007. The schema-aware tooling for parsing the TSV data was the main cause for the painful control plane UX mentioned earlier.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eStateful services:\u003c/strong\u003e Data events were first stored on disk and then forwarded to the EDI. This made us resilient to crashes, but made us vulnerable to data loss if a machine was taken down. Furthermore, Spotify could not take advantage of auto-scaling mechanisms or Kubernetes (without difficult workarounds) because the EDI made our service ecosystem stateful.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eLegacy perimeter:\u003c/strong\u003e Since data events were forwarded from disk to our EDI, all events triggered by Spotify clients needed to be emitted from our perimeter servers. These servers had to keep events on disk and were tightly coupled to our legacy logging mechanism. This caused some pain to perimeter administrators and hindered architectural innovations. Besides the additional complexity in the perimeter, the shared ownership of different teams with different goals caused alignment problems.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe situation\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe had hundreds of services sending events through a legacy EDI by logging data to disk. After being ingested by the infrastructure, events were consumed by hundreds of downstream data pipelines to produce derivative datasets (Figure 2). Our goal was to build a platform that takes advantage of the modern landscape in the cloud while also enabling legacy event types to be migrated easily. The workflow to create new events should be frictionless, while still following our data governance principles and applicable privacy laws.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"591\" height=\"302\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2.png 591w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-250x128.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig2-120x61.png 120w\" sizes=\"(max-width: 591px) 100vw, 591px\"/\u003e\u003cfigcaption\u003eFigure 2: Events produced by our internal services go through the legacy EDI and are consumed by hundreds of data pipelines.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eTransitioning event logging to a new infrastructure would need to take into consideration the long tail that mobile app updates have. A new version of our mobile apps takes several months to gain adoption from a high percentage of Spotify end users. We knew that we would have traffic coming to both the old and new EDIs for quite some time. Moreover, events emitted from embedded devices, such as TVs and speakers, would need special treatment as some of these devices are unlikely to ever be upgraded. We call this challenge “The Long Tail Problem”.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe partially solved “The Long Tail Problem” by designing a data transformation pipeline that reads events from legacy clients, converts them, and feeds them from the legacy EDI into the new infrastructure (Figure 3). Since we were breaking backwards compatibility, we took the opportunity to update our data model. The transformation to the new data model would not have all the necessary information available, so missing or inaccurate fields were expected occasionally. But since this transformation only applied to legacy clients, it would decrease as end users upgraded to the latest version of Spotify. This traffic would become negligible, eventually.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"355\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-700x355.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3-120x61.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-Migration_Fig3.png 1286w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 3: To handle clients which had not yet upgraded to the latest version, we implemented a job to export legacy data to the new EDI and transform it to our new data model.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWe gave data producers two options to adopt the new EDI: either redesign their instrumentation using the new data model, or stick with what they have and turn on exporting data from the legacy EDI to the new EDI. After producers onboarded, event consumers would migrate to read data from the new EDI. If producers and consumers agree to use the exporter, they would first need to update any downstream pipelines to read from the new infrastructure before making client-side changes.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGet in production with real use cases ASAP\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn order to validate our decisions, we had to find early adopters to start producing events with the new infrastructure. We presented the advantages and explained the limitations of our alpha product to potential interested teams. It was important to be able to experiment, break, and fix issues fast and safely without worrying about affecting critical production systems or data. Setting expectations with our internal users was important so we could make breaking changes when our assumptions were wrong.\u003c/p\u003e\n\n\n\n\u003cp\u003eNext, we found at least one real use case to migrate. We were looking for something specific, since different event types have different levels of importance, timeliness requirements, and downstream dependencies. We reached out to event owners to understand how their data was being used and how we could help them migrate.\u003c/p\u003e\n\n\n\n\u003cp\u003eGiven a set of eligible event types, we identified use cases that were satisfied by the limited features we had built so far. Learning which features our internal users were missing also helped prioritize our roadmap. The more features we added to the new EDI, the more event types we could onboard. We periodically revisited our design decisions and assumptions in order to identify potential problems in the new infrastructure as quickly as possible.\u003c/p\u003e\n\n\n\n\u003cp\u003eOnce we had a prototype that was working with real production traffic, we solidified the interfaces and data model and helped the alpha internal users adapt to the changes (Figure 4). This enabled us to decouple the significant work of migrating the 600+ event types which were running on the legacy infrastructure, and actually building the new EDI behind the abstractions.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"187\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-700x187.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-250x67.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-768x206.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-1536x411.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4-120x32.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery-MIgration_Fig4.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 4: New interfaces for the prototype infrastructure, so we could concurrently migrate internal users to the new Event Delivery Infrastructure while building it.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eJust-in-time optimizations\u003c/h2\u003e\n\n\n\n\u003cp\u003ePrematurely optimizing is generally a bad idea without motivating metrics. We always want to be as efficient as possible, but we had to prioritize and make trade-offs. Part of the challenge was to find a good balance between the desired efficiency of our infrastructure and the features we absolutely needed to release in order to accomplish our goals.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe learned from the first EDI that we needed to design for our targeted service availability from the start. Since transactional data collection was not required, there was no need for 100% delivery. We instead had to determine what level of service availability was acceptable and understand the trade-offs associated with that.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe design changes and decisions\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have two main interfaces to the EDI. The control plane is the starting point where internal users declare their events, design their schemas, and bind them to specific SDKs. The data plane receives events sent by those SDKs, divides them by event type, and makes them available as batch datasets or data-streaming topics. The events go through several other components that deduplicate, translate to our well-designed data model, and pseudonymize personal information. The output data is reliably stored for Spotify’s data community to consume and build data pipelines. Due to the evolving needs of internal users, as well as operational overhead and scalability concerns, we needed to make changes between the old and new infrastructure.\u003c/p\u003e\n\n\n\n\u003ch3\u003eClient re-sends and new deduplication\u003c/h3\u003e\n\n\n\n\u003cp\u003eIn order to reduce event loss and improve reliability, we implemented client resends. Due to connection stability and offline mode, these resends may happen immediately, within a few minutes, or maybe several days later. They may never happen! It’s actually impossible to tell if an event has been lost in transport, or if a user has used Spotify and then dropped their phone in the ocean, causing data loss. The combination of resend strategies and flaky network connections complicates things and introduces duplicate events.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the old infrastructure, we only deduplicate events within a small window of hours. However, due to the significant increase of duplicates, we hit some bottlenecks and decided to redesign the job. The biggest changes in the new job are the introduction of event message identifiers, and the adoption of Google’s Dataflow processing service instead of Hadoop. The event message identifiers were used to generate lookup indices and remove duplicates. This new strategy allowed us to look back across multiple weeks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eReceiver service — offline to online\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe legacy EDI used files on disk to store events before they were sent to a receiver service.  Spotify’s access point or other backend services would have their own availability guarantees, and we would read the data from disk eventually. In the new EDI, our receiver service needs its own availability guarantees, which was a paradigm shift in our infrastructure and for our team as \u003ca href=\"https://sre.google/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSREs\u003c/a\u003e. Furthermore, those files on disk were a blocker for Spotify to leverage auto-scaling fleets.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the new EDI, we have the receiver service as a highly available API used by SDKs to send events. In case of a receiver service outage, events would be temporarily stored on clients and, eventually, re-sent according to a predefined retry policy.\u003c/p\u003e\n\n\n\n\u003ch3\u003eManaged Dataflow\u003c/h3\u003e\n\n\n\n\u003cp\u003eWe wanted the new EDI to leverage cloud-managed services as much as possible. By rebuilding the architecture to run in the cloud, we can offload management responsibilities to Google, and our team can focus on providing additional value.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen building the legacy EDI, we needed to migrate a heavy Hadoop job from our on-premise cluster to the cloud. The easiest way was to run the same job on Google’s managed Hadoop solution, Dataproc, so that’s what we did. In the new EDI, the new implementation of that job uses \u003ca href=\"https://spotify.github.io/scio/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eScio\u003c/a\u003e (Scala API for Apache Beam) and runs on Google’s Dataflow instead. We considered Spark or Flink, but those had to run over Hadoop, which goes against our strategy to save us operational burden and cost.\u003c/p\u003e\n\n\n\n\u003cp\u003eBy using Dataflow, we no longer needed to keep long-lived Hadoop clusters to execute our jobs. These clusters had to be big enough to process the largest job without issues, and were overkill for almost everything else. Maintaining these clusters was incredibly expensive. Conversely, Dataflow recycles clusters for every job and supports auto-scaling, allowing us to use and pay only for the resources we need.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWrap-up\u003c/h2\u003e\n\n\n\n\u003cp\u003eOnce we decided to redesign our EDI, we evaluated new technologies and adopted new paradigms available to us in the cloud. We had been operating our old infrastructure for years, and that helped us to understand the main pain points and fragilities. We made decisions based on the technical direction of the company, the industry state of the art, and the known scalability issues with existing components. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe started by first designing new components for the new EDI, which we hacked together into a proof of concept and quickly evolved to a more robust prototype that could be used in production. Shipping as soon as possible was critical to validate the infrastructure end to end and catch issues fast. Having the internal users onboarded early was an important forcing function to keep quality and operational maturity high. Next, we solidified the interfaces to the prototype infrastructure and scaled up traffic by onboarding many noncritical event types. With the interfaces stable, we could improve or change out the internals without friction. This approach decoupled the mass migration from actually rebuilding the infrastructure and reduced wall-clock project time significantly.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs we neared the end of the migration, we had thrown out nearly all the old, obsolete infrastructure in favor of the state of the art. We successfully changed the wheels of the moving bus, and gave Spotify’s data community a smooth ride.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify, data rules all. We log a variety of data, from listening history, to results of A/B testing, to page load times so we can analyze and improve the Spotify service. We instrument and log data across every surface that is running Spotify code through a system called the Event Delivery Infra",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/Event-Delivery_Header-IMage.png",
      "date_published": "2021-10-20T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/",
      "title": "\n                                            A Product Story: Three Lessons We Learned from Developing the Mobile App\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 5, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/\" title=\"A Product Story: Three Lessons We Learned from Developing the Mobile App\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-120x63.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eRemember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we travel with our music forever.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eEpisode 02\u003c/a\u003e of our podcast series, \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eSpotify: A Product Story\u003c/em\u003e\u003c/a\u003e\u003cem\u003e,\u003c/em\u003e host and Chief R\u0026amp;D Officer Gustav Söderström chats with the engineers, executives, and other Spotifiers who helped make the mobile streaming revolution possible. Why was it so difficult to create Spotify for mobile even after we created the desktop app? Why were Spotify’s licensing deals such a game changer for us? Keep reading to find out the answers, and don’t forget to \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA\u0026amp;dl_branch=1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elisten to the podcast\u003c/a\u003e itself to learn even more.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eWhat’s user research really for?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we first released the Spotify desktop app in Sweden in 2008, the idea of creating an on-the-go version of that same experience seemed out of the question. After all, Spotify didn’t have access to iPods or the other dedicated digital music players, and besides, at the time, most phones were equipped with only enough memory to hold a handful of songs.\u003c/p\u003e\n\n\n\n\u003cp\u003eThen something happened. Smartphones happened. The iPhone happened. Suddenly the idea of creating a piece of software that people could carry around with them everywhere didn’t seem so far-fetched.\u003c/p\u003e\n\n\n\n\u003cp\u003eThough it was theoretically possible to create a mobile edition of Spotify, there was still one major issue. A free, offline music app, one that could truly compete with the already-offline MP3 players, just wasn’t in the cards. First off, it couldn’t be free — you can’t click on ads without an internet connection — so it would have to be a paid service. Plus, our listeners already had access to all the music they wanted through piracy. So, once again, we had to ask ourselves something: Why would anyone pay for Spotify? How do you charge for nothing? \u003c/p\u003e\n\n\n\n\u003cp\u003eAnd indeed, when we surveyed our users, we discovered that $10 per month — the number we settled on after months of licensing negotiations with labels (more on those in a bit) — was a number literally no one said they would pay.\u003c/p\u003e\n\n\n\n\u003cp\u003eWell, at least they \u003cem\u003esaid\u003c/em\u003e that. See, user research is great, but you have to know what it’s really there for. That was our \u003cstrong\u003efirst lesson:\u003c/strong\u003e \u003cstrong\u003euser research is for understanding what people \u003c/strong\u003e\u003cstrong\u003e\u003cem\u003ethink\u003c/em\u003e\u003c/strong\u003e\u003cstrong\u003e they will do, not what they will actually do.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThere’s a reason for that, as Gustav points out.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eGustav:\u003c/strong\u003e Humans are not very good at predicting our own future behavior, especially when it comes to what we are prepared to pay for something like convenience. We just can’t imagine our future selves being that lazy.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eExactly. In other words, just like we discovered while creating the desktop app, convenience is key. Of course people don’t think they would pay for something they were already getting for free — but they just might if it’s a lot more convenient than what they’re using. At least, that’s what we were betting on.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe just needed to find another magic trick.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBeware your optimization bias\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThat was easier said than done, though. Setting aside the business side of things, actually building a mobile experience convenient enough to convert music pirates was a tall order, one without an obvious solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eBack then, in the early days of Spotify, the desktop app may have technically been able to run on the iPhone, but that didn’t mean it was built for it. When Gustav asked Mattias Arrelid, one of Spotify’s early Mac developers, what the challenges were in translating the desktop app into a great mobile product, two things sprang immediately to Mattias’ mind:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e“One [challenge] is that … a phone doesn’t have a constant power supply. For example, it doesn’t have a constant high-bandwidth network connection. Spotify was, at that time, at least leveraging peer to peer … If you do that on the phone, you basically force it to keep the radio on quite a bit. And that is draining in terms of battery.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e“The other one is data. [When] you had Spotify running on the desktop, you relied on some kind of data connection that [you] don’t really need to care about … On the phone, you had many contracts that had limited bandwidth included in the monthly allowance. So you really wanted to be careful about your consumption of that.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eIt turns out by developing the desktop app to be as efficient as possible, we were actually developing something that was \u003cem\u003eonly\u003c/em\u003e efficient as a desktop app. Concerns like bandwidth and power consumption never factored into the equation, since you don’t need to worry about them when you’re a desktop user. And that’s where \u003cstrong\u003ethe second lesson\u003c/strong\u003e comes in: \u003cstrong\u003ewhether you realize it or not, you’re always optimizing for something.\u003c/strong\u003e This isn’t always a bad thing — after all, you’re still optimizing — but you are unwittingly creating blind spots for you and your product, ones that can hamper your growth down the road.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of the solutions Mattias and the engineering team came up with to improve battery performance involved changing up the codec we used to encode audio. (\u003ca href=\"https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDive into the full episode\u003c/a\u003e to hear all the details about why.) But changing up our codec didn’t just have technical ramifications — it had business implications as well, implications that dovetail nicely with our final lesson.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eYour tech is only half of the story\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpotify didn’t invent streaming, and it was only a matter of time until other companies could start to pull off a service similar to what we were offering. What we needed, in other words, was a breakthrough in our business strategy as well.\u003c/p\u003e\n\n\n\n\u003cp\u003eEnter licensing. Contrary to what you may expect, there isn’t a one-size-fits-all codec license for streaming songs. Different licenses are required for almost every use case — say, streaming on desktop versus streaming on mobile. This meant that hammering out licensing deals with record labels could end up being a lengthy, complex process. Rather than dealing with these complications, we saw plenty of other companies kick that sort of nitty-gritty work down the road, focusing instead on shipping code and getting to market quicker.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut to us, this wasn’t a sustainable strategy. So rather than take the same route, Spotify slowed things down, sweating the details in the negotiations for months on end, making sure that we had licenses that no one else had. That way, we could guarantee that we hit the ground running when we introduced a paid mobile tier. Here’s Spotify’s first general counsel, Petra Hansson, on the matter:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003ePetra:\u003c/strong\u003e We spent a lot of time getting those licenses right. Because, you know, back in the day, the norm was more to just sign any licenses and then, you know, try and flip the companies, so it became someone else’s problem or companies would go belly up. So we were sort of hell-bent. And that’s one of the reasons why I joined [Spotify], because what I really liked about the company was that [co-founder and CEO Daniel Ek]’s vision was always to build something long-term and sustainable.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAs it so happens, truly great product development is more than just a cool new feature or even a breakthrough new technology. \u003cstrong\u003eOur third lesson is that\u003c/strong\u003e \u003cstrong\u003etruly great product development, in short, almost always combines technological innovation with business innovation.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThat’s another three lessons down, but there are plenty more to go. The podcast series \u003cem\u003eSpotify: A Product Story\u003c/em\u003e shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join podcast host and Chief R\u0026amp;D Officer Gustav Söderström, and \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echeck out all the episodes right here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we trave",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png",
      "date_published": "2021-10-05T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/10/05/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/",
      "title": "\n                                            A Product Story: Three Lessons We Learned from Developing the Mobile App\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 5, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/10/05/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/\" title=\"A Product Story: Three Lessons We Learned from Developing the Mobile App\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png\" alt=\"\" loading=\"lazy\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-120x63.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eRemember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we travel with our music forever.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eEpisode 02\u003c/a\u003e of our podcast series, \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eSpotify: A Product Story\u003c/em\u003e\u003c/a\u003e\u003cem\u003e,\u003c/em\u003e host and Chief R\u0026amp;D Officer Gustav Söderström chats with the engineers, executives, and other Spotifiers who helped make the mobile streaming revolution possible. Why was it so difficult to create Spotify for mobile even after we created the desktop app? Why were Spotify’s licensing deals such a game changer for us? Keep reading to find out the answers, and don’t forget to \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA\u0026amp;dl_branch=1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elisten to the podcast\u003c/a\u003e itself to learn even more.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eWhat’s user research really for?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we first released the Spotify desktop app in Sweden in 2008, the idea of creating an on-the-go version of that same experience seemed out of the question. After all, Spotify didn’t have access to iPods or the other dedicated digital music players, and besides, at the time, most phones were equipped with only enough memory to hold a handful of songs.\u003c/p\u003e\n\n\n\n\u003cp\u003eThen something happened. Smartphones happened. The iPhone happened. Suddenly the idea of creating a piece of software that people could carry around with them everywhere didn’t seem so far-fetched.\u003c/p\u003e\n\n\n\n\u003cp\u003eThough it was theoretically possible to create a mobile edition of Spotify, there was still one major issue. A free, offline music app, one that could truly compete with the already-offline MP3 players, just wasn’t in the cards. First off, it couldn’t be free — you can’t click on ads without an internet connection — so it would have to be a paid service. Plus, our listeners already had access to all the music they wanted through piracy. So, once again, we had to ask ourselves something: Why would anyone pay for Spotify? How do you charge for nothing? \u003c/p\u003e\n\n\n\n\u003cp\u003eAnd indeed, when we surveyed our users, we discovered that $10 per month — the number we settled on after months of licensing negotiations with labels (more on those in a bit) — was a number literally no one said they would pay.\u003c/p\u003e\n\n\n\n\u003cp\u003eWell, at least they \u003cem\u003esaid\u003c/em\u003e that. See, user research is great, but you have to know what it’s really there for. That was our \u003cstrong\u003efirst lesson:\u003c/strong\u003e \u003cstrong\u003euser research is for understanding what people \u003c/strong\u003e\u003cstrong\u003e\u003cem\u003ethink\u003c/em\u003e\u003c/strong\u003e\u003cstrong\u003e they will do, not what they will actually do.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThere’s a reason for that, as Gustav points out.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eGustav:\u003c/strong\u003e Humans are not very good at predicting our own future behavior, especially when it comes to what we are prepared to pay for something like convenience. We just can’t imagine our future selves being that lazy.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eExactly. In other words, just like we discovered while creating the desktop app, convenience is key. Of course people don’t think they would pay for something they were already getting for free — but they just might if it’s a lot more convenient than what they’re using. At least, that’s what we were betting on.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe just needed to find another magic trick.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBeware your optimization bias\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThat was easier said than done, though. Setting aside the business side of things, actually building a mobile experience convenient enough to convert music pirates was a tall order, one without an obvious solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eBack then, in the early days of Spotify, the desktop app may have technically been able to run on the iPhone, but that didn’t mean it was built for it. When Gustav asked Mattias Arrelid, one of Spotify’s early Mac developers, what the challenges were in translating the desktop app into a great mobile product, two things sprang immediately to Mattias’ mind:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e“One [challenge] is that … a phone doesn’t have a constant power supply. For example, it doesn’t have a constant high-bandwidth network connection. Spotify was, at that time, at least leveraging peer to peer … If you do that on the phone, you basically force it to keep the radio on quite a bit. And that is draining in terms of battery.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e“The other one is data. [When] you had Spotify running on the desktop, you relied on some kind of data connection that [you] don’t really need to care about … On the phone, you had many contracts that had limited bandwidth included in the monthly allowance. So you really wanted to be careful about your consumption of that.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eIt turns out by developing the desktop app to be as efficient as possible, we were actually developing something that was \u003cem\u003eonly\u003c/em\u003e efficient as a desktop app. Concerns like bandwidth and power consumption never factored into the equation, since you don’t need to worry about them when you’re a desktop user. And that’s where \u003cstrong\u003ethe second lesson\u003c/strong\u003e comes in: \u003cstrong\u003ewhether you realize it or not, you’re always optimizing for something.\u003c/strong\u003e This isn’t always a bad thing — after all, you’re still optimizing — but you are unwittingly creating blind spots for you and your product, ones that can hamper your growth down the road.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of the solutions Mattias and the engineering team came up with to improve battery performance involved changing up the codec we used to encode audio. (\u003ca href=\"https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDive into the full episode\u003c/a\u003e to hear all the details about why.) But changing up our codec didn’t just have technical ramifications — it had business implications as well, implications that dovetail nicely with our final lesson.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eYour tech is only half of the story\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpotify didn’t invent streaming, and it was only a matter of time until other companies could start to pull off a service similar to what we were offering. What we needed, in other words, was a breakthrough in our business strategy as well.\u003c/p\u003e\n\n\n\n\u003cp\u003eEnter licensing. Contrary to what you may expect, there isn’t a one-size-fits-all codec license for streaming songs. Different licenses are required for almost every use case — say, streaming on desktop versus streaming on mobile. This meant that hammering out licensing deals with record labels could end up being a lengthy, complex process. Rather than dealing with these complications, we saw plenty of other companies kick that sort of nitty-gritty work down the road, focusing instead on shipping code and getting to market quicker.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut to us, this wasn’t a sustainable strategy. So rather than take the same route, Spotify slowed things down, sweating the details in the negotiations for months on end, making sure that we had licenses that no one else had. That way, we could guarantee that we hit the ground running when we introduced a paid mobile tier. Here’s Spotify’s first general counsel, Petra Hansson, on the matter:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003ePetra:\u003c/strong\u003e We spent a lot of time getting those licenses right. Because, you know, back in the day, the norm was more to just sign any licenses and then, you know, try and flip the companies, so it became someone else’s problem or companies would go belly up. So we were sort of hell-bent. And that’s one of the reasons why I joined [Spotify], because what I really liked about the company was that [co-founder and CEO Daniel Ek]’s vision was always to build something long-term and sustainable.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAs it so happens, truly great product development is more than just a cool new feature or even a breakthrough new technology. \u003cstrong\u003eOur third lesson is that\u003c/strong\u003e \u003cstrong\u003etruly great product development, in short, almost always combines technological innovation with business innovation.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThat’s another three lessons down, but there are plenty more to go. The podcast series \u003cem\u003eSpotify: A Product Story\u003c/em\u003e shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join podcast host and Chief R\u0026amp;D Officer Gustav Söderström, and \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echeck out all the episodes right here\u003c/a\u003e.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we trave",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png",
      "date_published": "2021-10-05T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/",
      "title": "\n                                            A Product Story: Three Lessons We Learned from Developing the Mobile App\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 5, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/\" title=\"A Product Story: Three Lessons We Learned from Developing the Mobile App\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-120x63.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eRemember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we travel with our music forever.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eEpisode 02\u003c/a\u003e of our podcast series, \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eSpotify: A Product Story\u003c/em\u003e\u003c/a\u003e\u003cem\u003e,\u003c/em\u003e host and Chief R\u0026amp;D Officer Gustav Söderström chats with the engineers, executives, and other Spotifiers who helped make the mobile streaming revolution possible. Why was it so difficult to create Spotify for mobile even after we created the desktop app? Why were Spotify’s licensing deals such a game changer for us? Keep reading to find out the answers, and don’t forget to \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA\u0026amp;dl_branch=1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elisten to the podcast\u003c/a\u003e itself to learn even more.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eWhat’s user research really for?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we first released the Spotify desktop app in Sweden in 2008, the idea of creating an on-the-go version of that same experience seemed out of the question. After all, Spotify didn’t have access to iPods or the other dedicated digital music players, and besides, at the time, most phones were equipped with only enough memory to hold a handful of songs.\u003c/p\u003e\n\n\n\n\u003cp\u003eThen something happened. Smartphones happened. The iPhone happened. Suddenly the idea of creating a piece of software that people could carry around with them everywhere didn’t seem so far-fetched.\u003c/p\u003e\n\n\n\n\u003cp\u003eThough it was theoretically possible to create a mobile edition of Spotify, there was still one major issue. A free, offline music app, one that could truly compete with the already-offline MP3 players, just wasn’t in the cards. First off, it couldn’t be free — you can’t click on ads without an internet connection — so it would have to be a paid service. Plus, our listeners already had access to all the music they wanted through piracy. So, once again, we had to ask ourselves something: Why would anyone pay for Spotify? How do you charge for nothing? \u003c/p\u003e\n\n\n\n\u003cp\u003eAnd indeed, when we surveyed our users, we discovered that $10 per month — the number we settled on after months of licensing negotiations with labels (more on those in a bit) — was a number literally no one said they would pay.\u003c/p\u003e\n\n\n\n\u003cp\u003eWell, at least they \u003cem\u003esaid\u003c/em\u003e that. See, user research is great, but you have to know what it’s really there for. That was our \u003cstrong\u003efirst lesson:\u003c/strong\u003e \u003cstrong\u003euser research is for understanding what people \u003c/strong\u003e\u003cstrong\u003e\u003cem\u003ethink\u003c/em\u003e\u003c/strong\u003e\u003cstrong\u003e they will do, not what they will actually do.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThere’s a reason for that, as Gustav points out.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eGustav:\u003c/strong\u003e Humans are not very good at predicting our own future behavior, especially when it comes to what we are prepared to pay for something like convenience. We just can’t imagine our future selves being that lazy.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eExactly. In other words, just like we discovered while creating the desktop app, convenience is key. Of course people don’t think they would pay for something they were already getting for free — but they just might if it’s a lot more convenient than what they’re using. At least, that’s what we were betting on.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe just needed to find another magic trick.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBeware your optimization bias\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThat was easier said than done, though. Setting aside the business side of things, actually building a mobile experience convenient enough to convert music pirates was a tall order, one without an obvious solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eBack then, in the early days of Spotify, the desktop app may have technically been able to run on the iPhone, but that didn’t mean it was built for it. When Gustav asked Mattias Arrelid, one of Spotify’s early Mac developers, what the challenges were in translating the desktop app into a great mobile product, two things sprang immediately to Mattias’ mind:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e“One [challenge] is that … a phone doesn’t have a constant power supply. For example, it doesn’t have a constant high-bandwidth network connection. Spotify was, at that time, at least leveraging peer to peer … If you do that on the phone, you basically force it to keep the radio on quite a bit. And that is draining in terms of battery.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e“The other one is data. [When] you had Spotify running on the desktop, you relied on some kind of data connection that [you] don’t really need to care about … On the phone, you had many contracts that had limited bandwidth included in the monthly allowance. So you really wanted to be careful about your consumption of that.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eIt turns out by developing the desktop app to be as efficient as possible, we were actually developing something that was \u003cem\u003eonly\u003c/em\u003e efficient as a desktop app. Concerns like bandwidth and power consumption never factored into the equation, since you don’t need to worry about them when you’re a desktop user. And that’s where \u003cstrong\u003ethe second lesson\u003c/strong\u003e comes in: \u003cstrong\u003ewhether you realize it or not, you’re always optimizing for something.\u003c/strong\u003e This isn’t always a bad thing — after all, you’re still optimizing — but you are unwittingly creating blind spots for you and your product, ones that can hamper your growth down the road.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of the solutions Mattias and the engineering team came up with to improve battery performance involved changing up the codec we used to encode audio. (\u003ca href=\"https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDive into the full episode\u003c/a\u003e to hear all the details about why.) But changing up our codec didn’t just have technical ramifications — it had business implications as well, implications that dovetail nicely with our final lesson.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eYour tech is only half of the story\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpotify didn’t invent streaming, and it was only a matter of time until other companies could start to pull off a service similar to what we were offering. What we needed, in other words, was a breakthrough in our business strategy as well.\u003c/p\u003e\n\n\n\n\u003cp\u003eEnter licensing. Contrary to what you may expect, there isn’t a one-size-fits-all codec license for streaming songs. Different licenses are required for almost every use case — say, streaming on desktop versus streaming on mobile. This meant that hammering out licensing deals with record labels could end up being a lengthy, complex process. Rather than dealing with these complications, we saw plenty of other companies kick that sort of nitty-gritty work down the road, focusing instead on shipping code and getting to market quicker.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut to us, this wasn’t a sustainable strategy. So rather than take the same route, Spotify slowed things down, sweating the details in the negotiations for months on end, making sure that we had licenses that no one else had. That way, we could guarantee that we hit the ground running when we introduced a paid mobile tier. Here’s Spotify’s first general counsel, Petra Hansson, on the matter:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003ePetra:\u003c/strong\u003e We spent a lot of time getting those licenses right. Because, you know, back in the day, the norm was more to just sign any licenses and then, you know, try and flip the companies, so it became someone else’s problem or companies would go belly up. So we were sort of hell-bent. And that’s one of the reasons why I joined [Spotify], because what I really liked about the company was that [co-founder and CEO Daniel Ek]’s vision was always to build something long-term and sustainable.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAs it so happens, truly great product development is more than just a cool new feature or even a breakthrough new technology. \u003cstrong\u003eOur third lesson is that\u003c/strong\u003e \u003cstrong\u003etruly great product development, in short, almost always combines technological innovation with business innovation.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThat’s another three lessons down, but there are plenty more to go. The podcast series \u003cem\u003eSpotify: A Product Story\u003c/em\u003e shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join podcast host and Chief R\u0026amp;D Officer Gustav Söderström, and \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echeck out all the episodes right here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we trave",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png",
      "date_published": "2021-10-05T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/10/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/",
      "title": "\n                                            A Product Story: Three Lessons We Learned from Developing the Mobile App\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 5, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/10/a-product-story-three-lessons-we-learned-from-developing-the-mobile-app/\" title=\"A Product Story: Three Lessons We Learned from Developing the Mobile App\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630-120x63.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eRemember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we travel with our music forever.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn \u003ca href=\"https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eEpisode 02\u003c/a\u003e of our podcast series, \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eSpotify: A Product Story\u003c/em\u003e\u003c/a\u003e\u003cem\u003e,\u003c/em\u003e host and Chief R\u0026amp;D Officer Gustav Söderström chats with the engineers, executives, and other Spotifiers who helped make the mobile streaming revolution possible. Why was it so difficult to create Spotify for mobile even after we created the desktop app? Why were Spotify’s licensing deals such a game changer for us? Keep reading to find out the answers, and don’t forget to \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA\u0026amp;dl_branch=1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elisten to the podcast\u003c/a\u003e itself to learn even more.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eWhat’s user research really for?\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we first released the Spotify desktop app in Sweden in 2008, the idea of creating an on-the-go version of that same experience seemed out of the question. After all, Spotify didn’t have access to iPods or the other dedicated digital music players, and besides, at the time, most phones were equipped with only enough memory to hold a handful of songs.\u003c/p\u003e\n\n\n\n\u003cp\u003eThen something happened. Smartphones happened. The iPhone happened. Suddenly the idea of creating a piece of software that people could carry around with them everywhere didn’t seem so far-fetched.\u003c/p\u003e\n\n\n\n\u003cp\u003eThough it was theoretically possible to create a mobile edition of Spotify, there was still one major issue. A free, offline music app, one that could truly compete with the already-offline MP3 players, just wasn’t in the cards. First off, it couldn’t be free — you can’t click on ads without an internet connection — so it would have to be a paid service. Plus, our listeners already had access to all the music they wanted through piracy. So, once again, we had to ask ourselves something: Why would anyone pay for Spotify? How do you charge for nothing? \u003c/p\u003e\n\n\n\n\u003cp\u003eAnd indeed, when we surveyed our users, we discovered that $10 per month — the number we settled on after months of licensing negotiations with labels (more on those in a bit) — was a number literally no one said they would pay.\u003c/p\u003e\n\n\n\n\u003cp\u003eWell, at least they \u003cem\u003esaid\u003c/em\u003e that. See, user research is great, but you have to know what it’s really there for. That was our \u003cstrong\u003efirst lesson:\u003c/strong\u003e \u003cstrong\u003euser research is for understanding what people \u003c/strong\u003e\u003cstrong\u003e\u003cem\u003ethink\u003c/em\u003e\u003c/strong\u003e\u003cstrong\u003e they will do, not what they will actually do.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThere’s a reason for that, as Gustav points out.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eGustav:\u003c/strong\u003e Humans are not very good at predicting our own future behavior, especially when it comes to what we are prepared to pay for something like convenience. We just can’t imagine our future selves being that lazy.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eExactly. In other words, just like we discovered while creating the desktop app, convenience is key. Of course people don’t think they would pay for something they were already getting for free — but they just might if it’s a lot more convenient than what they’re using. At least, that’s what we were betting on.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe just needed to find another magic trick.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBeware your optimization bias\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThat was easier said than done, though. Setting aside the business side of things, actually building a mobile experience convenient enough to convert music pirates was a tall order, one without an obvious solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eBack then, in the early days of Spotify, the desktop app may have technically been able to run on the iPhone, but that didn’t mean it was built for it. When Gustav asked Mattias Arrelid, one of Spotify’s early Mac developers, what the challenges were in translating the desktop app into a great mobile product, two things sprang immediately to Mattias’ mind:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e“One [challenge] is that … a phone doesn’t have a constant power supply. For example, it doesn’t have a constant high-bandwidth network connection. Spotify was, at that time, at least leveraging peer to peer … If you do that on the phone, you basically force it to keep the radio on quite a bit. And that is draining in terms of battery.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e“The other one is data. [When] you had Spotify running on the desktop, you relied on some kind of data connection that [you] don’t really need to care about … On the phone, you had many contracts that had limited bandwidth included in the monthly allowance. So you really wanted to be careful about your consumption of that.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eIt turns out by developing the desktop app to be as efficient as possible, we were actually developing something that was \u003cem\u003eonly\u003c/em\u003e efficient as a desktop app. Concerns like bandwidth and power consumption never factored into the equation, since you don’t need to worry about them when you’re a desktop user. And that’s where \u003cstrong\u003ethe second lesson\u003c/strong\u003e comes in: \u003cstrong\u003ewhether you realize it or not, you’re always optimizing for something.\u003c/strong\u003e This isn’t always a bad thing — after all, you’re still optimizing — but you are unwittingly creating blind spots for you and your product, ones that can hamper your growth down the road.\u003c/p\u003e\n\n\n\n\u003cp\u003eOne of the solutions Mattias and the engineering team came up with to improve battery performance involved changing up the codec we used to encode audio. (\u003ca href=\"https://open.spotify.com/episode/7oB1UYZtOiKqY1Gj3niptG?si=21630574510943f3\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDive into the full episode\u003c/a\u003e to hear all the details about why.) But changing up our codec didn’t just have technical ramifications — it had business implications as well, implications that dovetail nicely with our final lesson.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eYour tech is only half of the story\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSpotify didn’t invent streaming, and it was only a matter of time until other companies could start to pull off a service similar to what we were offering. What we needed, in other words, was a breakthrough in our business strategy as well.\u003c/p\u003e\n\n\n\n\u003cp\u003eEnter licensing. Contrary to what you may expect, there isn’t a one-size-fits-all codec license for streaming songs. Different licenses are required for almost every use case — say, streaming on desktop versus streaming on mobile. This meant that hammering out licensing deals with record labels could end up being a lengthy, complex process. Rather than dealing with these complications, we saw plenty of other companies kick that sort of nitty-gritty work down the road, focusing instead on shipping code and getting to market quicker.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut to us, this wasn’t a sustainable strategy. So rather than take the same route, Spotify slowed things down, sweating the details in the negotiations for months on end, making sure that we had licenses that no one else had. That way, we could guarantee that we hit the ground running when we introduced a paid mobile tier. Here’s Spotify’s first general counsel, Petra Hansson, on the matter:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003ePetra:\u003c/strong\u003e We spent a lot of time getting those licenses right. Because, you know, back in the day, the norm was more to just sign any licenses and then, you know, try and flip the companies, so it became someone else’s problem or companies would go belly up. So we were sort of hell-bent. And that’s one of the reasons why I joined [Spotify], because what I really liked about the company was that [co-founder and CEO Daniel Ek]’s vision was always to build something long-term and sustainable.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAs it so happens, truly great product development is more than just a cool new feature or even a breakthrough new technology. \u003cstrong\u003eOur third lesson is that\u003c/strong\u003e \u003cstrong\u003etruly great product development, in short, almost always combines technological innovation with business innovation.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThat’s another three lessons down, but there are plenty more to go. The podcast series \u003cem\u003eSpotify: A Product Story\u003c/em\u003e shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join podcast host and Chief R\u0026amp;D Officer Gustav Söderström, and \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echeck out all the episodes right here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Remember what life was like before smartphones? Remember manually having to sync your computer’s playlists with your iPod every time you added a few songs? One of Spotify’s core products, our mobile app, was designed specifically to leave all of that busywork in the past, changing how we trave",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/10/A-Product-Story_02-Illustration_1200x630.png",
      "date_published": "2021-10-05T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/",
      "title": "\n                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 23, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/\" title=\"How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w\" sizes=\"(max-width: 512px) 100vw, 512px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?\u003c/p\u003e\n\n\n\n\u003cp\u003eInstead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?\u003c/p\u003e\n\n\n\n\u003cp\u003ePia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the \u003ca href=\"https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eThoughtworks\u003c/em\u003e podcast\u003c/a\u003e: What types of problems do Spotify engineers face? And why did we create \u003ca href=\"http://backstage.spotify.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGrowing pains\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?\u003c/p\u003e\n\n\n\n\u003cp\u003ePia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”\u003cbr/\u003e\u003c/li\u003e\u003cli\u003e“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eConsidering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBackstage: a platform for your platforms\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Pia notes, Spotify developed \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhere our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our \u003ca href=\"https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGolden Paths\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eBecause of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because \u003ca href=\"https://github.com/backstage/backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage is a rapidly growing open source tool\u003c/a\u003e, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” \u003c/p\u003e\n\n\n\n\u003cp\u003eInterested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the \u003ca href=\"https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThoughtworks podcast episode\u003c/a\u003e. And if you’re curious about how to get started with Backstage, read more about that \u003ca href=\"https://backstage.spotify.com/blog/getting-started-with-backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png",
      "date_published": "2021-09-23T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/09/23/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/",
      "title": "\n                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 23, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/09/23/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/\" title=\"How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w\" sizes=\"(max-width: 512px) 100vw, 512px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/09/Backstage_Developers_Header.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?\u003c/p\u003e\n\n\n\n\u003cp\u003eInstead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?\u003c/p\u003e\n\n\n\n\u003cp\u003ePia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the \u003ca href=\"https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eThoughtworks\u003c/em\u003e podcast\u003c/a\u003e: What types of problems do Spotify engineers face? And why did we create \u003ca href=\"http://backstage.spotify.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGrowing pains\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?\u003c/p\u003e\n\n\n\n\u003cp\u003ePia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”\u003cbr/\u003e\u003c/li\u003e\u003cli\u003e“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eConsidering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBackstage: a platform for your platforms\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Pia notes, Spotify developed \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhere our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our \u003ca href=\"https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGolden Paths\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eBecause of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because \u003ca href=\"https://github.com/backstage/backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage is a rapidly growing open source tool\u003c/a\u003e, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” \u003c/p\u003e\n\n\n\n\u003cp\u003eInterested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the \u003ca href=\"https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThoughtworks podcast episode\u003c/a\u003e. And if you’re curious about how to get started with Backstage, read more about that \u003ca href=\"https://backstage.spotify.com/blog/getting-started-with-backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Backstage_Developers_Header.png",
      "date_published": "2021-09-23T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/09/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/",
      "title": "\n                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 23, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/09/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/\" title=\"How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w\" sizes=\"(max-width: 512px) 100vw, 512px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?\u003c/p\u003e\n\n\n\n\u003cp\u003eInstead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?\u003c/p\u003e\n\n\n\n\u003cp\u003ePia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the \u003ca href=\"https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eThoughtworks\u003c/em\u003e podcast\u003c/a\u003e: What types of problems do Spotify engineers face? And why did we create \u003ca href=\"http://backstage.spotify.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGrowing pains\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?\u003c/p\u003e\n\n\n\n\u003cp\u003ePia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”\u003cbr/\u003e\u003c/li\u003e\u003cli\u003e“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eConsidering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBackstage: a platform for your platforms\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Pia notes, Spotify developed \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhere our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our \u003ca href=\"https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGolden Paths\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eBecause of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because \u003ca href=\"https://github.com/backstage/backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage is a rapidly growing open source tool\u003c/a\u003e, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” \u003c/p\u003e\n\n\n\n\u003cp\u003eInterested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the \u003ca href=\"https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThoughtworks podcast episode\u003c/a\u003e. And if you’re curious about how to get started with Backstage, read more about that \u003ca href=\"https://backstage.spotify.com/blog/getting-started-with-backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png",
      "date_published": "2021-09-23T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/",
      "title": "\n                                            How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 23, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/how-backstage-made-our-developers-more-effective-and-how-it-can-help-yours-too/\" title=\"How Backstage Made Our Developers More Effective — And How It Can Help Yours, Too\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png 512w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header-120x63.png 120w\" sizes=\"(max-width: 512px) 100vw, 512px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers anyway?\u003c/p\u003e\n\n\n\n\u003cp\u003eInstead, it’s more helpful to think of performance in terms of “developer effectiveness”. Suddenly, it’s not about the quantity of work and time spent, but the quality. Are engineers wasting a bunch of their days just trying to find what they need to get started, or are they able to jump straight into the work they really want to do with as few blockers as possible?\u003c/p\u003e\n\n\n\n\u003cp\u003ePia Nilsson, Director of Engineering and Head of Platform Developer Experience at Spotify, addressed these and other questions on the \u003ca href=\"https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eThoughtworks\u003c/em\u003e podcast\u003c/a\u003e: What types of problems do Spotify engineers face? And why did we create \u003ca href=\"http://backstage.spotify.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e to address those issues? Read on to find out how exactly Backstage helped us, and how you can use Backstage to boost the effectiveness of your own team.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGrowing pains\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Pia explains in the podcast, when she started at Spotify in 2016, we were facing an interesting problem. We were in the middle of a hiring boom during a period of exponential growth. From the outside, everything seemed to be moving along swimmingly. But internally, a few metrics were giving us pause; specifically, our productivity wasn’t increasing at all, even with all the new hires.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo we did what we always do: we looked at the data. We had a few metrics for determining and monitoring developer effectiveness — deployment frequency, for instance — but the most crucial was our onboarding metric. You see, we gauge how well our onboarding process is working by measuring how long it takes for a new engineer to make their tenth pull request. And in the midst of our hiring frenzy, that number was getting incredibly high: over 60 days. Clearly something had to be done, but what were the issues developers were facing to begin with?\u003c/p\u003e\n\n\n\n\u003cp\u003ePia and her team looked into the issue, and this was the feedback she got back from the engineers, in her own words:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e“First, it was the context switching … because we had a very fragmented ecosystem. Why did we have a fragmented ecosystem? … Every single team is like a little startup, and it’s free to charge ahead and reach their mission by themselves … This is very conducive for speed, but when we grow, that’s where stuff starts to break down. Of course, this leads to a lot of cognitive load for our engineers.”\u003cbr/\u003e\u003c/li\u003e\u003cli\u003e“The number two blocker was that it’s just hard to find things. Which service should I be integrating with as an engineer? Should I use the user data service that the customer service team has built? Or should I use the slightly different user data service that the premium team has built? Or should I just go ahead and build my own? This, of course, leads to further fragmentation, and we’re back to problem number one.”\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eConsidering both of these challenges, it’s clear that as Spotify grew, our famously autonomous culture was also driving our working environment to become increasingly convoluted and disparate. No one was on the same page, and it was starting to weigh us down. The obvious solution, of course, would be to mandate our engineers use the same technologies and microservices so that we started acting more as a monolith.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut that just wouldn’t fly at Spotify. Again, our autonomous culture, and all the freedom that comes with it, was a big reason a lot of people liked working at Spotify to begin with. It’s key to our identity. Mandating our problems away was out of the question.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhat else could we do? What we needed was a solution that prioritized developers and their ways of working. What we needed was a place where everyone could go to find everything they needed, no matter what it was. What we needed was Backstage.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBackstage: a platform for your platforms\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs Pia notes, Spotify developed \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e to help our engineers do three different things: find stuff, manage stuff, and create stuff. In other words, it’s built to address all the blockers our engineers were facing, especially in terms of discoverability.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhere our engineers used to spend hours of their week just looking for things — documentation, platforms, systems and their owners — all over the internet, now they can find everything in one place: Backstage. Similarly, rather than moving from tab to tab, checking to see the health of, say, their Kubernetes clusters or the status of their recent deployment, engineers can now use Backstage to bring together monitoring tools, logging, their CI/CD pipeline, and whatever else our engineers needed to manage.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, let’s say our engineers want to spin up a new ML model, data pipeline, or some other component or microservice. Rather than building something on their own, introducing yet another instance of boilerplate code similar to a dozen others in our ecosystem, they can now use Backstage to do that work for them. Not only does this save them time if they choose to do this, but these new components and services are also set up using our own best practices and tech standards — what we call our \u003ca href=\"https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGolden Paths\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eBecause of this, we’re able to have our cake and eat it too. Our engineers and squads can remain entirely autonomous, even as Backstage nudges them toward walking down these Golden Paths, thereby increasing our teams’ alignment and keeping our ecosystem from becoming more fragmented. Additionally, because \u003ca href=\"https://github.com/backstage/backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage is a rapidly growing open source tool\u003c/a\u003e, more and more features and plugins are constantly being added for a variety of use cases beyond the ones mentioned here.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, with all that being said, was Backstage worth all the time and money we invested into it? Well, let’s go back to the onboarding metrics one more time. Remember when Pia discovered that it took over 60 days for onboarding engineers to merge their tenth pull request? After Backstage was introduced, that number dropped to only 20. “And if you have numbers like that in your organization,” mentions Pia, “I find that it’s easy to get buy-in for investments in developer experience.” \u003c/p\u003e\n\n\n\n\u003cp\u003eInterested in hearing more about Backstage and what it can do for you? To hear more from Pia discussing Backstage and developer effectiveness with other engineers, check out the \u003ca href=\"https://www.thoughtworks.com/insights/podcasts/technology-podcasts/developer-effectiveness\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThoughtworks podcast episode\u003c/a\u003e. And if you’re curious about how to get started with Backstage, read more about that \u003ca href=\"https://backstage.spotify.com/blog/getting-started-with-backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "What’s the best way to assess your developers’ experience and performance to discover what they need help with? Is it by measuring something arbitrary, like how many lines of code they’ve written or how many commits they’ve made? Nope. How much useful data are you really getting out of those numbers",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Backstage_Developers_Header.png",
      "date_published": "2021-09-23T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/09/introducing-pedalboard-spotifys-audio-effects-library-for-python/",
      "title": "\n                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4787\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/09/introducing-pedalboard-spotifys-audio-effects-library-for-python/\" title=\"Introducing Pedalboard: Spotify’s Audio Effects Library for Python\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWe’ve just open sourced \u003ca href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePedalboard\u003c/a\u003e, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro\u003csup\u003e®\u003c/sup\u003e, or Pro Tools\u003csup\u003e®\u003c/sup\u003e, or newer, more accessible tools like \u003ca rel=\"noreferrer noopener\" href=\"https://www.soundtrap.com/\" target=\"_blank\"\u003eSoundtrap\u003c/a\u003e or \u003ca rel=\"noreferrer noopener\" href=\"https://anchor.fm/\" target=\"_blank\"\u003eAnchor\u003c/a\u003e. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at \u003ca href=\"https://research.atspotify.com/audio-intelligence/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s Audio Intelligence Lab\u003c/a\u003e found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter \u003cem\u003ePedalboard\u003c/em\u003e, a new Python package.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"182\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003ePedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of \u003ca rel=\"noreferrer noopener\" href=\"https://juce.com/\" target=\"_blank\"\u003eJUCE\u003c/a\u003e, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3\u003csup\u003e® \u003c/sup\u003eand Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.\u003c/p\u003e\n\n\n\n\u003cp\u003eSimilar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3\u003csup\u003e® \u003c/sup\u003eor Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"538\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWe’ve found a number of great uses for Pedalboard at Spotify so far, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMachine Learning (ML):\u003c/strong\u003e Pedalboard makes the process of \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Data_augmentation\" target=\"_blank\"\u003edata augmentation\u003c/a\u003e for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eContent Creation: \u003c/strong\u003ePedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3\u003csup\u003e®\u003c/sup\u003e or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCreativity:\u003c/strong\u003e Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eSpotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you’re interested in trying out Pedalboard, it’s ready now. You can find its \u003ca href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecode and documentation on GitHub\u003c/a\u003e, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!\u003c/p\u003e\n\n\n\n\u003cp\u003e—\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eVST is a registered trademark of Steinberg Media Technologies GmbH.\u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/tech-research/\" rel=\"tag\"\u003etech research\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif",
      "date_published": "2021-09-07T00:00:00Z",
      "author": {
        "name": "Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/09/07/introducing-pedalboard-spotifys-audio-effects-library-for-python/",
      "title": "\n                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4787\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/09/07/introducing-pedalboard-spotifys-audio-effects-library-for-python/\" title=\"Introducing Pedalboard: Spotify’s Audio Effects Library for Python\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_header.gif\" alt=\"\" loading=\"lazy\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/09/Pedalboard_header.gif\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWe’ve just open sourced \u003ca href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePedalboard\u003c/a\u003e, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro\u003csup\u003e®\u003c/sup\u003e, or Pro Tools\u003csup\u003e®\u003c/sup\u003e, or newer, more accessible tools like \u003ca rel=\"noreferrer noopener\" href=\"https://www.soundtrap.com/\" target=\"_blank\"\u003eSoundtrap\u003c/a\u003e or \u003ca rel=\"noreferrer noopener\" href=\"https://anchor.fm/\" target=\"_blank\"\u003eAnchor\u003c/a\u003e. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at \u003ca href=\"https://research.atspotify.com/audio-intelligence/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s Audio Intelligence Lab\u003c/a\u003e found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter \u003cem\u003ePedalboard\u003c/em\u003e, a new Python package.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"182\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003ePedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of \u003ca rel=\"noreferrer noopener\" href=\"https://juce.com/\" target=\"_blank\"\u003eJUCE\u003c/a\u003e, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3\u003csup\u003e® \u003c/sup\u003eand Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.\u003c/p\u003e\n\n\n\n\u003cp\u003eSimilar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3\u003csup\u003e® \u003c/sup\u003eor Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"538\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWe’ve found a number of great uses for Pedalboard at Spotify so far, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMachine Learning (ML):\u003c/strong\u003e Pedalboard makes the process of \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Data_augmentation\" target=\"_blank\"\u003edata augmentation\u003c/a\u003e for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eContent Creation: \u003c/strong\u003ePedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3\u003csup\u003e®\u003c/sup\u003e or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCreativity:\u003c/strong\u003e Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eSpotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you’re interested in trying out Pedalboard, it’s ready now. You can find its \u003ca href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecode and documentation on GitHub\u003c/a\u003e, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!\u003c/p\u003e\n\n\n\n\u003cp\u003e—\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eVST is a registered trademark of Steinberg Media Technologies GmbH.\u003c/em\u003e\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/09/Pedalboard_header.gif",
      "date_published": "2021-09-07T00:00:00Z",
      "author": {
        "name": "Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/introducing-pedalboard-spotifys-audio-effects-library-for-python/",
      "title": "\n                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4787\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/introducing-pedalboard-spotifys-audio-effects-library-for-python/\" title=\"Introducing Pedalboard: Spotify’s Audio Effects Library for Python\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWe’ve just open sourced \u003ca href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePedalboard\u003c/a\u003e, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro\u003csup\u003e®\u003c/sup\u003e, or Pro Tools\u003csup\u003e®\u003c/sup\u003e, or newer, more accessible tools like \u003ca rel=\"noreferrer noopener\" href=\"https://www.soundtrap.com/\" target=\"_blank\"\u003eSoundtrap\u003c/a\u003e or \u003ca rel=\"noreferrer noopener\" href=\"https://anchor.fm/\" target=\"_blank\"\u003eAnchor\u003c/a\u003e. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at \u003ca href=\"https://research.atspotify.com/audio-intelligence/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s Audio Intelligence Lab\u003c/a\u003e found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter \u003cem\u003ePedalboard\u003c/em\u003e, a new Python package.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"182\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003ePedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of \u003ca rel=\"noreferrer noopener\" href=\"https://juce.com/\" target=\"_blank\"\u003eJUCE\u003c/a\u003e, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3\u003csup\u003e® \u003c/sup\u003eand Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.\u003c/p\u003e\n\n\n\n\u003cp\u003eSimilar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3\u003csup\u003e® \u003c/sup\u003eor Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"538\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWe’ve found a number of great uses for Pedalboard at Spotify so far, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMachine Learning (ML):\u003c/strong\u003e Pedalboard makes the process of \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Data_augmentation\" target=\"_blank\"\u003edata augmentation\u003c/a\u003e for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eContent Creation: \u003c/strong\u003ePedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3\u003csup\u003e®\u003c/sup\u003e or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCreativity:\u003c/strong\u003e Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eSpotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you’re interested in trying out Pedalboard, it’s ready now. You can find its \u003ca href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecode and documentation on GitHub\u003c/a\u003e, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!\u003c/p\u003e\n\n\n\n\u003cp\u003e—\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eVST is a registered trademark of Steinberg Media Technologies GmbH.\u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/tech-research/\" rel=\"tag\"\u003etech research\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif",
      "date_published": "2021-09-07T00:00:00Z",
      "author": {
        "name": "Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/introducing-pedalboard-spotifys-audio-effects-library-for-python/",
      "title": "\n                                            Introducing Pedalboard: Spotify’s Audio Effects Library for Python\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4787\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/introducing-pedalboard-spotifys-audio-effects-library-for-python/\" title=\"Introducing Pedalboard: Spotify’s Audio Effects Library for Python\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWe’ve just open sourced \u003ca href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePedalboard\u003c/a\u003e, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW).\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you ask any music or podcast producer where they spend most of their time, chances are they’ll say their DAW — the app that lets them edit, manipulate, and perfect their audio. DAWs are powerful software packages that are used in the production of the vast majority of audio today. Most music or podcast content that you hear on Spotify has probably been processed through popular DAWs like Ableton Live, Logic Pro\u003csup\u003e®\u003c/sup\u003e, or Pro Tools\u003csup\u003e®\u003c/sup\u003e, or newer, more accessible tools like \u003ca rel=\"noreferrer noopener\" href=\"https://www.soundtrap.com/\" target=\"_blank\"\u003eSoundtrap\u003c/a\u003e or \u003ca rel=\"noreferrer noopener\" href=\"https://anchor.fm/\" target=\"_blank\"\u003eAnchor\u003c/a\u003e. These apps are optimized for high performance and audio quality, and give producers both incredible flexibility and control over their audio.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis ability to play with sound is usually relegated to DAWs, and these apps are built for musicians, not programmers. But what if programmers want to use the power, speed, and sound quality of a DAW in their code? The engineers and researchers at \u003ca href=\"https://research.atspotify.com/audio-intelligence/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify’s Audio Intelligence Lab\u003c/a\u003e found themselves with that exact need as part of their cutting-edge audio research. They found that each existing solution met some (but not all) of the criteria they needed — so instead, they built their own. Enter \u003cem\u003ePedalboard\u003c/em\u003e, a new Python package.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"182\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-700x182.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-250x65.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-768x200.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-1536x400.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1-120x31.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_1.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003ePedalboard is a Python audio effects library designed to bridge the gap between professional audio software and Python code. It’s built on top of \u003ca rel=\"noreferrer noopener\" href=\"https://juce.com/\" target=\"_blank\"\u003eJUCE\u003c/a\u003e, the industry-standard framework for performant and reliable audio applications. Just like a professional DAW, Pedalboard supports a number of built-in audio effects, as well as third-party VST3\u003csup\u003e® \u003c/sup\u003eand Audio Unit plugins. And just like a DAW, Pedalboard prioritizes speed and quality: in basic tests on common developer hardware, it’s up to 300 times faster than the currently widely used packages for Python audio effects.\u003c/p\u003e\n\n\n\n\u003cp\u003eSimilar to the pedalboards used by guitar players, Pedalboard includes a variety of common stylistic effects and augmentations that you can use to alter sounds. You’ll find basic tools to control volume, like a noise gate, compressor, and limiter, as well as more stylistic tools like distortion, phaser, filter, and reverb. Pedalboard even includes a built-in convolution operator for high-quality simulation of speakers and microphones. If that’s not enough, any VST3\u003csup\u003e® \u003c/sup\u003eor Audio Unit effect plugin can be loaded to provide access to more sonic possibilities. Once you’ve got the sound you’re looking for, you can save your effects by grouping plugins together into a pedalboard, which has the added benefit of speeding up processing.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"538\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-700x538.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-250x192.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-768x591.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-1536x1181.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2-120x92.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_Audio_Effects_2.png 1874w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWe’ve found a number of great uses for Pedalboard at Spotify so far, including:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMachine Learning (ML):\u003c/strong\u003e Pedalboard makes the process of \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Data_augmentation\" target=\"_blank\"\u003edata augmentation\u003c/a\u003e for audio dramatically faster and produces more realistic results. Using Pedalboard, it’s easy to take a small dataset and augment it with audio effects — adding reverb, compression, distortion, and more — to vastly increase the size of your model’s training data and increase your model’s performance. Pedalboard has been thoroughly tested in high-performance and high-reliability ML use cases at Spotify, and is used heavily with TensorFlow.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eContent Creation: \u003c/strong\u003ePedalboard makes it easy to script the application of audio effects with small amounts of Python code. This can help automate parts of the audio creation process. Applying a VST3\u003csup\u003e®\u003c/sup\u003e or Audio Unit plugin no longer requires launching your DAW, importing audio, and exporting it; a couple of lines of code can do it all in one command, or as part of a larger workflow.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCreativity:\u003c/strong\u003e Artists, musicians, and producers with a bit of Python knowledge can use Pedalboard to produce new creative effects that would be extremely time consuming and difficult to produce in a DAW. And for those just getting started with Python, Pedalboard is a great place to begin, as it provides a bridge between code and music.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eSpotify has a long tradition of contributing to open source software, and our research labs are active participants in the open source and academic communities. To continue that tradition, we’re open sourcing the project after nearly a year of internal use in the hopes that it will open up new possibilities for researchers, engineers, musicians, and tinkerers. Pedalboard is “stage ready” — it supports macOS, Windows, and Linux out of the box, and we’ve used it internally at Spotify to process millions of hours of audio.\u003c/p\u003e\n\n\n\n\u003cp\u003eIf you’re interested in trying out Pedalboard, it’s ready now. You can find its \u003ca href=\"https://github.com/spotify/pedalboard\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecode and documentation on GitHub\u003c/a\u003e, where we welcome contributions to the code. Installing Pedalboard on your computer is as simple as running one command: pip install pedalboard. We can’t wait to hear what you use Pedalboard for!\u003c/p\u003e\n\n\n\n\u003cp\u003e—\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eVST is a registered trademark of Steinberg Media Technologies GmbH.\u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/tech-research/\" rel=\"tag\"\u003etech research\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "We’ve just open sourced Pedalboard, Spotify’s framework for adding effects to audio in Python. Pedalboard makes it easy to use studio-quality audio effects in your code, rather than just in your digital audio workstation (DAW). If you ask any music or podcast producer where they spend most of the",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/09/Pedalboard_header.gif",
      "date_published": "2021-09-07T00:00:00Z",
      "author": {
        "name": "Published by Peter Sobot, Staff Machine Learning Engineer - Spotify Audio Intelligence Lab"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/08/four-lessons-we-learned-from-creating-spotifys-desktop-app/",
      "title": "\n                                            Four Lessons We Learned from Creating Spotify’s Desktop App\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eAugust 4, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/08/four-lessons-we-learned-from-creating-spotifys-desktop-app/\" title=\"Four Lessons We Learned from Creating Spotify’s Desktop App\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eOver the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to \u003ca href=\"https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eweb players\u003c/a\u003e to \u003ca href=\"https://carthing.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecar things\u003c/a\u003e. But sitting at the core is our flagship product, the one that started it all: the desktop app. In \u003ca href=\"https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe first episode\u003c/a\u003e of our podcast series, “\u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify: A Product Story\u003c/a\u003e”, host and Chief R\u0026amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA\u0026amp;dl_branch=1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echeck out the podcast\u003c/a\u003e yourself to hear even more about how Spotify became, well, Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow do you steal from a pirate?\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. \u003c/p\u003e\n\n\n\n\u003cp\u003eSure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eDaniel: \u003c/strong\u003eI guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eIn other words, when it comes to guiding your product strategy, \u003cstrong\u003eour\u003c/strong\u003e \u003cstrong\u003efirst lesson is an important rule to live by: convenience trumps everything. \u003c/strong\u003eThe appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGo big or go home\u003c/h2\u003e\n\n\n\n\u003cp\u003eBut how exactly could we make a user experience \u003cem\u003ethat\u003c/em\u003e good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.\u003c/p\u003e\n\n\n\n\u003cp\u003eLuckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.\u003c/p\u003e\n\n\n\n\u003cp\u003eLittle surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” \u003c/p\u003e\n\n\n\n\u003cp\u003eBad news for other companies, but great news for us. You see, it’s always good to keep in mind \u003cstrong\u003eour second lesson:\u003c/strong\u003e \u003cstrong\u003egreat ambition attracts great talent. This is why companies always need to keep moving the goalposts.\u003c/strong\u003e When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe rules are only suggestions\u003c/h2\u003e\n\n\n\n\u003cp\u003eAnd it wasn’t long before he identified Spotify’s problem — and the solution.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eLudde: \u003c/strong\u003eDoing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eEssentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. \u003c/p\u003e\n\n\n\n\u003cp\u003eFortunately, Ludde already knew \u003cstrong\u003eour\u003c/strong\u003e \u003cstrong\u003ethird product lesson:\u003c/strong\u003e \u003cstrong\u003edon’t be afraid to break the rules. \u003c/strong\u003eThe problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.\u003c/p\u003e\n\n\n\n\u003cp\u003eNot to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (\u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c\" target=\"_blank\"\u003eCheck out Episode 01, “How do you steal from a pirate?”\u003c/a\u003e, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …\u003c/p\u003e\n\n\n\n\u003ch2\u003eDo you want to see a magic trick?\u003c/h2\u003e\n\n\n\n\u003cp\u003eAll of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eMichelle\u003c/strong\u003e: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThat’s our fourth and final lesson:\u003c/strong\u003e \u003cstrong\u003edifferentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.\u003c/strong\u003e Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R\u0026amp;D Officer Gustav Söderström and \u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003echeck out all the episodes right here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png",
      "date_published": "2021-08-04T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/08/04/four-lessons-we-learned-from-creating-spotifys-desktop-app/",
      "title": "\n                                            Four Lessons We Learned from Creating Spotify’s Desktop App\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eAugust 4, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/08/04/four-lessons-we-learned-from-creating-spotifys-desktop-app/\" title=\"Four Lessons We Learned from Creating Spotify’s Desktop App\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eOver the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to \u003ca href=\"https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eweb players\u003c/a\u003e to \u003ca href=\"https://carthing.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecar things\u003c/a\u003e. But sitting at the core is our flagship product, the one that started it all: the desktop app. In \u003ca href=\"https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe first episode\u003c/a\u003e of our podcast series, “\u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify: A Product Story\u003c/a\u003e”, host and Chief R\u0026amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA\u0026amp;dl_branch=1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echeck out the podcast\u003c/a\u003e yourself to hear even more about how Spotify became, well, Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow do you steal from a pirate?\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. \u003c/p\u003e\n\n\n\n\u003cp\u003eSure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eDaniel: \u003c/strong\u003eI guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eIn other words, when it comes to guiding your product strategy, \u003cstrong\u003eour\u003c/strong\u003e \u003cstrong\u003efirst lesson is an important rule to live by: convenience trumps everything. \u003c/strong\u003eThe appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGo big or go home\u003c/h2\u003e\n\n\n\n\u003cp\u003eBut how exactly could we make a user experience \u003cem\u003ethat\u003c/em\u003e good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.\u003c/p\u003e\n\n\n\n\u003cp\u003eLuckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.\u003c/p\u003e\n\n\n\n\u003cp\u003eLittle surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” \u003c/p\u003e\n\n\n\n\u003cp\u003eBad news for other companies, but great news for us. You see, it’s always good to keep in mind \u003cstrong\u003eour second lesson:\u003c/strong\u003e \u003cstrong\u003egreat ambition attracts great talent. This is why companies always need to keep moving the goalposts.\u003c/strong\u003e When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe rules are only suggestions\u003c/h2\u003e\n\n\n\n\u003cp\u003eAnd it wasn’t long before he identified Spotify’s problem — and the solution.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eLudde: \u003c/strong\u003eDoing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eEssentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. \u003c/p\u003e\n\n\n\n\u003cp\u003eFortunately, Ludde already knew \u003cstrong\u003eour\u003c/strong\u003e \u003cstrong\u003ethird product lesson:\u003c/strong\u003e \u003cstrong\u003edon’t be afraid to break the rules. \u003c/strong\u003eThe problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.\u003c/p\u003e\n\n\n\n\u003cp\u003eNot to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (\u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c\" target=\"_blank\"\u003eCheck out Episode 01, “How do you steal from a pirate?”\u003c/a\u003e, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …\u003c/p\u003e\n\n\n\n\u003ch2\u003eDo you want to see a magic trick?\u003c/h2\u003e\n\n\n\n\u003cp\u003eAll of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eMichelle\u003c/strong\u003e: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThat’s our fourth and final lesson:\u003c/strong\u003e \u003cstrong\u003edifferentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.\u003c/strong\u003e Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R\u0026amp;D Officer Gustav Söderström and \u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003echeck out all the episodes right here\u003c/a\u003e.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png",
      "date_published": "2021-08-04T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/four-lessons-we-learned-from-creating-spotifys-desktop-app/",
      "title": "\n                                            Four Lessons We Learned from Creating Spotify’s Desktop App\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eAugust 4, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/four-lessons-we-learned-from-creating-spotifys-desktop-app/\" title=\"Four Lessons We Learned from Creating Spotify’s Desktop App\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eOver the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to \u003ca href=\"https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eweb players\u003c/a\u003e to \u003ca href=\"https://carthing.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecar things\u003c/a\u003e. But sitting at the core is our flagship product, the one that started it all: the desktop app. In \u003ca href=\"https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe first episode\u003c/a\u003e of our podcast series, “\u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify: A Product Story\u003c/a\u003e”, host and Chief R\u0026amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA\u0026amp;dl_branch=1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echeck out the podcast\u003c/a\u003e yourself to hear even more about how Spotify became, well, Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow do you steal from a pirate?\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. \u003c/p\u003e\n\n\n\n\u003cp\u003eSure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eDaniel: \u003c/strong\u003eI guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eIn other words, when it comes to guiding your product strategy, \u003cstrong\u003eour\u003c/strong\u003e \u003cstrong\u003efirst lesson is an important rule to live by: convenience trumps everything. \u003c/strong\u003eThe appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGo big or go home\u003c/h2\u003e\n\n\n\n\u003cp\u003eBut how exactly could we make a user experience \u003cem\u003ethat\u003c/em\u003e good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.\u003c/p\u003e\n\n\n\n\u003cp\u003eLuckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.\u003c/p\u003e\n\n\n\n\u003cp\u003eLittle surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” \u003c/p\u003e\n\n\n\n\u003cp\u003eBad news for other companies, but great news for us. You see, it’s always good to keep in mind \u003cstrong\u003eour second lesson:\u003c/strong\u003e \u003cstrong\u003egreat ambition attracts great talent. This is why companies always need to keep moving the goalposts.\u003c/strong\u003e When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe rules are only suggestions\u003c/h2\u003e\n\n\n\n\u003cp\u003eAnd it wasn’t long before he identified Spotify’s problem — and the solution.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eLudde: \u003c/strong\u003eDoing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eEssentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. \u003c/p\u003e\n\n\n\n\u003cp\u003eFortunately, Ludde already knew \u003cstrong\u003eour\u003c/strong\u003e \u003cstrong\u003ethird product lesson:\u003c/strong\u003e \u003cstrong\u003edon’t be afraid to break the rules. \u003c/strong\u003eThe problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.\u003c/p\u003e\n\n\n\n\u003cp\u003eNot to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (\u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c\" target=\"_blank\"\u003eCheck out Episode 01, “How do you steal from a pirate?”\u003c/a\u003e, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …\u003c/p\u003e\n\n\n\n\u003ch2\u003eDo you want to see a magic trick?\u003c/h2\u003e\n\n\n\n\u003cp\u003eAll of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eMichelle\u003c/strong\u003e: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThat’s our fourth and final lesson:\u003c/strong\u003e \u003cstrong\u003edifferentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.\u003c/strong\u003e Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R\u0026amp;D Officer Gustav Söderström and \u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003echeck out all the episodes right here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png",
      "date_published": "2021-08-04T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/four-lessons-we-learned-from-creating-spotifys-desktop-app/",
      "title": "\n                                            Four Lessons We Learned from Creating Spotify’s Desktop App\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eAugust 4, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/four-lessons-we-learned-from-creating-spotifys-desktop-app/\" title=\"Four Lessons We Learned from Creating Spotify’s Desktop App\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png 1201w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-250x131.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-700x368.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-768x404.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1-120x63.png 120w\" sizes=\"(max-width: 1201px) 100vw, 1201px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eOver the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to \u003ca href=\"https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eweb players\u003c/a\u003e to \u003ca href=\"https://carthing.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ecar things\u003c/a\u003e. But sitting at the core is our flagship product, the one that started it all: the desktop app. In \u003ca href=\"https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe first episode\u003c/a\u003e of our podcast series, “\u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify: A Product Story\u003c/a\u003e”, host and Chief R\u0026amp;D Officer Gustav Söderström walks through how the app (and Spotify in general) came to be — and the product lessons you should take away from that journey. So read on to learn how Spotify had to completely rethink peer-to-peer (P2P) networking to improve our user experience, and why everyone needs a bit of magic to stand out from their competitors. And, of course, please \u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB?si=pX_Ez7ZHR3untiFXd5WaNA\u0026amp;dl_branch=1\" target=\"_blank\" rel=\"noreferrer noopener\"\u003echeck out the podcast\u003c/a\u003e yourself to hear even more about how Spotify became, well, Spotify.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow do you steal from a pirate?\u003c/h2\u003e\n\n\n\n\u003cp\u003eLet’s rewind to the mid-2000s. At this point, music piracy wasn’t a new phenomenon, but it was a newly popular one. No longer did you have to physically steal records, rip radio onto a cassette, or even burn a CD. With peer-to-peer technology, all it took was an internet connection and some software, and you could be moments away from nabbing a song for yourself, with almost no chance of getting punished for it. \u003c/p\u003e\n\n\n\n\u003cp\u003eSure, download speeds could be painfully slow, the programs were a bit janky, and that album you pirated might turn out to be low-quality, incomplete, or covertly smuggling a virus along with it. But, hey, it was pretty convenient and low risk, all things considered. It was also, most importantly, absolutely free. What could compete?\u003c/p\u003e\n\n\n\n\u003cp\u003eIn the very first episode of the podcast, Spotify co-founder and CEO Daniel Ek remembers considering what it would take to beat piracy at its own game when first conceiving of the company:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eDaniel: \u003c/strong\u003eI guess if you could take the concept of downloading all the world’s music, like you have on Napster and Kazaa, for a free price or a very low price, and you married it with the user experience of iTunes, so that it would feel like you had all the world’s music on your hard drive — that would be a much better experience than piracy. And then I think everyone would turn to that.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eIn other words, when it comes to guiding your product strategy, \u003cstrong\u003eour\u003c/strong\u003e \u003cstrong\u003efirst lesson is an important rule to live by: convenience trumps everything. \u003c/strong\u003eThe appeal of music piracy for most people, after all, wasn’t to deliberately sabotage the record industry or cut off a revenue stream for artists — it’s that it was free and relatively convenient. After realizing that, Spotify’s mission was set: if we could provide those same things with an even better user experience while still generating revenue, then, and only then, did we have a shot at success.\u003c/p\u003e\n\n\n\n\u003ch2\u003eGo big or go home\u003c/h2\u003e\n\n\n\n\u003cp\u003eBut how exactly could we make a user experience \u003cem\u003ethat\u003c/em\u003e good? How could we actually beat pirates at their own game? One obvious improvement was speed; waiting hours over a slow connection to download an album was a massive pain, no matter how free it was. But when we looked at the tech and tools out there, it wasn’t immediately clear how to make anything better.\u003c/p\u003e\n\n\n\n\u003cp\u003eLuckily, Spotify co-founder Martin Lorentzon ran into someone who knew how. Enter Ludde Strigeus, the creator of µTorrent, one of the world’s most popular BitTorrent clients and, ironically, one of the largest drivers of music piracy. If there were one person who could promise to push the limits of what client-server technology and P2P networking could do, it was him.\u003c/p\u003e\n\n\n\n\u003cp\u003eLittle surprise, then, that Ludde was fielding offers from Silicon Valley left and right. Even so, he wasn’t in any rush to accept any of them. As he puts it in the podcast, “When I find a project that interests me enough, I can’t really stop working on it. So the problem is to actually find these projects.” \u003c/p\u003e\n\n\n\n\u003cp\u003eBad news for other companies, but great news for us. You see, it’s always good to keep in mind \u003cstrong\u003eour second lesson:\u003c/strong\u003e \u003cstrong\u003egreat ambition attracts great talent. This is why companies always need to keep moving the goalposts.\u003c/strong\u003e When Daniel and Martin first approached Ludde to give him the hard sell on Spotify, a company whose goal, again, seemed impossible to achieve at the time, Ludde signed right up.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe rules are only suggestions\u003c/h2\u003e\n\n\n\n\u003cp\u003eAnd it wasn’t long before he identified Spotify’s problem — and the solution.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eLudde: \u003c/strong\u003eDoing it in the browser wasn’t even an alternative. There wasn’t any competitive way to do it in the browser at that time. The browsers weren’t mature enough.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eEssentially, if Spotify ran in a browser, it would only be able to run as fast as the rest of the browser-based internet, which included our competition. That meant that it would never provide a better experience than piracy if we went that route. \u003c/p\u003e\n\n\n\n\u003cp\u003eFortunately, Ludde already knew \u003cstrong\u003eour\u003c/strong\u003e \u003cstrong\u003ethird product lesson:\u003c/strong\u003e \u003cstrong\u003edon’t be afraid to break the rules. \u003c/strong\u003eThe problem was bigger than just a matter of finding the right tech; for Spotify to be what we wanted, we would need to custom-build everything in our entire infrastructure from the ground up. In short, we would need to go full stack.\u003c/p\u003e\n\n\n\n\u003cp\u003eNot to go into too much detail here, but at the time, most of the internet was made up of “thin clients,” like web pages or Flash-based clients that ran in-browser, and used more traditional, standardized protocols like HTTPS. Seeing the limitations of that, Ludde and a team of engineers ran in the exact opposite direction, creating a stand-alone “fat client,” building entirely new protocols and hybridizing client-server and P2P technology to suit their own ends. (\u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/episode/1jHRUXkeiUh44CK4KZQb0h?si=d8695b5f4e58491c\" target=\"_blank\"\u003eCheck out Episode 01, “How do you steal from a pirate?”\u003c/a\u003e, to hear more of that nitty-gritty stuff about persistent TCP connections and how our P2P implementation saved us bandwidth cost.) It was only by rethinking every layer of our infrastructure that we were able to pull Spotify off, to create that magic moment of double-clicking on a new song and having it instantly play. And speaking of magic …\u003c/p\u003e\n\n\n\n\u003ch2\u003eDo you want to see a magic trick?\u003c/h2\u003e\n\n\n\n\u003cp\u003eAll of that ambition would have meant nothing if we couldn’t secure the licensing deals we needed to actually play music on our app. We knew our tech was cool and groundbreaking, but would anyone else? The music industry was being ravaged by the same peer-to-peer technology that Spotify was using in the desktop app. Why would they want to strike a deal with someone who seemed like the enemy?\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen Michelle Kadir joined Universal Sweden in 2008 to vet new technologies that could potentially help the ailing music industry, she originally saw Spotify as just another start-up setting up a meeting, vying for her attention. That was, until she saw the product in action.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eMichelle\u003c/strong\u003e: The thing that happened that was kind of pure magic in that meeting was that [Daniel] did a comparison. He started playing a song on the software, and the song played so quick, so instant … I mean, I don’t know if people remember, but playback was slow back then. Even if you had an MP3 on your computer, and you played it via, you know, Winamp, iTunes, this was faster. And we were like, “You have the files on your computer, right?” And he was like, “No, it’s in the cloud.”\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eThat’s our fourth and final lesson:\u003c/strong\u003e \u003cstrong\u003edifferentiating yourself from your competitors is one thing. But if you can pull off something that no one thought was possible — a magic trick — now that’s captivating.\u003c/strong\u003e Captivating enough to potentially change the minds of not only users, but an entire industry that’s stuck in a rut.\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, sure, that’s four lessons, but why stop there? The podcast series “Spotify: A Product Story” shares all these stories and dozens more, filled with insider insight and product strategy lessons from the employees, collaborators, and musicians who made Spotify what it is today. Join host and Chief R\u0026amp;D Officer Gustav Söderström and \u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003echeck out all the episodes right here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Over the years, Spotify’s brand has expanded to encompass a number of products, from mobile apps to web players to car things. But sitting at the core is our flagship product, the one that started it all: the desktop app. In the first episode of our podcast series, “Spotify: A Product Story”,",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/08/A-Product-Story_01-Illustration_1200x630-1.png",
      "date_published": "2021-08-04T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/patrick-balestra-senior-engineer/",
      "title": "\n                                            Patrick Balestra: Senior Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4729\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w\" sizes=\"(max-width: 693px) 100vw, 693px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eOriginally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.\u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e8:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e10:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAfter breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. \u003c/p\u003e\n\n\n\n\u003cp\u003eMy team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eOne way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e2:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAfter lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source \u003ca rel=\"noreferrer noopener\" href=\"https://xcmetrics.io/\" target=\"_blank\"\u003eXCMetrics\u003c/a\u003e — \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/\" target=\"_blank\"\u003ea tool developed at Spotify\u003c/a\u003e for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e7:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"557\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png",
      "date_published": "2021-06-04T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/06/04/patrick-balestra-senior-engineer/",
      "title": "\n                                            Patrick Balestra: Senior Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4729\"\u003e\n     \u003cdiv\u003e\n         \n         \n         \n         \u003cdiv\u003e\n             \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w\" sizes=\"(max-width: 693px) 100vw, 693px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/06/Patrick-Balestra_Header-Image.png\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cp\u003e\u003cb\u003eOriginally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.\u003c/b\u003e\u003c/p\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e8:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e10:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAfter breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. \u003c/p\u003e\n\n\n\n\u003cp\u003eMy team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eOne way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e2:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAfter lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source \u003ca rel=\"noreferrer noopener\" href=\"https://xcmetrics.io/\" target=\"_blank\"\u003eXCMetrics\u003c/a\u003e — \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/\" target=\"_blank\"\u003ea tool developed at Spotify\u003c/a\u003e for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e7:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"557\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n         \n         \n\n         \u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/06/Patrick-Balestra_Header-Image.png",
      "date_published": "2021-06-04T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/06/patrick-balestra-senior-engineer/",
      "title": "\n                                            Patrick Balestra: Senior Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4729\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w\" sizes=\"(max-width: 693px) 100vw, 693px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eOriginally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.\u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e8:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e10:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAfter breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. \u003c/p\u003e\n\n\n\n\u003cp\u003eMy team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eOne way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e2:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAfter lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source \u003ca rel=\"noreferrer noopener\" href=\"https://xcmetrics.io/\" target=\"_blank\"\u003eXCMetrics\u003c/a\u003e — \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/\" target=\"_blank\"\u003ea tool developed at Spotify\u003c/a\u003e for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e7:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"557\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png",
      "date_published": "2021-06-04T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/patrick-balestra-senior-engineer/",
      "title": "\n                                            Patrick Balestra: Senior Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4729\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png 693w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-250x222.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image-120x106.png 120w\" sizes=\"(max-width: 693px) 100vw, 693px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eOriginally from Switzerland, Patrick has lived in Sweden for almost three years now and works as a Senior iOS Engineer in our Stockholm office.\u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e8:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day!\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e10:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAfter breakfast, I log into my computer, check my messages and settle down for my morning’s work. Officially, I’m an iOS engineer on the Infrastructure team, which means I take care of the iOS developer experience — creating tools, libraries and other innovative solutions to help developers work faster and more efficiently. But over the last year, I’ve transitioned to work more on a new monorepo and Bazel system project that’s designed for all kinds of developers, not just iOS. We’re unifying how the tooling works, where the code lives and so on. And it’s an ongoing project – we’re constantly finding ways to improve and make life easier for developers across Spotify. \u003c/p\u003e\n\n\n\n\u003cp\u003eMy team is divided between Stockholm and New York, so we’re accustomed to being far apart and have adapted to working from home pretty easily. But I’m a sociable person and really miss being in the office, chatting with other people and knowing what’s going on with other teams. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eOne way I stay connected is by meeting up with colleagues that live close by — we often take our lunch breaks together and grab a bite to eat in a local restaurant. Stockholm’s a great place to live — the winters can be rough but, as a Swiss-Italian, I’m no stranger to snow and ice. And at this time of year, the city is becoming more lively — the weather’s warmer, the restaurants are buzzing, and people are ready to enjoy themselves. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e2:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAfter lunch is when New York wakes up, so I tend to have more meetings and less time for writing code or reviewing documents. I also like to keep up-to-date with the wider engineering community and do open source work — I usually have a bunch of projects on the go. Recently, I helped to open source \u003ca rel=\"noreferrer noopener\" href=\"https://xcmetrics.io/\" target=\"_blank\"\u003eXCMetrics\u003c/a\u003e — \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/\" target=\"_blank\"\u003ea tool developed at Spotify\u003c/a\u003e for Apple’s developer software, Xcode, that allows people to collect, display, and track the metrics inside their team’s Xcode build logs. It can provide valuable insights to help improve both developer experience and productivity.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e7:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI tend to work fairly late in the evenings to make the most of the overlap with New York. But around 7pm, it’s time to relax — I watch movies, play video games, all the usual stuff. On Thursday evenings, I often play football with a few other folks from Spotify. And I’ve also got more into cooking over the last year — I love to get my girlfriend or a few friends over for dinner and try out one of my new recipes. Somehow, I’ve become the chef of the group, but I’m more than happy with that…\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"557\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-700x557.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-250x199.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-768x611.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2-120x96.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/MyBeat-Patrick-Balestra_Pie-chart2.png 1000w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/mobile/\" rel=\"tag\"\u003eMobile\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "8:30am I’m at my best after nine hours’ sleep, so I tend to wake up pretty late, shower and dress as though I’m going into the office. Although I’ve been in Stockholm a while now, I’m still not a fan of the Swedish breakfast of bread and cheese or salami – give me Nutella on toast any day! 10:",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/06/Patrick-Balestra_Header-Image.png",
      "date_published": "2021-06-04T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/achieving-team-purpose-and-pride-with-scrum/",
      "title": "\n                                            Achieving Team Purpose and Pride with Scrum\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 27, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/achieving-team-purpose-and-pride-with-scrum/\" title=\"Achieving Team Purpose and Pride with Scrum\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eTeam purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.\u003c/p\u003e\n\n\n\n\u003ch2\u003eOur problem\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur squad had long been following a process comprising bits and pieces from the \u003ca rel=\"noreferrer noopener\" href=\"https://scrumguides.org/scrum-guide.html\" target=\"_blank\"\u003eScrum\u003c/a\u003e framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe goals we ultimately wanted our process to achieve were:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eContinuous improvement: \u003c/strong\u003eWe wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eShared understanding and transparency: \u003c/strong\u003eWe wanted everyone on the team to know at any given time what work was happening, and what it entailed.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"3\"\u003e\u003cli\u003e\u003cstrong\u003eConfidence: \u003c/strong\u003eWe wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eOur approach\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. \u003c/p\u003e\n\n\n\n\u003ch3\u003eBacklog refinement\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBefore these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually \u003cem\u003epoint \u003c/em\u003ethose tickets.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSprint planning\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003ePreviously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSprint review\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eWhat work did we complete?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eIs there anything we need to extend or add to what we’ve done?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eDid we discover any tech debt?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eAre we on track to meet our longer-term goals?\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eThis allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.\u003c/p\u003e\n\n\n\n\u003ch3\u003eRetro\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, we continue to create those notes, but then vote on a \u003cem\u003esingle issue \u003c/em\u003eto spend the majority of the retro discussing and ideating to solve.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"746\" height=\"787\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w\" sizes=\"(max-width: 746px) 100vw, 746px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eNow, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBy the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.\u003c/p\u003e\n\n\n\n\u003ch3\u003eStand-ups\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIncorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.\u003c/p\u003e\n\n\n\n\u003ch2\u003eRecommendations\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e1. Try out a system holistically before making adjustments. \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAgile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e2. Ask the “stand-up question”.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAsking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e3. Focus on a single issue in retros.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAllow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e4. Plan sprints you can finish, and commit to finishing them.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eCreate multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers \u003ca href=\"https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ebecame more T-shaped\u003c/a\u003e to finish the sprint’s work in time. \u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  \u003c/p\u003e\n\n\n\n\u003cp\u003eMy team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png",
      "date_published": "2021-05-27T00:00:00Z",
      "author": {
        "name": "Published by Sophia Ciocca, Web Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/05/achieving-team-purpose-and-pride-with-scrum/",
      "title": "\n                                            Achieving Team Purpose and Pride with Scrum\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 27, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/05/achieving-team-purpose-and-pride-with-scrum/\" title=\"Achieving Team Purpose and Pride with Scrum\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eTeam purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.\u003c/p\u003e\n\n\n\n\u003ch2\u003eOur problem\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur squad had long been following a process comprising bits and pieces from the \u003ca rel=\"noreferrer noopener\" href=\"https://scrumguides.org/scrum-guide.html\" target=\"_blank\"\u003eScrum\u003c/a\u003e framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe goals we ultimately wanted our process to achieve were:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eContinuous improvement: \u003c/strong\u003eWe wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eShared understanding and transparency: \u003c/strong\u003eWe wanted everyone on the team to know at any given time what work was happening, and what it entailed.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"3\"\u003e\u003cli\u003e\u003cstrong\u003eConfidence: \u003c/strong\u003eWe wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eOur approach\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. \u003c/p\u003e\n\n\n\n\u003ch3\u003eBacklog refinement\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBefore these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually \u003cem\u003epoint \u003c/em\u003ethose tickets.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSprint planning\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003ePreviously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSprint review\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eWhat work did we complete?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eIs there anything we need to extend or add to what we’ve done?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eDid we discover any tech debt?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eAre we on track to meet our longer-term goals?\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eThis allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.\u003c/p\u003e\n\n\n\n\u003ch3\u003eRetro\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, we continue to create those notes, but then vote on a \u003cem\u003esingle issue \u003c/em\u003eto spend the majority of the retro discussing and ideating to solve.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"746\" height=\"787\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w\" sizes=\"(max-width: 746px) 100vw, 746px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eNow, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBy the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.\u003c/p\u003e\n\n\n\n\u003ch3\u003eStand-ups\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIncorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.\u003c/p\u003e\n\n\n\n\u003ch2\u003eRecommendations\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e1. Try out a system holistically before making adjustments. \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAgile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e2. Ask the “stand-up question”.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAsking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e3. Focus on a single issue in retros.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAllow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e4. Plan sprints you can finish, and commit to finishing them.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eCreate multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers \u003ca href=\"https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ebecame more T-shaped\u003c/a\u003e to finish the sprint’s work in time. \u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  \u003c/p\u003e\n\n\n\n\u003cp\u003eMy team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png",
      "date_published": "2021-05-27T00:00:00Z",
      "author": {
        "name": "Published by Sophia Ciocca, Web Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/05/27/achieving-team-purpose-and-pride-with-scrum/",
      "title": "\n                                            Achieving Team Purpose and Pride with Scrum\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 27, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/05/27/achieving-team-purpose-and-pride-with-scrum/\" title=\"Achieving Team Purpose and Pride with Scrum\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/05/Scrum-Post_Header.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eTeam purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.\u003c/p\u003e\n\n\n\n\u003ch2\u003eOur problem\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur squad had long been following a process comprising bits and pieces from the \u003ca rel=\"noreferrer noopener\" href=\"https://scrumguides.org/scrum-guide.html\" target=\"_blank\"\u003eScrum\u003c/a\u003e framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe goals we ultimately wanted our process to achieve were:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eContinuous improvement: \u003c/strong\u003eWe wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eShared understanding and transparency: \u003c/strong\u003eWe wanted everyone on the team to know at any given time what work was happening, and what it entailed.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"3\"\u003e\u003cli\u003e\u003cstrong\u003eConfidence: \u003c/strong\u003eWe wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eOur approach\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. \u003c/p\u003e\n\n\n\n\u003ch3\u003eBacklog refinement\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBefore these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually \u003cem\u003epoint \u003c/em\u003ethose tickets.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSprint planning\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003ePreviously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSprint review\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eWhat work did we complete?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eIs there anything we need to extend or add to what we’ve done?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eDid we discover any tech debt?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eAre we on track to meet our longer-term goals?\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eThis allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.\u003c/p\u003e\n\n\n\n\u003ch3\u003eRetro\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, we continue to create those notes, but then vote on a \u003cem\u003esingle issue \u003c/em\u003eto spend the majority of the retro discussing and ideating to solve.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"746\" height=\"787\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w\" sizes=\"(max-width: 746px) 100vw, 746px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eNow, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBy the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.\u003c/p\u003e\n\n\n\n\u003ch3\u003eStand-ups\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIncorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.\u003c/p\u003e\n\n\n\n\u003ch2\u003eRecommendations\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e1. Try out a system holistically before making adjustments. \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAgile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e2. Ask the “stand-up question”.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAsking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e3. Focus on a single issue in retros.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAllow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e4. Plan sprints you can finish, and commit to finishing them.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eCreate multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers \u003ca href=\"https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ebecame more T-shaped\u003c/a\u003e to finish the sprint’s work in time. \u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  \u003c/p\u003e\n\n\n\n\u003cp\u003eMy team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/Scrum-Post_Header.png",
      "date_published": "2021-05-27T00:00:00Z",
      "author": {
        "name": "Published by Sophia Ciocca, Web Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/achieving-team-purpose-and-pride-with-scrum/",
      "title": "\n                                            Achieving Team Purpose and Pride with Scrum\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 27, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/achieving-team-purpose-and-pride-with-scrum/\" title=\"Achieving Team Purpose and Pride with Scrum\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eTeam purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Letting teams adjust their processes to work for them promises many benefits (innovation, lower overhead, team happiness, speed, etc.), but it takes intentional team effort to make these adjustments.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile this international effort towards aligned autonomy has shown dazzling success and efficiency across the company, my team was struggling to make it work, finding ourselves with a process that wasn’t working for us. This is the story of how we changed that.\u003c/p\u003e\n\n\n\n\u003ch2\u003eOur problem\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur squad had long been following a process comprising bits and pieces from the \u003ca rel=\"noreferrer noopener\" href=\"https://scrumguides.org/scrum-guide.html\" target=\"_blank\"\u003eScrum\u003c/a\u003e framework, an agile methodology developed in the 1990s by Ken Schwaber and Jeff Sutherland. However, we hadn’t connected the Scrum practices we were using — like stand-ups, two-week sprints, and retros — to the principles behind them, and we hadn’t woven them together cohesively as a system. As a result, we found ourselves with a surprising lack of structure and clarity: our meetings often felt purposeless, we never finished our sprints, and our product manager had a difficult time knowing what could reasonably be expected to be delivered at any given time. We, as engineers, also had little sense of how our day-to-day work fit into a larger quarterly picture, or how close our team was to achieving its goals. This left many of us with a gnawing feeling that our team rhythm could be better, though we weren’t quite sure how to get there.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe goals we ultimately wanted our process to achieve were:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eContinuous improvement: \u003c/strong\u003eWe wanted to iterate better — to easily and fluidly understand our work and find opportunities where we could improve.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eShared understanding and transparency: \u003c/strong\u003eWe wanted everyone on the team to know at any given time what work was happening, and what it entailed.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"3\"\u003e\u003cli\u003e\u003cstrong\u003eConfidence: \u003c/strong\u003eWe wanted to be able to more confidently plan our long-term trajectory and communicate with stakeholders about what they could expect.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eOur approach\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo help us reach our goals, we sought the help of a Spotify Agile coach, who first guided us through an assessment of our existing ways of working. Since our team generally liked the Scrum framework but wasn’t using it holistically, our Agile coach helped us dig deeper into how the Scrum elements work together as a whole. Each piece has a specific role to play and interacts with each other piece. Ultimately, we unanimously agreed to adopt Scrum more or less “by the book”: that is to say, following the entire framework laid out in the Scrum Guide, rather than just disconnected bits of it. \u003c/p\u003e\n\n\n\n\u003ch3\u003eBacklog refinement\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Create a shared understanding of each ticket, as well as how “large” it is, so that the PM can prioritize accordingly.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBefore these process changes, we were itching for a succinct way to size our stories; sometimes stories would get pointed during a planning meeting, but more often than not, we were bringing many unsized stories into a sprint. This meant that we had virtually no gauge of how much work we were bringing in or committing to.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith the help of our coach, we began holding a weekly backlog refinement meeting. We alternate each week between “coarse refinement” — in which we hone in on tickets, ask questions, and find collective understanding — and “fine refinement”, in which we actually \u003cem\u003epoint \u003c/em\u003ethose tickets.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis system ensures that everyone has an opportunity to ask questions and shares a basic understanding of every ticket. We all know how much work we are committing to when we begin a sprint, and it also allows us to compare, sprint by sprint, how many points we are finishing as a team.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSprint planning\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Create a sprint full of stories ready to be picked up, and which we feel confident we can deliver on time.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003ePreviously, our sprint planning process didn’t allow for us to share a collective grasp of each of the tickets in the backlog before our sprint planning ceremony, so we spent most of the two hours reading about the tickets and trying to arrive at an agreement about which ones felt important to bring in.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, because all the tickets are pointed and prioritized in the backlog ahead of time, the process is very simple: we go down the backlog — full of tickets we’ve already pointed and discussed — and simply do any subtasking to get clearer on the actual work we’ll be doing. After each ticket we review and bring into the sprint, we check whether the team feels we can take on more. By the end, we have a sprint full of fully subtasked stories we thoroughly understand, and that we’re confident we can deliver within two weeks.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSprint review\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Review the sprint’s work, celebrate achievements, and note what new tasks came out of this sprint.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile we already had a retro in which we talked vaguely about the successes and challenges of the sprint, we didn’t evaluate the work in terms of our team’s product prioritization.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn a 30-minute sprint review, we demo the features completed, and ask ourselves some basic questions:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eWhat work did we complete?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eIs there anything we need to extend or add to what we’ve done?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eDid we discover any tech debt?\u003c/em\u003e\u003c/li\u003e\u003cli\u003e\u003cem\u003eAre we on track to meet our longer-term goals?\u003c/em\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eThis allows us to regroup and reprioritize work accordingly for the next sprint, which begins the following day.\u003c/p\u003e\n\n\n\n\u003ch3\u003eRetro\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Bring team celebrations and concerns to the table; arrive at an action item to implement in order to improve team process.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn previous retros, we all jotted down our notes and talked a little bit about the many things that had come up during the sprint, but we didn’t discuss action items sufficiently in order to implement them.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, we continue to create those notes, but then vote on a \u003cem\u003esingle issue \u003c/em\u003eto spend the majority of the retro discussing and ideating to solve.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"746\" height=\"787\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format.png 746w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-250x264.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-700x738.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum_Retro-Format-120x127.png 120w\" sizes=\"(max-width: 746px) 100vw, 746px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eNow, our retro format takes us step-by-step from ideation at the beginning, to the refining of a single idea at the end.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBy the end of the retro, we now have an implementable action item that can be tracked throughout the next few sprints. These action items allow us to actively resolve pain points and, in turn, make progress toward our broader goal of continuous self-improvement.\u003c/p\u003e\n\n\n\n\u003ch3\u003eStand-ups\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eGoal: Establish a shared understanding of the day-to-day state of the team’s work, and make any adjustments needed to unblock any team member.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIncorporating a key question to the end of stand-ups has helped the team prioritize and make adjustments where needed: “How likely are we to complete this sprint, on a scale of 1 to 5?” All at once, each team member holds up 1 to 5 fingers to communicate their answer. If anyone holds up three or fewer fingers, we invite a deeper discussion. This helps us catch and swarm on problems early, even if only one person has noticed them.\u003c/p\u003e\n\n\n\n\u003ch2\u003eRecommendations\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith simple adjustments to our Agile process, we found a meaningful change in our working rhythm. If you’re thinking about revamping your team’s Agile process, you can give these steps a try:\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e1. Try out a system holistically before making adjustments. \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAgile systems are designed with a lot of intention. Honoring all of the different parts will allow you to experience the originally intended benefits, before fine-tuning the nuances to your specific use case.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e2. Ask the “stand-up question”.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAsking “How confident are we that we will finish this sprint?” gives team members the opportunity to voice their concerns and offer potential solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e3. Focus on a single issue in retros.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAllow team members to vote on one or two issues to discuss at length, so there’s time and space to brainstorm actionable solutions.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003e4. Plan sprints you can finish, and commit to finishing them.\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eCreate multiple decision points during the sprint planning process where team members can decline work. Planning accurately sized sprints and committing to finishing them will help teams run like a well-oiled machine.\u003c/p\u003e\n\n\n\n\u003cp\u003eThese changes allowed our team to finally experience the great feeling of actually finishing a sprint and celebrating what we’ve accomplished, as well as giving us increased confidence when communicating our deliverables to stakeholders. We also found expanded opportunities to learn and collaborate, as backend and frontend engineers \u003ca href=\"https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ebecame more T-shaped\u003c/a\u003e to finish the sprint’s work in time. \u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, as we implemented these changes, the average time we took to complete a work item dropped from 8.1 days to just 3.9 days, and we were able to increase our product load from one product to three products, tripling our monthly active users (MAU) without any change in the number of engineers on our team. These quantitative improvements aligned with our impression that, with the help of our improved process, we were working with greater efficiency.  \u003c/p\u003e\n\n\n\n\u003cp\u003eMy team’s practical work of recommitting to the principles behind our Agile practices speaks to a larger theme here at Spotify: finding the right level of alignment to help navigate the flexibility of autonomy. By increasing the structure in our team processes (through adoption of Scrum, in our case), we found enhanced clarity in our work, which allowed us to ensure we always felt aligned towards our shared goals. Ultimately, we finished our process upgrade with an increased sense of pride, direction, and responsibility for our success.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAcknowledgments\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany thanks are in order to our Agile coach, Matthieu Cornillon, for guiding us through every step of this process! And of course to my teammates: Isaac Ezer, Joshua Freeberg, Rishabh Jain, Linda Liu, Yani Metaxas, Nithya Muralidharan, Sabrina Siu, Jim Thomson, Hui Yuan, and Veronica Yurovsky.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Team purpose and pride — my team hit those high marks, but it was a long journey to get there from where we started. At Spotify, we strive for “aligned autonomy” among our teams. Meaning: we align on what it is we set out to do, but preserve flexibility to choose how we’ll achieve those goals. Le",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/Scrum-Post_Header.png",
      "date_published": "2021-05-27T00:00:00Z",
      "author": {
        "name": "Published by Sophia Ciocca, Web Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/05/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/",
      "title": "\n                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 18, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/05/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/\" title=\"A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR;\u003c/strong\u003e In episode 08 of our podcast series “\u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify: A Product Story\u003c/a\u003e”, we share stories and lessons from building and open sourcing \u003ca href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. \u003ca href=\"https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eListen to the episode now\u003c/a\u003e and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow it started: “Like a cold shower”\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? \u003c/p\u003e\n\n\n\n\u003cp\u003eThe answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R\u0026amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eGustav\u003c/strong\u003e: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePia\u003c/strong\u003e: Having spent 15 years as an engineer at other companies, it was like a cold shower.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBrrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?\u003c/p\u003e\n\n\n\n\u003ch2\u003eEngineers are users, too\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Pia, two issues emerged as common causes for declining productivity:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eContext switching\u003c/strong\u003e: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eDiscoverability\u003c/strong\u003e: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many \u003cem\u003ealmost\u003c/em\u003e duplications — not pure duplications — because people are very smart and they would recognize that.” \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eThere would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “\u003ca href=\"https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erumour-driven development\u003c/a\u003e”.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd as Spotify continued to grow, the problem only got worse.\u003c/p\u003e\n\n\n\n\u003ch2\u003eSpeed, scale, autonomy… pick two?\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?\u003c/p\u003e\n\n\n\n\u003cp\u003eTurns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:\u003c/p\u003e\n\n\n\n\u003cp\u003e“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”\u003c/p\u003e\n\n\n\n\u003cp\u003eSpotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow it’s going: Not just adopted, but embraced\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. \u003c/p\u003e\n\n\n\n\u003cp\u003eEven more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.\u003c/p\u003e\n\n\n\n\u003cp\u003ePeople can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.\u003c/p\u003e\n\n\n\n\u003cp\u003eWant to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003ca href=\"https://backstage.io/\"\u003e\u003cimg loading=\"lazy\" width=\"350\" height=\"350\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/backstage-service-catalog-icon-4.gif\" alt=\"\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\n\n\n\n\u003cp\u003eListen to episode 08 — “\u003ca href=\"https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWhen to build vs buy — and when to open source\u003c/a\u003e” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWant to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn each episode, Spotify’s Chief R\u0026amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from \u003ca href=\"https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMetallica’s Lars Ulrich and Napster’s Sean Parker\u003c/a\u003e, to \u003ca href=\"https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eML legend Andrew Ng\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eHow did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running \u003ca href=\"https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle Cloud’s largest Dataflow jobs ever\u003c/a\u003e? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? \u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003eYou can find all the podcast episodes here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif",
      "date_published": "2021-05-18T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/",
      "title": "\n                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 18, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/\" title=\"A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR;\u003c/strong\u003e In episode 08 of our podcast series “\u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify: A Product Story\u003c/a\u003e”, we share stories and lessons from building and open sourcing \u003ca href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. \u003ca href=\"https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eListen to the episode now\u003c/a\u003e and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow it started: “Like a cold shower”\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? \u003c/p\u003e\n\n\n\n\u003cp\u003eThe answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R\u0026amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eGustav\u003c/strong\u003e: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePia\u003c/strong\u003e: Having spent 15 years as an engineer at other companies, it was like a cold shower.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBrrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?\u003c/p\u003e\n\n\n\n\u003ch2\u003eEngineers are users, too\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Pia, two issues emerged as common causes for declining productivity:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eContext switching\u003c/strong\u003e: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eDiscoverability\u003c/strong\u003e: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many \u003cem\u003ealmost\u003c/em\u003e duplications — not pure duplications — because people are very smart and they would recognize that.” \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eThere would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “\u003ca href=\"https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erumour-driven development\u003c/a\u003e”.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd as Spotify continued to grow, the problem only got worse.\u003c/p\u003e\n\n\n\n\u003ch2\u003eSpeed, scale, autonomy… pick two?\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?\u003c/p\u003e\n\n\n\n\u003cp\u003eTurns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:\u003c/p\u003e\n\n\n\n\u003cp\u003e“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”\u003c/p\u003e\n\n\n\n\u003cp\u003eSpotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow it’s going: Not just adopted, but embraced\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. \u003c/p\u003e\n\n\n\n\u003cp\u003eEven more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.\u003c/p\u003e\n\n\n\n\u003cp\u003ePeople can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.\u003c/p\u003e\n\n\n\n\u003cp\u003eWant to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003ca href=\"https://backstage.io/\"\u003e\u003cimg loading=\"lazy\" width=\"350\" height=\"350\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/backstage-service-catalog-icon-4.gif\" alt=\"\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\n\n\n\n\u003cp\u003eListen to episode 08 — “\u003ca href=\"https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWhen to build vs buy — and when to open source\u003c/a\u003e” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWant to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn each episode, Spotify’s Chief R\u0026amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from \u003ca href=\"https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMetallica’s Lars Ulrich and Napster’s Sean Parker\u003c/a\u003e, to \u003ca href=\"https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eML legend Andrew Ng\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eHow did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running \u003ca href=\"https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle Cloud’s largest Dataflow jobs ever\u003c/a\u003e? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? \u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003eYou can find all the podcast episodes here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif",
      "date_published": "2021-05-18T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/",
      "title": "\n                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 18, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/\" title=\"A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR;\u003c/strong\u003e In episode 08 of our podcast series “\u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify: A Product Story\u003c/a\u003e”, we share stories and lessons from building and open sourcing \u003ca href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. \u003ca href=\"https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eListen to the episode now\u003c/a\u003e and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow it started: “Like a cold shower”\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? \u003c/p\u003e\n\n\n\n\u003cp\u003eThe answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R\u0026amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eGustav\u003c/strong\u003e: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePia\u003c/strong\u003e: Having spent 15 years as an engineer at other companies, it was like a cold shower.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBrrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?\u003c/p\u003e\n\n\n\n\u003ch2\u003eEngineers are users, too\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Pia, two issues emerged as common causes for declining productivity:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eContext switching\u003c/strong\u003e: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eDiscoverability\u003c/strong\u003e: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many \u003cem\u003ealmost\u003c/em\u003e duplications — not pure duplications — because people are very smart and they would recognize that.” \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eThere would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “\u003ca href=\"https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erumour-driven development\u003c/a\u003e”.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd as Spotify continued to grow, the problem only got worse.\u003c/p\u003e\n\n\n\n\u003ch2\u003eSpeed, scale, autonomy… pick two?\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?\u003c/p\u003e\n\n\n\n\u003cp\u003eTurns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:\u003c/p\u003e\n\n\n\n\u003cp\u003e“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”\u003c/p\u003e\n\n\n\n\u003cp\u003eSpotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow it’s going: Not just adopted, but embraced\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. \u003c/p\u003e\n\n\n\n\u003cp\u003eEven more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.\u003c/p\u003e\n\n\n\n\u003cp\u003ePeople can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.\u003c/p\u003e\n\n\n\n\u003cp\u003eWant to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003ca href=\"https://backstage.io/\"\u003e\u003cimg loading=\"lazy\" width=\"350\" height=\"350\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/backstage-service-catalog-icon-4.gif\" alt=\"\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\n\n\n\n\u003cp\u003eListen to episode 08 — “\u003ca href=\"https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWhen to build vs buy — and when to open source\u003c/a\u003e” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWant to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn each episode, Spotify’s Chief R\u0026amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from \u003ca href=\"https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMetallica’s Lars Ulrich and Napster’s Sean Parker\u003c/a\u003e, to \u003ca href=\"https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eML legend Andrew Ng\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eHow did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running \u003ca href=\"https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle Cloud’s largest Dataflow jobs ever\u003c/a\u003e? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? \u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003eYou can find all the podcast episodes here\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e, \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/a-product-story-backstage-1.gif",
      "date_published": "2021-05-18T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/05/18/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/",
      "title": "\n                                            A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 18, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/05/18/a-product-story-the-lessons-of-backstage-and-spotifys-autonomous-culture/\" title=\"A Product Story: The Lessons of Backstage and Spotify’s Autonomous Culture\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/a-product-story-backstage-1.gif\" alt=\"\" loading=\"lazy\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/05/a-product-story-backstage-1.gif\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR;\u003c/strong\u003e In episode 08 of our podcast series “\u003ca href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify: A Product Story\u003c/a\u003e”, we share stories and lessons from building and open sourcing \u003ca href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy is prized, not top-down mandates) and why that ends up making Backstage such a flexible fit for other companies, too. \u003ca href=\"https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eListen to the episode now\u003c/a\u003e and get all our hard-earned lessons in entertaining podcast form — or read on for episode highlights and to learn more about this critical time in Spotify’s growth.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow it started: “Like a cold shower”\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe story begins five years ago when Spotify had a problem: we were growing fast. Really, really fast. This should be a great problem to have, except that instead of speeding us up, adding new hires was actually slowing us down. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs Director of Engineering Pia Nilsson explains in the podcast, one of the metrics Spotify’s Platform team used to measure productivity was onboarding time: how long did it take for a new engineer to merge their tenth pull request at Spotify? \u003c/p\u003e\n\n\n\n\u003cp\u003eThe answer was not good — over 60 days. That is, from the day an engineer walked through Spotify’s doors, it would be two more months before they were able to contribute code in the form of their tenth pull request. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the number alone doesn’t capture the whole feeling. Gustav Söderström, Spotify’s Chief R\u0026amp;D Officer and the podcast’s host, asks Pia if she remembers what it was like seeing that “60 days” metric for the first time:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e\u003cstrong\u003eGustav\u003c/strong\u003e: Was it like, “Maybe that’s OK”? Or was it like, “That seems super long”?\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePia\u003c/strong\u003e: Having spent 15 years as an engineer at other companies, it was like a cold shower.\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBrrr. So the first thing Pia’s team had to do was figure out what was putting the chill on new hires. Why did productivity keep dropping as the headcount kept rising?\u003c/p\u003e\n\n\n\n\u003ch2\u003eEngineers are users, too\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen it comes to their own employees, companies will often skip doing user research — after all, why ask when you can just dictate? \u003c/p\u003e\n\n\n\n\u003cp\u003eBut the Platform team at Spotify sees Spotify’s developers as their customers. Their priorities are our priorities. Their pain points are our problems to solve. So, to find out what was holding back our engineers, the first thing to do was ask our engineers. \u003c/p\u003e\n\n\n\n\u003cp\u003eAccording to Pia, two issues emerged as common causes for declining productivity:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eContext switching\u003c/strong\u003e: “People are interrupted constantly … New joiners had to tap someone on the shoulder because very seldom was there any documentation.” \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eDiscoverability\u003c/strong\u003e: “People couldn’t find things. It was simple as that. It took forever to just find the right service. There were so many \u003cem\u003ealmost\u003c/em\u003e duplications — not pure duplications — because people are very smart and they would recognize that.” \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cp\u003eThere would be 15 different versions of the same service, each speaking to the slightly different needs of different teams. And if a new team needed a similar service? Instead of sorting through all those versions … they would just build yet another version of the same service for themselves. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn a way, this is what worked for Spotify before: small, autonomous teams building fast. But that basic agile approach was reaching its limits. More teams meant more confusion, as evidenced by our onboarding metric. New hires didn’t even know where to begin — let alone how to decipher our “spaghetti” codebase — without tapping another engineer on the shoulder. It was a way of working that was becoming so common, we gave it a name — “\u003ca href=\"https://engineering.atspotify.com/2020/08/17/how-we-use-golden-paths-to-solve-fragmentation-in-our-software-ecosystem/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003erumour-driven development\u003c/a\u003e”.\u003c/p\u003e\n\n\n\n\u003cp\u003eAnd as Spotify continued to grow, the problem only got worse.\u003c/p\u003e\n\n\n\n\u003ch2\u003eSpeed, scale, autonomy… pick two?\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that the problem was clear, the solution was also obvious: centralization. But just as obvious was the fact that a centralized team will always be much slower than many small teams. Would Spotify have to trade speed for scale?\u003c/p\u003e\n\n\n\n\u003cp\u003eTurns out, the question was moot. Tasked with restoring productivity, the Platform team realized that a top-down, centralized approach wouldn’t work at Spotify for another, much more fundamental reason: it just wasn’t part of Spotify’s DNA. As Pia explains in the podcast:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e“So we basically knew we couldn’t build a centralized solution. It would never work. No one would use it. And no one really believed in it even among ourselves. We had joined Spotify for the reason that we all loved autonomy. We thought that was brilliant to set people free. So the culture really spoke to us there: “Well, you don’t have the option of building something central and mandating everyone.”\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat made Spotify engineering great was now slowing it down: too much autonomy. But that culture of autonomy would also lead to an even better solution than a simplistic tech requirements list or top-down mandates. As Spotify’s VP of Engineering, Tyson Singer, says, for Backstage to succeed with our engineers, it had to be the better solution, not the only solution:\u003c/p\u003e\n\n\n\n\u003cp\u003e“For the most part, if we go out and we tell people to do X, they just shrug, and they do wherever they want. So we really do have to sell to them. We have to basically make their lives better with everything that we do. And so [our culture] really did inform our approach, if we wanted to take control of this fragmentation problem in our tech ecosystem.”\u003c/p\u003e\n\n\n\n\u003cp\u003eSpotify wanted something that could give us everything: speed, scale — and a new idea at Spotify — aligned autonomy. And that’s how Backstage was conceived and born.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow it’s going: Not just adopted, but embraced\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo if we can’t make anyone use it, how do we know it’s working? Every day, we see the 280 engineering teams inside Spotify use Backstage to manage over 2,000 backend services, 300 websites, 4,000 data pipelines, and 200 mobile features. \u003c/p\u003e\n\n\n\n\u003cp\u003eEven more impressive are the contribution numbers. More than 200 engineers inside Spotify have contributed features to Backstage. We now have 120+ plugins developed by 50+ teams. And 80% of contributions came from Spotifiers outside the Backstage core team.\u003c/p\u003e\n\n\n\n\u003cp\u003ePeople can find what they need — without constantly interrupting their fellow developers. Any Spotifier — not just engineers, but also compliance and security team members — can easily discover all the software in our ecosystem, see who owns it, and access technical documentation in a centralized location. In an environment optimized for speed and as decentralized as Spotify, having this information so easily accessible makes all the difference. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor a company growing as fast as ours, this is a game-changing improvement to both productivity and developer happiness — which we believe go hand in hand. And we know the open source version will be able to transform other tech organizations as well. As a product, Backstage is what happens when you treat your developers with the same thoughtfulness as your users. According to our company-wide surveys, 80% of our internal users are satisfied with Backstage.\u003c/p\u003e\n\n\n\n\u003cp\u003eWant to know what happens next? How much were we able to lower that bone-chilling “60 days to tenth pull request” onboarding metric? How did our homegrown developer portal go on to become Spotify’s biggest open source project? And the significance of this humble GIF?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003ca href=\"https://backstage.io/\"\u003e\u003cimg loading=\"lazy\" width=\"350\" height=\"350\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/backstage-service-catalog-icon-4.gif\" alt=\"\"/\u003e\u003c/a\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\n\n\n\n\u003cp\u003eListen to episode 08 — “\u003ca href=\"https://open.spotify.com/episode/7iuQ3ew1Wwpuiq6LbBKzCl\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWhen to build vs buy — and when to open source\u003c/a\u003e” — to get the whole story. You’ll hear from Gustav, Tyson, and Pia, as well as Jeremiah Lowin, CEO of Prefect.io, a company that runs on what is called an “open core” model. Now streaming on Spotify — or wherever you listen to podcasts!\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWant to hear more about how Spotify was built, straight from the people who built it? The podcast series “Spotify: A Product Story” shares the stories behind the most important product strategy lessons we’ve learned at Spotify, all told in the words of the people who were actually there. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn each episode, Spotify’s Chief R\u0026amp;D Officer, Gustav Söderström, is joined by Spotify insiders and special guests, from \u003ca href=\"https://open.spotify.com/episode/5mEUQUycl3Wgx8hfWjCexD\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eMetallica’s Lars Ulrich and Napster’s Sean Parker\u003c/a\u003e, to \u003ca href=\"https://open.spotify.com/episode/0T3nb0PcpvqA4o1BbbQWpp\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eML legend Andrew Ng\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cp\u003eHow did P2P networking and local caching create a feeling of magic in the very first Spotify app? How did we go from stashing servers in a cupboard to running \u003ca href=\"https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGoogle Cloud’s largest Dataflow jobs ever\u003c/a\u003e? What does it mean to build truly ML-first products? And what’s the next frontier for creators and audio formats? \u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003eYou can find all the podcast episodes here\u003c/a\u003e.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR; In episode 08 of our podcast series “Spotify: A Product Story”, we share stories and lessons from building and open sourcing Backstage, our homegrown developer portal. Hear why a developer-friendly, market-based platform like Backstage could only have been developed at Spotify (where autonomy",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/a-product-story-backstage-1.gif",
      "date_published": "2021-05-18T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/05/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/",
      "title": "\n                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4585\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 11, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/05/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/\" title=\"Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eLast week, Spotify won an award — and we’re not playing it cool. We \u003ca href=\"https://twitter.com/SpotifyEng/status/1389988765725245441\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etweeted\u003c/a\u003e. We bragged \u003ca href=\"https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eon LinkedIn\u003c/a\u003e. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe award is \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/\" target=\"_blank\"\u003eCloud Native Computing Foundation\u003c/a\u003e’s Top End User Award, \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/\" target=\"_blank\"\u003eannounced\u003c/a\u003e at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/enduser/\" target=\"_blank\"\u003eEnd User Community\u003c/a\u003e, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like \u003ca rel=\"noreferrer noopener\" href=\"https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e, \u003ca rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=fMq3IpPE3TU\" target=\"_blank\"\u003egRPC\u003c/a\u003e, and \u003ca href=\"https://www.youtube.com/watch?v=HfRU414cjjQ\"\u003eEnvoy\u003c/a\u003e), our leadership in CNCF forums and meetups (\u003ca rel=\"noreferrer noopener\" href=\"https://community.cncf.io/stockholm/\" target=\"_blank\"\u003elook out for the next Stockholm one here\u003c/a\u003e), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" target=\"_blank\"\u003eBackstage\u003c/a\u003e, which is now in \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\"\u003ethe CNCF Sandbox\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBut in the end, this is a win for all of Spotify R\u0026amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou can read more about the award on the \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/\" target=\"_blank\"\u003eCNCF website\u003c/a\u003e. And to learn more about what got us here, listen to our podcast series “\u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003eSpotify: A Product Story\u003c/a\u003e”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cfigcaption\u003eHear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cfigcaption\u003eHear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png",
      "date_published": "2021-05-11T00:00:00Z",
      "author": {
        "name": "Published by Dave Zolotusky, Principal Engineer \u0026 CNCF Technical Oversight Committee Representative"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/",
      "title": "\n                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4585\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 11, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/\" title=\"Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eLast week, Spotify won an award — and we’re not playing it cool. We \u003ca href=\"https://twitter.com/SpotifyEng/status/1389988765725245441\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etweeted\u003c/a\u003e. We bragged \u003ca href=\"https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eon LinkedIn\u003c/a\u003e. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe award is \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/\" target=\"_blank\"\u003eCloud Native Computing Foundation\u003c/a\u003e’s Top End User Award, \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/\" target=\"_blank\"\u003eannounced\u003c/a\u003e at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/enduser/\" target=\"_blank\"\u003eEnd User Community\u003c/a\u003e, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like \u003ca rel=\"noreferrer noopener\" href=\"https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e, \u003ca rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=fMq3IpPE3TU\" target=\"_blank\"\u003egRPC\u003c/a\u003e, and \u003ca href=\"https://www.youtube.com/watch?v=HfRU414cjjQ\"\u003eEnvoy\u003c/a\u003e), our leadership in CNCF forums and meetups (\u003ca rel=\"noreferrer noopener\" href=\"https://community.cncf.io/stockholm/\" target=\"_blank\"\u003elook out for the next Stockholm one here\u003c/a\u003e), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" target=\"_blank\"\u003eBackstage\u003c/a\u003e, which is now in \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\"\u003ethe CNCF Sandbox\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBut in the end, this is a win for all of Spotify R\u0026amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou can read more about the award on the \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/\" target=\"_blank\"\u003eCNCF website\u003c/a\u003e. And to learn more about what got us here, listen to our podcast series “\u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003eSpotify: A Product Story\u003c/a\u003e”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cfigcaption\u003eHear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cfigcaption\u003eHear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png",
      "date_published": "2021-05-11T00:00:00Z",
      "author": {
        "name": "Published by Dave Zolotusky, Principal Engineer \u0026 CNCF Technical Oversight Committee Representative"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/05/11/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/",
      "title": "\n                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4585\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 11, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/05/11/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/\" title=\"Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eLast week, Spotify won an award — and we’re not playing it cool. We \u003ca href=\"https://twitter.com/SpotifyEng/status/1389988765725245441\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etweeted\u003c/a\u003e. We bragged \u003ca href=\"https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eon LinkedIn\u003c/a\u003e. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe award is \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/\" target=\"_blank\"\u003eCloud Native Computing Foundation\u003c/a\u003e’s Top End User Award, \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/\" target=\"_blank\"\u003eannounced\u003c/a\u003e at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/enduser/\" target=\"_blank\"\u003eEnd User Community\u003c/a\u003e, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like \u003ca rel=\"noreferrer noopener\" href=\"https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e, \u003ca rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=fMq3IpPE3TU\" target=\"_blank\"\u003egRPC\u003c/a\u003e, and \u003ca href=\"https://www.youtube.com/watch?v=HfRU414cjjQ\"\u003eEnvoy\u003c/a\u003e), our leadership in CNCF forums and meetups (\u003ca rel=\"noreferrer noopener\" href=\"https://community.cncf.io/stockholm/\" target=\"_blank\"\u003elook out for the next Stockholm one here\u003c/a\u003e), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" target=\"_blank\"\u003eBackstage\u003c/a\u003e, which is now in \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\"\u003ethe CNCF Sandbox\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBut in the end, this is a win for all of Spotify R\u0026amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou can read more about the award on the \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/\" target=\"_blank\"\u003eCNCF website\u003c/a\u003e. And to learn more about what got us here, listen to our podcast series “\u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003eSpotify: A Product Story\u003c/a\u003e”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cfigcaption\u003eHear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cfigcaption\u003eHear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png",
      "date_published": "2021-05-11T00:00:00Z",
      "author": {
        "name": "Published by Dave Zolotusky, Principal Engineer \u0026 CNCF Technical Oversight Committee Representative"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/",
      "title": "\n                                            Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-4585\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMay 11, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/spotify-wins-cncfs-top-end-user-award-and-toots-own-horn-about-it/\" title=\"Spotify Wins CNCF’s Top End User Award and Toots Own Horn About It\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-700x351.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-2048x1028.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eLast week, Spotify won an award — and we’re not playing it cool. We \u003ca href=\"https://twitter.com/SpotifyEng/status/1389988765725245441\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etweeted\u003c/a\u003e. We bragged \u003ca href=\"https://www.linkedin.com/posts/spotify_congratulations-to-everyone-in-spotify-r-activity-6797492373665394688-lrcs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eon LinkedIn\u003c/a\u003e. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe award is \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/\" target=\"_blank\"\u003eCloud Native Computing Foundation\u003c/a\u003e’s Top End User Award, \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/\" target=\"_blank\"\u003eannounced\u003c/a\u003e at last week’s KubeCon + CloudNativeCon. Voted on by the 140+ organizations in the \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/enduser/\" target=\"_blank\"\u003eEnd User Community\u003c/a\u003e, we’re honored to receive this recognition from our peers in the CNCF — home to so many outstanding open source projects (and people!).\u003cbr/\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWe joined the CNCF three years ago, and we’ve come a long way with the community in a short time. The award recognizes Spotify for our adoption and evangelizing of cloud native technology (like \u003ca rel=\"noreferrer noopener\" href=\"https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e, \u003ca rel=\"noreferrer noopener\" href=\"https://www.youtube.com/watch?v=fMq3IpPE3TU\" target=\"_blank\"\u003egRPC\u003c/a\u003e, and \u003ca href=\"https://www.youtube.com/watch?v=HfRU414cjjQ\"\u003eEnvoy\u003c/a\u003e), our leadership in CNCF forums and meetups (\u003ca rel=\"noreferrer noopener\" href=\"https://community.cncf.io/stockholm/\" target=\"_blank\"\u003elook out for the next Stockholm one here\u003c/a\u003e), our contributions to both the code and the direction of CNCF projects (more than 27,000 contributions to 13 different projects), and our industry-leading work on \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" target=\"_blank\"\u003eBackstage\u003c/a\u003e, which is now in \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\"\u003ethe CNCF Sandbox\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eBut in the end, this is a win for all of Spotify R\u0026amp;D. It’s recognition for our commitment to technical excellence across the entire company and our desire to always give back to the community. Thank you to everyone at Spotify for your contributions across the open source ecosystem. You should be proud.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou can read more about the award on the \u003ca rel=\"noreferrer noopener\" href=\"https://www.cncf.io/announcements/2021/05/05/cloud-native-computing-foundation-grants-spotify-the-top-end-user-award/\" target=\"_blank\"\u003eCNCF website\u003c/a\u003e. And to learn more about what got us here, listen to our podcast series “\u003ca rel=\"noreferrer noopener\" href=\"https://open.spotify.com/show/3L9tzrt0CthF6hNkxYIeSB\" target=\"_blank\"\u003eSpotify: A Product Story\u003c/a\u003e”. You’ll hear the stories and lessons from building Spotify, as told by the people who were there. For more on our journey to becoming cloud native, check out these episodes:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\n\u003cfigure\u003e\u003cfigcaption\u003eHear how we started with servers in a closet in an apartment in Stockholm, all the way to becoming one of Google Cloud’s biggest customers\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cfigcaption\u003eHear what we learned from building and open sourcing Backstage, the open platform for building developer portals, which we donated to the CNCF last year.\u003c/figcaption\u003e\u003c/figure\u003e\n\u003c/div\u003e\n\n\n\n\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Last week, Spotify won an award — and we’re not playing it cool. We tweeted. We bragged on LinkedIn. Our internal Slack is alive with emoji and exclamation points. We’re really very proud of this one. The award is Cloud Native Computing Foundation’s Top End User Award, announced at last week’s Ku",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/05/spotify-wins-cncf-end-user-award-1.png",
      "date_published": "2021-05-11T00:00:00Z",
      "author": {
        "name": "Published by Dave Zolotusky, Principal Engineer \u0026 CNCF Technical Oversight Committee Representative"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/customization-vs-configuration-in-evolving-design-systems/",
      "title": "\n                                            Customization vs. Configuration in Evolving Design Systems\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 28, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/customization-vs-configuration-in-evolving-design-systems/\" title=\"Customization vs. Configuration in Evolving Design Systems\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhen a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.\u003c/p\u003e\n\n\n\n\u003cp\u003eHow do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?\u003c/p\u003e\n\n\n\n\u003cp\u003eAs a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    \u003c/p\u003e\n\n\n\n\u003ch2\u003eAbstraction\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eFor a more thorough view of abstraction in software, and in life, check out \u003c/em\u003e\u003ca href=\"https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eLevels of Abstraction, A Key Concept in Systems Design\u003c/em\u003e\u003c/a\u003e\u003cem\u003e by Daniel Jhin Yoo. \u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.\u003c/p\u003e\n\n\n\n\u003ch2\u003eCurrent landscape\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eCustomization — \u003c/em\u003eCustom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eConfiguration — \u003c/em\u003eThe original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-768x433.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-1536x865.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1.png 1672w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSome highlights of our available options:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePowerhouse definitions\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrepacked guidelines\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProperty passthroughs\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDirect overrides\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eCustomization vs. configuration\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCustomization\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePros: Autonomy, speed, innovation\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCons: Lack of coherency, loss of maintainability, potential duplication\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eA local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.\u003c/p\u003e\n\n\n\n\u003ch3\u003eConfiguration\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePros: Consistency, contribution, maintainability\u003c/strong\u003e \u003c/p\u003e\n\n\n\n\u003cp\u003eIf emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCons: Can become a bottleneck, rigidness, vocabulary awareness\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow to decide which approach to use\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eFeature maturity\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProduct maturity\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTimeline\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReusability\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eKey takeaways\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.\u003c/p\u003e\n\n\n\n\u003cp\u003eLastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.\u003c/p\u003e\n\n\n\n\u003cp\u003e—\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA huge shout out to Krist Wongsuphasawat and his article \u003c/em\u003e\u003ca href=\"https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eNavigating the Wide World of Data Visualization Libraries\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. \u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/web/\" rel=\"tag\"\u003eweb\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png",
      "date_published": "2021-04-28T00:00:00Z",
      "author": {
        "name": "Published by Charlie Backus"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/04/customization-vs-configuration-in-evolving-design-systems/",
      "title": "\n                                            Customization vs. Configuration in Evolving Design Systems\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 28, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/04/customization-vs-configuration-in-evolving-design-systems/\" title=\"Customization vs. Configuration in Evolving Design Systems\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhen a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.\u003c/p\u003e\n\n\n\n\u003cp\u003eHow do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?\u003c/p\u003e\n\n\n\n\u003cp\u003eAs a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    \u003c/p\u003e\n\n\n\n\u003ch2\u003eAbstraction\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eFor a more thorough view of abstraction in software, and in life, check out \u003c/em\u003e\u003ca href=\"https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eLevels of Abstraction, A Key Concept in Systems Design\u003c/em\u003e\u003c/a\u003e\u003cem\u003e by Daniel Jhin Yoo. \u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.\u003c/p\u003e\n\n\n\n\u003ch2\u003eCurrent landscape\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eCustomization — \u003c/em\u003eCustom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eConfiguration — \u003c/em\u003eThe original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-768x433.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-1536x865.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1.png 1672w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSome highlights of our available options:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePowerhouse definitions\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrepacked guidelines\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProperty passthroughs\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDirect overrides\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eCustomization vs. configuration\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCustomization\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePros: Autonomy, speed, innovation\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCons: Lack of coherency, loss of maintainability, potential duplication\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eA local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.\u003c/p\u003e\n\n\n\n\u003ch3\u003eConfiguration\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePros: Consistency, contribution, maintainability\u003c/strong\u003e \u003c/p\u003e\n\n\n\n\u003cp\u003eIf emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCons: Can become a bottleneck, rigidness, vocabulary awareness\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow to decide which approach to use\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eFeature maturity\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProduct maturity\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTimeline\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReusability\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eKey takeaways\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.\u003c/p\u003e\n\n\n\n\u003cp\u003eLastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.\u003c/p\u003e\n\n\n\n\u003cp\u003e—\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA huge shout out to Krist Wongsuphasawat and his article \u003c/em\u003e\u003ca href=\"https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eNavigating the Wide World of Data Visualization Libraries\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. \u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/web/\" rel=\"tag\"\u003eweb\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png",
      "date_published": "2021-04-28T00:00:00Z",
      "author": {
        "name": "Published by Charlie Backus"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/04/28/customization-vs-configuration-in-evolving-design-systems/",
      "title": "\n                                            Customization vs. Configuration in Evolving Design Systems\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 28, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/04/28/customization-vs-configuration-in-evolving-design-systems/\" title=\"Customization vs. Configuration in Evolving Design Systems\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x60.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/04/image3.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhen a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.\u003c/p\u003e\n\n\n\n\u003cp\u003eHow do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?\u003c/p\u003e\n\n\n\n\u003cp\u003eAs a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    \u003c/p\u003e\n\n\n\n\u003ch2\u003eAbstraction\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eFor a more thorough view of abstraction in software, and in life, check out \u003c/em\u003e\u003ca href=\"https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eLevels of Abstraction, A Key Concept in Systems Design\u003c/em\u003e\u003c/a\u003e\u003cem\u003e by Daniel Jhin Yoo. \u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.\u003c/p\u003e\n\n\n\n\u003ch2\u003eCurrent landscape\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eCustomization — \u003c/em\u003eCustom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eConfiguration — \u003c/em\u003eThe original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x394.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x394.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-250x141.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-768x433.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-1536x865.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-120x68.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1.png 1672w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSome highlights of our available options:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePowerhouse definitions\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrepacked guidelines\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProperty passthroughs\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDirect overrides\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eCustomization vs. configuration\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCustomization\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePros: Autonomy, speed, innovation\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCons: Lack of coherency, loss of maintainability, potential duplication\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eA local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.\u003c/p\u003e\n\n\n\n\u003ch3\u003eConfiguration\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePros: Consistency, contribution, maintainability\u003c/strong\u003e \u003c/p\u003e\n\n\n\n\u003cp\u003eIf emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCons: Can become a bottleneck, rigidness, vocabulary awareness\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow to decide which approach to use\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eFeature maturity\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProduct maturity\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTimeline\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReusability\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eKey takeaways\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.\u003c/p\u003e\n\n\n\n\u003cp\u003eLastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.\u003c/p\u003e\n\n\n\n\u003cp\u003e—\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA huge shout out to Krist Wongsuphasawat and his article \u003c/em\u003e\u003ca href=\"https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eNavigating the Wide World of Data Visualization Libraries\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. \u003c/em\u003e\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png",
      "date_published": "2021-04-28T00:00:00Z",
      "author": {
        "name": "Published by Charlie Backus"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/customization-vs-configuration-in-evolving-design-systems/",
      "title": "\n                                            Customization vs. Configuration in Evolving Design Systems\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 28, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/customization-vs-configuration-in-evolving-design-systems/\" title=\"Customization vs. Configuration in Evolving Design Systems\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhen a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deployed.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs the product grows — and so does the team — it can sometimes seem like the team is outgrowing the current set of components and styles. Your once-perfect button doesn’t quite cover the new specs needed for a new feature. Some restrictions in the way a component is coded means it would be quicker and easier to spin up something new, rather than pull from the component library.\u003c/p\u003e\n\n\n\n\u003cp\u003eHow do we grow a design system to meet the needs of an evolving product? How do we ensure designers and developers have the tools they need to build the product or feature, even when they are not sitting next to the maintainers of the relevant design system?\u003c/p\u003e\n\n\n\n\u003cp\u003eAs a system grows more complex, this evolution can be handled by developing an abstract shared vocabulary around component properties or by ensuring that base properties remain accessible for modification by end consumers.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen working on Encore, the design system for Spotify, we try hard to ensure our customers (fellow Spotifiers) are given as much autonomy and control as possible. While we have the option to enable configuration in our components, it’s not always the first thing we reach for. Why might this be? We’ll explore these considerations in a bit more detail later on.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this post, we’ll dive into the factors at play as a design system evolves, and the pros and cons of this range of approaches.    \u003c/p\u003e\n\n\n\n\u003ch2\u003eAbstraction\u003c/h2\u003e\n\n\n\n\u003cp\u003eSo what is an abstraction? In this context, we define it as a simplified version of a more complex concept. Abstraction can make some concepts easier by obscuring underlying characters of a system in favor of a more high-level representation. We are looking at abstraction here as a measure of how different the code we write is from the HTML and CSS that is ultimately rendered. For the scope of this piece, we will be discussing abstraction from the lens of frontend development using React, starting with written code through to what is rendered in the browser. \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eFor a more thorough view of abstraction in software, and in life, check out \u003c/em\u003e\u003ca href=\"https://medium.com/@danieljyoo/levels-of-abstraction-a-key-concept-in-systems-design-7fdb33d288af\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eLevels of Abstraction, A Key Concept in Systems Design\u003c/em\u003e\u003c/a\u003e\u003cem\u003e by Daniel Jhin Yoo. \u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eIn this context, a low level of abstraction would define something that touches CSS or HTML elements directly, whereas a high level of abstraction would define changing custom properties that have their own subjective meaning and value, that in turn modify some underlying CSS or elements within the component.\u003c/p\u003e\n\n\n\n\u003ch2\u003eCurrent landscape\u003c/h2\u003e\n\n\n\n\u003cp\u003eNow that we understand what abstraction means in terms of defining web components, let’s take a look at some of the common approaches to handling evolving use cases. Some definitions that will help us understand what’s going on here:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eCustomization — \u003c/em\u003eCustom styles are added external to the component. These styles reference HTML elements and touch CSS properties directly. A low level of abstraction.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cem\u003eConfiguration — \u003c/em\u003eThe original component is made more flexible. Additional parameters are passed to the component for more varied behavior. A high level of abstraction.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x394.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-250x141.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-768x433.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-1536x865.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-120x68.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1.png 1672w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eSome highlights of our available options:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePowerhouse definitions\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e By assigning a definition to a whole set of underlying properties, this category of abstraction can get a lot done without a ton of input from the end user. Configurations like enum props allow us to add configuration to our components in a semantic way, while remaining typesafe. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003ePrepacked guidelines\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e Utility classes allow us to modify CSS properties in a granular way that still references the underlying style guide of the design system, and without having to touch CSS directly.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProperty passthroughs\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e These strategies pass the elements and properties through to ultimately be rendered to the page. Children, className, and props allow feature developers to pass their custom styles and components into the design system’s components.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDirect overrides\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e These strategies are the closest to the CSS and JSX itself. Direct overrides of existing classes and CSS properties give the most granular control of look and feel, but at the cost of unchecked specificity.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eCustomization vs. configuration\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith the range of approaches made more tangible, let’s now look at the pros and cons of different ends of the spectrum.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCustomization\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePros: Autonomy, speed, innovation\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe greatest benefit of this approach is that feature developers have the freedom to modify components in order to meet their specific needs. Developers are not tied to the system’s release cadence, which can be very appealing to teams who have pressing deadlines to meet. Not being tied to the constraints of a design system can also provide more freedom and flexibility, which can lead to more innovative approaches.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCons: Lack of coherency, loss of maintainability, potential duplication\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eA local override may solve the problem in a pinch, but those style overrides are less likely to be in close alignment with the system’s broader standards. What’s more, if this pattern emerges more broadly, this local code is not accessible for other feature developers to pick up and use — it would have to be duplicated. Further problems arise if we are looking at more sweeping updates to the design system — any sort of override (think padding, headings, spacing, even colors) made to a local version of the component will stay in place, even if the official version changes drastically.\u003c/p\u003e\n\n\n\n\u003ch3\u003eConfiguration\u003c/h3\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003ePros: Consistency, contribution, maintainability\u003c/strong\u003e \u003c/p\u003e\n\n\n\n\u003cp\u003eIf emerging variations all find their way back to the parent component, then they can be reused and tracked, to ensure that consistency is maintained. If changes need to be made to the main component, folks using the system will need to contribute back to it to meet their needs. As components are updated, consumers may safely upgrade to the latest version with less concern of breaking local overrides in the process.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eCons: Can become a bottleneck, rigidness, vocabulary awareness\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eThe other side of the contribution coin — relying on updates to the system means that code must be developed and released in a separate library before it can be used in features. This can slow down feature development, and it introduces a dependency, often on another team. The system also becomes more rigid when consumers are given fewer options — this is good for consistency, but can stifle innovation by setting constraints on how components can be manipulated. Understanding of the abstract vocabulary you have defined in configurations is an additional responsibility maintainers must take on, since you are no longer relying on baseline properties of CSS and HTML that are already thoroughly documented on the web.\u003c/p\u003e\n\n\n\n\u003ch2\u003eHow to decide which approach to use\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith both ends of the abstraction spectrum carrying implications for the key functions of your design system, it should come as no surprise that you will end up with a mix of approaches. Here are some factors to consider in deciding what approach is best for your use case:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eFeature maturity\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e If a feature is still taking shape, odds are the design is yet to be fully realized. This isn’t a bad thing — iteration is the name of the game. But when you are still experimenting with what the exact look will be, customization is your friend because you have access to any properties you may realize you need. On the flip side, if you are working with an established component, you have a wealth of existing use cases available to you to reference and establish patterns from, resulting in modifications with a more meaningful configuration for all to use.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProduct maturity\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e As with feature maturity, the less developed the product is, the harder it is to know what conventions will stick around. If you are seeing a pattern for the first time, customization may be the right move, but if you start to see it emerging in other aspects of the product, use that opportunity to take inventory of your variations and move into a more maintainable configuration approach.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eTimeline\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e While design system engineers would rather look at the best-case scenario, the feature teams who consume design systems don’t often have the same luxury. Customization is going to get something out the door quicker, but this is a great opportunity to utilize the full spectrum of approaches — what is an approach closest to configuration which will still allow you to deliver on time?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReusability\u003c/strong\u003e\u003cstrong\u003e:\u003c/strong\u003e If a pattern emerges that you can see applying across features, odds are someone else is looking for the same thing — configuration will benefit you here, and can cut down on duplication that is more likely in a customization approach.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eKey takeaways\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen evolving a design system, there is a range of strategies you can take. A more abstract configuration approach can increase consistency and maintainability, but at the risk of the system being a bottleneck for outgoing features. The less abstract customization approach enables quicker feature development; however, overall consistency of the product can suffer as a result.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe more mature a product or feature is, the more beneficial and feasible a configuration approach is. However, the iterative and low-level nature of customization makes it more suitable for prototyping and features which are bespoke, or are still subject to change.\u003c/p\u003e\n\n\n\n\u003cp\u003eLastly, one size does not fit all. In viewing the pros and cons of these different approaches, think of how those tradeoffs relate to your company’s broader values. At Spotify, the ability for teams to work autonomously is highly valued, and thus we generally lean more towards customization as a result.  Though we have the maturity to support a more configurable design system, that doesn’t mean we need to solve all of our challenges through configuration — it’s just another tool in the set that we can choose from.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile there is no right or wrong approach on how to best evolve your design system, I hope the measures above helped broaden your understanding of the tools available and the context surrounding them.\u003c/p\u003e\n\n\n\n\u003cp\u003e—\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA huge shout out to Krist Wongsuphasawat and his article \u003c/em\u003e\u003ca href=\"https://medium.com/nightingale/navigating-the-wide-world-of-web-based-data-visualization-libraries-798ea9f536e7\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eNavigating the Wide World of Data Visualization Libraries\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. While the subject matter is different, the format of Krist’s article was a huge inspiration, and the content opened my eyes to how abstraction is a huge part of the equation, even in the frontend world. \u003c/em\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/web/\" rel=\"tag\"\u003eweb\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "When a design system first starts out, the promise of visual consistency glows bright — the ideal product would have only one set of buttons, a unified typography scale, and elements that look the same no matter which designer made the design or which developer programmed them to be real and deploye",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png",
      "date_published": "2021-04-28T00:00:00Z",
      "author": {
        "name": "Published by Charlie Backus"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/nour-daoud-bosing-security-engineer/",
      "title": "\n                                            Nour Daoud Bösing: Security Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4531\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-120x69.png 120w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eNour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. \u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e6:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eThese days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e7:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThings are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00 noon\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTwice a week, my team has a virtual \u003cem\u003efika\u003c/em\u003e (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e4:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! \u003c/p\u003e\n\n\n\n\u003cp\u003eOn nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to \u003ca href=\"https://jusoorsyria.com/\"\u003eJussor\u003c/a\u003e, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"583\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/security/\" rel=\"tag\"\u003esecurity\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png",
      "date_published": "2021-04-26T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/nour-daoud-bosing-security-engineer/",
      "title": "\n                                            Nour Daoud Bösing: Security Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4531\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-120x69.png 120w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eNour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. \u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e6:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eThese days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e7:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThings are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00 noon\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTwice a week, my team has a virtual \u003cem\u003efika\u003c/em\u003e (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e4:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! \u003c/p\u003e\n\n\n\n\u003cp\u003eOn nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to \u003ca href=\"https://jusoorsyria.com/\"\u003eJussor\u003c/a\u003e, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"583\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/security/\" rel=\"tag\"\u003esecurity\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png",
      "date_published": "2021-04-26T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/04/26/nour-daoud-bosing-security-engineer/",
      "title": "\n                                            Nour Daoud Bösing: Security Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4531\"\u003e\n     \u003cdiv\u003e\n         \n         \n         \n         \u003cdiv\u003e\n             \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png 1200w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1-120x69.png 120w\" sizes=\"(max-width: 1200px) 100vw, 1200px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/04/Nour-edit-1.png\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cp\u003e\u003cb\u003eNour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. \u003c/b\u003e\u003c/p\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e6:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eThese days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e7:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThings are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00 noon\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTwice a week, my team has a virtual \u003cem\u003efika\u003c/em\u003e (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e4:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! \u003c/p\u003e\n\n\n\n\u003cp\u003eOn nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to \u003ca href=\"https://jusoorsyria.com/\"\u003eJussor\u003c/a\u003e, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"583\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n         \n         \n\n         \u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Nour-edit-1.png",
      "date_published": "2021-04-26T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/04/nour-daoud-bosing-security-engineer/",
      "title": "\n                                            Nour Daoud Bösing: Security Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4531\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png 1200w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-250x144.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-700x404.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-768x444.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1-120x69.png 120w\" sizes=\"(max-width: 1200px) 100vw, 1200px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eNour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 11-month-old daughter, Leya. \u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e6:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eThese days, I don’t need an alarm clock – I get woken up bright and early by my daughter! I live in Jersey City, just across the river from the Spotify NY office, and often used to take the ferry into work. But now, there’s no need to commute – instead, I squeeze in an hour of yoga whilst Leya is entertained by her dad. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e7:30am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI work as a Security Engineer in the Product Security team, which involves a lot of collaboration with colleagues in Sweden, so I start my day early to bridge the gap between the two time zones. My role really differs from project to project and from phase to phase of projects – some weeks will be mostly consultancy and design work, whereas others will be almost all programming. For instance, when Spotify acquired the podcasting platform Anchor, I did their security assessment, enumerated their issues and prioritized what to tackle first. Then, I put on my engineer hat and embedded with them for three weeks – getting hands-on, working through the issues that needed fixing and making sure their security was completely up to par.  \u003c/p\u003e\n\n\n\n\u003cp\u003eThings are just as varied when it comes to big internal development projects, like the Security Tiers project we rolled out last quarter. Here, the goal was to shift to a more targeted approach in addressing security risks at Spotify. I worked across every phase, from design and architecture to implementation – taking on the initial detective work, finding ways to automate our information and assigning products with their appropriate security tier. It was a lot of work and very complex at times. But being involved across all the different phases definitely kept things interesting! \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00 noon\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTwice a week, my team has a virtual \u003cem\u003efika\u003c/em\u003e (coffee break) to help us all stay connected while we’re working remotely. We also have a monthly get-together, like a baking challenge, happy hour or yoga session. I really miss the ‘water cooler chat’ that comes with working in an office. But being at home means I get to see much more of Leya – most days, she’s looked after by my mum who lives in the next block, so I stop by and take her for a walk at lunchtime. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy afternoons tend to be less meeting-heavy than my mornings, so I get more focus time to spend on things like coding and reviewing RFCs.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e4:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI finish up around 4pm, collect Leya and head to the park – the weather’s so nice at the moment and there’s lots of other babies there for her to look at. Then it’s home for dinner and a bit more playtime, although some days I need to study too – I’m doing a Master’s degree in Cyber Security at NYU and I’m just five weeks away from graduating, so right now it’s the final push! \u003c/p\u003e\n\n\n\n\u003cp\u003eOn nights when I can just kick back and relax, my husband and I usually play cards, read or watch something on Netflix. I also make a huge effort to keep in touch with my family back in Syria – I was lucky enough to escape the war and come here on a scholarship, but I have plenty of loved ones still living there. And I’m so grateful to \u003ca href=\"https://jusoorsyria.com/\"\u003eJussor\u003c/a\u003e, an amazing organization that funded my education in the US (please support them if you can!) – without their help, I wouldn’t be here in New York or doing what I do at Spotify.\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"583\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-700x583.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-250x208.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-768x640.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1-120x100.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/MyBeat_Nour-Daoud-graph-1.png 1520w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/security/\" rel=\"tag\"\u003esecurity\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Nour is a Security Engineer at Spotify New York – juggling her busy day job with completing her Masters in Cyber Security and looking after her 10-month-old daughter, Leya.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Nour-edit-1.png",
      "date_published": "2021-04-26T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/rethinking-spotify-search/",
      "title": "\n                                            Rethinking Spotify Search\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 15, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/rethinking-spotify-search/\" title=\"Rethinking Spotify Search\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003ch2\u003eSearch @ Spotify\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eSearch is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.\u003c/p\u003e\u003cp\u003eSince Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eWhen more doesn’t mean more\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003e2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “\u003cem\u003eIf each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”\u003c/em\u003e That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eBy the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"2101\" height=\"879\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg 2101w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-120x50.jpg 120w\" sizes=\"(max-width: 2101px) 100vw, 2101px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. \u003c/p\u003e\n\n\n\n\u003ch2\u003eSearch as a platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eGreat — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom user obsession to developer satisfaction\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWhile these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “\u003cem\u003e[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.\u003c/em\u003e” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.\u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eKatarina Berg, Chief HR Officer, says “\u003cem\u003eGrowth is our mantra\u003c/em\u003e” and “\u003cem\u003eChange is our constant.”\u003c/em\u003e This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReferences\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAng Li et al., “Search Mindsets:\u003cem\u003e \u003c/em\u003eUnderstanding Focused and Non-Focused Information Seeking  in\u003cem\u003e \u003c/em\u003eMusic Search,” \u003ca href=\"https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epublication\u003c/a\u003e \u003cem\u003eWWW ’19: The World Wide Web Conference\u003c/em\u003e (May 2019): 2971–2977. \u003ca href=\"https://doi.org/10.1145/3308558.3313627\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://doi.org/10.1145/3308558.3313627\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Shareholder Letter Q4 2020” (February 3, 2021) \u003ca href=\"https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Spotify — Company Info,” For the Record, \u003ca href=\"https://newsroom.spotify.com/company-info/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://newsroom.spotify.com/company-info/\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“The Band Manifesto.” \u003ca href=\"https://www.spotifyjobs.com/culture/the-band-manifesto\"\u003ehttps://www.spotifyjobs.com/culture/the-band-manifesto\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).\u003c/p\u003e\n\n\n\n\u003cp\u003e“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) \u003ca href=\"https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e “SPOTIFY PODCASTS DATASET.” \u003ca href=\"https://podcastsdataset.byspotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://podcastsdataset.byspotify.com/\u003c/a\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif",
      "date_published": "2021-04-15T00:00:00Z",
      "author": {
        "name": "Published by Hugo Galvão and Daniel Doro"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/04/15/rethinking-spotify-search/",
      "title": "\n                                            Rethinking Spotify Search\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 15, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/04/15/rethinking-spotify-search/\" title=\"Rethinking Spotify Search\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Search-gif.gif\" alt=\"\" loading=\"lazy\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/04/Search-gif.gif\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003ch2\u003eSearch @ Spotify\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eSearch is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.\u003c/p\u003e\u003cp\u003eSince Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eWhen more doesn’t mean more\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-700x352.jpg\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image5.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003e2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “\u003cem\u003eIf each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”\u003c/em\u003e That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-700x352.jpg\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image2.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eBy the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"2101\" height=\"879\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15.jpg\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15.jpg 2101w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Frame-15-120x50.jpg 120w\" sizes=\"(max-width: 2101px) 100vw, 2101px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. \u003c/p\u003e\n\n\n\n\u003ch2\u003eSearch as a platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eGreat — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom user obsession to developer satisfaction\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.jpg\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x352.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x126.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x386.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x772.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x60.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWhile these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “\u003cem\u003e[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.\u003c/em\u003e” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.\u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eKatarina Berg, Chief HR Officer, says “\u003cem\u003eGrowth is our mantra\u003c/em\u003e” and “\u003cem\u003eChange is our constant.”\u003c/em\u003e This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReferences\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAng Li et al., “Search Mindsets:\u003cem\u003e \u003c/em\u003eUnderstanding Focused and Non-Focused Information Seeking  in\u003cem\u003e \u003c/em\u003eMusic Search,” \u003ca href=\"https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epublication\u003c/a\u003e \u003cem\u003eWWW ’19: The World Wide Web Conference\u003c/em\u003e (May 2019): 2971–2977. \u003ca href=\"https://doi.org/10.1145/3308558.3313627\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://doi.org/10.1145/3308558.3313627\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Shareholder Letter Q4 2020” (February 3, 2021) \u003ca href=\"https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Spotify — Company Info,” For the Record, \u003ca href=\"https://newsroom.spotify.com/company-info/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://newsroom.spotify.com/company-info/\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“The Band Manifesto.” \u003ca href=\"https://www.spotifyjobs.com/culture/the-band-manifesto\"\u003ehttps://www.spotifyjobs.com/culture/the-band-manifesto\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).\u003c/p\u003e\n\n\n\n\u003cp\u003e“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) \u003ca href=\"https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e “SPOTIFY PODCASTS DATASET.” \u003ca href=\"https://podcastsdataset.byspotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://podcastsdataset.byspotify.com/\u003c/a\u003e\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/Search-gif.gif",
      "date_published": "2021-04-15T00:00:00Z",
      "author": {
        "name": "Published by Hugo Galvão and Daniel Doro"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/rethinking-spotify-search/",
      "title": "\n                                            Rethinking Spotify Search\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 15, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/rethinking-spotify-search/\" title=\"Rethinking Spotify Search\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003ch2\u003eSearch @ Spotify\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eSearch is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.\u003c/p\u003e\u003cp\u003eSince Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eWhen more doesn’t mean more\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003e2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “\u003cem\u003eIf each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”\u003c/em\u003e That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eBy the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"2101\" height=\"879\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg 2101w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-120x50.jpg 120w\" sizes=\"(max-width: 2101px) 100vw, 2101px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. \u003c/p\u003e\n\n\n\n\u003ch2\u003eSearch as a platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eGreat — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom user obsession to developer satisfaction\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWhile these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “\u003cem\u003e[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.\u003c/em\u003e” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.\u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eKatarina Berg, Chief HR Officer, says “\u003cem\u003eGrowth is our mantra\u003c/em\u003e” and “\u003cem\u003eChange is our constant.”\u003c/em\u003e This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReferences\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAng Li et al., “Search Mindsets:\u003cem\u003e \u003c/em\u003eUnderstanding Focused and Non-Focused Information Seeking  in\u003cem\u003e \u003c/em\u003eMusic Search,” \u003ca href=\"https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epublication\u003c/a\u003e \u003cem\u003eWWW ’19: The World Wide Web Conference\u003c/em\u003e (May 2019): 2971–2977. \u003ca href=\"https://doi.org/10.1145/3308558.3313627\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://doi.org/10.1145/3308558.3313627\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Shareholder Letter Q4 2020” (February 3, 2021) \u003ca href=\"https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Spotify — Company Info,” For the Record, \u003ca href=\"https://newsroom.spotify.com/company-info/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://newsroom.spotify.com/company-info/\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“The Band Manifesto.” \u003ca href=\"https://www.spotifyjobs.com/culture/the-band-manifesto\"\u003ehttps://www.spotifyjobs.com/culture/the-band-manifesto\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).\u003c/p\u003e\n\n\n\n\u003cp\u003e“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) \u003ca href=\"https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e “SPOTIFY PODCASTS DATASET.” \u003ca href=\"https://podcastsdataset.byspotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://podcastsdataset.byspotify.com/\u003c/a\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif",
      "date_published": "2021-04-15T00:00:00Z",
      "author": {
        "name": "Published by Hugo Galvão and Daniel Doro"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/04/rethinking-spotify-search/",
      "title": "\n                                            Rethinking Spotify Search\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 15, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/04/rethinking-spotify-search/\" title=\"Rethinking Spotify Search\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003ch2\u003eSearch @ Spotify\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eSearch is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is no exception, helping a vast majority of users find joy through search, regardless of the language or method used to search, both typed and spoken.\u003c/p\u003e\u003cp\u003eSince Spotify’s launch in 2008, Search has been a core piece of the user journey, and it’s where we’ve increased our investment and focus over time. Earlier on, only a small group of people were responsible for the end-to-end experience that encompassed the infrastructure that held Search together, the backend system that powered the personalized results, and the desktop and mobile interface that delighted our users. Spotify continued to grow, reaching 345 million users in December 2020, and Search grew with it. This post details the challenges that emerged as teams began to scale.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eWhen more doesn’t mean more\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the beginning of 2019, we already had a handful of teams working across the Search infrastructure, as well as the machine learning and backend systems. Given the increasing number of user issues we were trying to solve at the time, we decided to organize ourselves around these problems. But we quickly learned that problems come and go, new problems arise, and priorities can change unexpectedly. This meant that we were creating new teams all the time (well, not all the time, but almost every quarter).\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image5.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003e2020 was the year for us to try something new. We decided to take a different approach to organizing our teams. Each team would become responsible for one piece of our Search stack — meaning one team would be responsible for getting data into Search, another team would be responsible for the quality of personalized Search results, another team would be responsible for our Search APIs, another team for insights, and finally, a team would be focused on our company bet, podcast Search. You might be wondering, “\u003cem\u003eIf each team were responsible for one part of the Search stack, how would we solve problems that required the expertise of different parts of our time around specific issues such as query intent, retrieval, and ranking?”\u003c/em\u003e That is a great question — one that was highlighted as one of the original risks when we formed this  organization. But we believed that nurturing our tech stack was the way to go. And guess what? We learned new things, which made us reconsider old ways of thinking. Or better, made us want to try something different.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image2.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eBy the end of 2020, we had grown our internal efforts by more than 100% compared to 2018, and 500% compared to 2016. But we were not seeing the same boom in terms of speed of delivery, experimentation, and number of problems we were solving. Each time a team outside our Search area wanted to collaborate with us or use our systems to solve their problems, they would need to involve multiple experts from each Search stack part, meaning sometimes five different Search teams. There were also varied rhythms and maturities among teams and systems, despite having the same priorities.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were experiencing these problems on a daily basis, but we weren’t sure if we were blindsided by our previous learnings and our own beliefs or if we were biased because of the people we asked for feedback. We decided to check some numbers. Supported by Spotify’s Chief Architect Niklas Gustavsson’s latest research, we focused on two data points: system centrality and system congestion. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"2101\" height=\"879\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15.jpg 2101w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-250x105.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-700x293.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-768x321.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-1536x643.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-2048x857.jpg 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Frame-15-120x50.jpg 120w\" sizes=\"(max-width: 2101px) 100vw, 2101px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eIn the image above, congestion represents the unique teams contributing to the codebase over a period of time. Centrality is subdivided into two buckets: indegree centrality — how many teams have a dependency on a given service; and outdegree centrality — how many teams the service depends on. Search was in high demand across all these dimensions. \u003c/p\u003e\n\n\n\n\u003ch2\u003eSearch as a platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eGreat — we received feedback from users, from other teams, from our own Search teams, and we also had data from our own systems. Was there something else we could use to have a better understanding and make a more informed decision on how to make our Search better? We knew that users from different markets were experiencing different levels of Search satisfaction, and that the forecast was that new-user growth would come from outside North America and Western Europe. We also knew that we had dedicated Spotify teams focused on improving the overall experience for these new markets. Spotify had, as well, much experience building internal tools and platforms to scale our business and improve productivity. So we wondered, should we build a Search platform? We believed so.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom user obsession to developer satisfaction\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith insights about Spotify’s growth, system centrality and system congestion, along with team and user feedback, we decided, in 2021, to evolve our organization to try to solve for the needs of both external (Spotify end users) and internal (Spotify developers) users. In order to accomplish that, we created two groups inside our Search area —  one focused on our personalized core Search experience, with Spotify end user satisfaction as the measure of success, and the other aiming to improve Spotify developer happiness, encouraging experimentation while maintaining the services SLOs. Below you can see what our Search organization looks like today.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"352\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-700x352.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-250x126.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-768x386.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-1536x772.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-120x60.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWhile these two groups don’t necessarily share the same metrics, we believe that they do share the same goal: “\u003cem\u003e[T]o unlock the potential of human creativity — by giving a million creative artists the opportunity to live off their art and billions of fans the opportunity to enjoy and be inspired by it.\u003c/em\u003e” Making these groups autonomous and independent in ways of working and goal setting — leading with context instead of control — is what we believe makes us better prepared to support Spotify users and growth.\u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eKatarina Berg, Chief HR Officer, says “\u003cem\u003eGrowth is our mantra\u003c/em\u003e” and “\u003cem\u003eChange is our constant.”\u003c/em\u003e This means that our Search journey will not end here. Nor will this be the last iteration of our organization. But we are eager to give it a try, learn new things, tweak them, and try again —  especially now that we are expanding into more markets and languages, investing in podcast topic search, podcast understanding, and retrieval, and rolling out many other new features in the future.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReferences\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eAng Li et al., “Search Mindsets:\u003cem\u003e \u003c/em\u003eUnderstanding Focused and Non-Focused Information Seeking  in\u003cem\u003e \u003c/em\u003eMusic Search,” \u003ca href=\"https://research.atspotify.com/publications/search-mindsets-understanding-focused-and-non-focused-information-needs-in-music-search/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003epublication\u003c/a\u003e \u003cem\u003eWWW ’19: The World Wide Web Conference\u003c/em\u003e (May 2019): 2971–2977. \u003ca href=\"https://doi.org/10.1145/3308558.3313627\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://doi.org/10.1145/3308558.3313627\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Shareholder Letter Q4 2020” (February 3, 2021) \u003ca href=\"https://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://s22.q4cdn.com/540910603/files/doc_financials/2020/q4/Shareholder-Letter-Q4-2020_FINAL.pdf\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Spotify — Company Info,” For the Record, \u003ca href=\"https://newsroom.spotify.com/company-info/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://newsroom.spotify.com/company-info/\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“The Band Manifesto.” \u003ca href=\"https://www.spotifyjobs.com/culture/the-band-manifesto\"\u003ehttps://www.spotifyjobs.com/culture/the-band-manifesto\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e“Spotify Expands International Footprint, Bringing Audio to 80+ New Markets,” For the Record (February 22, 2021).\u003c/p\u003e\n\n\n\n\u003cp\u003e“Today’s Spotify Stream On Announcements,” For the Record (February 22, 2021) \u003ca href=\"https://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://newsroom.spotify.com/2021-02-22/todays-spotify-stream-on-announcements/\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e “SPOTIFY PODCASTS DATASET.” \u003ca href=\"https://podcastsdataset.byspotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehttps://podcastsdataset.byspotify.com/\u003c/a\u003e\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/backend/\" rel=\"tag\"\u003ebackend\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "Search @ Spotify Search is a well-established functionality across different industries, devices, and applications. When users come to any kind of search, they already have something in mind, whether they come looking for one thing in particular or are open to becoming inspired. Spotify Search is",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/Search-gif.gif",
      "date_published": "2021-04-15T00:00:00Z",
      "author": {
        "name": "Published by Hugo Galvão and Daniel Doro"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/04/building-the-future-of-our-desktop-apps/",
      "title": "\n                                            Building the Future of Our Desktop Apps\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/04/building-the-future-of-our-desktop-apps/\" title=\"Building the Future of Our Desktop Apps\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-120x60.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eFor the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.\u003c/p\u003e\n\n\n\n\u003ch2\u003eIn the beginning, there were two clients\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eTowards the end of 2018, our team was the owner of a recently built \u003ca href=\"https://open.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWeb Player\u003c/a\u003e, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.\u003c/p\u003e\u003cp\u003eBecause the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to \u003ca href=\"https://en.wikipedia.org/wiki/Conway%27s_law\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eConway’s law\u003c/a\u003e). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eIn addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.\u003c/p\u003e\u003cp\u003eWe had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWe knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.\u003c/p\u003e\n\n\n\n\u003cp\u003eThere were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.\u003c/p\u003e\n\n\n\n\u003cp\u003eLet’s talk more in detail about how we turned the idea into reality.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"259\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-250x93.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-768x285.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-1536x569.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-120x44.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eOne UI, multiple containers\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Spotify Desktop client is a Windows and Mac native application that uses CEF (\u003ca href=\"https://bitbucket.org/chromiumembedded/cef/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eChromium Embedded Framework\u003c/a\u003e) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png\" alt=\"\" width=\"450\" height=\"451\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-250x250.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-150x150.png 150w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-768x769.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-120x120.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7.png 1200w\" sizes=\"(max-width: 450px) 100vw, 450px\"/\u003e\u003cfigcaption\u003ePrevious architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow \u003ca href=\"http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplayback to begin as soon as a listener clicked\u003c/a\u003e. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eThe Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.\u003c/p\u003e\u003cp\u003eThe Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThis concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"375\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-250x134.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-768x412.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-120x64.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879.png 1199w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eThe new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWith this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"387\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-250x138.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-768x425.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-120x66.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x483.png\" alt=\"\"/\u003e\u003cfigcaption\u003eOld vs New: the Web Player UI has come a long way since the project started.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eSolving the organizational challenge\u003c/h2\u003e\n\n\n\n\u003cp\u003eFrom the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?\u003c/p\u003e\n\n\n\n\u003cp\u003eThere was also a large information gap we had to solve. What features in the existing Desktop application \u003cem\u003ehad\u003c/em\u003e to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.\u003c/p\u003e\n\n\n\n\u003cp\u003eAfter three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.\u003c/p\u003e\n\n\n\n\u003ch2\u003eEvaluating success\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png\" alt=\"\" width=\"500\" height=\"500\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch3\u003e1. \u003cstrong\u003eReusability\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eReusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.\u003c/p\u003e\n\n\n\n\u003ch3\u003e2. \u003cstrong\u003eUnification\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eUser experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.\u003c/p\u003e\n\n\n\n\u003cp\u003eCritically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. \u003c/p\u003e\n\n\n\n\u003ch3\u003e3. \u003cstrong\u003eSpeed\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eAn important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.\u003c/p\u003e\n\n\n\n\u003ch3\u003e4. \u003cstrong\u003eSatisfaction: Meeting Desktop and Web Player user needs\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat does all this mean for you as a user?\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eAs a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.\u003c/p\u003e\u003cp\u003eAs time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eIs this your jam? Join us!\u003c/h2\u003e\n\n\n\n\u003cp\u003eWant to join the band and build the future of Spotify? Head over to our \u003ca href=\"https://www.spotifyjobs.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ejob board\u003c/a\u003e and see if anything catches your eye. We’ve just announced our \u003ca href=\"https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWorking From Anywhere\u003c/a\u003e policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian\u003c/em\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/web/\" rel=\"tag\"\u003eweb\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png",
      "date_published": "2021-04-07T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/building-the-future-of-our-desktop-apps/",
      "title": "\n                                            Building the Future of Our Desktop Apps\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/building-the-future-of-our-desktop-apps/\" title=\"Building the Future of Our Desktop Apps\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-120x60.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eFor the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.\u003c/p\u003e\n\n\n\n\u003ch2\u003eIn the beginning, there were two clients\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eTowards the end of 2018, our team was the owner of a recently built \u003ca href=\"https://open.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWeb Player\u003c/a\u003e, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.\u003c/p\u003e\u003cp\u003eBecause the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to \u003ca href=\"https://en.wikipedia.org/wiki/Conway%27s_law\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eConway’s law\u003c/a\u003e). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eIn addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.\u003c/p\u003e\u003cp\u003eWe had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWe knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.\u003c/p\u003e\n\n\n\n\u003cp\u003eThere were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.\u003c/p\u003e\n\n\n\n\u003cp\u003eLet’s talk more in detail about how we turned the idea into reality.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"259\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-250x93.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-768x285.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-1536x569.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-120x44.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eOne UI, multiple containers\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Spotify Desktop client is a Windows and Mac native application that uses CEF (\u003ca href=\"https://bitbucket.org/chromiumembedded/cef/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eChromium Embedded Framework\u003c/a\u003e) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png\" alt=\"\" width=\"450\" height=\"451\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-250x250.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-150x150.png 150w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-768x769.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-120x120.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7.png 1200w\" sizes=\"(max-width: 450px) 100vw, 450px\"/\u003e\u003cfigcaption\u003ePrevious architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow \u003ca href=\"http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplayback to begin as soon as a listener clicked\u003c/a\u003e. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eThe Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.\u003c/p\u003e\u003cp\u003eThe Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThis concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"375\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-250x134.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-768x412.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-120x64.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879.png 1199w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eThe new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWith this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"387\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-250x138.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-768x425.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-120x66.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x483.png\" alt=\"\"/\u003e\u003cfigcaption\u003eOld vs New: the Web Player UI has come a long way since the project started.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eSolving the organizational challenge\u003c/h2\u003e\n\n\n\n\u003cp\u003eFrom the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?\u003c/p\u003e\n\n\n\n\u003cp\u003eThere was also a large information gap we had to solve. What features in the existing Desktop application \u003cem\u003ehad\u003c/em\u003e to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.\u003c/p\u003e\n\n\n\n\u003cp\u003eAfter three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.\u003c/p\u003e\n\n\n\n\u003ch2\u003eEvaluating success\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png\" alt=\"\" width=\"500\" height=\"500\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch3\u003e1. \u003cstrong\u003eReusability\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eReusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.\u003c/p\u003e\n\n\n\n\u003ch3\u003e2. \u003cstrong\u003eUnification\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eUser experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.\u003c/p\u003e\n\n\n\n\u003cp\u003eCritically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. \u003c/p\u003e\n\n\n\n\u003ch3\u003e3. \u003cstrong\u003eSpeed\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eAn important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.\u003c/p\u003e\n\n\n\n\u003ch3\u003e4. \u003cstrong\u003eSatisfaction: Meeting Desktop and Web Player user needs\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat does all this mean for you as a user?\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eAs a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.\u003c/p\u003e\u003cp\u003eAs time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eIs this your jam? Join us!\u003c/h2\u003e\n\n\n\n\u003cp\u003eWant to join the band and build the future of Spotify? Head over to our \u003ca href=\"https://www.spotifyjobs.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ejob board\u003c/a\u003e and see if anything catches your eye. We’ve just announced our \u003ca href=\"https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWorking From Anywhere\u003c/a\u003e policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian\u003c/em\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/web/\" rel=\"tag\"\u003eweb\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png",
      "date_published": "2021-04-07T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/",
      "title": "\n                                            Building the Future of Our Desktop Apps\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/04/07/building-the-future-of-our-desktop-apps/\" title=\"Building the Future of Our Desktop Apps\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-1536x771.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX-120x60.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/04/ClientX.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eFor the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.\u003c/p\u003e\n\n\n\n\u003ch2\u003eIn the beginning, there were two clients\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eTowards the end of 2018, our team was the owner of a recently built \u003ca href=\"https://open.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWeb Player\u003c/a\u003e, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.\u003c/p\u003e\u003cp\u003eBecause the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to \u003ca href=\"https://en.wikipedia.org/wiki/Conway%27s_law\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eConway’s law\u003c/a\u003e). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eIn addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.\u003c/p\u003e\u003cp\u003eWe had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWe knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.\u003c/p\u003e\n\n\n\n\u003cp\u003eThere were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.\u003c/p\u003e\n\n\n\n\u003cp\u003eLet’s talk more in detail about how we turned the idea into reality.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"259\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-700x259.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-700x259.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-250x93.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-768x285.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-1536x569.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8-120x44.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image8.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eOne UI, multiple containers\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Spotify Desktop client is a Windows and Mac native application that uses CEF (\u003ca href=\"https://bitbucket.org/chromiumembedded/cef/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eChromium Embedded Framework\u003c/a\u003e) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-700x701.png\" alt=\"\" width=\"450\" height=\"451\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-700x701.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-250x250.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-150x150.png 150w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-768x769.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7-120x120.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image7.png 1200w\" sizes=\"(max-width: 450px) 100vw, 450px\"/\u003e\u003cfigcaption\u003ePrevious architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow \u003ca href=\"http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplayback to begin as soon as a listener clicked\u003c/a\u003e. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eThe Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.\u003c/p\u003e\u003cp\u003eThe Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThis concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"375\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-700x375.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-700x375.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-250x134.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-768x412.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879-120x64.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/path879.png 1199w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eThe new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWith this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"387\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-700x387.jpg\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-700x387.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-250x138.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-768x425.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2-120x66.jpg 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-2.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"483\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x483.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-700x483.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-250x172.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-768x529.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-1536x1059.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1-120x83.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image1.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eOld vs New: the Web Player UI has come a long way since the project started.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eSolving the organizational challenge\u003c/h2\u003e\n\n\n\n\u003cp\u003eFrom the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?\u003c/p\u003e\n\n\n\n\u003cp\u003eThere was also a large information gap we had to solve. What features in the existing Desktop application \u003cem\u003ehad\u003c/em\u003e to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.\u003c/p\u003e\n\n\n\n\u003cp\u003eAfter three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.\u003c/p\u003e\n\n\n\n\u003ch2\u003eEvaluating success\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png\" alt=\"\" width=\"500\" height=\"500\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-250x250.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-700x700.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-150x150.png 150w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-768x768.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-1536x1536.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/image3-120x120.png 120w\" sizes=\"(max-width: 500px) 100vw, 500px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch3\u003e1. \u003cstrong\u003eReusability\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eReusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.\u003c/p\u003e\n\n\n\n\u003ch3\u003e2. \u003cstrong\u003eUnification\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eUser experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.\u003c/p\u003e\n\n\n\n\u003cp\u003eCritically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. \u003c/p\u003e\n\n\n\n\u003ch3\u003e3. \u003cstrong\u003eSpeed\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eAn important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.\u003c/p\u003e\n\n\n\n\u003ch3\u003e4. \u003cstrong\u003eSatisfaction: Meeting Desktop and Web Player user needs\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat does all this mean for you as a user?\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eAs a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.\u003c/p\u003e\u003cp\u003eAs time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eIs this your jam? Join us!\u003c/h2\u003e\n\n\n\n\u003cp\u003eWant to join the band and build the future of Spotify? Head over to our \u003ca href=\"https://www.spotifyjobs.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ejob board\u003c/a\u003e and see if anything catches your eye. We’ve just announced our \u003ca href=\"https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWorking From Anywhere\u003c/a\u003e policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian\u003c/em\u003e.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/04/ClientX.png",
      "date_published": "2021-04-07T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/building-the-future-of-our-desktop-apps/",
      "title": "\n                                            Building the Future of Our Desktop Apps\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eApril 7, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/building-the-future-of-our-desktop-apps/\" title=\"Building the Future of Our Desktop Apps\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-1536x771.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX-120x60.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eFor the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.\u003c/p\u003e\n\n\n\n\u003ch2\u003eIn the beginning, there were two clients\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eTowards the end of 2018, our team was the owner of a recently built \u003ca href=\"https://open.spotify.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWeb Player\u003c/a\u003e, as well as Spotify’s Desktop client. The Desktop was our rich, full-featured experience and the Web Player was a much lighter, simpler experience.\u003c/p\u003e\u003cp\u003eBecause the Web Player was implemented with a modern React app architecture, we had success onboarding new engineers to the Web Player code. But those same engineers were having difficulties with the Desktop client, which used a very diverse range of web technologies (thanks to \u003ca href=\"https://en.wikipedia.org/wiki/Conway%27s_law\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eConway’s law\u003c/a\u003e). Due to having to implement many of the features twice at different levels of complexity while dealing with context switching, we were not shipping new features at the pace we would have liked to.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eIn addition, there were accessibility issues in our clients that we needed to solve. We discovered that making our Web Player accessible was going to be a difficult, yet achievable, challenge. Making the Desktop application accessible, in contrast, would be nearly impossible.\u003c/p\u003e\u003cp\u003eWe had many discussions on how to solve these problems. The team figured out that converging the clients into a single codebase and user experience would be the best way forward. We considered several approaches and did tech spikes to test many of the ideas — component sharing, feature sharing — always trying to find the right balance between fixing our technical debt problem while continuing to improve the experience for our users.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWe knew we were embarking on a long-term project, so our biggest priority was to de-risk delivery and avoid trapping ourselves into a big bang rewrite. We settled on a bold solution: focus on iterating on top of the existing Web Player codebase until it reached a Desktop-grade feature set. Since our Web Player is continuously deployed, we could ship and test with real users every change made towards our final goal.\u003c/p\u003e\n\n\n\n\u003cp\u003eThere were risks, of course. Desktop had (and has) many more users than Web Player, and Spotify’s Desktop client is the place most of Spotify’s “power users” call home. We knew we would have a lot of work to do to bring our Web Player up to those power users’ exacting standards.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow, at the beginning of 2021, we have created one maintainable codebase for both of our clients with the high standard of accessibility and speed of development we hoped for.\u003c/p\u003e\n\n\n\n\u003cp\u003eLet’s talk more in detail about how we turned the idea into reality.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"259\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-700x259.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-250x93.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-768x285.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-1536x569.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8-120x44.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image8.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eOne UI, multiple containers\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Spotify Desktop client is a Windows and Mac native application that uses CEF (\u003ca href=\"https://bitbucket.org/chromiumembedded/cef/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eChromium Embedded Framework\u003c/a\u003e) to display a web-based user interface. That’s still true today, but for the previous version of Desktop, every “page” in the client was built as a standalone “app” to run inside its own iframe. This architecture was designed to foster autonomy, allowing multiple teams — and potentially partners — to own the development and maintenance of the features. Eventually, however, one team became responsible for the user interface of the entire application.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png\" alt=\"\" width=\"450\" height=\"451\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-700x701.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-250x250.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-150x150.png 150w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-768x769.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7-120x120.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image7.png 1200w\" sizes=\"(max-width: 450px) 100vw, 450px\"/\u003e\u003cfigcaption\u003ePrevious architecture (simplified) of the Desktop client. Each page in the application would be sandboxed in an iframe and built in different ways. The UI would access the backend through the native container.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe previous version of the Desktop client had many strengths, including Spotify’s original “killer feature” from its very first client, which would allow \u003ca href=\"http://www.csc.kth.se/~gkreitz/spotify-p2p10/spotify-p2p10.pdf\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplayback to begin as soon as a listener clicked\u003c/a\u003e. It also boasted a comprehensive set of features we know Spotify listeners value. But, at the same time, this architecture was causing severe friction for developers.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eThe Web Player’s codebase, however, was considered a much more solid foundation to build upon. It allowed us to develop new features quickly. It was developed with the web in mind, meaning it was small in size, more performant, and worked with various browsers. The client was delivered continuously, allowing changes to get to users almost immediately. We decided, then, to use the Web Player as the starting point for a single user experience shared between the Web Player and Desktop. One of the main challenges we encountered was that this approach would require us to ship and run the Web Player UI with the Desktop container.\u003c/p\u003e\u003cp\u003eThe Web Player was also tightly coupled to our web servers, relying on them for all data and authentication. The playback system used by Web Player was not compatible with Desktop. Authentication worked differently — we needed to support our web OAuth login on Web Player and our native login on Desktop. Desktop would also need features its users expect, such as downloading and offline playback, that are not supported by the Web Player.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThis concept of running the same user interface on two similar but different infrastructures is what informed the architecture we developed. In order to keep the UI platform agnostic, we built TypeScript Platform APIs that would abstract the different sources of data and different playback stacks, as well as provide helpful information to the user interface about what functionality was available to it. We also rewrote the whole client in TypeScript along the way, as we were rebuilding the experience bit by bit.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhile work was done outside of our team to make certain kinds of data available via the web, we focused on decoupling the Web Player not just from the web servers but also from any hard-coded dependencies from being run in a normal browser.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe final architecture looks like a layer of Platform APIs that expose the underlying Spotify ecosystem to clients, with a React-based user interface and the Platform APIs exposed via React Hooks. Thus, the new UI can run on the web, and it can run in our Desktop container, and never know, or care, if the data is coming from our C++ stack or our web infrastructure.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"375\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-700x375.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-250x134.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-768x412.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879-120x64.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/path879.png 1199w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eThe new architecture of Web Player (left) and Desktop (right) clients. The UI is built as a React application that reaches the backend through our GraphQL and Web API services, and in some cases achieves this through the native Desktop APIs due to their increased performance and capabilities.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWith this architecture in place, the team’s velocity began increasing rapidly. We added downloading, offline mode, local files, lyrics, a “Now Playing” queue, as well as advanced features such as sorting and filtering of playlists and albums. In just over a year, the new shared UI included all the features of the original Desktop client and was, in some areas, actually more advanced, including features previously seen only on the mobile client.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"387\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-700x387.jpg 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-250x138.jpg 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-768x425.jpg 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-1536x850.jpg 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2-120x66.jpg 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3-2.jpg 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image1-700x483.png\" alt=\"\"/\u003e\u003cfigcaption\u003eOld vs New: the Web Player UI has come a long way since the project started.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eSolving the organizational challenge\u003c/h2\u003e\n\n\n\n\u003cp\u003eFrom the moment we decided on the product strategy for the new Desktop client, we began work on solving the engineering challenge — but there was also the organizational challenge: how could we actually make this happen in a reasonable amount of time without dropping the everyday “business as usual” work that needed to continue?\u003c/p\u003e\n\n\n\n\u003cp\u003eThere was also a large information gap we had to solve. What features in the existing Desktop application \u003cem\u003ehad\u003c/em\u003e to be implemented in the new one? What should the new client look like? Almost immediately the design and product insight teams began to investigate how our users use our software, so that we could draw up a road map towards being able to ship.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt the same time we created a small “virtual team” made up of engineers from several teams to begin the very first engineering experiments and answer some fundamental questions: Was the desired solution even possible? How much work would it actually require? This virtual team’s priority was simply to get the Web Player, as it was, running inside the Desktop container. They would solve the problem of playback and authentication, explore how the UI was bundled with the container, and set the engineering blueprint for the rest of the project. The team was aided by other teams within Spotify to create a single UI that could run on multiple platforms having different capabilities — for example, televisions. The fact that both codebases were co-located in the same monorepo as a result of previous efforts to converge the clients was key to facilitating this task.\u003c/p\u003e\n\n\n\n\u003cp\u003eAfter three months, the team’s work concluded successfully. We established our roadmap and priorities, and we knew exactly what we would be doing for the upcoming year. It would require a full commitment from everyone on our wider team, with constant testing and analysis to ensure we were on the correct path. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn reality, this project only happened because of the commitment of our engineering, design, and product management teams to envision a product that engineers could iterate on quickly, and that would fully support the Spotify vision. We had to iterate longer than we’d hoped before shipping to users, but the speed at which the team was able to implement these features in the new shared UI is what gave everyone the confidence that we were heading in the right direction.\u003c/p\u003e\n\n\n\n\u003ch2\u003eEvaluating success\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe had four primary goals at the start of this project: make our code reusable, unify our user experience and visual design, improve speed to deliver more quickly, and do all of this while meeting Desktop and Web Player users’ needs. With the results of the project now shipped, how have we performed against these metrics?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/image3.png\" alt=\"\" width=\"500\" height=\"500\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch3\u003e1. \u003cstrong\u003eReusability\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eReusing the same code in multiple clients (i.e., the Web Player and Desktop) allows us to write the code once and reap the benefits in multiple places. When we need to implement a design change, it’s much more efficient to make it in one location and have it propagate to all receiving endpoints. We would like to expand our reusability in the future, sharing more of our Platform APIs with even more clients.\u003c/p\u003e\n\n\n\n\u003ch3\u003e2. \u003cstrong\u003eUnification\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eUser experience and visual design are important yet time-consuming areas to improve within an application. Thus, having one set of components that service multiple clients ensures that we can implement designs more thoroughly, thereby improving our users’ experiences.\u003c/p\u003e\n\n\n\n\u003cp\u003eCritically, we have been able to achieve a degree of unification with the rest of the Spotify ecosystem, moving our clients to Spotify’s shared design language. The result is a more consistent experience when users switch between mobile and desktop, as well as a more modern, contemporary, accessible, and user-oriented experience for everyone. \u003c/p\u003e\n\n\n\n\u003ch3\u003e3. \u003cstrong\u003eSpeed\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eAn important justification for this project was the argument that a modernized codebase with a single, easy-to-understand architecture would increase our velocity as engineers. While we need more time to conclusively prove success in the long term, the large number of features the team has already completed since the project began is a positive indicator. Speed, however, is merely an outcome — the result of engineers with clear goals working with a healthy codebase. We measure code health in terms of test coverage, maintainability, readability, and how easy code is to remove. The architecture we chose had unexpected benefits in terms of making UI coding simpler and easier to understand as developers, and so we are hopeful this platform is going to be a solid foundation for us to build on in the years to come.\u003c/p\u003e\n\n\n\n\u003ch3\u003e4. \u003cstrong\u003eSatisfaction: Meeting Desktop and Web Player user needs\u003c/strong\u003e\u003c/h3\u003e\n\n\n\n\u003cp\u003eThe new experience has been developed with Spotify users in mind — both existing Desktop power users, and new users coming from the mobile app or completely new to the Spotify ecosystem. From the very beginning, we’ve been evaluating and testing our progress at each step to make sure we deliver an experience that fulfills our users’ needs. We’ve conducted extensive user research and run continuous tests over the past year that have informed us of the direction we should take. We’ve made the experience more accessible than ever, so everyone can enjoy using the application.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are looking closely at the feedback received and are continuously shaping the application to satisfy users’ needs. The new architecture lets us move faster, and users can expect the client to evolve more quickly than ever before.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat does all this mean for you as a user?\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cp\u003eAs a music listener using the Spotify Desktop client or Web Player, we hope it feels like a fresh new experience, but with all the features you use and love still there. You’ll notice a few new features that you might have seen on Spotify on mobile appearing for the first time too.\u003c/p\u003e\u003cp\u003eAs time goes on, you’ll begin to notice brand-new features appearing more often, making your experience of music and podcasts even better. The launch of the new Desktop, for us, is not the end. It’s just a new beginning for the app that started everything here at Spotify.\u003c/p\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eIs this your jam? Join us!\u003c/h2\u003e\n\n\n\n\u003cp\u003eWant to join the band and build the future of Spotify? Head over to our \u003ca href=\"https://www.spotifyjobs.com/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ejob board\u003c/a\u003e and see if anything catches your eye. We’ve just announced our \u003ca href=\"https://hrblog.spotify.com/2021/02/12/introducing-working-from-anywhere/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWorking From Anywhere\u003c/a\u003e policy, which allows employees to choose whether they want to work from home full time, at the office full time, or a combination of the two.\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA shout out to everyone who contributed to this project, especially Felix Bruns, Peter Johansson, Alberto Núñez Acosta, Guido Kessels, Tryggvi Gylfason, Craig Spence, Lucas Lencinas and Emma Bostian\u003c/em\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/web/\" rel=\"tag\"\u003eweb\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "For the past couple of years, we’ve been on a mission to modernize our Spotify clients by creating one single desktop UI for both the Desktop application and the Web Player. We couldn’t build everything we wanted to for our users with our old setup, so we decided to do something about it.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/04/ClientX.png",
      "date_published": "2021-04-07T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/30/my-beat-david-riordan/",
      "title": "\n                                            David Riordan: Product Manager\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4054\"\u003e\n     \u003cdiv\u003e\n         \n         \n         \n         \u003cdiv\u003e\n             \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w\" sizes=\"(max-width: 500px) 100vw, 500px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2020/11/MyBeat_David-Riordan.png\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cp\u003e\u003cb\u003eDavid is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  \u003c/b\u003e\u003c/p\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e5:00 am \u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e9:00 am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBack home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor the past year, I’ve been part of a project called \u003ca href=\"https://klio.io/\"\u003eKlio\u003c/a\u003e – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  \u003c/p\u003e\n\n\n\n\u003cp\u003eKlio has been a long time in the making, so \u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\"\u003eit was amazing to finally open-source it\u003c/a\u003e last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e11:30 am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWorking on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00 noon\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTime to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00 pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWhilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e6:00 pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWe’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png\" alt=\"\" width=\"700\" height=\"685\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\n\n         \n         \n\n         \u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/MyBeat_David-Riordan.png",
      "date_published": "2021-03-30T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/my-beat-david-riordan/",
      "title": "\n                                            David Riordan: Product Manager\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4054\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w\" sizes=\"(max-width: 500px) 100vw, 500px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eDavid is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  \u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e5:00 am \u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e9:00 am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBack home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor the past year, I’ve been part of a project called \u003ca href=\"https://klio.io/\"\u003eKlio\u003c/a\u003e – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  \u003c/p\u003e\n\n\n\n\u003cp\u003eKlio has been a long time in the making, so \u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\"\u003eit was amazing to finally open-source it\u003c/a\u003e last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e11:30 am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWorking on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00 noon\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTime to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00 pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWhilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e6:00 pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWe’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png\" alt=\"\" width=\"700\" height=\"685\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png",
      "date_published": "2021-03-30T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/my-beat-david-riordan/",
      "title": "\n                                            David Riordan: Product Manager\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4054\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w\" sizes=\"(max-width: 500px) 100vw, 500px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eDavid is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  \u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e5:00 am \u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e9:00 am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBack home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor the past year, I’ve been part of a project called \u003ca href=\"https://klio.io/\"\u003eKlio\u003c/a\u003e – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  \u003c/p\u003e\n\n\n\n\u003cp\u003eKlio has been a long time in the making, so \u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\"\u003eit was amazing to finally open-source it\u003c/a\u003e last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e11:30 am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWorking on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00 noon\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTime to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00 pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWhilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e6:00 pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWe’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png\" alt=\"\" width=\"700\" height=\"685\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png",
      "date_published": "2021-03-30T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/my-beat-david-riordan/",
      "title": "\n                                            David Riordan: Product Manager\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4054\"\u003e\n     \u003cdiv\u003e\n         \n         \n        \n         \u003cdiv\u003e\n            \u003cdiv\u003e\n            \u003ch2\u003ePutting the Spotlight on our technical employees\u003c/h2\u003e\n    \u003cp\u003eMy beat is a blog series that turns the spotlight towards technical employees across various desciplines and roles to showcase what a typical day as a Spotifier consists of.\u003c/p\u003e\n            \u003c/div\u003e\n \n             \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png 500w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-250x175.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-120x84.png 120w\" sizes=\"(max-width: 500px) 100vw, 500px\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cdiv\u003e\n             \n                 \u003cp\u003e\u003cb\u003eDavid is a Product Manager at Spotify in New York. But since the start of the pandemic, he’s been working from the Greenpoint apartment he shares with his wife, dog and 21-month-old son, Zev. Here, he talks us through his day-to-day…  \u003c/b\u003e\u003c/p\u003e\n             \u003c/div\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e5:00 am \u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw Zev in the back carrier and walk all together through the park to his nanny-share. It’s a really lovely way to start the day. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e9:00 am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eBack home, the first thing I do is check in with my To Do list – I have a love-hate relationship with task management software, but it’s great to have all my personal and professional commitments in one place. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs a Product Manager in Spotify’s Data and Insights team, I work on the audio-processing infrastructure – which means I get to hang out with brilliant researchers and build the tools they need to take big leaps in knowledge, as well as in the application of that new knowledge. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor the past year, I’ve been part of a project called \u003ca href=\"https://klio.io/\"\u003eKlio\u003c/a\u003e – creating a software framework that allows researchers, engineers and data scientists to process audio files easily and at scale, as part of a commodity data pipeline. It means that algorithms that could previously only run in a very bespoke manner on a small or medium-sized scale can now work for a relatively unbounded amount of input data. And they can do so in a unified, standardized way – meaning there’s no need for people to reinvent the wheel every time and freeing them up to go further, faster, with their research.  \u003c/p\u003e\n\n\n\n\u003cp\u003eKlio has been a long time in the making, so \u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\"\u003eit was amazing to finally open-source it\u003c/a\u003e last month. Now, our tools and methodology are available to everyone and will help drive groundbreaking work across the research community worldwide. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e11:30 am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWorking on a project like this has required lots of collaboration, so I’m glad to be part of a strong, supportive team. Even though we’re now spread out geographically, we’re always there for each other on Slack. And we all get together for a Hangout every morning to check in on what everyone’s doing and discuss the most important actions for the day. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e12:00 noon\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eTime to leave my desk and break for lunch – my wife and I try to eat together and grab a bit of fresh air if we can. We’re lucky to have a communal outdoor space at our apartment block and plenty of parks nearby. And one of the great perks of staying in New York throughout the pandemic has been seeing other people out and about – bumping into neighbours and keeping up that sense of connection. It feels extra special right now.\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00 pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWhilst my mornings tend to be fairly unstructured, my afternoons are when most of my regularly scheduled meetings happen, particularly those involving colleagues in the US. But outside of these meetings, my work routine is highly variable – I might spend some focus time on a specific issue, check in with one of my fellow Product Managers, or run a workshop or user research session with one of our current customers. One of the things I love is that, at the moment, our community is small enough for us to know every single customer on a personal level – we can get to know their pain points and problems precisely and really understand the impact of any changes we make. Obviously, I’d love us to grow our customer base and I know it won’t always be possible to be so personally connected. But right now, it feels like we’re doing favours for friends – for extraordinary people that we admire and have the privilege of working with. And that brings a lot of meaning to everything we do. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e6:00 pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eWe’re getting into planning season now – both for a new quarter and a new year – so some nights, I find myself working a bit later than usual on my laptop. Other times, I get an idea in my head and can’t stop till I’ve got it out! But mostly, I log off in the early evening, spend time with my family, walk the dog and then collapse. Like busy parents all over the world, right?\u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png\" alt=\"\" width=\"700\" height=\"685\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-700x685.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-250x245.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-768x752.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2-120x117.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan-2.png 1140w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003e\n\n         Tags: \u003ca href=\"https://engineering.atspotify.com/tag/machine-learning/\" rel=\"tag\"\u003emachine learning\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "5:00 am My days begin with an early morning wake-up call from Zev – he comes through at around 5am and we get a couple of dedicated hours of playtime before the rest of the world gets up. It’s fun – this morning, we baked oatmeal cookies. Then once my wife and I are ready, we take the dog, throw",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2020/11/MyBeat_David-Riordan.png",
      "date_published": "2021-03-30T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/why-you-should-pair-with-non-engineers/",
      "title": "\n                                            Why You Should Pair with Non-Engineers\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 23, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/03/why-you-should-pair-with-non-engineers/\" title=\"Why You Should Pair with Non-Engineers\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eSpotify encourages engineers to become \u003ca href=\"https://jchyip.medium.com/why-t-shaped-people-e8706198e437\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eT-shaped\u003c/a\u003e and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and \u003ca href=\"https://www.youtube.com/watch?v=4jckqGVtyAA\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elearning from peers\u003c/a\u003e outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. \u003c/p\u003e\n\n\n\n\u003cp\u003ePrior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBroaden scope of knowledge (T shape)\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eEngineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.\u003c/p\u003e\n\n\n\n\u003cp\u003eWithin web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.\u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eStress-test documentation\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eI had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had \u003cem\u003enever\u003c/em\u003e been used for writing code for the web.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.\u003c/p\u003e\n\n\n\n\u003cp\u003eStress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBuild empathy between disciplines\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUnderstanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. \u003c/p\u003e\n\n\n\n\u003cp\u003eI once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an \u003ca href=\"https://en.wikipedia.org/wiki/Integrated_development_environment\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eIDE\u003c/a\u003e and also display in \u003ca href=\"http://www.figma.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFigma\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eShared language\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This \u003cem\u003ecan\u003c/em\u003e be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.\u003c/p\u003e\n\n\n\n\u003cp\u003eSomething we use extensively across Spotify is SEMVER (\u003ca href=\"https://semver.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esemantic versioning\u003c/a\u003e), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. \u003c/p\u003e\n\n\n\n\u003cp\u003eDoing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eSummary\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.\u003c/p\u003e\n\n\n\n\u003cp\u003eShare more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn't always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png",
      "date_published": "2021-03-23T00:00:00Z",
      "author": {
        "name": "Published by Tyce Clee"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/why-you-should-pair-with-non-engineers/",
      "title": "\n                                            Why You Should Pair with Non-Engineers\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 23, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/why-you-should-pair-with-non-engineers/\" title=\"Why You Should Pair with Non-Engineers\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eSpotify encourages engineers to become \u003ca href=\"https://jchyip.medium.com/why-t-shaped-people-e8706198e437\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eT-shaped\u003c/a\u003e and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and \u003ca href=\"https://www.youtube.com/watch?v=4jckqGVtyAA\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elearning from peers\u003c/a\u003e outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. \u003c/p\u003e\n\n\n\n\u003cp\u003ePrior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBroaden scope of knowledge (T shape)\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eEngineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.\u003c/p\u003e\n\n\n\n\u003cp\u003eWithin web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.\u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eStress-test documentation\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eI had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had \u003cem\u003enever\u003c/em\u003e been used for writing code for the web.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.\u003c/p\u003e\n\n\n\n\u003cp\u003eStress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBuild empathy between disciplines\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUnderstanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. \u003c/p\u003e\n\n\n\n\u003cp\u003eI once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an \u003ca href=\"https://en.wikipedia.org/wiki/Integrated_development_environment\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eIDE\u003c/a\u003e and also display in \u003ca href=\"http://www.figma.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFigma\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eShared language\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This \u003cem\u003ecan\u003c/em\u003e be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.\u003c/p\u003e\n\n\n\n\u003cp\u003eSomething we use extensively across Spotify is SEMVER (\u003ca href=\"https://semver.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esemantic versioning\u003c/a\u003e), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. \u003c/p\u003e\n\n\n\n\u003cp\u003eDoing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eSummary\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.\u003c/p\u003e\n\n\n\n\u003cp\u003eShare more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn't always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png",
      "date_published": "2021-03-23T00:00:00Z",
      "author": {
        "name": "Published by Tyce Clee"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/",
      "title": "\n                                            Why You Should Pair with Non-Engineers\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 23, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/03/23/why-you-should-pair-with-non-engineers/\" title=\"Why You Should Pair with Non-Engineers\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/03/Pairing_02.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eSpotify encourages engineers to become \u003ca href=\"https://jchyip.medium.com/why-t-shaped-people-e8706198e437\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eT-shaped\u003c/a\u003e and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and \u003ca href=\"https://www.youtube.com/watch?v=4jckqGVtyAA\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elearning from peers\u003c/a\u003e outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. \u003c/p\u003e\n\n\n\n\u003cp\u003ePrior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBroaden scope of knowledge (T shape)\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eEngineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.\u003c/p\u003e\n\n\n\n\u003cp\u003eWithin web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.\u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eStress-test documentation\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eI had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had \u003cem\u003enever\u003c/em\u003e been used for writing code for the web.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.\u003c/p\u003e\n\n\n\n\u003cp\u003eStress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBuild empathy between disciplines\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUnderstanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. \u003c/p\u003e\n\n\n\n\u003cp\u003eI once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an \u003ca href=\"https://en.wikipedia.org/wiki/Integrated_development_environment\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eIDE\u003c/a\u003e and also display in \u003ca href=\"http://www.figma.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFigma\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eShared language\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This \u003cem\u003ecan\u003c/em\u003e be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.\u003c/p\u003e\n\n\n\n\u003cp\u003eSomething we use extensively across Spotify is SEMVER (\u003ca href=\"https://semver.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esemantic versioning\u003c/a\u003e), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. \u003c/p\u003e\n\n\n\n\u003cp\u003eDoing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eSummary\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.\u003c/p\u003e\n\n\n\n\u003cp\u003eShare more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn't always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pairing_02.png",
      "date_published": "2021-03-23T00:00:00Z",
      "author": {
        "name": "Published by Tyce Clee"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/why-you-should-pair-with-non-engineers/",
      "title": "\n                                            Why You Should Pair with Non-Engineers\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 23, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/why-you-should-pair-with-non-engineers/\" title=\"Why You Should Pair with Non-Engineers\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003eSpotify encourages engineers to become \u003ca href=\"https://jchyip.medium.com/why-t-shaped-people-e8706198e437\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eT-shaped\u003c/a\u003e and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn’t always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-creation process and \u003ca href=\"https://www.youtube.com/watch?v=4jckqGVtyAA\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elearning from peers\u003c/a\u003e outside of your discipline. Design Systems Engineering Manager at Spotify Tyce Clee talks about his experiences and the benefits of pairing with teams outside your strict discipline. \u003c/p\u003e\n\n\n\n\u003cp\u003ePrior to becoming an engineering manager, I spent the majority of my career as an engineer, primarily working on web interfaces and applications. The experiences and relationships built during my time as an engineer were essential to a successful transition into management. I’ve had firsthand experience with pairing with many other disciplines to achieve a goal or take a feature to production, and I want to share some of those learnings with you.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBroaden scope of knowledge (T shape)\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eEngineers tend to skew towards a particular focus area within their chosen path, allowing for more advanced skills in some areas, but less in others. A JavaScript engineer who loves to work on data visualization, for example, may work less with GraphQL or Node.JS, or a more UI-focused engineer who lives within the world of CSS may not get a ton of exposure to complex routing or performance-based optimizations.\u003c/p\u003e\n\n\n\n\u003cp\u003eWithin web engineering, we can look to our other developer colleagues and learn from them and their processes and workflows and bring that back to our discipline. This could be done by embedding with a backend team to understand how to map out an API schema for the very first time, build that together, and finally serve it to the front end. Then, returning to your team as a web engineer, you can have a much better understanding of how the schema was made and exactly what’s returned when making requests. Methods such as these can extend your “T-shapedness” by expanding your knowledge in areas that you don’t necessarily focus on in your day to day.\u003c/p\u003e\n\n\n\n\u003cp\u003eAdditionally, when pairing with those outside of our discipline, we get to expand our thinking more laterally beyond just code, understanding more of the why and how behind a product-creation process and not just the final piece of the puzzle.\u003c/p\u003e\n\n\n\n\u003cp\u003eWhen I paired with a UX prototyper, I was able to gain early insights into product inception. This allowed me to get face time with real users through user-testing sessions, to have conversations with product managers on the importance of the new feature or product, and even to pair with designers on early mockups of the UI itself. Then, when the time came to write code, I had a much more well-rounded and cohesive background on the product we were building, and could be more invested in why it’s important for the business.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eStress-test documentation\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eI had a humbling and great learning moment with a designer who was attempting to write code for the first time on their computer. My team had written and rewritten our contribution documentation multiple times in previous weeks, and were confident it was thorough and had accounted for all use cases and disciplines. One thing we forgot to include, however, was the scenario when a computer had \u003cem\u003enever\u003c/em\u003e been used for writing code for the web.\u003c/p\u003e\n\n\n\n\u003cp\u003eThis meant the machine didn’t have Xcode command-line tools, Node.JS, npm, Homebrew, etc. After watching the designer try to figure out why nothing was working, I had to interject and explain what was missing. We then paired on the pull request to update the contribution docs with a new section purely for those who had never run a frontend web environment before.\u003c/p\u003e\n\n\n\n\u003cp\u003eStress-testing your documentation is critical for the success of your product, and we’ve found it best to simply observe when someone is attempting to read through the docs. Try to hold back your thoughts and tips in order to really test what you’ve written down.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eBuild empathy between disciplines\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eUnderstanding the work that your colleagues do is a key piece to building better products together. One way to achieve this relatively quickly is to spend a “day in the life” with someone outside of your discipline. Go to every meeting, ask questions, take notes and, critically, attempt to do a piece of work as they would. A great example would be pairing with a designer to work on a small piece of a project or to spend time with a UX writer to understand the importance of tone of voice and language.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe next step would be to return the favor and encourage your non-engineer teammates to spend time with you. Dedicate the day to it, and treat it like an open conversation with some learning goals to achieve by the end of the day. Building this level of empathy between disciplines can only help with future planning, prioritization of work, and overall understanding of the difficulties faced by all the disciplines required to build digital products. \u003c/p\u003e\n\n\n\n\u003cp\u003eI once spent half a day pairing with a designer to brainstorm ways to better capture key descriptions of each component in our design system, and together we came up with a way to store that data to then use as code hints in an \u003ca href=\"https://en.wikipedia.org/wiki/Integrated_development_environment\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eIDE\u003c/a\u003e and also display in \u003ca href=\"http://www.figma.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFigma\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou might be thinking “I don’t have time for this,” or “I can’t justify prioritizing this over other things in my sprint,” but I would argue that spending a “day in the life” with someone else will forever affect the way you interact with that person, discipline, or product. Diversity of thought and background is key to building the best products imaginable, and by sharing your day with someone else you will exponentially increase your ability to build better products that will ultimately impact a broader group of people due to that expanded way of thinking.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eShared language\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany workplaces these days have their own unique acronyms, slang, and more to help make sense of the slew of historical information a company has. This \u003cem\u003ecan\u003c/em\u003e be helpful, but only when you’re aware of what those acronyms mean and why they’re important. It’s vital to help all new starters or internal transfers understand these terms and to explain them in a manner that makes sense to those outside of your team and/or discipline.\u003c/p\u003e\n\n\n\n\u003cp\u003eSomething we use extensively across Spotify is SEMVER (\u003ca href=\"https://semver.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esemantic versioning\u003c/a\u003e), and this technique for releasing software doesn’t always translate 1:1 to other disciplines without a little bit of explanation. I remember multiple times where my team took the time to walk through the fundamentals of this strategy with non-engineers to help them better understand the terminology and intent. \u003c/p\u003e\n\n\n\n\u003cp\u003eDoing this helped create a bridge of understanding between our disciplines, and such collaboration might even assist those outside of engineering with understanding release schedules and how they can play a key part in releasing software. Conversely, a designer explaining how a design critique works, the names given to various flows within their design tool, and even the difference between vector- and pixel-based image creation, can go a long way to helping an engineer better understand and relate to design.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eSummary\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe made it to the end! So what did we learn? It’s always important to stress-test your onboarding documentation and procedures, and the best way to do that is with someone that’s never done it before. Don’t be afraid of this; embrace the awkward moment your lack of documentation leads to a brick wall for the person onboarding. Make a note and fix it before the next person stumbles into the same problem.\u003c/p\u003e\n\n\n\n\u003cp\u003eShare more between disciplines, and encourage each other to translate words and phrases that  may otherwise be confusing and isolating. Consider being T-shaped in more unorthodox ways —take up a design course, learn more about UX writing, study how accessibility in the browser works. I’ve personally spent time on all of these things, and see myself as having broader knowledge in areas I would’ve otherwise overlooked in favor of focusing on purely engineering-based areas.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR Spotify encourages engineers to become T-shaped and lean into technologies and skill sets outside of their core specialization. Being a T-shaped developer doesn't always mean having to learn more code, additional languages, or frameworks. It can be about broadening your outlook on the product-",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Pairing_02.png",
      "date_published": "2021-03-23T00:00:00Z",
      "author": {
        "name": "Published by Tyce Clee"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/",
      "title": "\n                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 16, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/03/16/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" title=\"Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Backstage-BDay-Blog_v002.gif\" alt=\"\" loading=\"lazy\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/03/Backstage-BDay-Blog_v002.gif\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR:\u003c/strong\u003e As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. \u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom Hack Week hunch to CNCF Sandbox\u003c/h2\u003e\n\n\n\n\u003cp\u003eLast year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? \u003c/p\u003e\n\n\n\n\u003cp\u003eThe team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later \u003ca href=\"https://backstage.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e was out in the wild — making its official open source debut \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eone year ago today\u003c/a\u003e. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at \u003ca href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe CNCF\u003c/a\u003e (also home to Kubernetes, Envoy, and Helm).\u003c/p\u003e\n\n\n\n\u003cp\u003eLooking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"1999\" height=\"1016\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2.png 1999w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-250x127.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-700x356.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-768x390.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-1536x781.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-120x61.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e\u003cfigcaption\u003e\u003cem\u003e“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” \u003c/em\u003e\u003cbr/\u003e\u003cem\u003e— Expedia Group Developer Experience Team\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eCompanies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. \u003c/p\u003e\n\n\n\n\u003ch2\u003eWhy a developer portal?\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard \u003ca href=\"https://backstage.io/docs/overview/background\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethis story\u003c/a\u003e before, feel free to skip ahead.) \u003c/p\u003e\n\n\n\n\u003cp\u003eIn March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. \u003c/p\u003e\n\n\n\n\u003cp\u003eOur small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe faster we grew, the more this fragmentation slowed us down again. \u003c/p\u003e\n\n\n\n\u003ch2\u003eA single pane of glass\u003c/h2\u003e\n\n\n\n\u003cp\u003eDesigned first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. \u003c/p\u003e\n\n\n\n\u003cp\u003eSpeed was the key. We saw firsthand that faster developers aren’t just \u003ca href=\"https://martinfowler.com/articles/developer-effectiveness.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emore productive developers\u003c/a\u003e, they’re happier developers.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom internal portal to open platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhat’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike \u003ca href=\"https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe internal version of Backstage\u003c/a\u003e, which has more than 120 different \u003ca href=\"https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplugins\u003c/a\u003e built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. \u003c/p\u003e\n\n\n\n\u003cp\u003eSince that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe introduced four core features: the \u003ca href=\"https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eService Catalog\u003c/a\u003e, \u003ca href=\"https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSoftware Templates\u003c/a\u003e, \u003ca href=\"https://backstage.io/blog/2020/09/08/announcing-tech-docs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eTechDocs\u003c/a\u003e, and our \u003ca href=\"https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew Kubernetes monitoring tool\u003c/a\u003e. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe launched the \u003ca href=\"https://backstage.io/plugins\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePlugin Marketplace\u003c/a\u003e: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like \u003ca href=\"https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSnyk\u003c/a\u003e. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe created the \u003ca href=\"https://backstage.io/blog/2020/09/30/backstage-design-system\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage Design System\u003c/a\u003e: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eStabilizing the core\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: \u003ca href=\"https://backstage.io/blog/2020/12/22/stability-index\" target=\"_blank\" rel=\"noreferrer noopener\"\u003estabilizing those features and APIs\u003c/a\u003e so that more companies can adopt the platform for production use. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn the coming weeks, our team will:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eBring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eCreate an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eUpdate other parts of the core app — notably, improving search and incorporating GraphQL systemwide.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eYou can learn more in \u003ca href=\"https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe project roadmap\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAdopters: Backstage in the wild!\u003c/h2\u003e\n\n\n\n\u003cp\u003eBeyond the \u003ca href=\"https://github.com/backstage/backstage/blob/master/ADOPTERS.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eofficial adopters list\u003c/a\u003e, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eEarly adopters Zalando and SDA SE shared \u003ca href=\"https://youtu.be/4-VX9tDdJYY?t=1756\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etheir adoption experiences\u003c/a\u003e last month at our first community session. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eExpedia has a team dedicated to rolling out Backstage.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003eAmerican Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:\u003cp\u003e“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eDoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. \u003cp\u003e“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eA world of contributors grows into a community\u003c/h2\u003e\n\n\n\n\u003cp\u003eOf course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged \u003ca href=\"https://twitter.com/SpotifyEng/status/1341376341636239364\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etwo new contributors a week\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the \u003ca href=\"https://github.com/backstage/community\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage Community\u003c/a\u003e page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in \u003ca href=\"https://github.com/backstage/backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe main repo\u003c/a\u003e.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Backstage-BDay-Blog_v002.gif",
      "date_published": "2021-03-16T00:00:00Z",
      "author": {
        "name": "Published by Tyson Singer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/",
      "title": "\n                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 16, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/03/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" title=\"Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR:\u003c/strong\u003e As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. \u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom Hack Week hunch to CNCF Sandbox\u003c/h2\u003e\n\n\n\n\u003cp\u003eLast year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? \u003c/p\u003e\n\n\n\n\u003cp\u003eThe team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later \u003ca href=\"https://backstage.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e was out in the wild — making its official open source debut \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eone year ago today\u003c/a\u003e. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at \u003ca href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe CNCF\u003c/a\u003e (also home to Kubernetes, Envoy, and Helm).\u003c/p\u003e\n\n\n\n\u003cp\u003eLooking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"1999\" height=\"1016\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-700x356.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-1536x781.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-120x61.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e\u003cfigcaption\u003e\u003cem\u003e“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” \u003c/em\u003e\u003cbr/\u003e\u003cem\u003e— Expedia Group Developer Experience Team\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eCompanies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. \u003c/p\u003e\n\n\n\n\u003ch2\u003eWhy a developer portal?\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard \u003ca href=\"https://backstage.io/docs/overview/background\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethis story\u003c/a\u003e before, feel free to skip ahead.) \u003c/p\u003e\n\n\n\n\u003cp\u003eIn March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. \u003c/p\u003e\n\n\n\n\u003cp\u003eOur small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe faster we grew, the more this fragmentation slowed us down again. \u003c/p\u003e\n\n\n\n\u003ch2\u003eA single pane of glass\u003c/h2\u003e\n\n\n\n\u003cp\u003eDesigned first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. \u003c/p\u003e\n\n\n\n\u003cp\u003eSpeed was the key. We saw firsthand that faster developers aren’t just \u003ca href=\"https://martinfowler.com/articles/developer-effectiveness.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emore productive developers\u003c/a\u003e, they’re happier developers.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom internal portal to open platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhat’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike \u003ca href=\"https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe internal version of Backstage\u003c/a\u003e, which has more than 120 different \u003ca href=\"https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplugins\u003c/a\u003e built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. \u003c/p\u003e\n\n\n\n\u003cp\u003eSince that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe introduced four core features: the \u003ca href=\"https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eService Catalog\u003c/a\u003e, \u003ca href=\"https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSoftware Templates\u003c/a\u003e, \u003ca href=\"https://backstage.io/blog/2020/09/08/announcing-tech-docs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eTechDocs\u003c/a\u003e, and our \u003ca href=\"https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew Kubernetes monitoring tool\u003c/a\u003e. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe launched the \u003ca href=\"https://backstage.io/plugins\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePlugin Marketplace\u003c/a\u003e: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like \u003ca href=\"https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSnyk\u003c/a\u003e. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe created the \u003ca href=\"https://backstage.io/blog/2020/09/30/backstage-design-system\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage Design System\u003c/a\u003e: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eStabilizing the core\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: \u003ca href=\"https://backstage.io/blog/2020/12/22/stability-index\" target=\"_blank\" rel=\"noreferrer noopener\"\u003estabilizing those features and APIs\u003c/a\u003e so that more companies can adopt the platform for production use. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn the coming weeks, our team will:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eBring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eCreate an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eUpdate other parts of the core app — notably, improving search and incorporating GraphQL systemwide.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eYou can learn more in \u003ca href=\"https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe project roadmap\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAdopters: Backstage in the wild!\u003c/h2\u003e\n\n\n\n\u003cp\u003eBeyond the \u003ca href=\"https://github.com/backstage/backstage/blob/master/ADOPTERS.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eofficial adopters list\u003c/a\u003e, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eEarly adopters Zalando and SDA SE shared \u003ca href=\"https://youtu.be/4-VX9tDdJYY?t=1756\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etheir adoption experiences\u003c/a\u003e last month at our first community session. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eExpedia has a team dedicated to rolling out Backstage.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003eAmerican Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:\u003cp\u003e“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eDoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. \u003cp\u003e“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eA world of contributors grows into a community\u003c/h2\u003e\n\n\n\n\u003cp\u003eOf course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged \u003ca href=\"https://twitter.com/SpotifyEng/status/1341376341636239364\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etwo new contributors a week\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the \u003ca href=\"https://github.com/backstage/community\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage Community\u003c/a\u003e page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in \u003ca href=\"https://github.com/backstage/backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe main repo\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif",
      "date_published": "2021-03-16T00:00:00Z",
      "author": {
        "name": "Published by Tyson Singer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/",
      "title": "\n                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 16, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" title=\"Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR:\u003c/strong\u003e As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. \u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom Hack Week hunch to CNCF Sandbox\u003c/h2\u003e\n\n\n\n\u003cp\u003eLast year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? \u003c/p\u003e\n\n\n\n\u003cp\u003eThe team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later \u003ca href=\"https://backstage.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e was out in the wild — making its official open source debut \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eone year ago today\u003c/a\u003e. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at \u003ca href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe CNCF\u003c/a\u003e (also home to Kubernetes, Envoy, and Helm).\u003c/p\u003e\n\n\n\n\u003cp\u003eLooking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"1999\" height=\"1016\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-700x356.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-1536x781.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-120x61.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e\u003cfigcaption\u003e\u003cem\u003e“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” \u003c/em\u003e\u003cbr/\u003e\u003cem\u003e— Expedia Group Developer Experience Team\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eCompanies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. \u003c/p\u003e\n\n\n\n\u003ch2\u003eWhy a developer portal?\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard \u003ca href=\"https://backstage.io/docs/overview/background\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethis story\u003c/a\u003e before, feel free to skip ahead.) \u003c/p\u003e\n\n\n\n\u003cp\u003eIn March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. \u003c/p\u003e\n\n\n\n\u003cp\u003eOur small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe faster we grew, the more this fragmentation slowed us down again. \u003c/p\u003e\n\n\n\n\u003ch2\u003eA single pane of glass\u003c/h2\u003e\n\n\n\n\u003cp\u003eDesigned first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. \u003c/p\u003e\n\n\n\n\u003cp\u003eSpeed was the key. We saw firsthand that faster developers aren’t just \u003ca href=\"https://martinfowler.com/articles/developer-effectiveness.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emore productive developers\u003c/a\u003e, they’re happier developers.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom internal portal to open platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhat’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike \u003ca href=\"https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe internal version of Backstage\u003c/a\u003e, which has more than 120 different \u003ca href=\"https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplugins\u003c/a\u003e built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. \u003c/p\u003e\n\n\n\n\u003cp\u003eSince that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe introduced four core features: the \u003ca href=\"https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eService Catalog\u003c/a\u003e, \u003ca href=\"https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSoftware Templates\u003c/a\u003e, \u003ca href=\"https://backstage.io/blog/2020/09/08/announcing-tech-docs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eTechDocs\u003c/a\u003e, and our \u003ca href=\"https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew Kubernetes monitoring tool\u003c/a\u003e. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe launched the \u003ca href=\"https://backstage.io/plugins\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePlugin Marketplace\u003c/a\u003e: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like \u003ca href=\"https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSnyk\u003c/a\u003e. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe created the \u003ca href=\"https://backstage.io/blog/2020/09/30/backstage-design-system\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage Design System\u003c/a\u003e: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eStabilizing the core\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: \u003ca href=\"https://backstage.io/blog/2020/12/22/stability-index\" target=\"_blank\" rel=\"noreferrer noopener\"\u003estabilizing those features and APIs\u003c/a\u003e so that more companies can adopt the platform for production use. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn the coming weeks, our team will:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eBring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eCreate an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eUpdate other parts of the core app — notably, improving search and incorporating GraphQL systemwide.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eYou can learn more in \u003ca href=\"https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe project roadmap\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAdopters: Backstage in the wild!\u003c/h2\u003e\n\n\n\n\u003cp\u003eBeyond the \u003ca href=\"https://github.com/backstage/backstage/blob/master/ADOPTERS.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eofficial adopters list\u003c/a\u003e, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eEarly adopters Zalando and SDA SE shared \u003ca href=\"https://youtu.be/4-VX9tDdJYY?t=1756\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etheir adoption experiences\u003c/a\u003e last month at our first community session. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eExpedia has a team dedicated to rolling out Backstage.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003eAmerican Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:\u003cp\u003e“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eDoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. \u003cp\u003e“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eA world of contributors grows into a community\u003c/h2\u003e\n\n\n\n\u003cp\u003eOf course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged \u003ca href=\"https://twitter.com/SpotifyEng/status/1341376341636239364\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etwo new contributors a week\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the \u003ca href=\"https://github.com/backstage/community\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage Community\u003c/a\u003e page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in \u003ca href=\"https://github.com/backstage/backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe main repo\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif",
      "date_published": "2021-03-16T00:00:00Z",
      "author": {
        "name": "Published by Tyson Singer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/",
      "title": "\n                                            Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 16, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/happy-birthday-backstage-spotifys-biggest-open-source-project-grows-up-fast/\" title=\"Happy Birthday, Backstage: Spotify’s Biggest Open Source Project Grows Up Fast\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif\" alt=\"\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR:\u003c/strong\u003e As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with. \u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom Hack Week hunch to CNCF Sandbox\u003c/h2\u003e\n\n\n\n\u003cp\u003eLast year, a small team of Spotifiers had a hunch about our homegrown developer portal: if Backstage could help our 1,600+ engineers manage the 14,000+ software components we use at Spotify, then couldn’t it do the same for other growing tech companies, too? \u003c/p\u003e\n\n\n\n\u003cp\u003eThe team began building a proof of concept for an external version of Backstage during Hack Week. Just six weeks later \u003ca href=\"https://backstage.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e was out in the wild — making its official open source debut \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eone year ago today\u003c/a\u003e. A few months and a few thousand pull requests later, what started as a hunch became an early stage Sandbox project at \u003ca href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe CNCF\u003c/a\u003e (also home to Kubernetes, Envoy, and Helm).\u003c/p\u003e\n\n\n\n\u003cp\u003eLooking back, the Backstage open source project feels like it has come incredibly far in a short amount of time. But on its first anniversary — as we prepare Backstage for a more stable release and wider adoption — we’re even more excited for what lies ahead. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"1999\" height=\"1016\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2.png 1999w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-250x127.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-700x356.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-768x390.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-1536x781.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image2-120x61.png 120w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"/\u003e\u003cfigcaption\u003e\u003cem\u003e“We recognized the need to drive increased productivity and collaboration for our developer community. We could only accomplish this by removing friction along the developer journey and by prioritizing pain points that got in the way of our developers. Building a unified developer front door for all things developers need was critical to us. Backstage provided the foundation that allowed us to accelerate on this promise.” \u003c/em\u003e\u003cbr/\u003e\u003cem\u003e— Expedia Group Developer Experience Team\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eCompanies as varied as Expedia, Zalando, TELUS, American Airlines, and DoorDash have already started using Backstage. And we remain committed to our long-term vision of seeing Backstage become the standard for all kinds of companies. We think the past year has given us a good head start. \u003c/p\u003e\n\n\n\n\u003ch2\u003eWhy a developer portal?\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo understand the problems Backstage solves, let’s go back to its beginnings at Spotify — and why we built it in the first place. (If you’ve heard \u003ca href=\"https://backstage.io/docs/overview/background\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethis story\u003c/a\u003e before, feel free to skip ahead.) \u003c/p\u003e\n\n\n\n\u003cp\u003eIn March 2020, our internal version of Backstage was already a mature product; our developers had started using a primitive version of it four years earlier. During that period, we were growing fast. We seemed to be adding new developers, new software components, and new tooling at an equally breakneck pace. \u003c/p\u003e\n\n\n\n\u003cp\u003eOur small, autonomous developer teams have always been our strength. But as we scaled, we didn’t have one way to create a microservice, we had a dozen. We didn’t have one new developer trying to find their way around our stack, we had hundreds. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe faster we grew, the more this fragmentation slowed us down again. \u003c/p\u003e\n\n\n\n\u003ch2\u003eA single pane of glass\u003c/h2\u003e\n\n\n\n\u003cp\u003eDesigned first as a basic service catalog, our engineering teams began to gravitate to Backstage on their own — recognizing its ability to streamline workflows, help them align with work being done across the organization, and reduce the daily frustrations that slow developers down.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt became the “single pane of glass” for all our tooling. Everything our developers needed to create, manage, and monitor their projects was in one place. We began to rely on Backstage more and more — from managing data pipelines to software migrations — until it became the hub for all our development work.\u003c/p\u003e\n\n\n\n\u003cp\u003eWith Backstage, infrastructure tooling got out of our engineers’ way so they could build and test faster. And since it simplified discovery — from ownership and documentation to best practices — we could onboard new developers faster, too. \u003c/p\u003e\n\n\n\n\u003cp\u003eSpeed was the key. We saw firsthand that faster developers aren’t just \u003ca href=\"https://martinfowler.com/articles/developer-effectiveness.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emore productive developers\u003c/a\u003e, they’re happier developers.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom internal portal to open platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhat’s the biggest difference between the internal version of Backstage and the version we released a year ago? We didn’t want to ship you Spotify’s developer portal. We wanted to ship the best platform for you to build your own developer portal — one that fits your particular needs and use cases.\u003c/p\u003e\n\n\n\n\u003cp\u003eUnlike \u003ca href=\"https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe internal version of Backstage\u003c/a\u003e, which has more than 120 different \u003ca href=\"https://backstage.io/docs/FAQ#what-is-a-plugin-in-backstage\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplugins\u003c/a\u003e built by 60 different teams, the first open source version was mostly an empty shell. Shiny, new, and full of potential — yes. But less like a brand new car and more like a blank canvas. \u003c/p\u003e\n\n\n\n\u003cp\u003eSince that first day, the promise of that empty shell has been filled in and shaped into a full-featured product, thanks to feedback from early adopters and contributions from the open source community. In the last year:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe introduced four core features: the \u003ca href=\"https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eService Catalog\u003c/a\u003e, \u003ca href=\"https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSoftware Templates\u003c/a\u003e, \u003ca href=\"https://backstage.io/blog/2020/09/08/announcing-tech-docs\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eTechDocs\u003c/a\u003e, and our \u003ca href=\"https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003enew Kubernetes monitoring tool\u003c/a\u003e. This is functionality that we think defines the Backstage experience and that everyone would want out of the box.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe launched the \u003ca href=\"https://backstage.io/plugins\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePlugin Marketplace\u003c/a\u003e: the ecosystem for open source plugins for Backstage continues to grow, including contributions from individuals, from other tech companies, and software providers, like \u003ca href=\"https://github.com/snyk-tech-services/backstage-plugin-snyk/blob/main/README.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSnyk\u003c/a\u003e. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe created the \u003ca href=\"https://backstage.io/blog/2020/09/30/backstage-design-system\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage Design System\u003c/a\u003e: consistent frontend design is integral to creating a seamless experience inside Backstage, so we developed tools and guidelines anyone can use, including non-designers.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eStabilizing the core\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe work we did last year — identifying the core features and iterating on them quickly — has prepared us for what’s next: \u003ca href=\"https://backstage.io/blog/2020/12/22/stability-index\" target=\"_blank\" rel=\"noreferrer noopener\"\u003estabilizing those features and APIs\u003c/a\u003e so that more companies can adopt the platform for production use. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn the coming weeks, our team will:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eBring both the Service Catalog and the Software Templates scaffolder into beta, resulting in a more stable release ready for wider adoption.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eCreate an easy, standardized way for developers to build plugins that will encourage contributions and lead to a richer ecosystem for everyone.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eUpdate other parts of the core app — notably, improving search and incorporating GraphQL systemwide.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eYou can learn more in \u003ca href=\"https://github.com/backstage/backstage/blob/master/docs/overview/roadmap.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe project roadmap\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAdopters: Backstage in the wild!\u003c/h2\u003e\n\n\n\n\u003cp\u003eBeyond the \u003ca href=\"https://github.com/backstage/backstage/blob/master/ADOPTERS.md\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eofficial adopters list\u003c/a\u003e, we’ve consulted with hundreds of other companies evaluating Backstage — from digital natives to Fortune 50’s undergoing digital transformations. Our rule of thumb has been that once your org reaches 100 engineers, it’s time to stop managing your infrastructure solely with spreadsheets and Slack channels.\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eEarly adopters Zalando and SDA SE shared \u003ca href=\"https://youtu.be/4-VX9tDdJYY?t=1756\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etheir adoption experiences\u003c/a\u003e last month at our first community session. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eExpedia has a team dedicated to rolling out Backstage.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003eAmerican Airlines has 20 teams using their version of Backstage, which they named Runway. They’re already seeing some good internal traction:\u003cp\u003e“We now get upwards of 500+ hits a day from people using not only “Create an App” but also consuming other components in Runway, like Catalog, and our custom plugins. Just a few months ago, this was maybe 50/day.” — Jason Walker, Director, Technology Transformation, American Airlines\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eDoorDash is one of our most recent adopters and we’ve been working closely to get them up and running. \u003cp\u003e“The support we received from the Spotify team, GitHub collaborators, and Discord members enabled us to stand up our initial environment quickly and painlessly, while also inspiring a robust roadmap that will make Backstage our engineering hub.” — Adam Rogal, Director, Developer Platform, DoorDash\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eA world of contributors grows into a community\u003c/h2\u003e\n\n\n\n\u003cp\u003eOf course, none of this would have been possible without our ever-growing community of contributors from around the world. Since the project’s beginning, the project has averaged \u003ca href=\"https://twitter.com/SpotifyEng/status/1341376341636239364\" target=\"_blank\" rel=\"noreferrer noopener\"\u003etwo new contributors a week\u003c/a\u003e. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis year, we’ve given the global community of maintainers, contributors, adopters, and an official home on the \u003ca href=\"https://github.com/backstage/community\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage Community\u003c/a\u003e page. As our excitement for Backstage open source continues to grow at Spotify, we hope you will join us there — and in \u003ca href=\"https://github.com/backstage/backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ethe main repo\u003c/a\u003e.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/engineering-leadership/\" rel=\"tag\"\u003eengineering leadership\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR: As Backstage turns one, we’re doubling down on our commitment to the open source project and the community we’re building it with.",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Backstage-BDay-Blog_v002.gif",
      "date_published": "2021-03-16T00:00:00Z",
      "author": {
        "name": "Published by Tyson Singer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/",
      "title": "\n                                            Spotify’s New Experimentation Coordination Strategy\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 10, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/\" title=\"Spotify’s New Experimentation Coordination Strategy\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/03/Bucket-Reuse_01-1.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time. These are well-known problems among tech companies — take, for example, Google’s solution in Tang et al (2010). But the statistical implications of different solutions have not been properly investigated. In a recent paper (\u003ca rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/2012.10202\" target=\"_blank\"\u003eM. Schultzberg, O. Kjellin, and J. Rydberg, 2020\u003c/a\u003e), we investigate important statistical properties of a common technical solution to the coordination — called “Bucket Reuse”. In this blog post we highlight some interesting results and present some details about how Spotify will coordinate experiments from now on.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat is Bucket Reuse?\u003c/h2\u003e\n\n\n\n\u003cp\u003eBucket Reuse is a simple idea utilizing the power of hashing. Essentially the steps are as follows: decide on a number of buckets (B). Take the unique user ID and hash it together with a  random salt into B “buckets” such that all users hash into one and only one bucket. Once the hash map is established, all sampling is performed on the bucket level. This implies that a bucket either is or is not in a sample at any given time point. If we want to sample N number of users, we sample the number of buckets that contain the number of users closest to the desired number N. If, e.g., the desired N is 20 and each bucket contains 3 users, we would sample 7 buckets and end up with 21 users. Note that a bucket is simply a logical group of units to which we assign a certain user by a fixed hash map. Figure 1 illustrates such a map. The second part of the name Bucket \u003cem\u003eReuse\u003c/em\u003e comes from the fact that we reuse the same buckets over and over again in the sampling for all experiments. That is, the random salt for the hashing is selected only once; after that, the hash map and the number of buckets is fixed. \u003cstrong\u003eImportantly,\u003c/strong\u003e when we talk about Bucket Reuse for experimentation, we always mean the following: the random sampling is performed on the bucket level; the random treatment allocation is performed at the user level on the users in the sample. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"625\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x625.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x625.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-250x223.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-768x686.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-120x107.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4.png 1366w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: \u003cem\u003eSchematic illustration of a hash map. A user ID is hashed together with a random salt to map each user to a unique bucket. \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eWhat is experiment coordination?\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo get into the interesting parts of experiment coordination, we need to establish some key concepts. Figure 2 illustrates the concepts of exclusive and nonexclusive experiments. That two or more experiments “are exclusive” to each other simply means that they are run on distinct sets of users. Experiments that are nonexclusive are nonexclusive to \u003cem\u003eall\u003c/em\u003e experiments at Spotify, meaning that they all randomly overlap in terms of users.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"383\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-700x383.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-700x383.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-250x137.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-768x420.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5-120x66.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image5.png 1498w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 2: \u003cem\u003eIllustration of exclusive and nonexclusive experiments. Exclusive experiments never overlap with each other in terms of users; nonexclusive experiments randomly overlap with exclusive experiments and other nonexclusive experiments. Note that the allocations in this figure were selected for illustration; in a true random sample we would expect exclusive experiments to also be spread out uniformly.  \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe most critical challenge from a statistical perspective is imposed by what we call \u003cem\u003eprograms of exclusive experiments\u003c/em\u003e. A program of exclusive experiments is a set of experiments run over time where all simultaneous experiments are exclusive to each other. That is, a unit is in at most one, and only one, of the experiments in the program at any given time point. At Spotify we have such programs for several surfaces in the app, for example Search, the Home screen, and certain parts of the backend code base. To better understand the limitations imposed on the sampling by running programs of exclusive experiments, it is helpful to introduce the concepts of paths. A path is simply a sequence of experiments that a unit can be in. Figure 3 illustrates a program of exclusive experiments containing 5 experiments over time. Below the experiments, their possible paths are displayed. For example, it is not possible to go through both Experiments 3 and 4 as they overlap in time and are exclusive, and must therefore be run on distinct users. The number of unique possible paths explodes combinatorially after a relatively short time period in most programs, and only a small partition of the possible paths can be taken by any unit regardless of the sample strategies discussed in this post.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"806\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-700x806.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-700x806.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-250x288.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-768x885.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-1334x1536.png 1334w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3-120x138.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Figure3.png 1370w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 3: \u003cem\u003ePaths of experiences possible during a hypothetical program of 5 exclusive experiments.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eHighlights from \u003ca rel=\"noreferrer noopener\" href=\"https://arxiv.org/abs/2012.10202\" target=\"_blank\"\u003eSchultzberg et al.\u003c/a\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSchultzberg et al. presents two key results:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eUnder unrestricted sampling of buckets, i.e., Bucket Reuse in nonexclusive experiments, the properties of the difference-in-means estimator of the average treatment effect is approximately equivalent to the properties under random sampling of units. In other words, standard t-tests can be used for inference. The key to this remarkable finding is connecting Bucket Reuse to the existing literature on randomized experiments embedded in complex sampling designs (Horvitz and Thompson, 1952;  van den Brakel and Renssen, 2005). \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003eThe bias imposed by the restricted sampling of buckets implied by programs of exclusive experiments is derived. It is shown that this bias is often restricted to the history right before the experiment. Moreover, the length of the window of the history that affects the bias can be estimated for any empirical experimentation program. One way to phrase this finding is that the sample at a time point T is not random with respect to the last D days leading up to time T, but random with respect to everything that happened before the time point T-D. If things happened during the last D days that make the set of buckets available for sampling at time T different from the population with respect to the treatment effect, the estimator is biased. This insight makes it possible for experimenters to evaluate the risk for biases by checking what experiments have been run in the program over the last D days, and if those risks for biases are likely to affect the average treatment effect in the experiment that is about to be started. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eSpotify’s new experimentation coordination strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have migrated our experimentation platform to using Bucket Reuse for all experiments at Spotify. There are a few key reasons why we prefer Bucket Reuse over other solutions:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eIt is simple to implement and understand. \u003c/li\u003e\u003cli\u003eIt is a technically feasible solution that allows us to do complex coordination without losing speed in our systems. As new users come into Spotify, they are uniformly hashed into the existing buckets — that is, the system scales as Spotify’s user base grows.\u003c/li\u003e\u003cli\u003eUsing one company-global bucket structure makes it easy to coordinate experiments arbitrarily. For example, two programs that have been run independently can easily be merged into one program of exclusive experiments at any time point for any period of time. And, a sample from a previous experiment with a broken experience can easily be quarantined and avoided in any future experiments, as the sampling units are always the same over time.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eAt Spotify we have chosen Bucket Reuse with 1M (1,000,000) buckets to coordinate all experiments. That is, all users are hashed into 1M buckets, and these buckets are used for all experiments in all experimentation programs, exclusive and nonexclusive. Although Schultzberg et al.  establishes that smaller numbers of buckets can have statistical properties enabling straight forward inference, it should be clear that the larger the number of buckets, the better. Even though the inference for the average treatment effect is unaffected by the bucket sampling, it is well known that the effect of cluster sampling on other estimands decreases when the number of buckets increases (Kumar Pradhan). That is, imposing a bucket structure is not preferred from a statistical perspective, but it is a technical necessity. The choice of 1M buckets was made because it is close to the largest number of buckets we can have while still keeping the selected buckets within an executable script stored in a database without having to resort to \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Binary_large_object\" target=\"_blank\"\u003eBLOB\u003c/a\u003e storage. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe are not planning to have any full stops or to reshuffle users into new buckets. However, there are naturally occurring periods of low experimentation that will help decrease the dependency between the samples in programs of exclusive experiments. For example, many programs run fewer general product experiments over the winter holidays due to unusual listening behaviors. These natural pauses effectively reset the programs in terms of dependencies.  \u003c/p\u003e\n\n\n\n\u003cp\u003eTo help experimenters running programs of exclusive experiments, we are implementing a few tools to keep track of the short-term dependencies. For each program, we will estimate the length of the history that can bias the results. Moreover, we are also implementing a tool to see the history of the available buckets at any time point. Figure 4 displays a prototype of this tool. It allows experimenters to evaluate if the experiments that the available user came out of are likely to bias the estimator in their experiment. It also provides information about the size of the overlap with previous experiments. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"1600\" height=\"1394\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-10-23-10.58.56.gif\" alt=\"\"/\u003e\u003cfigcaption\u003eFigure 4: \u003cem\u003eDependency graph that shows the experimenter where the free space, and thereby their sample, will come from in terms of previous experiments in the exclusive program. Each rectangle corresponds to a previously run experiment. The numbers in the yellow circles indicate the percentage points of the population that went from one experiment into another, and finally into the proportion of the population that is available (“free space”) for sampling right now. The experimenter can see what effects the previous experiments had on the metrics of interest in their experiment. \u003c/em\u003e\u003cbr/\u003e\u003cem\u003eFigure above for illustrative purposes only.\u003c/em\u003e\u003cbr/\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\n\n\n\n\u003cp\u003eStatistical validity, derived from proper random sampling and random treatment allocation, is the cornerstone of a successful experimentation program. However, implementing systems that can serve hundreds of experiments to hundreds of millions users — while retaining the ability to conveniently coordinate experiments to be exclusive, without overlap — requires compromises between technical feasibility and statistical properties. At Spotify, we have migrated the internal experimentation platform to rely fully on \u003cem\u003ebucket reuse\u003c/em\u003e, a technically desirable solution that provides speed, simplicity, and flexibility. In this post we establish the statistical properties under bucket reuse and conclude that the validity is unaffected. This migration enables more experiments of higher quality at Spotify.  \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eExperimentation Platform team, Spotify\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReferences\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBrakel, Jan van den, and Robbert H. Renssen. “Analysis of Experiments Embedded in Complex Sampling Designs.” \u003cem\u003eSurvey Methodology \u003c/em\u003e 31, no. 1 (2005): 23–40.\u003c/p\u003e\n\n\n\n\u003cp\u003eHorvitz, D. G. and D. J. Thompson. “A Generalization of Sampling Without Replacement from a Finite Universe.” \u003cem\u003eJournal of the American Statistical Association\u003c/em\u003e 47, no. 260 (1952): 663–685. https://doi.org/10.2307/2280784\u003c/p\u003e\n\n\n\n\u003cp\u003eKumar Pradhan, Bijoy. “On efficiency of cluster sampling on sampling on two occasions.” \u003cem\u003eStatistica\u003c/em\u003e 64, no. 1 (2004): 183–191. \u003ca href=\"https://doi.org/10.6092/issn.1973-2201/31\"\u003ehttps://doi.org/10.6092/issn.1973-2201/31\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eSchultzberg, Mårten, Oskar Kjellin, and Johan Rydberg. “Statistical Properties of Exclusive and Non-exclusive Online Randomized Experiments using Bucket Reuse.” \u003cem\u003earXiv preprint arXiv:2012.10202 \u003c/em\u003e(2020).\u003c/p\u003e\n\n\n\n\u003cp\u003eTang, Diane, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. “Overlapping experiment infrastructure: more, better, faster experimentation.” \u003cem\u003eKDD\u003c/em\u003e \u003cem\u003e’10: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining\u003c/em\u003e, (July 2010): 17–26.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Bucket-Reuse_01-1.png",
      "date_published": "2021-03-10T00:00:00Z",
      "author": {
        "name": "Published by Mårten Schultzberg, Oskar Kjellin, and Johan Rydberg"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/spotifys-new-experimentation-coordination-strategy/",
      "title": "\n                                            Spotify’s New Experimentation Coordination Strategy\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 10, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/spotifys-new-experimentation-coordination-strategy/\" title=\"Spotify’s New Experimentation Coordination Strategy\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time. These are well-known problems among tech companies — take, for example, Google’s solution in Tang et al (2010). But the statistical implications of different solutions have not been properly investigated. In a recent paper (\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-030-89906-6_50\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eM. Schultzberg, O. Kjellin, and J. Rydberg, 2020\u003c/a\u003e), we investigate important statistical properties of a common technical solution to the coordination — called “Bucket Reuse”. In this blog post we highlight some interesting results and present some details about how Spotify will coordinate experiments from now on.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat is Bucket Reuse?\u003c/h2\u003e\n\n\n\n\u003cp\u003eBucket Reuse is a simple idea utilizing the power of hashing. Essentially the steps are as follows: decide on a number of buckets (B). Take the unique user ID and hash it together with a  random salt into B “buckets” such that all users hash into one and only one bucket. Once the hash map is established, all sampling is performed on the bucket level. This implies that a bucket either is or is not in a sample at any given time point. If we want to sample N number of users, we sample the number of buckets that contain the number of users closest to the desired number N. If, e.g., the desired N is 20 and each bucket contains 3 users, we would sample 7 buckets and end up with 21 users. Note that a bucket is simply a logical group of units to which we assign a certain user by a fixed hash map. Figure 1 illustrates such a map. The second part of the name Bucket \u003cem\u003eReuse\u003c/em\u003e comes from the fact that we reuse the same buckets over and over again in the sampling for all experiments. That is, the random salt for the hashing is selected only once; after that, the hash map and the number of buckets is fixed. \u003cstrong\u003eImportantly,\u003c/strong\u003e when we talk about Bucket Reuse for experimentation, we always mean the following: the random sampling is performed on the bucket level; the random treatment allocation is performed at the user level on the users in the sample. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"625\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-700x625.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-700x625.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-250x223.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-768x686.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-120x107.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4.png 1366w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: \u003cem\u003eSchematic illustration of a hash map. A user ID is hashed together with a random salt to map each user to a unique bucket. \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eWhat is experiment coordination?\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo get into the interesting parts of experiment coordination, we need to establish some key concepts. Figure 2 illustrates the concepts of exclusive and nonexclusive experiments. That two or more experiments “are exclusive” to each other simply means that they are run on distinct sets of users. Experiments that are nonexclusive are nonexclusive to \u003cem\u003eall\u003c/em\u003e experiments at Spotify, meaning that they all randomly overlap in terms of users.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"383\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-700x383.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-700x383.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-250x137.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-768x420.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-120x66.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5.png 1498w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 2: \u003cem\u003eIllustration of exclusive and nonexclusive experiments. Exclusive experiments never overlap with each other in terms of users; nonexclusive experiments randomly overlap with exclusive experiments and other nonexclusive experiments. Note that the allocations in this figure were selected for illustration; in a true random sample we would expect exclusive experiments to also be spread out uniformly.  \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe most critical challenge from a statistical perspective is imposed by what we call \u003cem\u003eprograms of exclusive experiments\u003c/em\u003e. A program of exclusive experiments is a set of experiments run over time where all simultaneous experiments are exclusive to each other. That is, a unit is in at most one, and only one, of the experiments in the program at any given time point. At Spotify we have such programs for several surfaces in the app, for example Search, the Home screen, and certain parts of the backend code base. To better understand the limitations imposed on the sampling by running programs of exclusive experiments, it is helpful to introduce the concepts of paths. A path is simply a sequence of experiments that a unit can be in. Figure 3 illustrates a program of exclusive experiments containing 5 experiments over time. Below the experiments, their possible paths are displayed. For example, it is not possible to go through both Experiments 3 and 4 as they overlap in time and are exclusive, and must therefore be run on distinct users. The number of unique possible paths explodes combinatorially after a relatively short time period in most programs, and only a small partition of the possible paths can be taken by any unit regardless of the sample strategies discussed in this post.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"806\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-700x806.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-700x806.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-250x288.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-768x885.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-1334x1536.png 1334w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-120x138.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3.png 1370w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 3: \u003cem\u003ePaths of experiences possible during a hypothetical program of 5 exclusive experiments.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eHighlights from \u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-030-89906-6_50\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSchultzberg et al.\u003c/a\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSchultzberg et al. presents two key results:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eUnder unrestricted sampling of buckets, i.e., Bucket Reuse in nonexclusive experiments, the properties of the difference-in-means estimator of the average treatment effect is approximately equivalent to the properties under random sampling of units. In other words, standard t-tests can be used for inference. The key to this remarkable finding is connecting Bucket Reuse to the existing literature on randomized experiments embedded in complex sampling designs (Horvitz and Thompson, 1952;  van den Brakel and Renssen, 2005). \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003eThe bias imposed by the restricted sampling of buckets implied by programs of exclusive experiments is derived. It is shown that this bias is often restricted to the history right before the experiment. Moreover, the length of the window of the history that affects the bias can be estimated for any empirical experimentation program. One way to phrase this finding is that the sample at a time point T is not random with respect to the last D days leading up to time T, but random with respect to everything that happened before the time point T-D. If things happened during the last D days that make the set of buckets available for sampling at time T different from the population with respect to the treatment effect, the estimator is biased. This insight makes it possible for experimenters to evaluate the risk for biases by checking what experiments have been run in the program over the last D days, and if those risks for biases are likely to affect the average treatment effect in the experiment that is about to be started. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eSpotify’s new experimentation coordination strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have migrated our experimentation platform to using Bucket Reuse for all experiments at Spotify. There are a few key reasons why we prefer Bucket Reuse over other solutions:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eIt is simple to implement and understand. \u003c/li\u003e\u003cli\u003eIt is a technically feasible solution that allows us to do complex coordination without losing speed in our systems. As new users come into Spotify, they are uniformly hashed into the existing buckets — that is, the system scales as Spotify’s user base grows.\u003c/li\u003e\u003cli\u003eUsing one company-global bucket structure makes it easy to coordinate experiments arbitrarily. For example, two programs that have been run independently can easily be merged into one program of exclusive experiments at any time point for any period of time. And, a sample from a previous experiment with a broken experience can easily be quarantined and avoided in any future experiments, as the sampling units are always the same over time.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eAt Spotify we have chosen Bucket Reuse with 1M (1,000,000) buckets to coordinate all experiments. That is, all users are hashed into 1M buckets, and these buckets are used for all experiments in all experimentation programs, exclusive and nonexclusive. Although Schultzberg et al.  establishes that smaller numbers of buckets can have statistical properties enabling straight forward inference, it should be clear that the larger the number of buckets, the better. Even though the inference for the average treatment effect is unaffected by the bucket sampling, it is well known that the effect of cluster sampling on other estimands decreases when the number of buckets increases (Kumar Pradhan). That is, imposing a bucket structure is not preferred from a statistical perspective, but it is a technical necessity. The choice of 1M buckets was made because it is close to the largest number of buckets we can have while still keeping the selected buckets within an executable script stored in a database without having to resort to \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Binary_large_object\" target=\"_blank\"\u003eBLOB\u003c/a\u003e storage. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe are not planning to have any full stops or to reshuffle users into new buckets. However, there are naturally occurring periods of low experimentation that will help decrease the dependency between the samples in programs of exclusive experiments. For example, many programs run fewer general product experiments over the winter holidays due to unusual listening behaviors. These natural pauses effectively reset the programs in terms of dependencies.  \u003c/p\u003e\n\n\n\n\u003cp\u003eTo help experimenters running programs of exclusive experiments, we are implementing a few tools to keep track of the short-term dependencies. For each program, we will estimate the length of the history that can bias the results. Moreover, we are also implementing a tool to see the history of the available buckets at any time point. Figure 4 displays a prototype of this tool. It allows experimenters to evaluate if the experiments that the available user came out of are likely to bias the estimator in their experiment. It also provides information about the size of the overlap with previous experiments. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"1600\" height=\"1394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/2020-10-23-10.58.56.gif\" alt=\"\"/\u003e\u003cfigcaption\u003eFigure 4: \u003cem\u003eDependency graph that shows the experimenter where the free space, and thereby their sample, will come from in terms of previous experiments in the exclusive program. Each rectangle corresponds to a previously run experiment. The numbers in the yellow circles indicate the percentage points of the population that went from one experiment into another, and finally into the proportion of the population that is available (“free space”) for sampling right now. The experimenter can see what effects the previous experiments had on the metrics of interest in their experiment. \u003c/em\u003e\u003cbr/\u003e\u003cem\u003eFigure above for illustrative purposes only.\u003c/em\u003e\u003cbr/\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\n\n\n\n\u003cp\u003eStatistical validity, derived from proper random sampling and random treatment allocation, is the cornerstone of a successful experimentation program. However, implementing systems that can serve hundreds of experiments to hundreds of millions users — while retaining the ability to conveniently coordinate experiments to be exclusive, without overlap — requires compromises between technical feasibility and statistical properties. At Spotify, we have migrated the internal experimentation platform to rely fully on \u003cem\u003ebucket reuse\u003c/em\u003e, a technically desirable solution that provides speed, simplicity, and flexibility. In this post we establish the statistical properties under bucket reuse and conclude that the validity is unaffected. This migration enables more experiments of higher quality at Spotify.  \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eExperimentation Platform team, Spotify\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReferences\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBrakel, Jan van den, and Robbert H. Renssen. “Analysis of Experiments Embedded in Complex Sampling Designs.” \u003cem\u003eSurvey Methodology \u003c/em\u003e 31, no. 1 (2005): 23–40.\u003c/p\u003e\n\n\n\n\u003cp\u003eHorvitz, D. G. and D. J. Thompson. “A Generalization of Sampling Without Replacement from a Finite Universe.” \u003cem\u003eJournal of the American Statistical Association\u003c/em\u003e 47, no. 260 (1952): 663–685. https://doi.org/10.2307/2280784\u003c/p\u003e\n\n\n\n\u003cp\u003eKumar Pradhan, Bijoy. “On efficiency of cluster sampling on sampling on two occasions.” \u003cem\u003eStatistica\u003c/em\u003e 64, no. 1 (2004): 183–191. \u003ca href=\"https://doi.org/10.6092/issn.1973-2201/31\"\u003ehttps://doi.org/10.6092/issn.1973-2201/31\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eSchultzberg, Mårten, Oskar Kjellin, and Johan Rydberg. “Statistical Properties of Exclusive and Non-exclusive Online Randomized Experiments using Bucket Reuse.” \u003cem\u003earXiv preprint arXiv:2012.10202 \u003c/em\u003e(2020).\u003c/p\u003e\n\n\n\n\u003cp\u003eTang, Diane, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. “Overlapping experiment infrastructure: more, better, faster experimentation.” \u003cem\u003eKDD\u003c/em\u003e \u003cem\u003e’10: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining\u003c/em\u003e, (July 2010): 17–26.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png",
      "date_published": "2021-03-10T00:00:00Z",
      "author": {
        "name": "Published by Mårten Schultzberg, Oskar Kjellin, and Johan Rydberg"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/spotifys-new-experimentation-coordination-strategy/",
      "title": "\n                                            Spotify’s New Experimentation Coordination Strategy\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 10, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/spotifys-new-experimentation-coordination-strategy/\" title=\"Spotify’s New Experimentation Coordination Strategy\"\u003e\n                        \u003cimg src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png 2105w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-250x126.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-700x352.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-768x386.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-1536x772.png 1536w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-2048x1029.png 2048w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be coordinated in the sense that the same user cannot be in two experiments at the same time. These are well-known problems among tech companies — take, for example, Google’s solution in Tang et al (2010). But the statistical implications of different solutions have not been properly investigated. In a recent paper (\u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-030-89906-6_50\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eM. Schultzberg, O. Kjellin, and J. Rydberg, 2020\u003c/a\u003e), we investigate important statistical properties of a common technical solution to the coordination — called “Bucket Reuse”. In this blog post we highlight some interesting results and present some details about how Spotify will coordinate experiments from now on.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat is Bucket Reuse?\u003c/h2\u003e\n\n\n\n\u003cp\u003eBucket Reuse is a simple idea utilizing the power of hashing. Essentially the steps are as follows: decide on a number of buckets (B). Take the unique user ID and hash it together with a  random salt into B “buckets” such that all users hash into one and only one bucket. Once the hash map is established, all sampling is performed on the bucket level. This implies that a bucket either is or is not in a sample at any given time point. If we want to sample N number of users, we sample the number of buckets that contain the number of users closest to the desired number N. If, e.g., the desired N is 20 and each bucket contains 3 users, we would sample 7 buckets and end up with 21 users. Note that a bucket is simply a logical group of units to which we assign a certain user by a fixed hash map. Figure 1 illustrates such a map. The second part of the name Bucket \u003cem\u003eReuse\u003c/em\u003e comes from the fact that we reuse the same buckets over and over again in the sampling for all experiments. That is, the random salt for the hashing is selected only once; after that, the hash map and the number of buckets is fixed. \u003cstrong\u003eImportantly,\u003c/strong\u003e when we talk about Bucket Reuse for experimentation, we always mean the following: the random sampling is performed on the bucket level; the random treatment allocation is performed at the user level on the users in the sample. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"625\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-700x625.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-700x625.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-250x223.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-768x686.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4-120x107.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image4.png 1366w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 1: \u003cem\u003eSchematic illustration of a hash map. A user ID is hashed together with a random salt to map each user to a unique bucket. \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eWhat is experiment coordination?\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo get into the interesting parts of experiment coordination, we need to establish some key concepts. Figure 2 illustrates the concepts of exclusive and nonexclusive experiments. That two or more experiments “are exclusive” to each other simply means that they are run on distinct sets of users. Experiments that are nonexclusive are nonexclusive to \u003cem\u003eall\u003c/em\u003e experiments at Spotify, meaning that they all randomly overlap in terms of users.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"383\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-700x383.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-700x383.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-250x137.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-768x420.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5-120x66.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/image5.png 1498w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 2: \u003cem\u003eIllustration of exclusive and nonexclusive experiments. Exclusive experiments never overlap with each other in terms of users; nonexclusive experiments randomly overlap with exclusive experiments and other nonexclusive experiments. Note that the allocations in this figure were selected for illustration; in a true random sample we would expect exclusive experiments to also be spread out uniformly.  \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe most critical challenge from a statistical perspective is imposed by what we call \u003cem\u003eprograms of exclusive experiments\u003c/em\u003e. A program of exclusive experiments is a set of experiments run over time where all simultaneous experiments are exclusive to each other. That is, a unit is in at most one, and only one, of the experiments in the program at any given time point. At Spotify we have such programs for several surfaces in the app, for example Search, the Home screen, and certain parts of the backend code base. To better understand the limitations imposed on the sampling by running programs of exclusive experiments, it is helpful to introduce the concepts of paths. A path is simply a sequence of experiments that a unit can be in. Figure 3 illustrates a program of exclusive experiments containing 5 experiments over time. Below the experiments, their possible paths are displayed. For example, it is not possible to go through both Experiments 3 and 4 as they overlap in time and are exclusive, and must therefore be run on distinct users. The number of unique possible paths explodes combinatorially after a relatively short time period in most programs, and only a small partition of the possible paths can be taken by any unit regardless of the sample strategies discussed in this post.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"806\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-700x806.png\" alt=\"\" srcset=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-700x806.png 700w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-250x288.png 250w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-768x885.png 768w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-1334x1536.png 1334w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3-120x138.png 120w, https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Figure3.png 1370w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigure 3: \u003cem\u003ePaths of experiences possible during a hypothetical program of 5 exclusive experiments.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eHighlights from \u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-030-89906-6_50\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSchultzberg et al.\u003c/a\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eSchultzberg et al. presents two key results:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003eUnder unrestricted sampling of buckets, i.e., Bucket Reuse in nonexclusive experiments, the properties of the difference-in-means estimator of the average treatment effect is approximately equivalent to the properties under random sampling of units. In other words, standard t-tests can be used for inference. The key to this remarkable finding is connecting Bucket Reuse to the existing literature on randomized experiments embedded in complex sampling designs (Horvitz and Thompson, 1952;  van den Brakel and Renssen, 2005). \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003eThe bias imposed by the restricted sampling of buckets implied by programs of exclusive experiments is derived. It is shown that this bias is often restricted to the history right before the experiment. Moreover, the length of the window of the history that affects the bias can be estimated for any empirical experimentation program. One way to phrase this finding is that the sample at a time point T is not random with respect to the last D days leading up to time T, but random with respect to everything that happened before the time point T-D. If things happened during the last D days that make the set of buckets available for sampling at time T different from the population with respect to the treatment effect, the estimator is biased. This insight makes it possible for experimenters to evaluate the risk for biases by checking what experiments have been run in the program over the last D days, and if those risks for biases are likely to affect the average treatment effect in the experiment that is about to be started. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003ch2\u003eSpotify’s new experimentation coordination strategy\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have migrated our experimentation platform to using Bucket Reuse for all experiments at Spotify. There are a few key reasons why we prefer Bucket Reuse over other solutions:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eIt is simple to implement and understand. \u003c/li\u003e\u003cli\u003eIt is a technically feasible solution that allows us to do complex coordination without losing speed in our systems. As new users come into Spotify, they are uniformly hashed into the existing buckets — that is, the system scales as Spotify’s user base grows.\u003c/li\u003e\u003cli\u003eUsing one company-global bucket structure makes it easy to coordinate experiments arbitrarily. For example, two programs that have been run independently can easily be merged into one program of exclusive experiments at any time point for any period of time. And, a sample from a previous experiment with a broken experience can easily be quarantined and avoided in any future experiments, as the sampling units are always the same over time.  \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eAt Spotify we have chosen Bucket Reuse with 1M (1,000,000) buckets to coordinate all experiments. That is, all users are hashed into 1M buckets, and these buckets are used for all experiments in all experimentation programs, exclusive and nonexclusive. Although Schultzberg et al.  establishes that smaller numbers of buckets can have statistical properties enabling straight forward inference, it should be clear that the larger the number of buckets, the better. Even though the inference for the average treatment effect is unaffected by the bucket sampling, it is well known that the effect of cluster sampling on other estimands decreases when the number of buckets increases (Kumar Pradhan). That is, imposing a bucket structure is not preferred from a statistical perspective, but it is a technical necessity. The choice of 1M buckets was made because it is close to the largest number of buckets we can have while still keeping the selected buckets within an executable script stored in a database without having to resort to \u003ca rel=\"noreferrer noopener\" href=\"https://en.wikipedia.org/wiki/Binary_large_object\" target=\"_blank\"\u003eBLOB\u003c/a\u003e storage. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe are not planning to have any full stops or to reshuffle users into new buckets. However, there are naturally occurring periods of low experimentation that will help decrease the dependency between the samples in programs of exclusive experiments. For example, many programs run fewer general product experiments over the winter holidays due to unusual listening behaviors. These natural pauses effectively reset the programs in terms of dependencies.  \u003c/p\u003e\n\n\n\n\u003cp\u003eTo help experimenters running programs of exclusive experiments, we are implementing a few tools to keep track of the short-term dependencies. For each program, we will estimate the length of the history that can bias the results. Moreover, we are also implementing a tool to see the history of the available buckets at any time point. Figure 4 displays a prototype of this tool. It allows experimenters to evaluate if the experiments that the available user came out of are likely to bias the estimator in their experiment. It also provides information about the size of the overlap with previous experiments. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"1600\" height=\"1394\" src=\"https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/2020-10-23-10.58.56.gif\" alt=\"\"/\u003e\u003cfigcaption\u003eFigure 4: \u003cem\u003eDependency graph that shows the experimenter where the free space, and thereby their sample, will come from in terms of previous experiments in the exclusive program. Each rectangle corresponds to a previously run experiment. The numbers in the yellow circles indicate the percentage points of the population that went from one experiment into another, and finally into the proportion of the population that is available (“free space”) for sampling right now. The experimenter can see what effects the previous experiments had on the metrics of interest in their experiment. \u003c/em\u003e\u003cbr/\u003e\u003cem\u003eFigure above for illustrative purposes only.\u003c/em\u003e\u003cbr/\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\n\n\n\n\u003cp\u003eStatistical validity, derived from proper random sampling and random treatment allocation, is the cornerstone of a successful experimentation program. However, implementing systems that can serve hundreds of experiments to hundreds of millions users — while retaining the ability to conveniently coordinate experiments to be exclusive, without overlap — requires compromises between technical feasibility and statistical properties. At Spotify, we have migrated the internal experimentation platform to rely fully on \u003cem\u003ebucket reuse\u003c/em\u003e, a technically desirable solution that provides speed, simplicity, and flexibility. In this post we establish the statistical properties under bucket reuse and conclude that the validity is unaffected. This migration enables more experiments of higher quality at Spotify.  \u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eExperimentation Platform team, Spotify\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cstrong\u003eReferences\u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eBrakel, Jan van den, and Robbert H. Renssen. “Analysis of Experiments Embedded in Complex Sampling Designs.” \u003cem\u003eSurvey Methodology \u003c/em\u003e 31, no. 1 (2005): 23–40.\u003c/p\u003e\n\n\n\n\u003cp\u003eHorvitz, D. G. and D. J. Thompson. “A Generalization of Sampling Without Replacement from a Finite Universe.” \u003cem\u003eJournal of the American Statistical Association\u003c/em\u003e 47, no. 260 (1952): 663–685. https://doi.org/10.2307/2280784\u003c/p\u003e\n\n\n\n\u003cp\u003eKumar Pradhan, Bijoy. “On efficiency of cluster sampling on sampling on two occasions.” \u003cem\u003eStatistica\u003c/em\u003e 64, no. 1 (2004): 183–191. \u003ca href=\"https://doi.org/10.6092/issn.1973-2201/31\"\u003ehttps://doi.org/10.6092/issn.1973-2201/31\u003c/a\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eSchultzberg, Mårten, Oskar Kjellin, and Johan Rydberg. “Statistical Properties of Exclusive and Non-exclusive Online Randomized Experiments using Bucket Reuse.” \u003cem\u003earXiv preprint arXiv:2012.10202 \u003c/em\u003e(2020).\u003c/p\u003e\n\n\n\n\u003cp\u003eTang, Diane, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. “Overlapping experiment infrastructure: more, better, faster experimentation.” \u003cem\u003eKDD\u003c/em\u003e \u003cem\u003e’10: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining\u003c/em\u003e, (July 2010): 17–26.\u003c/p\u003e\n        \u003cp\u003e\n\n        Tags: \u003ca href=\"https://engineering.atspotify.com/tag/data/\" rel=\"tag\"\u003eData\u003c/a\u003e\u003cbr/\u003e        \n            \u003c/p\u003e\u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify we run hundreds of experiments at any given time. Coordinating these experiments, i.e., making sure the right user is receiving the right “treatment” with a population of hundreds of millions of users, poses technical challenges. Adding to the complexity, some of these experiments must be",
      "image": "https://engineering.atspotify.com/wp-content/uploads/sites/2/2021/03/Bucket-Reuse_01-1.png",
      "date_published": "2021-03-10T00:00:00Z",
      "author": {
        "name": "Published by Mårten Schultzberg, Oskar Kjellin, and Johan Rydberg"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/09/my-beat-matt-clarke/",
      "title": "\n                                            Matt Clarke: Senior Backend Infrastructure Engineer\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                 \n \u003carticle id=\"post-4139\"\u003e\n     \u003cdiv\u003e\n         \n         \n         \n         \u003cdiv\u003e\n             \u003cp\u003e\u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg\" alt=\"Matt Clarke\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg 800w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-250x192.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-700x537.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-768x589.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit-120x92.jpg 120w\" sizes=\"(max-width: 800px) 100vw, 800px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/02/Matt-Clarke-edit.jpg\"/\u003e\n                                  \n             \u003c/p\u003e\n             \u003cp\u003e\u003cstrong\u003eMatt is a Senior Backend Infrastructure Engineer and has been at Spotify for two-and-a-half years. This time last year, he was living and working in London – but that’s all changed since the start of the pandemic… \u003c/strong\u003e\u003c/p\u003e\n         \u003c/div\u003e\n\n         \n\n\n         \n\n         \n\u003cblockquote\u003e\u003cp\u003e8:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eAt the moment, I’m living with my wife and step daughter in London and working from home like most people. Which means I get up around 8am, drink lots of coffee (way too much!) and log onto my computer an hour later – ready to check my messages and start getting my brain into thinking mode. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e10:00am\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eMy team is actually based in New York, so I tend to work 10–6 to overlap as much as possible with their hours. But the time difference means my mornings are fairly quiet and meeting-free – a chance for me to get down to focussed, individual tasks, without too many interruptions. \u003c/p\u003e\n\n\n\n\u003cp\u003eMy work is mostly to do with Kubernetes – the technology we use to deploy our services at Spotify – and a lot of my time is spent helping other engineers get to grips with the system, debugging their issues and developing our infrastructure services, so they can deploy more easily and reliably. \u003c/p\u003e\n\n\n\n\u003cp\u003eRecently, I’ve been working on something called the k8s plug in, which vastly simplifies the Kubernetes experience for developers and means they can operate without a huge understanding of the platform under the hood. Earlier this year, we open-sourced this plug in, which felt like a really great moment – it’s amazing to think it’s now available to everyone in the tech community worldwide and can benefit so many people outside our organization. That to me is the magical thing about open-source. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e1:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI try to grab a bite to eat around 1pm, although I’m really bad at taking breaks – I get so sucked into what I’m doing that I forget the time, especially if I’m coding. It used to happen when I worked in the office too, even though there was an awesome canteen and a table tennis table to tempt me away from my desk!\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e2:00pm\u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eThis is when New York starts to wake up, so my work becomes more team-based – I often pair remotely with one of the developers out there, which means jumping on a hangout and sharing our screens, so we can collaborate on a piece of code. Alternatively, we might work together to write documents like RFCs, debug production issues or help other developers with their infrastructure issues. It’s a bit of a mix, really, \u003c/p\u003e\n\n\n\n\u003cp\u003eWeirdly, I’ve only met one of team-mates in real life, when I first joined up and spent two weeks in the New York office as part of an embed. But we still all work together really well – it’s friendly and we joke around a lot. I think there are some rules you need to learn for remote working and being in different zones – you need to be a bit flexible and not always expect to get your answers straight away. But once you’ve got used to that, things are surprisingly easy – it’s really not a big deal at all. \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003e6:00pm \u003c/p\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eI usually finish up at 6ish, although I’m terrible for checking my emails in the evening. To try and switch off, I watch TV, listen to podcasts or play video games – I also started up an engineering book club at the start of the first lockdown. And I’m really into cooking at the moment – there’s something about following a recipe and going through a series of orchestrated steps that reminds me of coding. Although, as the old joke goes, at least your potato peeler never turns out to be ten versions out of date… \u003c/p\u003e\n\n\n\n\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"111\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-700x111.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-250x40.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-768x121.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering-120x19.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/My-Beat-Breakdown-Engineering.png 1525w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"557\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-700x557.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-700x557.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-250x199.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-768x611.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph-120x96.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/MyBeat_Matt-Clarke-graph.png 1520w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n         \n         \n\n         \u003cp\u003e\n             Published by Spotify Engineering         \u003c/p\u003e\n     \u003c/div\u003e\n\n     \n     \n\n\n \u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Matt is a Senior Backend Infrastructure Engineer and has been at Spotify for two-and-a-half years. This time last year, he was living and working in London – but that’s all changed since the start of the pandemic…",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Matt-Clarke-edit.jpg",
      "date_published": "2021-03-09T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/01/2020-unwrapped-the-people-behind-the-numbers/",
      "title": "\n                                            2020 Unwrapped: The people behind the numbers\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 1, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/03/01/2020-unwrapped-the-people-behind-the-numbers/\" title=\"2020 Unwrapped: The people behind the numbers\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg 1510w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-250x122.jpg 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-700x341.jpg 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-768x374.jpg 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped-120x58.jpg 120w\" sizes=\"(max-width: 1510px) 100vw, 1510px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/03/2020-Wrapped.jpg\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e2020 Wrapped is a story of gratitude and resilience. And we’re grateful for the people and teams behind the curtain who built this product experience (👏🏽Give them a hand!). \u003c/p\u003e\n\n\n\n\u003cp\u003eThe effort behind Wrapped spans the entire company and is founded on communication and collaboration. With the shift to working from home, we needed to create a structure that enabled us to collaborate and communicate remotely, and prioritized asynchronous communication over synchronous communication when possible. We were able to adapt to unprecedented challenges and join forces to deliver a personalized product experience for our listeners. Now, let’s take a look at the numbers behind the 2020 Wrapped experience.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eDistributed execution\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003e2020 Wrapped, like the Wrapped campaigns before it, was a company-wide project that involved collaboration among hundreds of Spotifiers. Over a span of 4 months, two key groups dedicated themselves to building the product experience — \u003cstrong\u003ePersonalization\u003c/strong\u003e \u003cstrong\u003e(PZN)\u003c/strong\u003e, the minds behind the \u003ca href=\"https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edata\u003c/a\u003e magic, and \u003cstrong\u003eEdison\u003c/strong\u003e, drivers of the rich end-user experience. Around twenty engineers from 4 different squads within Edison and 6 engineers from other squads joined as embeds, all working as a virtual team covering 4 different time zones. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe adapted our ways of working to allow team members to contribute, communicate and collaborate regardless of their location. The team asynchronously worked on 10 technical specification documents, 8 RFC documents, 4 project planners, and more than 15 slide decks to facilitate information sharing.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cul\u003e\u003cli\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"1245\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-700x1245.png\" alt=\"\" data-id=\"4235\" data-full-url=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer.png\" data-link=\"https://engineering.atspotify.com/?attachment_id=4235\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Pioneer.png 822w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/li\u003e\u003cli\u003e\u003cfigure\u003e\u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3-700x1245.png\" alt=\"\" data-id=\"4225\" data-full-url=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.png\" data-link=\"https://engineering.atspotify.com/?attachment_id=4225\"/\u003e\u003c/figure\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWith a distributed-first mindset, Slack became our place to build traceability and transparency, allowing us to easily retrace our steps and find documentation that led to business decisions and actions. By the end of the project,  we had created 11 Slack channels each dedicated to brief messages that required quick answers or a brief team discussion around specific topics. One particular channel existed solely for the purpose of sharing memes — 8,116 and counting. 😬\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cul\u003e\u003cli\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"1245\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-700x1245.png\" alt=\"\" data-id=\"4237\" data-full-url=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack.png\" data-link=\"https://engineering.atspotify.com/?attachment_id=4237\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Slack.png 822w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/li\u003e\u003cli\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"1245\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-700x1245.png\" alt=\"\" data-id=\"4238\" data-full-url=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison.png\" data-link=\"https://engineering.atspotify.com/?attachment_id=4238\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Edison.png 822w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eUltimately, we kept the team connected and created team spirit by holding brainstorm meetings and building on ideas we generated as a group. It was especially helpful when complex problems arose and we needed to get people on the same page.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eCollaboration\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe core \u003ca href=\"https://spotify.design/article/how-we-brought-2020-wrapped-to-life-in-the-mobile-app\" target=\"_blank\" rel=\"noreferrer noopener\"\u003emobile experience\u003c/a\u003e was built by a few teams, but to deliver the product experience, it took the effort of 8 additional squads working simultaneously. We defaulted to open communication and, in most cases, preferred to overshare than undershare. Given the number of teams, oversharing ensured a successful flow of information. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe focused a great deal on expanding the mobile experience for our listeners in 2019, and we were able to reuse some of the components and apply some of the \u003ca href=\"https://engineering.atspotify.com/2020/09/21/spotify-unwrapped-2019-how-we-built-an-in-app-experience-just-for-you/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003elessons from 2019 Wrapped\u003c/a\u003e to the 2020 experience. Compared to 2019, we were able to reduce our engineering effort by 50% for Android development, reduce our timeline for iOS development by 30%, and repeat with 60 engineering weeks for Backend. For Web, we increased our scope with an ambitious vision and made a significant investment in our engineering effort compared to 2019.\u003c/p\u003e\n\n\n\n\u003cp\u003eOn the day of launch, individuals from multiple functions — Marketing, Brand + Creative, R\u0026amp;D, Customer Support, Local Marketing, Public Relations, and Localization — gathered in a war room (aka a virtual meeting) testing, troubleshooting, and tackling live issues with tight coordination across 8 hours.\u003c/p\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eWrapped around the world\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eWith 12 new markets and 4 new languages, 2020 Wrapped was the most global iteration of the project to date 🌎🌍🌏. We served 26 total languages, 8 different fonts, and 87,343 words total — approximately 4,000 words per language in both left-to-right and right-to-left text orientations. Though a challenging task, it was absolutely critical that we embed localization into our process to improve the listening experience for our users across the globe. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cul\u003e\u003cli\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"1245\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-700x1245.png\" alt=\"\" data-id=\"4240\" data-full-url=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily.png\" data-link=\"https://engineering.atspotify.com/?attachment_id=4240\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/the-daily.png 822w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/li\u003e\u003cli\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"1245\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-700x1245.png\" alt=\"\" data-id=\"4241\" data-full-url=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts.png\" data-link=\"https://engineering.atspotify.com/?attachment_id=4241\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-700x1245.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-250x445.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-768x1366.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts-120x213.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/top-fonts.png 822w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/li\u003e\u003c/ul\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eShareable cards, like the ones above, hold a number of different corner cases to test, and quality remains top of mind. Because we weren’t physically in the office to pair and review pieces, it became imperative that we find creative solutions to allow for effective and efficient collaboration. For Localization testing and the Design Review process, we created a dedicated tool to reduce the feedback loop and enable the team to use it asynchronously. Using this tool, we were able to increase productivity in our distributed work environment, while respecting flexible work schedules and work/life boundaries.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"358\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-700x358.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-700x358.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-250x128.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-768x393.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-1536x786.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8-120x61.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/I-want-to-believe8.png 1749w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"359\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-700x359.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-700x359.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-250x128.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-768x393.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-1536x787.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton-120x61.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/Lionel-Hampton.png 1743w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003e\u003cstrong\u003eClosing words from the Wrapped fellowship\u003c/strong\u003e\u003c/h2\u003e\n\n\n\n\u003cp\u003eIn the shift to working from home on a project that spanned the company, we needed to (quickly) set up a structure to keep productivity high and collaboration and communication flowing, while ensuring we were taking care of ourselves, our family and friends, and our team members. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe hope you enjoyed reading about the people behind the numbers, and hope that Wrapped made the end of your 2020 just a little bit better.\u003c/p\u003e\n\n\n\n\u003cp\u003eThank you to everyone who made another year of Wrapped.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "2020 Wrapped is a story of gratitude and resilience. And we’re grateful for the people and teams behind the curtain who built this product experience (👏🏽Give them a hand!). The effort behind Wrapped spans the entire company and is founded on communication and collaboration. With the shift to wor",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/2020-Wrapped.jpg",
      "date_published": "2021-03-01T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/",
      "title": "\n                                            Designing a Better Kubernetes Experience for Developers\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eMarch 1, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/03/01/designing-a-better-kubernetes-experience-for-developers/\" title=\"Designing a Better Kubernetes Experience for Developers\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.gif\" alt=\"\" loading=\"lazy\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/03/image3.gif\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTLDR;\u003c/strong\u003e If you’re deploying a service with Kubernetes, you shouldn’t have to use all of your cluster management skills just to perform everyday developer tasks (like seeing which pods are experiencing errors or checking autoscaler limits). Backstage Kubernetes simplifies your deployment workflow by connecting to your existing Kubernetes implementation and aggregating the status of all your deployments into a single view — even across multiple clusters in multiple regions. \u003c/p\u003e\n\n\n\n\u003ch2\u003eNavigating the complexity of Kubernetes\u003c/h2\u003e\n\n\n\n\u003cp\u003eIf you’re building a service today, you’re likely deploying it as a container, which is inside a pod, which is inside a cluster (alongside a bunch of other services that don’t belong to you), with deployments on different clusters spinning up and down all around the world. It can be hard to keep track of everything.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut despite widespread adoption of Kubernetes, all the tools for navigating this complexity have been focussed on the needs of cluster admins. This can make something as simple as checking the health of your service somewhat complicated. \u003c/p\u003e\n\n\n\n\u003cp\u003eThat’s why we built a Kubernetes monitoring tool focussed on the needs of service owners and made it a core feature of \u003ca href=\"https://backstage.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our open platform for building developer portals. We wanted to make the experience of managing services deployed on Kubernetes easier for all developers. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut first, how did we get here?\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe rise of Kubernetes and DevOps\u003c/h2\u003e\n\n\n\n\u003cp\u003eSince its release in 2014, Kubernetes has become one of the most widely adopted and important open source projects. Capabilities like autoscaling and cost optimisation through container scheduling used to be time-consuming and tricky to get right — now they’ve been democratised. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt the same time, the concept of DevOps has become mainstream. Developers now regularly perform tasks that were traditionally the domain of operations experts. \u003c/p\u003e\n\n\n\n\u003cp\u003eSo, while everyday engineers can do more than ever before, their new powers have also come along with a new set of responsibilities.\u003c/p\u003e\n\n\n\n\u003ch2\u003eNew powers, shifting roles\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen I first started using Kubernetes, cluster admins and service owners were one and the same: the people who built a cluster were usually the same people who owned the services that ran in the cluster. That’s not how it is today. As Kubernetes has achieved widespread adoption there has been a shift in Kubernetes usage as well as a shift in how Kubernetes is managed at the organisation level. \u003c/p\u003e\n\n\n\n\u003cp\u003eNow organisations tend to have a separate infrastructure team (sometimes not-so-ironically called the “DevOps” team) who build and maintain clusters for the feature developers and service owners. As the teams have become more specialized, the setups have become more advanced. For instance, the infrastructure team might set up Kubernetes clusters in multiple geographic regions in order to reduce end-user latency, wherever the user is in the world. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis is a better experience for the user, and it’s an optimization you might not have considered before Kubernetes existed or without a dedicated infrastructure team. But it also comes with productivity costs for the developer.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrustration also scales\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen your deployment environment reaches this kind of complexity and scale, the maintenance overhead for service owners increases. It forces them to use multiple kubectl contexts or multiple UIs just to get an overall view of their system. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt’s a small overhead — but adds up over time — and multiplies as service owners build more services and deploy them to more regions. Just checking the status of a service first requires hunting for it across multiple clusters. This can reduce productivity (and patience) company-wide.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBetter tools for the job\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe believed we could solve the problem through developer tooling. But we soon discovered the available tools weren’t suitable, because they:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eDon’t cater well for deploying to multiple Kubernetes clusters,\u003c/li\u003e\u003cli\u003eUsually require that users have clusterwide permissions, or\u003c/li\u003e\u003cli\u003eDisplay everything on a cluster and aren’t focused on the service the user cares about.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eAs we often do when we want to \u003ca href=\"https://backstage.io/blog/2020/10/22/cost-insights-plugin\"\u003esolve a problem involving infrastructure complexity\u003c/a\u003e, we wondered, why not build a custom plugin for Backstage, our homegrown developer portal?\u003c/p\u003e\n\n\n\n\u003ch2\u003eBackstage Kubernetes: Manage your services, not clusters\u003c/h2\u003e\n\n\n\n\u003cp\u003eBackstage provides vital information from Kubernetes — specifically focussed on the developer’s service. At a glance, the developer can see:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eThe current status of their systems running in Kubernetes\u003cul\u003e\u003cli\u003eIncluding information aggregated from multiple clusters/regions\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003eAny errors reported by Kubernetes\u003c/li\u003e\u003cli\u003eHow close the system is to its autoscaling limits\u003c/li\u003e\u003cli\u003eContainer restarts\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"419\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-700x419.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-700x419.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-250x150.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-768x459.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-1536x919.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1-120x72.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image1.png 1999w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eA default Kubernetes UI provides a cluster-centric view, including info about software you don’t own. \u003c/em\u003e\u003cbr/\u003e\u003cem\u003e(Source: \u003c/em\u003e\u003ca href=\"https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/\"\u003e\u003cem\u003ekubernetes.io\u003c/em\u003e\u003c/a\u003e\u003cem\u003e)\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image4-700x394.png\" alt=\"\"/\u003e\u003cfigcaption\u003e\u003cem\u003eThe Backstage Kubernetes UI provides a service-centric view, showing you the status of your service no matter how many clusters it’s been deployed to. \u003c/em\u003e\u003cbr/\u003e\u003cem\u003eFigures above are for illustrative purposes.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cfigure\u003e\u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image2-700x382.png\" alt=\"\"/\u003e\u003cfigcaption\u003e\u003cem\u003eMore detail about your deployments is just a click away. You can see autoscaler limits, errors, and the status of individual pods — all at a glance — and without a trip to the CLI.\u003c/em\u003e\u003cbr/\u003e\u003cem\u003eFigures above are for illustrative purposes.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eInstead of spending 20 minutes in a CLI trying to track down which clusters your service has been deployed to, you get all the information you need to know at a glance. You can learn more about these features on the \u003ca href=\"https://backstage.io/blog/2021/01/12/new-backstage-feature-kubernetes-for-service-owners\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage blog\u003c/a\u003e — or watch the demo video below to get an overview.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cp\u003e\n\u003ciframe loading=\"lazy\" title=\"How to monitor your services on Kubernetes with Backstage (Demo)\" width=\"900\" height=\"506\" src=\"https://www.youtube.com/embed/VivuOxn3VQ8?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n\u003c/p\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eEverything about your service in one place\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs a standalone monitoring tool, we think Backstage Kubernetes can improve the experience of any developer who deploys to Kubernetes. Combined with the other features of Backstage, developers get a complete solution for building and managing their services.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt the core of Backstage is its \u003ca href=\"https://backstage.io/blog/2020/05/22/phase-2-service-catalog\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eservice catalog\u003c/a\u003e, which aggregates information about software systems together so you have a consistent UI and one tool for developers to use. For years, Backstage has provided one place for Spotify’s developers to see everything they need to know about their services (APIs, documentation, ownership, etc.). Now that includes the current status of their service, regardless of how many Kubernetes clusters they deploy to.\u003c/p\u003e\n\n\n\n\u003cp\u003eNow that Backstage is open source, we want to improve on what we have built internally and provide Kubernetes as a core component of Backstage for anyone to contribute to and benefit from. \u003c/p\u003e\n\n\n\n\u003ch2\u003eFuture Iteration\u003c/h2\u003e\n\n\n\n\u003cp\u003eAs we continue to grow and develop Kubernetes in Backstage with the community, we hope to offer support for Kubernetes resources beyond Deployments and Custom Resource Definitions. \u003c/p\u003e\n\n\n\n\u003cp\u003eAlthough at Spotify we currently use GKE extensively, Kubernetes in Backstage communicates directly with the Kubernetes API and is cloud agnostic, accordingly. It will work with other cloud providers, including AWS and Azure, as well as managed Kubernetes services, like Red Hat OpenShift.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo contribute or get more information on Kubernetes in Backstage, \u003ca href=\"https://discord.gg/MUpMjP2\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ejoin the discussion on Discord\u003c/a\u003e!\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eAsk us anything: Matthew and the Backstage team will be hosting a Reddit AMA on March 3 at 4:00pm GMT. Send questions in \u003ca href=\"https://www.reddit.com/r/kubernetes/comments/lwb31v/were_the_engineers_rethinking_kubernetes_at/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003er/kubernetes\u003c/a\u003e starting March 2.\u003c/em\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003cem\u003eA version of this article first appeared on \u003c/em\u003e\u003ca href=\"https://thenewstack.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eThe New Stack\u003c/em\u003e\u003c/a\u003e\u003cem\u003e.\u003c/em\u003e\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TLDR; If you’re deploying a service with Kubernetes, you shouldn’t have to use all of your cluster management skills just to perform everyday developer tasks (like seeing which pods are experiencing errors or checking autoscaler limits). Backstage Kubernetes simplifies your deployment workflow by co",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/03/image3.gif",
      "date_published": "2021-03-01T00:00:00Z",
      "author": {
        "name": "Published by Matthew Clarke, Senior Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/",
      "title": "\n                                            How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eFebruary 11, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/\" title=\"How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image1.gif\" alt=\"\" loading=\"lazy\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/02/image1.gif\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eIn this post we’ll discuss how Spotify optimized and sped up elements from our largest Dataflow job, \u003ca rel=\"noreferrer noopener\" href=\"https://engineering.atspotify.com/2020/02/18/spotify-unwrapped-how-we-brought-you-a-decade-of-data/\" target=\"_blank\"\u003eWrapped 2019\u003c/a\u003e, for \u003ca href=\"https://open.spotify.com/genre/2020-page\"\u003eWrapped 2020\u003c/a\u003e using a technique called Sort Merge Bucket (SMB) join. We’ll present the design and implementation of SMB and how we incorporated it into our data pipelines.\u003c/p\u003e\n\n\n\n\u003ch2\u003eIntroduction\u003c/h2\u003e\n\n\n\n\u003cp\u003eShuffle is the core building block for many big data transforms, such as a join, GroupByKey, or other reduce operations. Unfortunately, it’s also one of the most expensive steps in many pipelines. Sort Merge Bucket is an optimization that reduces shuffle by doing work up front on the producer side. The intuition is that for datasets commonly and frequently joined on a known key, e.g., user events with user metadata on a user ID, we can write them in bucket files with records bucketed and sorted by that key. By knowing which files contain a subset of keys and in what order, shuffle becomes a matter of merge-sorting values from matching bucket files, completely eliminating costly disk and network I/O of moving key–value pairs around. Andrea Nardelli carried out the original investigation on Sort Merge Buckets for his \u003ca href=\"http://kth.diva-portal.org/smash/get/diva2:1334587/FULLTEXT01.pdf\"\u003e2018 master’s thesis\u003c/a\u003e, and we started looking into generalizing the idea as a \u003ca rel=\"noreferrer noopener\" href=\"https://spotify.github.io/scio/extras/Sort-Merge-Bucket.html\" target=\"_blank\"\u003eScio module\u003c/a\u003e afterwards.\u003c/p\u003e\n\n\n\n\u003ch2\u003eDesign and Implementation\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe majority of the data pipelines at Spotify are written in \u003ca rel=\"noreferrer noopener\" href=\"https://github.com/spotify/scio\" target=\"_blank\"\u003eScio\u003c/a\u003e, a Scala API for \u003ca href=\"https://beam.apache.org/\"\u003eApache Beam\u003c/a\u003e, and run on the \u003ca href=\"https://cloud.google.com/dataflow\"\u003eGoogle Cloud Dataflow\u003c/a\u003e service. We implemented SMB in Java to be closer to the native Beam SDK (and even wrote and collaborated on a \u003ca href=\"https://docs.google.com/document/d/1AQlonN8t4YJrARcWzepyP7mWHTxHAd6WIECwk1s3LQQ/edit?usp=sharing\"\u003edesign document with the Beam community\u003c/a\u003e), and provide Scala syntactic sugar in Scio like many other I/Os. The design is modularized into the main components listed below — we’ll start with the two top-level SMB \u003ca href=\"https://beam.apache.org/documentation/programming-guide/#transforms\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePTransforms\u003c/a\u003e — the write and read operations SortedBucketSink and SortedBucketSource.\u003c/p\u003e\n\n\n\n\u003ch3\u003eSortedBucketSink\u003c/h3\u003e\n\n\n\n\u003cp\u003eThis transform writes a \u003ca rel=\"noreferrer noopener\" href=\"https://beam.apache.org/documentation/programming-guide/#pcollections\" target=\"_blank\"\u003ePCollection\u003c/a\u003e\u0026lt;T\u0026gt; (where T has a corresponding \u003ca href=\"https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/FileOperations.java\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eFileOperations\u0026lt;T\u0026gt;\u003c/a\u003e instance) in SMB format. It first extracts keys and assigns bucket IDs using logic provided by \u003ca href=\"https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/BucketMetadata.java\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBucketMetadata\u003c/a\u003e, groups key–values by the ID, sorts all values, and then writes them into files corresponding to bucket IDs using the FileOperations instance.\u003c/p\u003e\n\n\n\n\u003cp\u003eIn addition to the bucket files, a JSON file is also written to the output directory representing the information from BucketMetadata that’s necessary to read the source: the number of buckets, the hashing scheme, and the instructions to extract the key from each record (for example, for Avro records we can encode this instruction with the name of the GenericRecord field containing the key).\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"255\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-250x91.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-768x280.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-120x44.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5.png 1180w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch3\u003eSortedBucketSource\u003c/h3\u003e\n\n\n\n\u003cp\u003eThis transform reads from one or more sources written in SMB format with the same key and hashing scheme. It opens file handles for corresponding buckets from each source (using FileOperations\u0026lt;T\u0026gt; for that input type) and merges them while maintaining sorted order. Results are emitted as \u003ca rel=\"noreferrer noopener\" href=\"https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java\" target=\"_blank\"\u003eCoGbkResult\u003c/a\u003e objects per key group, the same class Beam uses for regular Cogroup operations, so the user can extract the results per source with the correct parameterized type.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"365\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-250x130.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-768x400.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-120x63.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7.png 1067w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch3\u003eFileOperations\u003c/h3\u003e\n\n\n\n\u003cp\u003eFileOperations abstracts away the reading and writing of individual bucket files. Since we need fine-grained control over the exact elements and their order in every file, we cannot leverage the existing Beam file I/Os, which operate on a PCollection level and abstract away the locality and order of elements. Instead, SMB file operations happen at a lower level of BoundedSource for input and ParDo for output. Currently Avro, BigQuery TableRow JSON, and TensorFlow TFRecord/Example records are supported. We plan to add other formats like Parquet as well.\u003c/p\u003e\n\n\n\n\u003ch3\u003eBucketMetadata\u003c/h3\u003e\n\n\n\n\u003cp\u003eThis class abstracts the keying and bucketing of elements, and includes information such as key field, class, number of buckets, shards, and hash function. The metadata is serialized as a JSON file alongside data files when writing, and used to check compatibility when reading SMB sources.\u003c/p\u003e\n\n\n\n\u003ch3\u003eOptimizations and Variants\u003c/h3\u003e\n\n\n\n\u003cp\u003eOver the last year and a half we’ve been adopting SMB at Spotify for various use cases, and accumulated many improvements to handle the scale and complexity of our data pipelines.\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eDate partitioning:\u003c/strong\u003e At Spotify, event data is written to Google Cloud Services (GCS) in hourly or daily partitions. A common data engineering use case is to read many partitions in a single pipeline — for example, to compute stream count over the last seven days. For a non-SMB read, this can be easily done in a single PTransform using wildcard file patterns to match files across multiple directories. However, unlike most File I/Os in Beam, the SMB Read API requires the input to be specified as a directory, rather than a file pattern (this is because we need to check the directory’s metadata.json file as well as the actual record files). Additionally, it must match up bucket files across partitions as well as across different sources, while ensuring that the CoGbkResult output correctly groups data from all partitions of a source into the same TupleTag key. We evolved the SMB Read API to accept one or more directories \u003cem\u003eper source\u003c/em\u003e. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSharding:\u003c/strong\u003e Although the Murmur class of hash functions we use during bucket assignment usually ensures an even distribution of records across buckets, in some instances one or more buckets may be disproportionately large if the key space is skewed, creating possible OOM errors when grouping and sorting records. In this case, we allow users to specify a number of \u003cem\u003eshards\u003c/em\u003e to further split each bucket file. During the bucket assignment step, a value between [0, numShards) is generated randomly \u003ca href=\"https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence\"\u003e\u003cem\u003eper bundle\u003c/em\u003e\u003c/a\u003e. Since this value is computed completely orthogonally to the bucket ID, it can break up large key groups across files. Since each shard is still written in sorted order, they can simply be merged together at read time.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eParallelism:\u003c/strong\u003e Since the number of buckets in an SMB sink is always a power of 2, we can come up with a joining scheme across sources with different numbers of buckets based off of a desired level of parallelism specified by the user. For example, if the user wants to join Source 1 with 4 buckets and Source 2 with 2 buckets, they can specify either:\u003cul\u003e\u003cli\u003e\u003cstrong\u003eMinimum parallelism,\u003c/strong\u003e or “Merge Greatest Buckets” strategy: 2 parallel readers will be created. Each reader will read 2 buckets from source A and 1 from source B, merging them together. Because bucket IDs are assigned by taking the integer hash value of the key modulo the desired number of buckets, mathematically we know that the key spaces of the merged buckets overlap.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eMaximum parallelism,\u003c/strong\u003e or “Least Bucket Replication” strategy: 4 parallel readers will be created. Each reader will read 1 bucket from Source A and 1 from Source B. After merging each key group, the reader will have to rehash the key modulo the greatest number of buckets, to avoid emitting duplicate values. Therefore, even though this strategy achieves a higher level of parallelism, there is some overhead of computing duplicate values and rehashing to eliminate them.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eAuto parallelism:\u003c/strong\u003e Creates a number of readers between minimal and maximal amounts, based on a desired split size value provided by the Runner at runtime.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"459\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-250x164.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-768x504.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-120x79.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3.png 1115w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSortedBucketTransform:\u003c/strong\u003e A common usage pattern is for pipelines to enrich an existing dataset by joining it with one or more other sources, then writing it to an output location. We decided to specifically support this in SMB with a unique PTransform that reads, transforms, and writes output using the same keying and bucketing scheme. By doing the read/transform/write logic per bucket on the same worker, we can avoid having to reshuffle the data and recompute buckets — since the key is the same, we know that the transformed elements from bucket M of the inputs also correspond to bucket M in the output, in the same sorted order as they were read from.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"320\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-250x114.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-768x351.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-120x55.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4.png 902w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExternal Sort:\u003c/strong\u003e We made a number of improvements to Beam’s \u003ca href=\"https://github.com/apache/beam/tree/master/sdks/java/extensions/sorter\"\u003eexternal sorter extension\u003c/a\u003e, including replacing the Hadoop sequence file with the native file I/O, removing the 2GB memory limit, and reducing disk usage and coder overhead.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eAdoption — Core Data Producers\u003c/h2\u003e\n\n\n\n\u003cp\u003eSince SMB requires data to be bucketed and sorted in a specific fashion, the adoption naturally starts from the producer of that data. A majority of the Spotify data processing relies on a few core data sets that act as single sources of truth for various business domains like streaming activities, user metadata and streaming context. We worked with the maintainer of these data sets to convert a year’s worth of data to SMB format.\u003c/p\u003e\n\n\n\n\u003cp\u003eImplementation was straightforward since SortedBucketSink is mostly a drop-in replacement for the vanilla Avro sink with some extra settings. We were using Avro sink with the sharding option to control the number and size of output files. After migrating to SMB, we did not notice any major bump in terms of vCPU, vRAM, or wall time since sharding requires a full shuffle similar to the additional cost of SMB sinks. A few other settings we have since had to tweak:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eAgree on user_id as a hexadecimal string as bucket and sort key, since we need the same key type and semantic across all SMB datasets.\u003c/li\u003e\u003cli\u003eSet compression to DEFLATE with level 6 to be consistent with the default Avro sink in Scio. As a nice side effect of data being bucketed and sorted by key, we observed ~50% reduction in storage from better compression due to collocation of similar records.\u003c/li\u003e\u003cli\u003eMake sure output files are backwards compatible. SMB output files have “bucket-X-shard-Y” in their names but otherwise contain the same records with the same schema. So existing pipelines can consume them without any code change; they just do not leverage the speedup in certain join cases.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eAdoption — Wrapped 2020\u003c/h2\u003e\n\n\n\n\u003cp\u003eOnce the core datasets were available in SMB format, we started Wrapped 2020, building off the work left from the Wrapped 2019 campaign. The architecture was meant to be reusable and was a great place to start. However, the source of data was a large, expensive Bigtable cluster that had to be scaled further up to handle the load of Wrapped jobs. We wanted to save cost and time by moving from Bigtable to SMB sources. This year we also needed to handle new complex requirements for filtering and aggregating streams. This required us to join a large dataset containing stream contextual information to the user’s listening history. This would have been nearly impossible or at the very least extremely expensive because of the considerable size of each of these joins. Instead we tried using SMB to eliminate that join completely and avoid using Bigtable as our listening history source.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo compute Wrapped 2020, we had to read from three main data sources for streaming activity, user metadata and streaming context. These three sources had all the data we needed to generate each person’s Wrapped while filtering based on listening context. Previously, the Bigtable had 5 years’ worth of listening history already keyed by user_id. Now, we are able to read data already keyed by user_id from these three sources through SMB. We then aggregated a year’s worth of data per key to calculate each user’s Wrapped.\u003c/p\u003e\n\n\n\n\u003cp\u003eBecause 1 of the 3 main sources are partitioned hourly while the other 2 are partitioned daily, it would be problematic to read a year’s worth of data in one job due to the excessive number of concurrent reads from the hourly partitioned source. Instead, we first ran smaller jobs that would aggregate a week’s or day’s worth of play counts, msPlayed, and other information on each user. From there, we then aggregated all these smaller partitions to a singular partition of data that would hold a year’s worth of data. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"218\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-700x218.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-700x218.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-250x78.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-768x240.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-1536x479.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2-120x37.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image2.png 1692w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eSMB made this relatively easy. We used sortMergeTransform to combine our three sources of data, read each one keyed by user_id, and write our Wrapped output (play counts, ms played, play context, etc.) in SMB format. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"353\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-700x353.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-700x353.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-768x387.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM-120x60.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/Screen-Shot-2021-02-11-at-9.59.05-AM.png 1246w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eFinally, we ran our aggregate job that uses sortMergeGroupByKey to read all Wrapped weekly partitions of SMB, combine a year’s worth of data, and write the output so later jobs can calculate the rest of Wrapped. A key point of flexibility here is that the aggregate job can take any mix of weekly and daily partitions, which is incredibly helpful logistically when running these jobs. The end result in practice looks something like this:\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"218\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-700x218.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-700x218.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-250x78.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-768x240.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-1536x479.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6-120x37.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image6.png 1692w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThis ended up being a huge cost savings for us in this year’s Wrapped project. By leveraging SMB, we managed to join roughly a total of 1PB data without using conventional shuffle or Bigtable. We estimate around a 50% decrease in Dataflow costs this year compared to previous years’ Bigtable-based approach. Additionally, we avoided scaling the Bigtable cluster up two to three times its normal capacity (up to around 1,500 nodes at peak) to support the heavy Wrapped jobs. This was a huge win in this year’s campaign as we were able to bring a wonderful experience in a more cost effective way than ever before.\u003c/p\u003e\n\n\n\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\n\n\n\u003cp\u003eBy adopting SMB, we were able to perform extremely large joins that were previously either unfeasible or cost-prohibitive, or that required custom workarounds like Bigtable. We achieved significant cost savings and opened up more ways of optimizing our workflows. There’s still much work to be done. We look forward to migrating more workflows to SMB, while handling more edge cases like data skew, composite keys, and more file formats.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "In this post we'll discuss how Spotify optimized and sped up elements from our largest Dataflow job, Wrapped 2019, for Wrapped 2020 using a technique called Sort Merge Bucket (SMB) join. We'll present the design and implementation of SMB and how we incorporated it into our data pipelines. Introdu",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image1.gif",
      "date_published": "2021-02-11T00:00:00Z",
      "author": {
        "name": "Published by Neville Li, Claire McGinty, Sahith Nallapareddy, \u0026 Joel Östlund"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/",
      "title": "\n                                            Introducing XCMetrics: Our All-in-One Tool for Tracking Xcode Build Metrics\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eJanuary 20, 2021\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/\" title=\"Introducing XCMetrics: Our All-in-One Tool for Tracking Xcode Build Metrics\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif\" alt=\"\" loading=\"lazy\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e We just open sourced \u003ca href=\"https://xcmetrics.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCMetrics\u003c/a\u003e — a tool for Apple’s developer software, Xcode, that lets you collect, display, and track the valuable metrics hiding inside your team’s Xcode build logs. Are your build times improving or regressing? Which version of Xcode is slowest? Which hardware setup is fastest? XCMetrics makes it easy to find out all this and more. Made for iOS engineers, by iOS engineers, the tool is written completely in Swift, so it’s easy to customize. Use it to track the metrics you want — and get insights that can help improve both developer experience and productivity.\u003c/p\u003e\n\n\n\n\u003ch2\u003eThe problem: Where do you get good Xcode build data?\u003c/h2\u003e\n\n\n\n\u003cp\u003eYou’ve read before on this blog about how our infrastructure teams are always finding new and innovative ways to make \u003ca href=\"https://engineering.atspotify.com/2020/07/22/leveraging-mobile-infrastructure-with-data-driven-decisions/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003edata-driven decisions\u003c/a\u003e. But what if you don’t have access to good data in the first place? This is especially challenging for iOS engineers given that most of that platform’s tools are closed source, making it especially tricky to customize them to your needs.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor example, when we first introduced Swift to our music app, a requirement that we set for ourselves was not to worsen the developer experience. One metric for that is build time: is adopting Swift slowing our Xcode build times down or speeding them up? And how do we accurately measure that (without using a stopwatch every time we hit run)?\u003c/p\u003e\n\n\n\n\u003ch2\u003eOur first solution: Parse the data from Xcode’s log files\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhenever you run a build in Xcode, whether it’s a test build or a continuous integration build in production, xcodebuild produces a log file called xcactivitylog. Many developers don’t know that this file exists or that it’s useful for inspecting warnings, errors, and other data from past builds, like build times. So, over a year ago we developed and released an open source tool called \u003ca href=\"https://github.com/spotify/XCLogParser\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eXCLogParser\u003c/a\u003e — which parses those xcactivitylog files and makes all that build data more accessible to developers.\u003c/p\u003e\n\n\n\n\u003cp\u003eXCLogParser was created for a simple purpose: unearth the data buried in Xcode’s build logs and make it more human readable. But one piece of feedback we received from various teams after open sourcing XCLogParser is that it still requires substantial time to build the infrastructure for continuously collecting those build logs and maintaining them over time. \u003c/p\u003e\n\n\n\n\u003cp\u003eIt was time for us to build a more full-featured tool — one that could integrate with a production environment composed of distributed teams, and provide better insights over time. A collector and a tracker, not just a parser. And that’s how XCMetrics was born.\u003c/p\u003e\n\n\n\n\u003ch2\u003eA complete solution: Collect, parse, store, track, repeat\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe’ve been developing and testing XCMetrics over the last year, building a whole suite of tools in order to create a complete solution for tracking Xcode build metrics. \u003cstrong\u003eSince introducing this system at Spotify, the tools have been used to collect over one million builds and billions of compilation steps — producing over 10TB of data. \u003c/strong\u003e\u003c/p\u003e\n\n\n\n\u003cp\u003eWith this amount of data, we’ve been able to answer complex questions for our developer teams, such as:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWhich function takes the longest to typecheck in our project every day? \u003c/li\u003e\u003cli\u003eWhich pull requests introduce a specific warning or compilation failure?\u003c/li\u003e\u003cli\u003eHow should we configure our engineers’ machines in order to maximize their productivity (hardware specs, installed software, etc.)?\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eWe’ve used these insights to improve the everyday experience and productivity of our developers, and we think other organizations will find these kinds of insights valuable, as well. So we are happy to open source XCMetrics with the world — we’re especially excited to see and learn from the insights other teams uncover.\u003c/p\u003e\n\n\n\n\u003ch2\u003eArchitectural overview: Designed for scale and customization\u003c/h2\u003e\n\n\n\n\u003cp\u003eXCMetrics is an all-in-one tool that tracks Xcode build metrics for teams of all sizes. We built it with a flexible and extensible architecture in order to fit as many requirements as possible into its plugin system, allowing for customization of the information collected in every build. \u003c/p\u003e\n\n\n\n\u003cp\u003eXCMetrics is made up of the following components:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eA Swift CLI tool\u003c/strong\u003e that should be invoked in a post-scheme action after every build completes, whose task is to cache and upload build metrics.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eA backend service\u003c/strong\u003e written in Swift receives the log and attaches metadata via a multipart request. The data can be parsed and saved synchronously or asynchronously.\u003cul\u003e\u003cli\u003eIf the configuration specifies parsing logs asynchronously, they are enqueued for processing in a Redis instance.\u003c/li\u003e\u003c/ul\u003e\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eA PostgreSQL database\u003c/strong\u003e — once the log is parsed, the data for each build is inserted into the database, partitioned by day, for easy retrieval and historical analysis.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"280\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-700x280.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-700x280.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-250x100.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-768x307.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-1536x615.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3-120x48.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image3.png 1837w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eGetting started: Which metrics do you want to track?\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe did our best to make XCMetrics as generic and customizable as possible. The only decisions you have to make are where the backend service should be deployed — and, more interestingly, what type of data you would like to collect.\u003c/p\u003e\n\n\n\n\u003ch3\u003eStandard metrics\u003c/h3\u003e\n\n\n\n\u003cp\u003eXCMetrics is distributed as an executable from our GitHub releases page. You can follow the \u003ca href=\"https://xcmetrics.io/docs/getting-started.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGetting Started guide\u003c/a\u003e to learn how to get XCMetrics on your developer’s machine and execute it in a post-action scheme. Once that’s done, the default set of build metrics will be collected and uploaded to your service. You can check out the default set of collected metrics here.\u003c/p\u003e\n\n\n\n\u003ch3\u003eCustom metrics\u003c/h3\u003e\n\n\n\n\u003cp\u003eIf you would like to collect even more metrics, you can wrap the XCMetrics Swift Package in your own package in order to invoke it manually. By doing so, you’ll be able to provide even more metrics to be attached to every build. Some examples are:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eAnonymized version control information to correlate build times with dirty checkout state\u003c/li\u003e\u003cli\u003eThermal throttling of the machine that could affect build times\u003c/li\u003e\u003cli\u003eProject configuration information that could affect build metrics\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eThis is the minimal example of a XCMetrics plugin that collects the thermal throttling state of the machine and attaches it to each build.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"593\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-700x593.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-700x593.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-250x212.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-768x651.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-1536x1302.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2-120x102.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image2.png 1930w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eThe main method forwards the arguments parsing to \u003cem\u003eXCMetrics\u003c/em\u003e. You proceed to create a \u003cem\u003eXCMetricsConfiguration\u003c/em\u003e and add \u003cem\u003eXCMetricsPlugin\u003c/em\u003e to it. Each plugin takes a dictionary of the environment variables passed to the post-action scheme environment and returns a dictionary of the metrics to be collected. You would then distribute your own custom version of XCMetrics and execute it with the same arguments to upload the logs with the new metrics attached.\u003c/p\u003e\n\n\n\n\u003ch3\u003eService deployment\u003c/h3\u003e\n\n\n\n\u003cp\u003eWe provide a \u003ca href=\"https://hub.docker.com/r/spotify/xcmetrics\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDocker image\u003c/a\u003e that has everything needed to deploy the XCMetrics backend in any infrastructure. We also support a one-click deployment to Google Cloud via Google Cloud Run. Our documentation also contains examples on how to deploy to Kubernetes, if you fancy that.\u003c/p\u003e\n\n\n\n\u003cp\u003eNeedless to say, you don’t need a complex DevOps team to deploy and run XCMetrics. It’s made by iOS engineers, for iOS engineers, so simplicity is at its heart.\u003c/p\u003e\n\n\n\n\u003ch2\u003eUsing XCMetrics at Spotify\u003c/h2\u003e\n\n\n\n\u003cp\u003eXCMetrics has been in use in production at Spotify for over one year, and it has allowed us to make more informed decisions in regards to our project structure and investments. We have data pipelines and dashboards that are used every day to monitor the state of our codebase and tools. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"424\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-700x424.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-700x424.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-250x151.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-768x465.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-1536x930.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4-120x73.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/image4.png 1792w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFigures above are for illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWe hope XCMetrics will inspire and help other teams keep track of their build metrics and improve their developer experience.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou can learn more and watch a demo at \u003ca href=\"https://xcmetrics.io\"\u003eXCMetrics.io\u003c/a\u003e. We are happy to receive bug fixes and improvements on \u003ca href=\"https://github.com/spotify/XCMetrics/\"\u003eGitHub\u003c/a\u003e. And make sure to check out our \u003ca href=\"https://github.com/spotify/XCMetrics/blob/master/CONTRIBUTING.md\"\u003econtribution guide\u003c/a\u003e, which explains more advanced concepts of the project.\u003c/p\u003e\n\n\n\n\n\n\n\n\n\n\n\n\u003cp\u003e\u003cem\u003eXcode is a trademark of Apple Inc., registered in the U.S. and other countries.\u003c/em\u003e\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR We just open sourced XCMetrics — a tool for Apple’s developer software, Xcode, that lets you collect, display, and track the valuable metrics hiding inside your team’s Xcode build logs. Are your build times improving or regressing? Which version of Xcode is slowest? Which hardware setup is fas",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2021/01/xcmetrics-open-source-xcode-tool-1.gif",
      "date_published": "2021-01-20T00:00:00Z",
      "author": {
        "name": "Published by Patrick Balestra, Sr. Engineer"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2020/12/03/how-we-built-it-spotify-lite-one-year-later/",
      "title": "\n                                            How We Built It: Spotify Lite, One Year Later\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eDecember 3, 2020\u003c/span\u003e\n                \u003cspan\u003e\n                    Published by Erik Ghonyan (Senior Engineer), Slava Savitskiy (Senior Engineer), and Tommy Tynjä (Engineering Manager)                \u003c/span\u003e\n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2020/12/03/how-we-built-it-spotify-lite-one-year-later/\" title=\"How We Built It: Spotify Lite, One Year Later\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2020/12/Spotify-Lite_B.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eWhat if, for some users, the very best Spotify is a little less Spotify? Spotify Lite started as an experiment that had to be proven, both from a technical and a product-market fit perspective. In 2017, we found that a significant portion of registrations in some of our fastest-growing markets were happening on Android devices much older than what we were used to seeing in North American and European markets. Because of storage constraints, many of our potential users couldn’t install Spotify, and the ones that could weren’t getting the full “Spotify experience”.\u003c/p\u003e\n\n\n\n\u003cp\u003eOur mission was clear: we needed to make Spotify accessible to users with constrained resources, i.e., unreliable networks or phones with limited storage, memory, and low-resolution screens. Now we needed a team.\u003c/p\u003e\n\n\n\n\u003ch2\u003eA flexible, Lite team\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe main Spotify Music Android client is divided into multiple features, all owned by separate teams. But for Spotify Lite, we formed a single, autonomous team to fully own the entire process of designing, developing, and releasing the app. This allowed us to roll out an MVP product in record time.\u003c/p\u003e\n\n\n\n\u003cp\u003eBefore building the new app, the Spotify Lite team — a cross-functional mix of insights, design, product, and engineering — travelled to a number of locations where Lite would be available in order to experience the network and device constraints firsthand. It was absolutely critical to design Spotify Lite with our users in mind, and to experiment and iterate on the streaming experience for cases when devices have poor connectivity or are completely offline. Only then were we able to come up with an optimal, performant solution.\u003c/p\u003e\n\n\n\n\u003cp\u003eIt should be noted that we wouldn’t have achieved success had it not been for the existing tooling that we were able to reuse — tools for enabling recommendations, playback, search, browsing, and instrumentation. We were building on all the work, experience, and knowledge that came before us, giving us the ability to focus on finding solutions for our users.\u003c/p\u003e\n\n\n\n\u003ch2\u003eSpotify Lite: Spotify’s first separate app\u003c/h2\u003e\n\n\n\n\u003cp\u003eCreating a more performant and smaller version of the Spotify app proved to be more challenging than we liked, as the codebase hadn’t been modularized. With these challenges in mind, we decided to build a new separate app from scratch, giving us the ability to quickly iterate, obtain feedback, and innovate freely.\u003c/p\u003e\n\n\n\n\u003cp\u003eSpotify Lite was initially built on an entirely different playback stack than the regular Android app. This allowed Lite to be as small as possible, with minimal memory and network data usage. Having a separate app enabled us to test new performance ideas and to gain insights, such as understanding how application size impacts the new user funnel. We no longer use the initial playback stack, and have evolved towards a tailored setup that guarantees stability and playback quality on unreliable networks.\u003c/p\u003e\n\n\n\n\u003cp\u003eBuilding Lite was a lot like packing a backpack for your travels. With limited space, you have to be selective in what you bring. Only the most crucial and necessary components were carried forward.\u003c/p\u003e\n\n\n\n\u003ch2\u003eA balancing act\u003c/h2\u003e\n\n\n\n\u003cp\u003eShrinking the original Spotify app to create Spotify Lite brought up two crucial questions: What key elements of the original Spotify should remain intact to ensure listeners still get the “Spotify experience”? And what sacrifices do we need to make to ensure Spotify Lite does, in fact, remain light? \u003c/p\u003e\n\n\n\n\u003cp\u003eIn answering the first question, we knew that keeping the brand look and feel was absolutely critical to giving listeners a Spotify they could recognize. So, we used the same design philosophy as the original Spotify Android app. However, given a range of constraints (smaller screens, quick/performant interactions), we had to adapt some of our design choices. For example, information density has to be reviewed with smaller screen sizes and lower resolutions in mind, as well as whether information is still readable on a broken or scratched screen (these phones have been around for a while!). We’ve recently added our heuristics for how to design for these constraints to the overall design strategy so that this is kept in mind for other apps and surfaces, as well.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"444\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-700x444.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-700x444.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-250x159.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-768x487.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-1536x975.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-2048x1300.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_Iphone-Mockups-01-120x76.png 120w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAlong with streamlining the design, we also had to shrink things down under the hood. The team put a lot of work into implementing all the known techniques for binary size reduction, as well as making tradeoffs when selecting features. As the app includes a native shared library for playback, we have experimented with many compiler and linker flags to prioritize a small app size. This includes, for instance, switching to the \u003cem\u003elld\u003c/em\u003e linker, employing link time code optimizations, and disabling certain language features like RTTI. \u003c/p\u003e\n\n\n\n\u003cp\u003eOn the Android side, it was the use of App Bundles for application publishing, optimizing our R8 shrinking, carefully choosing dependency libraries, and stripping unused translations. We made an effort to reduce the install size, too. We store the shared library unpacked in the APK without copying it to the install folder, and allow users to store both the app and its cache and downloads separately on the SD card.\u003c/p\u003e\n\n\n\n\u003cp\u003eAfter the initial larger gains, it became harder and harder to reduce the app size. It was a constant balance between keeping it small while adding additional requested features. Along with monitoring the app download and install sizes in the Google Play Store, we added checks to our continuous delivery pipelines to prevent size bloat.\u003c/p\u003e\n\n\n\n\u003ch2\u003eLite is different\u003c/h2\u003e\n\n\n\n\u003cp\u003eBecause Lite was a brand-new concept, some of our work went beyond the app itself, leading to improvements to Spotify systems that other teams could benefit from, too.\u003c/p\u003e\n\n\n\n\u003cp\u003eBefore Lite, developers could safely assume there was only one Spotify app for any given platform — the Android platform and the Android app were considered one and the same. Backend services — including those providing application views and deciding which features are enabled — were built with that assumption in mind. Some of these assumptions cascaded through many different parts of our internal systems. \u003c/p\u003e\n\n\n\n\u003cp\u003eWhen we added Lite to the mix, developers needed to know exactly which app a user was using, not just what platform they were on. We generalized that issue beyond our own app and built ways to identify all the apps in the Spotify ecosystem. That work paid off again each time anyone introduced a new Spotify app to the Android platform, including our sister apps \u003ca rel=\"noreferrer noopener\" href=\"https://www.spotify.com/us/kids/?utm_source=us-en_brand_contextual_text\u0026amp;utm_medium=paidsearch\u0026amp;utm_campaign=alwayson_ucanz_us_premiumbusiness_kids_brand+contextual-desktop+text+exact+us-en+google\u0026amp;gclid=CjwKCAiA8Jf-BRB-EiwAWDtEGnamKsxw1Yx_w3KgzFDyJ1g4NKVvIUkc9jRA8fBFdlHCkR8pD4iHmBoCMLAQAvD_BwE\u0026amp;gclsrc=aw.ds\" target=\"_blank\"\u003eSpotify Kids\u003c/a\u003e, \u003ca rel=\"noreferrer noopener\" href=\"https://www.spotify.com/us/stations/\" target=\"_blank\"\u003eSpotify Stations\u003c/a\u003e, and \u003ca href=\"https://spotify-everywhere.com/collections/car-audio/products/polestar\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAndroid Automotive\u003c/a\u003e.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe also had to redesign parts of Spotify’s playback library with Lite constraints in mind — taking into account smaller download and installation sizes, memory usage, and the reduced feature set. Similar considerations have been applied to Spotify’s music and image transcoding services.\u003c/p\u003e\n\n\n\n\u003ch2\u003eMaking Lite a big deal\u003c/h2\u003e\n\n\n\n\u003cp\u003eOur ambition is to be the best-in-class Lite app. We are constantly modifying and updating the app to adapt to our users and their ever-evolving needs. As we’ve seen positive adoption of Spotify Lite since launch, we’ve invested in performance improvements, quality, and resilience. We recently rolled out an overhaul of our client architecture to cater to our growing user base and to reduce playback latencies.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe birth of Spotify Lite has given us flexible solutions that our other apps have benefited from. One such example is our backend service that scales down images to use less network traffic. Another is the support for App Bundles, which has allowed us to reduce the app size significantly so that users only download the assets needed for their particular device. Creating a separate app was a first for our build system — one that laid the groundwork for building native dependencies, sharing code components, and setting up crash and ANR reporting for tracking app quality.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are continuing our work to lower the barrier for people to access Spotify. We have our backlog full of ideas and performance improvements we want to keep investing in, not only for Lite but also for our other apps to benefit from.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "What if, for some users, the very best Spotify is a little less Spotify? Spotify Lite started as an experiment that had to be proven, both from a technical and a product-market fit perspective. In 2017, we found that a significant portion of registrations in some of our fastest-growing markets were",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2020/12/Spotify-Lite_B.png",
      "date_published": "2020-12-03T00:00:00Z",
      "author": {}
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/",
      "title": "\n                                            It’s All Just Wiggly Air: Building Infrastructure to Support Audio Research\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 4, 2020\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2020/11/04/its-all-just-wiggly-air-building-infrastructure-to-support-audio-research/\" title=\"It’s All Just Wiggly Air: Building Infrastructure to Support Audio Research\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio-Blog2.gif\" alt=\"\" loading=\"lazy\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2020/10/Klio-Blog2.gif\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003e\u003cstrong\u003eTL;DR \u003c/strong\u003e\u003ca href=\"https://venturebeat.com/2020/10/13/spotify-open-sources-klio-a-framework-for-ai-audio-research/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eWe just open sourced Klio\u003c/a\u003e — our framework for building smarter data pipelines for audio and other media processing. Based on Python and Apache Beam, \u003ca href=\"https://klio.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eKlio\u003c/a\u003e helps our teams process Spotify’s massive catalog of music and podcasts, faster and more efficiently. We think Klio’s \u003ca href=\"https://docs.klio.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eease of use\u003c/a\u003e — and its ability to let anyone leverage modern cloud infrastructure and tooling — has the potential to unlock new possibilities in media and ML research everywhere, from big tech companies to universities and libraries. \u003c/p\u003e\n\n\n\n\u003cp\u003eBut now we’re getting ahead of ourselves. What exactly is Klio and what does it do? Let’s start with the problem of audio itself.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAudio is hard \u003c/h2\u003e\n\n\n\n\u003cp\u003eReally, sound is just wiggly air. At a basic level, every violin concerto, love song, dog bark, and knock-knock joke is the result of air compressing and vibrating, which we sense as it moves bones and hair in our ears. Sound is an invisible force that reaches us in ways that we can’t see, but can feel. And that’s what also makes audio so difficult for machines to parse: Humans can tell the difference between a swooning vocal, a danceable beat, and a buzzing bee. Can we teach machines to hear those differences, too? \u003c/p\u003e\n\n\n\n\u003cp\u003eMachine listening, the field of research focused on getting computers to understand audio, combines expertise and methods from signal processing, music information retrieval, and machine learning — so that all those vibrations in the air result in data that makes a bit more sense for an engineer to work with. When encoded, compressed, and stored on a computer, you’re left with ones and zeroes packed into relatively large binary files. At a glance, a guitar solo can look just like a yodel. So, how do we begin to make sense of it all? And at scale?\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_01-Which-is-which-01-1.gif\" alt=\"\" width=\"580\" height=\"525\"/\u003e\u003cfigcaption\u003e\u003cem\u003eOne is a \u003c/em\u003e\u003ca href=\"https://open.spotify.com/show/71mvGXupfKcmO6jlmOJQTP?si=6vu_5jg7TUaxQmySGfzliA\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003epopular podcast\u003c/em\u003e\u003c/a\u003e\u003cem\u003e, one is \u003c/em\u003e\u003ca href=\"https://open.spotify.com/track/3Zb3SXZdtyNA0Cdq0DWLeC\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e\u003cem\u003eacoustic guitar\u003c/em\u003e\u003c/a\u003e\u003cem\u003e. It’s all just wiggly air. Software can help process the audio — identify voices, find beats per minute, analyze frequencies. But all at once? And 60+ million tracks at a time?\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eOne problem multiplied 60 million times\u003c/h2\u003e\n\n\n\n\u003cp\u003eProcessing massive amounts of large binary files: It was a problem that was only getting bigger at Spotify. We’re adding about 40,000 songs a day and are processing our music catalog — about 60 million songs — on a regular basis, with multiple teams around the world doing work at the same time. Besides the problem of engineering that kind of scale and parallelization, we also wanted a way to tie the processing jobs more closely with the work our audio and ML research teams were doing.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe were already building sophisticated data pipelines that supported AI and ML jobs using \u003ca href=\"https://spotify.github.io/scio\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eScio\u003c/a\u003e, a precursor to Klio. Scio proved to be a flexible, scalable framework that any team could use to \u003ca href=\"https://engineering.atspotify.com/tag/scio/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ebuild smarter data pipelines at scale\u003c/a\u003e. By tying together large database queries, map-filter-reduce operations, natural language processing, and ML models, teams could create better, more personalized playlists, like Discover Weekly, Release Radar, and dozens of others. \u003c/p\u003e\n\n\n\n\u003cp\u003eSo, Scio created a platform for processing massive amounts of data about the audio. But what about processing the audio itself? \u003c/p\u003e\n\n\n\n\u003ch2\u003eA uniquely Spotify problem, a uniquely Spotify solution\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhile processing metadata for the libraries of 299+ million users is impressive, it’s not the same as processing the content itself — those tens of millions of binary audio files that Spotify hosts and serves all over the world. On top of that, Java-based languages weren’t interfacing well with our Python-based research tools for audio and ML.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe knew that if we could build data pipelines that supported large-scale audio processing, there were untold features and personalizations waiting to be unlocked. We just needed a framework that supported it — and that worked as well with our research tools as our engineering tools. \u003c/p\u003e\n\n\n\n\u003cp\u003eIn 2019, an ad hoc team of data engineers, ML researchers, and audio experts outlined the requirements for creating a framework designed especially for processing media. Scio was a model of success, but still just a starting point. This new framework would need to support:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eLarge-file input/output: \u003c/strong\u003eWe wanted to transform audio, videos, images — all kinds of heavy-duty binary media files — in dozens of ways, with both streaming and batch processing.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eScalability, reproducibility, efficiency: \u003c/strong\u003eWhen you’re working with a dataset as large as the world’s music, as well as a burgeoning ecosystem of podcasts, you don’t want to have to redo your work over and over again.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCloser collaboration between researchers and engineers:\u003c/strong\u003e This translated into support for both Python (the lingua franca of both audio processing and ML) as well as non-Python dependencies (e.g., libsndfile, ffmpeg, etc.).\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eIn short, we needed a framework that could production-ize audio processing. This wasn’t just about creating data pipelines for media. It was about doing it at Spotify scale and with support for the latest audio and ML research. Let’s dig into that last requirement first.\u003c/p\u003e\n\n\n\n\u003ch2\u003eResearchers, engineers, and Python: The importance of speaking a common language\u003c/h2\u003e\n\n\n\n\u003cp\u003eAround this time, we noticed that both our researchers and engineers were beginning to get a little tired of the roadblocks preventing their audio work from getting adopted. Audio researchers were making promising breakthroughs, but the cost of getting new approaches integrated into shipping products was becoming increasingly high. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs much as their counterparts in data and ML engineering wanted to help, those engineers were spending much of their time looking after several distinct, bespoke systems for production audio processing, all built and customized for individual teams. In other words, we had smart people all over the company working on audio, but our \u003ca href=\"https://research.atspotify.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eworld-class researchers\u003c/a\u003e and engineers couldn’t work together, until most of the research was rewritten by the engineers. And even then, all that work and effort was siloed.\u003c/p\u003e\n\n\n\n\u003cp\u003eThe solution was simple: Python. It’s the native language of research and well-suited for the engineering problems at hand. Most importantly, allowing everyone to speak without a translation layer puts everyone in a position to focus on what they excel at. Audio and ML researchers get to focus on experimentation and building cutting-edge research tools. Engineers get to focus on building clean, reliable code.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhat is Klio?\u003c/h2\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"2409\" height=\"1868\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_02-What-is-02-B.gif\" alt=\"\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eKlio is a framework for building smarter data pipelines for audio and other binary files, enabling you to production-ize media processing at scale.\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eStreamlined Apache Beam\u003c/strong\u003e for a more ergonomic, Python-native experience for researchers and engineers\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpen graph of job dependencies\u003c/strong\u003e with support for top-down and bottom-up executions\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIntegration with cloud processing engines\u003c/strong\u003e for managed resources and autoscaling production pipelines\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eContainerization of custom dependencies\u003c/strong\u003e for simplified development and easily  reproducible deployment\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eBatch and streaming pipelines\u003c/strong\u003e for continuous processing\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eApache Beam under the hood, Klio in the driver’s seat\u003c/h2\u003e\n\n\n\n\u003cp\u003eIt’s no surprise then that Klio is built on top of \u003ca href=\"https://beam.apache.org/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eApache Beam\u003c/a\u003e for Python, while also aiming to be a more Pythonic experience of Beam. Additionally, Klio offers several advantages over traditional Python Beam for media processing — providing a substantial reduction in boilerplate code (an average of 60%), a focus on heavy file I/O, and standards for connecting multiple streaming jobs together in a jobs dependency graph (with top-down and bottom-up execution). This allows teams to immediately focus on writing new pipelines, with the knowledge that they can easily be extended and connected later. \u003c/p\u003e\n\n\n\n\u003cp\u003eThis ease of use and streamlining of Apache Beam means we can get our state-of-the-art audio research into people’s hands and ears, faster. And while Klio offers this more opinionated way to use Apache Beam for common media processing use cases by default, it also allows the use of core Python Beam at any time if Klio’s opinions don’t fit your use case.\u003c/p\u003e\n\n\n\n\u003ch2\u003eEfficiency, \u003cs\u003eefficiency,\u003c/s\u003e \u003cs\u003eefficiency\u003c/s\u003e (DRY: Don’t Repeat Yourself)\u003c/h2\u003e\n\n\n\n\u003cp\u003eWhen we were developing Klio, we decided to test it by downsampling every track in Spotify’s 60-million song catalog — amounting to well over 100 million audio files in all (including multiple releases of the same song). Downsampling is often the first step of audio analysis, so it’s a great benchmark of what real-world performance might look like. Previously, the fastest we had accomplished this at Spotify was about three or four weeks. With Klio, we did it in six days, and reduced costs by four times. When you think about the number of songs in our catalog, and our quickly growing podcast library, Klio can have a tremendous impact on our teams and our business.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-700x409.png\" alt=\"\" width=\"580\" height=\"338\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-700x409.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-250x146.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-768x449.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-1536x898.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-2048x1197.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio_Blog-Images_10-22_03-Before-After-01-2-120x70.png 120w\" sizes=\"(max-width: 580px) 100vw, 580px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eWith Klio’s streamlined framework, pipelines are more efficient and reliable. We can do in days what took weeks. And since jobs don’t have to be repeated (missing dependencies can be recursively created), you don’t have to run files through the whole pipeline again just to apply one more transformation at the end. \u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eYou’ll find these kinds of optimizations throughout Klio’s implementation. Klio pipelines improve processing time and costs by avoiding duplicate work on already processed audio. And the framework is opinionated — encouraging engineers and researchers to write a pipeline focused on one thing, like finding the timestamps of all the beats to a song or measuring a song’s loudness. By creating reusable building blocks, Klio allows for researchers to build more easily on top of previous research and create graphs of pipelines, leading to features like infinite playlists optimized for your current mood, internal tools that help automate the review of new content, and powerful data that personalizes the Spotify experience for each user.\u003c/p\u003e\n\n\n\n\u003ch2\u003eScale, reproducibility, and clouds. No infra team required.\u003c/h2\u003e\n\n\n\n\u003cp\u003eKlio can be run locally, but it really shines in the cloud — and is ready-made for it. In order to achieve the large-scale processing and reproducibility that we require at Spotify, Klio leverages the best parts of modern cloud infrastructures (like managed resources to autoscale production pipelines) and tooling (like containerization for easier deployments).\u003c/p\u003e\n\n\n\n\u003cp\u003eKlio was designed to be cloud agnostic, and the underlying Apache Beam project is designed to run workloads across any data workflow engine. Right now, it’s configured to work with Google Cloud Platform, but we welcome \u003ca href=\"https://docs.klio.io/en/latest/contributors.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003econtributions\u003c/a\u003e to help get Klio running on AWS, Azure, or another infrastructure. \u003c/p\u003e\n\n\n\n\u003cp\u003eOne thing to note: Current limitations to Beam Python prevent all of its features from being used on every engine, but we expect increased compatibility with Apache Flink and Apache Spark as Apache Beam extends its underlying compatibility with these engines. Preliminary work has also been done testing Klio on Amazon AWS and S3 using Klio’s Direct Runner.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe think this cloud integration (infrastructure as a service) can unlock production bottlenecks, as well as encourage experimentation. Engineering teams can rely on Klio to standardize media processing — using data processing and monitoring tools they’re already familiar with — rather than creating architectures from the ground up. Klio’s ability to autoscale production pipelines to handle variable workloads lets engineers focus on the next thing, rather than constantly tuning workloads.\u003c/p\u003e\n\n\n\n\u003ch2\u003eFrom Sing Along to dolphin songs: Open and the great unknown\u003c/h2\u003e\n\n\n\n\u003cp\u003eKlio began as a proof of concept a little less than two years ago. It was invented out of necessity — to overcome challenges we were facing internally. But even from the very beginning, it was built with the intention of being free and open source software. \u003c/p\u003e\n\n\n\n\u003cp\u003eAs we’ve seen with \u003ca href=\"https://backstage.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our open platform for building developer portals, \u003ca href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eSpotify is committed to open source and developer experience\u003c/a\u003e. We want to make the lives of engineers easier, so they can focus on building amazing things. So we’re excited to see not only how Klio can help others and advance audio/media research, but also what we can learn from others’ contributions and how Klio can evolve as a result. \u003c/p\u003e\n\n\n\n\u003cp\u003eBefore and after Klio, Spotify has been doing this kind of large-scale audio analysis for nearly a decade, extracting and transforming tracks in our catalog on a weekly, daily, and streaming basis. Audio analysis algorithms power our \u003ca href=\"https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eAudio Features API\u003c/a\u003e for fingerprinting songs by their unique attributes (illustrated in this interactive \u003ca href=\"https://www.nytimes.com/interactive/2018/08/09/opinion/do-songs-of-the-summer-sound-the-same.html\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eNew York Times\u003c/a\u003e article), in-house tools, like our automated content review screener; and market-specific features, like our Sing Along feature in Japan — which \u003ca href=\"https://research.atspotify.com/making-sense-of-music-by-extracting-and-analyzing-individual-instruments-in-a-song/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eseparates the vocals from the instruments\u003c/a\u003e as songs are uploaded to the catalog to create interactive versions that people can sing along with.\u003c/p\u003e\n\n\n\n\u003cp\u003eBut as we saw when we open sourced Backstage, the open source community will come up with use cases we never dreamed of. And since Klio enables anyone to do this kind of heavy-duty media processing at scale (not just big tech companies), we’re particularly curious to see what academics and research institutions will build with it. (\u003ca href=\"https://twitter.com/tomncooper/status/1316071741131759617\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eDolphin speech, anyone\u003c/a\u003e?)\u003c/p\u003e\n\n\n\n\u003cp\u003eSo, thank you to the Klio team and to everyone who’s ever used Klio or contributed to its development over the years (including its sibling framework, Scio). And thank you to all those reading this right now and who will contribute to its development in the future. It’s a product that only Spotify could have built. But we’re even more proud now that it’s out there for the world to share. Now let’s \u003ca href=\"https://docs.klio.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eget started\u003c/a\u003e.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "TL;DR We just open sourced Klio — our framework for building smarter data pipelines for audio and other media processing. Based on Python and Apache Beam, Klio helps our teams process Spotify’s massive catalog of music and podcasts, faster and more efficiently. We think Klio’s ease of use — and its",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Klio-Blog2.gif",
      "date_published": "2020-11-04T00:00:00Z",
      "author": {
        "name": "Published by David Riordan and Lynn Root"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/",
      "title": "\n                                            Spotify’s New Experimentation Platform (Part 2)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eNovember 2, 2020\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/\" title=\"Spotify’s New Experimentation Platform (Part 2)\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2020/11/Experimentation-Platform_Part-II_A.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eSo you’ve read \u003ca href=\"https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePart I\u003c/a\u003e of our two-part series about the new Experimentation Platform we’ve built at Spotify, and now know why we decided to invest in a new platform. In Part II, you’ll get a more detailed look at how we assigned users to experiments, how we analyze results and ensure test integrity. \u003c/p\u003e\n\n\n\n\u003ch2\u003eCoordination, holdbacks, and exclusivity\u003c/h2\u003e\n\n\n\n\u003cp\u003eA lot of the experiments we run change some small aspect of the user experience in one of our prime surfaces and it’s important for teams to be aware of what other experiments are running at the same time, as well as what other experiments are running in their field of interest.\u003c/p\u003e\n\n\n\n\u003cp\u003eTo accommodate for this, we allow experiments to be put into a “domain”. Domains roughly map different surfaces or systems in our service. Each domain has a timeline that shows what experiments have been running and what’s upcoming.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"486\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-700x486.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-700x486.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-250x174.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-768x533.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-1536x1067.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1-120x83.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_1.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003eFor illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWhen a lot of teams experiment in the same proximity, there’s risk of interaction effects. For this reason many experiments need to run in an exclusive manner, where a user can only be in one of a set of experiments that can potentially impact each other. Currently only experiments in a single domain can be exclusive to each other. We’re planning to decouple exclusivity from the domain concept, to allow for experiments across domains to also be exclusive to each other. \u003c/p\u003e\n\n\n\n\u003cp\u003eWe implement holdbacks (the practice of exempting a set of users from experiments and new features, in order to see long-term effects and combined evaluation) in domains. Each domain can have a set of holdbacks. Users in these holdbacks are exempt from the general experimentation that happens in the domain. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt Spotify we have established a pattern where at the start of a quarter, we create a new holdback. Experiments that run throughout the quarter will never be assigned to any of those users subject to the holdback. When the quarter ends, a single test is run on these users where the combined experience of all (successful) experiments is given to the treatment group. This way we can get a read for the compound effect of everything the team decided to ship during the quarter. Once this test is done, the holdback is released and these users will go into new experiments. \u003c/p\u003e\n\n\n\n\u003ch2\u003eThe Salt Machine\u003c/h2\u003e\n\n\n\n\u003cp\u003eAt Spotify, autonomous are teams free to move at schedules that fit them best. This means that they need to be able to start and stop experiments at any time. With requirements of exclusivity and holdbacks, assigning users to experiments gets quite complex if we do not want to compromise on randomization (and we do not want to).\u003c/p\u003e\n\n\n\n\u003cp\u003eWe have developed something we call the “salt machine” that automatically reshuffles users without the need to stop all experiments. This is done by hashing users into buckets using a tree of “salts” (it’s worth noting that if two experiments are disjoint because of targeting, we do not have to use the same salt tree for them).\u003c/p\u003e\n\n\n\n\u003cp\u003eFor this article, imagine that we split users into 8 buckets:\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"126\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-700x126.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-700x126.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-250x45.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-768x138.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-1536x276.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2-120x22.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_2.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eA user ends up in bucket 1 if HASH(user id, SALT) % 8 = 1 and so forth. We allocate buckets to experiments. In the image below, experiment E1 has been allocated buckets 0 and 1. Note that we also have a per-experiment salt to spread users from the allocated buckets over treatments, but for simplicity we omit that from the images in this article. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"146\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-700x146.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-700x146.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-250x52.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-768x160.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-1536x320.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3-120x25.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_3.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eSince we’re experimenting a lot, most buckets are always allocated to an experiment. So what happens when two experiments (E1and E1) end, releasing some space that can be allocated to a new experiment?\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"289\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-700x289.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-700x289.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-250x103.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-768x317.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-1536x635.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4-120x50.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_4.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eNow 50% of the buckets are free and we want to start E4 that needs 25% of the population.  How can we allocate buckets safely without jeopardizing randomization? If we were to pick only bucket 0 and 1 we would have a 100% overlap with experiment E1 that just ended, which might lead to biased results due to carryover effects. Not good.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"122\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-700x122.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-700x122.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-250x43.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-768x133.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-1536x267.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6-120x21.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_6.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhat if we shuffled the free users into new buckets using a new salt?\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"279\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-700x279.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-700x279.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-250x100.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-768x306.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-1536x612.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7-120x48.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_7.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eNow we have eight free buckets we can use for experiments. But because of the dilution of bucket size (those new eight buckets only get 50% of the traffic), we need to allocate four of them to E4 to get 25% of the population. We call the amount of required overallocation of buckets the “compensation factor” — and in this case it’s 1/.50 = 2. The remaining four buckets can be allocated to some other experiment.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-700x302.png\" alt=\"\" width=\"579\" height=\"250\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-700x302.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-250x108.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-768x332.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-1536x663.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8-120x52.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_8.png 1600w\" sizes=\"(max-width: 579px) 100vw, 579px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWhen experiment E3 ends we can completely get rid of salt 1 — but because we diluted the buckets the released space cannot be used until E4 finishes. In effect, we’re wasting 50% of the users. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe compensation factor changes all the time as experiments start and end. Over time we have learned that it’s good practice to not start new experiments if the compensation factor is higher than 5 (the higher the compensation factor, the more space is being wasted).\u003c/p\u003e\n\n\n\n\u003cp\u003eWe’re currently working on the second iteration of our allocation scheme where we believe we waste less space but still maintain the benefits of randomization.\u003c/p\u003e\n\n\n\n\u003ch2\u003eAnalysis\u003c/h2\u003e\n\n\n\n\u003cp\u003eTo conduct a well-designed experiment we need to decide up front what we want to measure and test. The Experiment Planner asks that all necessary information be specified when an experiment is created.  A metric can have one of two roles in an experiment:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eSuccess metrics to find evidence for the hypothesis.\u003c/li\u003e\u003cli\u003eGuardrail metrics to find evidence that the experiment is not introducing any harmful side effects. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eFor each success metric it is possible to choose either a one- or two-sided statistical test. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"417\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-700x417.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-700x417.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-250x149.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-768x457.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-1536x914.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9-120x71.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_9.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eFor success metrics, we perform superiority tests and require a relative minimum detectable effect (MDE) to be specified. This is used in power calculations in the result analysis and also in the sample size calculator. \u003c/p\u003e\n\n\n\n\u003cp\u003eFor guardrails, we perform a non-inferiority test where a non-inferiority margin has to be specified so we know when a change is considered non-inferior or not. \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"243\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-700x243.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-700x243.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-250x87.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-768x266.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-1536x533.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10-120x42.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_10.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eIf the experimenter wants to see results as the experiment is running, they need to choose sequential testing. If they decide to do a fixed horizon test, the results will only be available once the experiment is stopped. Regardless, to minimize weekday biases we recommend that tests are always run for the planned period and are only stopped early if harmful side effects are detected. We also have an optional (but highly recommended) gradual ramp-up assignment of the experiment over a time period to further minimize possible weekday effects. \u003c/p\u003e\n\n\n\n\u003cp\u003eWith a potentially large number of metrics, targeting, different statistical tests, and many treatment groups, it’s not always easy to calculate an accurate required sample size. For this reason we’ve built a sample size calculator and put it into the platform (it’s optional to use for fixed horizon tests, but required for sequential testing).  \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"266\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-700x266.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-700x266.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-250x95.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-768x291.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-1536x583.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11-120x46.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation_11.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eAs can be seen above (numbers redacted), the sample size calculator shows how many users are needed to power the metric for the specified target population. The calculator automatically queries historical data for the specified target population to proxy the control group average and variance needed for the calculations.\u003c/p\u003e\n\n\n\n\u003ch2\u003eValidity checks\u003c/h2\u003e\n\n\n\n\u003cp\u003eMany things can go wrong when we run an experiment, and even subtle issues can have a big impact on the result. For this reason we’re continuously monitoring all running experiments for potential problems. If a problem is detected, we notify the owning team so that they can decide what to do.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe have the following checks in place:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSample ratio mismatch:\u003c/strong\u003e We make sure that the targeted proportion between the treatment groups align with exposure. If we see a statistically significant difference, we sound the alarm.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ePre-exposure activity: \u003c/strong\u003eWe see if there’s any difference in activity between the groups prior to the experiment starting. \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eIncreases in crashes:\u003c/strong\u003e We ensure that we do not see an increase in client crashes.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProperty collisions:\u003c/strong\u003e If two experiments use the same Remote Configuration properties (and are not exclusive to each other), we will warn that the experiments might not get the exposure that was expected. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eFor checks that require a statistical test, we deploy sequential testing and correct for multiple comparisons. \u003c/p\u003e\n\n\n\n\u003ch2\u003eRollouts\u003c/h2\u003e\n\n\n\n\u003cp\u003eA use case supported by the Experimentation Platform, in addition to experimentation, is gradual rollouts. Once we learn that our change improves the user experience, we want to ship it, and with gradual rollouts we can do that while protecting against unexpected regressions. \u003c/p\u003e\n\n\n\n\u003cp\u003eThere are two ways of doing rollouts: with or without statistical testing. If we select the latter, we will be able to select a set of guardrail metrics and deploy sequential testing so we can continuously monitor the progress. Every day we also provide one of three recommendations to the owning team:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe cannot detect any harmful effects, so the recommendation is to continue the rollout.\u003c/li\u003e\u003cli\u003eWe have statistical evidence of harmful effects, so we recommend aborting the rollout.\u003c/li\u003e\u003cli\u003eWe do not know yet, and we recommend continuing with caution or wait until we have more data.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eThe ability to get metrics for rollouts is fairly new so we’re still iterating on it, but we plan to make it the default option going forward.\u003c/p\u003e\n\n\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have spent the last two years rebuilding our experimentation capabilities at Spotify. The new platform is a step change in ease of use and capabilities, but we still feel it’s early for experimentation at Spotify.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are constantly evolving our Experimentation Platform and practices. If you would like to know more, or if you’re interested in joining the team and contribute to our journey, do not hesitate to reach out.\u003c/p\u003e\n\n\n\n\u003cp\u003eJohan Rydberg, \u003ca href=\"mailto:jrydberg@spotify.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ejrydberg@spotify.com\u003c/a\u003e / @datamishap\u003cbr/\u003eExperimentation Lead\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "So you’ve read Part I of our two-part series about the new Experimentation Platform we’ve built at Spotify, and now know why we decided to invest in a new platform. In Part II, you’ll get a more detailed look at how we assigned users to experiments, how we analyze results and ensure test integrity.",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2020/11/Experimentation-Platform_Part-II_A.png",
      "date_published": "2020-11-02T00:00:00Z",
      "author": {
        "name": "Published by Johan Rydberg"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/",
      "title": "\n                                            Spotify’s New Experimentation Platform (Part 1)\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eOctober 29, 2020\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/\" title=\"Spotify’s New Experimentation Platform (Part 1)\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png 4209w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01-120x60.png 120w\" sizes=\"(max-width: 4209px) 100vw, 4209px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2020/10/Experimentation-Platform_01.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eAt Spotify we try to be as scientific as possible about how we build our products. Teams generate hypotheses that we test by running experiments — normally in the form of an A/B test — to learn what works and what doesn’t. The learnings give us insights and fuel new product ideas.\u003c/p\u003e\n\n\n\n\u003cp\u003eWant to know why Spotify decided to build a new Experimentation Platform and how it works? In this two-part series, we’ll share what led us to throw out our old A/B testing platform (called ABBA) and details around the new architecture we’ve chosen to substitute it.\u003c/p\u003e\n\n\n\n\u003ch2\u003eEarly days\u003c/h2\u003e\n\n\n\n\u003cp\u003eToday almost all product decisions are made with some input from one or more A/B tests. But it hasn’t always been like that. Back when Spotify was a small startup in Sweden, a team, simply called Analytics, played around with various kinds of tests. \u003c/p\u003e\n\n\n\n\u003cp\u003eOver time, interest in A/B testing grew, and in 2013 we decided to spin up a team to take on building a more robust system. Thus was born \u003ca href=\"https://open.spotify.com/track/2PzCOP5Aj9SABiBgNEZ52G\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eABBA\u003c/a\u003e, an A/B testing system that allowed us to (more) easily run experiments. Now we had a place to see what A/B tests were actually running, and a pipeline that computed results. The introduction of the system was a step change in productivity, and over time it was integrated into pretty much every aspect of Spotify — in our desktop clients and mobile clients, backend services and data pipelines, in-app messaging, and email campaigns. \u003c/p\u003e\n\n\n\n\u003cp\u003eABBA as a system was quite simple. Each experiment (or rollout) mapped one to one to a feature flag, named after the experiment. When a client fetched the value of the feature flag it got back the name of the treatment group — e.g., “Control” or “Enabled” or “Sort according to color” — anything the user decided to name the group. (Fun trivia: some users of ABBA started encoding more elaborate configurations as JSON in the group names. Life finds a way). Every time a feature flag value was resolved, an event was logged, which fed into the exposure and results pipelines. For each experiment, only a small number of metrics were calculated. Many of these metrics were not very sensitive, leading to almost all analysis being performed manually in notebooks.\u003c/p\u003e\n\n\n\n\u003ch2\u003eWhy we decided to build a new system\u003c/h2\u003e\n\n\n\n\u003cp\u003eAround 2017, the system began to reveal its limitations. We had a few big projects that required a lot of experimentation, and the sentiment at the company was that the system needed to improve.\u003c/p\u003e\n\n\n\n\u003cp\u003eAt a hack week in late 2017, a few senior engineers gathered to sketch out a new system, which aimed to address the following challenges (as well as some others):\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eReduce Time:\u003c/strong\u003e The 1-1 mapping between an experiment and the feature flag led to some interesting side effects. If there were a problem with the experiment (and often there were) and it needed to be restarted, we simply couldn’t just … restart it. A new experiment would have to be created, and the software would need to be updated to use the new feature flag. The new system would have to reduce the time it took to complete this cycle.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eProduce less events\u003c/strong\u003e: The volume of events that were logged by the A/B testing system had over time grown to almost 25% of our total event volume. This drove up the cost of processing, and the volume of events caused incidents in the event delivery system.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eImproved analysis:\u003c/strong\u003e The metrics that ABBA provided out of the box were no way near enough for our analysis needs, and our data scientists were getting tired of performing analyses in notebooks. It was time consuming, and we also didn’t have any consistency across the company when it came to how experiments were analyzed. The new system would have to allow us to add custom metrics and we needed a solid analysis methodology.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSophisticated coordination:\u003c/strong\u003e Over time our needs for how we allocated users to experiments changed, which was done manually by coordinating bucket ranges between teams. This was of course error prone — if someone ended up using the wrong buckets a whole slew of experiments would be impacted; the new system would have to address this.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch2\u003eThe Experimentation Platform\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe new experimentation system, dubbed “The Experimentation Platform”, is composed of three parts:\u003c/p\u003e\n\n\n\n\u003col\u003e\u003cli\u003e\u003cstrong\u003eRemote Configuration\u003c/strong\u003e – replaces our feature-flagging service. Instead of “flags”, its model is based on “properties” — a configurable aspect of one of our clients or backend services. An example of a property could be the color of our buttons, or the number of tracks in the top list. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"2\"\u003e\u003cli\u003e\u003cstrong\u003eMetrics Catalog\u003c/strong\u003e – a managed environment for running SQL pipelines to ingest metrics into a data warehouse, from where data can be served with sub-second latency to UIs and notebooks. \u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003col start=\"3\"\u003e\u003cli\u003e\u003cstrong\u003eExperiment Planner\u003c/strong\u003e – manages and orchestrates experiments. This is the part of the platform users interact with when they want to run an experiment.\u003c/li\u003e\u003c/ol\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"505\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-700x505.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-700x505.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-250x180.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-768x554.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-1536x1109.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1-120x87.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-1.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003ch2\u003eRemote Configuration\u003c/h2\u003e\n\n\n\n\u003cp\u003eRemote Configuration is a way to change the experience a user receives. This is done through controlling the values of a set of “properties” of the client. A property is a variable with a type (enum or integer) and a default value, and can represent the appearance or behavior of pretty much anything. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"525\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-700x525.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-700x525.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-250x188.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-768x576.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-1536x1153.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2-120x90.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-2.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe example above shows an imaginary version of Spotify’s home page in our mobile apps. It’s made up of a set of shelves, and each shelf has a set of cards. With Remote Configuration properties we can control elements for any purpose, i.e. the number of shelves or font sizes on the home page for experiments, rollouts, or personalization or localization. \u003c/p\u003e\n\n\n\n\u003cp\u003eThe properties are defined in a yaml file living next to the code that uses it. When the code is built, all properties and their default values are gathered and published via an API to the admin interface together with the ID of the client being built and the version number. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"351\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-700x351.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-700x351.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-250x125.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-768x385.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3-120x60.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-3.png 1420w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe default value is critical. It allows us to have a programmatic understanding of what the end user experience will be if a client fails to fetch or apply property values. Also, we only have to transfer values to the client when they differ from the default, which saves a lot of time and data traffic when the client starts up. We know what defaults a client has since it identifies itself with the version number. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"234\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-700x234.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-700x234.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-250x84.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-768x257.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-1536x514.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4-120x40.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-4.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eThe way different users get different values for properties is through a set of policies that is evaluated when a client requests the configuration. The policy has a set of filtering criteria and a property-value mapping to apply if the filters match. The actual implementation of the policy is a \u003ca href=\"https://facebook.github.io/planout/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ePlanOut\u003c/a\u003e script that the Remote Configuration service executes. \u003c/p\u003e\n\n\n\n\u003cp\u003eAn important side effect of the fetching of property values is that two events are being logged:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eConfig Assigned,\u003c/strong\u003e which lets us know that a user has fetched its values. Besides user information, this log message also identifies which policies were applied. This information is later used to determine which experiments a user was exposed to.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eConfig Applied,\u003c/strong\u003e which lets us know that the device has actually started using the property values. We use this event as the trigger event for exposure. \u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eProperty values are re-fetched in the background at regular intervals, but are only applied when the app is relaunched. The main reason for this is that we do not want the user experience to change mid-session. \u003c/p\u003e\n\n\n\n\u003ch2\u003eMetrics Catalog\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Metrics Catalog is where we manage, store, and serve metrics to the Experimentation Platform. \u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"336\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-700x336.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-700x336.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-250x120.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-768x369.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-1536x737.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5-120x58.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-5.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eOn a high level, raw metric data is fed into a pipeline where it’s joined with information on which experiment groups a user belongs to. This data is then aggregated into a OLAP cube and put into a data warehouse. In front of the data warehouse sits an API that allows other parties to query for information without knowing too much about the underlying storage.\u003c/p\u003e\n\n\n\n\u003cp\u003eExposure is assembled from the Config Assigned and Config Applied messages from Remote Configuration.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"292\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-700x292.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-700x292.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-250x104.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-768x320.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-1536x640.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6-120x50.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-6.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eA user is considered exposed to an experiment if the following is true:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe have a Config Assigned event that assigns the user to one of the groups in the experiment,\u003cp\u003e\u003cstrong\u003eAND\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eWe have a Config Applied event that tells us that the user started using the configuration of the experiment.\u003cp\u003e\u003cstrong\u003eAND OPTIONALLY\u003c/strong\u003e\u003c/p\u003e\u003c/li\u003e\u003cli\u003eThe user exists in one specified “custom exposure source”.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eThe custom exposure sources allow us to define finer-grained exposure events, such as when a user visited a certain page in the mobile app.\u003c/p\u003e\n\n\n\n\u003ch2\u003eExperiment Planner\u003c/h2\u003e\n\n\n\n\u003cp\u003eThe Experiment Planner sits as an orchestrating layer on top of Metrics Catalog and Remote Configuration. This is where we create, launch, and stop experiments, as well as analyze test results.\u003cbr/\u003eThe UI lives in \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our developer portal. All of our internal teams have access to our internal instance of Backstage and are free to create as many experiments as they like.\u003c/p\u003e\n\n\n\n\u003cdiv\u003e\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"512\" height=\"404\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7.png 512w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7-250x197.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-7-120x95.png 120w\" sizes=\"(max-width: 512px) 100vw, 512px\"/\u003e\u003cfigcaption\u003eFor illustrative purposes only.\u003c/figcaption\u003e\u003c/figure\u003e\u003c/div\u003e\n\n\n\n\u003cp\u003eWhen creating an experiment, we have to define the test treatments, what experience users should get for each treatment (by specifying property values), and all the things that go into testing the hypothesis. Having programmatic understanding of available properties in Remote Configuration and their types helps this process and reduces configuration errors. It’s possible to define values for properties belonging to different systems in a single experiment. For example, if Android and iOS are implemented differently, we still can run a single experiment on both platforms.\u003c/p\u003e\n\n\n\n\u003ch2\u003eSummary\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe have spent the last two years rebuilding our experimentation capabilities at Spotify. The new platform is a step change in ease of use and capabilities, but we still feel it’s early for experimentation at Spotify.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe are constantly evolving our Experimentation Platform and practices. If you would like to know more, or if you’re interested in joining the team and contribute to our journey, do not hesitate to reach out.\u003c/p\u003e\n\n\n\n\u003cp\u003eJohan Rydberg, Experimentation Lead\u003c/p\u003e\n\n\n\n\u003cp\u003e\u003ca rel=\"noreferrer noopener\" href=\"mailto:jrydberg@spotify.com\" target=\"_blank\"\u003ejrydberg@spotify.com\u003c/a\u003e / \u003ca rel=\"noreferrer noopener\" href=\"https://twitter.com/datamishap\" target=\"_blank\"\u003e@datamishap\u003c/a\u003e\u003c/p\u003e\n\n\n\n\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\u003c/div\u003e",
      "summary": "At Spotify we try to be as scientific as possible about how we build our products. Teams generate hypotheses that we test by running experiments — normally in the form of an A/B test — to learn what works and what doesn’t. The learnings give us insights and fuel new product ideas. Want to know wh",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2020/10/Experimentation-Platform_01.png",
      "date_published": "2020-10-29T00:00:00Z",
      "author": {
        "name": "Published by Johan Rydberg"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2020/09/29/managing-clouds-from-the-ground-up-cost-engineering-at-spotify/",
      "title": "\n                                            Managing Clouds from the Ground Up: Cost Engineering at Spotify\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-3958\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 29, 2020\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2020/09/29/managing-clouds-from-the-ground-up-cost-engineering-at-spotify/\" title=\"Managing Clouds from the Ground Up: Cost Engineering at Spotify\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png\" alt=\"\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png 2105w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-250x126.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-700x352.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-768x386.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-1536x772.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-2048x1029.png 2048w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1-120x60.png 120w\" sizes=\"(max-width: 2105px) 100vw, 2105px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2020/09/Cost-Engineering_01C-1.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eLike many of those in tech, we invest heavily in our cloud and data infrastructure. While seemingly routine, the ability to manage and scale our infrastructure to support our 299+ million listeners worldwide, 24/7, without missing a beat (or syllable) is crucial for the business and our brand. \u003c/p\u003e\n\n\n\n\u003cp\u003eOn top of that, our infrastructure teams are resolute when it comes to upholding a highly valued cultural goal: enabling our autonomous engineering teams (called squads) to work as freely and quickly as they possibly can. Finish that off with the fact that we’re a growing public company, and we’ve created a challenging problem for our cost engineering team.\u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"493\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-700x493.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-700x493.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-250x176.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-768x541.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-1536x1082.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1-120x85.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Big-Problem_Image-1.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003c/figure\u003e\n\n\n\n\u003ch2\u003eIntroducing Spotify’s Cost Insights tool\u003c/h2\u003e\n\n\n\n\u003cp\u003eManaging costs in our unique situation is no easy feat, but that certainly doesn’t stop us from innovating on the process. We’re leaving behind the days of reducing costs via top-down requests and moving on to finding fun and rewarding ways engineers can strengthen technology while improving the company’s bottom line. Our new \u003ca href=\"https://github.com/spotify/backstage/tree/master/plugins/cost-insights\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCost Insights\u003c/a\u003e product in \u003ca href=\"https://backstage.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage.io\u003c/a\u003e explains cloud costs in a way our engineers can relate to and identifies optimizations that have resulted in some big wins for Spotify.  \u003c/p\u003e\n\n\n\n\u003cfigure\u003e\u003cimg loading=\"lazy\" width=\"700\" height=\"460\" src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-700x460.png\" alt=\"\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-700x460.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-250x164.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-768x505.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-1536x1010.png 1536w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights-120x79.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Insights.png 1600w\" sizes=\"(max-width: 700px) 100vw, 700px\"/\u003e\u003cfigcaption\u003e\u003cem\u003eFigure above for illustrative purposes only.\u003c/em\u003e\u003c/figcaption\u003e\u003c/figure\u003e\n\n\n\n\u003cp\u003eWe’ve found that our engineers see these optimizations as an interesting challenge — they improve cost, performance, and reliability, turning our infrastructure into a lean, green execution machine. The Cost Insights tool brings those optimization opportunities to light, so engineers can move quickly in achieving those wins. \u003c/p\u003e\n\n\n\n\u003ch3\u003eSpotify’s unique take on cost management\u003c/h3\u003e\n\n\n\n\u003cp\u003eMany great cloud cost management tools are on the market, but we were looking for a way to better understand the relationship between a business’s cloud cost and its overall growth. Without broader context, it’s difficult to determine whether $10,000, for example, might be an appropriate amount to spend. \u003c/p\u003e\n\n\n\n\u003cp\u003eAt Spotify, we believe showcasing cost and business data in a meaningful way will empower our engineers to understand where they are spending and optimize quickly. To encourage engineers to take action, their cloud cost tool should be located where other frequently used products and services are to increase productivity.\u003c/p\u003e\n\n\n\n\u003ch3\u003eGrowing the business while being cost conscious\u003c/h3\u003e\n\n\n\n\u003cp\u003eSpotify continues to be heavily focused on growth, allowing teams that may be high spending on cloud costs to continue to do so if it results in growth opportunities for the business. We needed a tool that showcases costs and helps engineers, engineering managers, and product managers to be information driven when deciding between growth initiatives and worthwhile cost optimizations. Cost Insights is a solution that allows for engineering teams to:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003eBecome aware of their cloud spend and how it relates to their business unit’s growth.\u003c/li\u003e\u003cli\u003eUnderstand how cost optimizations should be prioritized compared to the goals for business growth.\u003c/li\u003e\u003cli\u003eReceive clear recommendations on how they can optimize or reduce their spend.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch3\u003eCost Insights features\u003c/h3\u003e\n\n\n\n\u003cp\u003eOur goal was to launch the Cost Insights plugin with a strong foundation and a great potential for growth. The open source version includes three key features:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCost vs. business graph:\u003c/strong\u003e Users track how their team or a specific GCP project is trending compared to their company’s business growth.  \u003c/li\u003e\u003cli\u003e\u003cstrong\u003eDetailed product panels:\u003c/strong\u003e Cost Insights currently includes detailed information on six cloud products. The product panels currently cater to GCP but can be configured to utilize other cloud providers. These panels help users understand the cost of their products down to the resource level and compare their growth over time.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eProject alerting:\u003c/strong\u003e Teams are alerted when project costs exceed a chosen threshold, allowing them to deep dive into cost changes at the resource level.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003ch3\u003ePreventing over-optimizations\u003c/h3\u003e\n\n\n\n\u003cp\u003eDeveloping a cost tool in a growth-focused, autonomous culture can have severe consequences if executed incorrectly. The Cost Insights product is accessible by any Spotifier, so it became essential to be explicit with teams when there is a cost increase to review. We’ve set several thresholds to capture how much a team is spending and to track their growth trends. Only teams that are growing faster than Spotify’s business will be nudged to investigate their spending. \u003c/p\u003e\n\n\n\n\u003ch3\u003eUpcoming on the roadmap\u003c/h3\u003e\n\n\n\n\u003cp\u003eCost Insights allows teams to determine for themselves if the time invested in an optimization is valuable compared to the costs saved. Now that it’s available in open source with \u003ca href=\"http://backstage.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our focus will shift to open sourcing the backend, creating detailed cost breakdowns (SKU level), and delivering alert dismissals that incorporate user feedback.\u003c/p\u003e\n\n\n\n\u003cp\u003eFor more information about Cost Insights, reach out to Janisa Anandamohan at \u003ca href=\"mailto:janisa@spotify.com\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ejanisa@spotify.com\u003c/a\u003e.\u003c/p\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "Like many of those in tech, we invest heavily in our cloud and data infrastructure. While seemingly routine, the ability to manage and scale our infrastructure to support our 299+ million listeners worldwide, 24/7, without missing a beat (or syllable) is crucial for the business and our brand. O",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cost-Engineering_01C-1.png",
      "date_published": "2020-09-29T00:00:00Z",
      "author": {
        "name": "Published by Janisa Anandamohan"
      }
    },
    {
      "id": "",
      "url": "https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/",
      "title": "\n                                            Cloud Native Computing Foundation Accepts Backstage as a Sandbox Project\n",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003csection role=\"main\" id=\"main\"\u003e\n\n                        \n\n                \u003carticle id=\"post-3947\"\u003e\n    \u003cdiv\u003e\n        \n        \n\n        \u003cdiv\u003e\n            \u003cp\u003e\u003cimg src=\"https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cspan\u003eSeptember 24, 2020\u003c/span\u003e\n                \n            \u003c/p\u003e\n        \u003c/div\u003e\n        \n        \u003cp\u003e\u003ca href=\"https://engineering.atspotify.com/2020/09/24/cloud-native-computing-foundation-accepts-backstage-as-a-sandbox-project/\" title=\"Cloud Native Computing Foundation Accepts Backstage as a Sandbox Project\"\u003e\n                        \u003cimg src=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png\" alt=\"Cloud Native Computing Foundation Accepts Backstage\" loading=\"lazy\" srcset=\"https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png 753w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-250x121.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-700x337.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage-120x58.png 120w\" sizes=\"(max-width: 753px) 100vw, 753px\" data-image-size=\"post-thumbnail\" data-stateless-media-bucket=\"rnd-atspotify\" data-stateless-media-name=\"sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png\"/\u003e                    \u003c/a\u003e\n                        \n        \u003c/p\u003e\n\n        \n\n        \n\u003cp\u003eIf you hear faint whooping in the background of your playlists today, it’s just us celebrating a new milestone for Spotify’s open source efforts: \u003ca href=\"https://www.cncf.io/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eThe Cloud Native Computing Foundation (CNCF)\u003c/a\u003e has accepted \u003ca href=\"https://backstage.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage\u003c/a\u003e, our open source developer portal, as an early stage project in the \u003ca href=\"https://www.cncf.io/sandbox-projects/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eCNCF Sandbox\u003c/a\u003e. It’s just the first step in a longer journey with the CNCF, but it’s an important one for Spotify as it underlines our renewed commitment to open source — and developers everywhere.\u003c/p\u003e\n\n\n\n\u003ch2\u003eBackstage + CNCF = 🎉\u003c/h2\u003e\n\n\n\n\u003cp\u003eFor those of you unfamiliar with the CNCF, you may recognize them as the home of such hits as Google’s Kubernetes and Lyft’s Envoy. With such a strong foundation watching over our community’s efforts and such an impressive roster of projects leading the way before us, we have high hopes for the future of Backstage — one of our most ambitious open source projects to date.\u003c/p\u003e\n\n\n\n\u003cp\u003eYou’ve heard us talk about Backstage before on this blog — back in March when we \u003ca href=\"https://engineering.atspotify.com/2020/03/17/what-the-heck-is-backstage-anyway/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eannounced the open source project\u003c/a\u003e and later when we shared \u003ca href=\"https://engineering.atspotify.com/2020/04/21/how-we-use-backstage-at-spotify/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003ehow we use Backstage internally at Spotify\u003c/a\u003e. In a nutshell: Backstage is an open platform for building developer portals. Built around a centralized service catalog, it’s designed to streamline your development environment from end to end. We built it to improve the everyday experience and productivity of developers — initially, our own developers, and then when we open sourced it, all developers, everywhere.\u003c/p\u003e\n\n\n\n\u003ch2\u003eOur commitment to improving developer experience\u003c/h2\u003e\n\n\n\n\u003cp\u003eAt Spotify, Backstage enables us to scale safely and onboard quickly, helping us build and ship the product that hundreds of millions of people around the world use every day. We believe it has the potential to transform how all engineers work together, whether they’re in a 50-person startup or a Fortune 50. \u003c/p\u003e\n\n\n\n\u003cp\u003eHere’s what Backstage can do for companies and tech organizations, and how it improves developer experience:\u003c/p\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eRestore order to software ecosystems.\u003c/strong\u003e For companies whose infrastructure has become a wilderness of competing technologies and orphaned dependencies hiding in the dark corners of their tech stack, the \u003ca rel=\"noreferrer noopener\" href=\"https://backstage.io/blog/2020/06/22/backstage-service-catalog-alpha\" target=\"_blank\"\u003eBackstage Service Catalog\u003c/a\u003e brings back discoverability, accountability, and control — not to mention sanity. Instead of being overwhelmed by fragmentation and information sprawl, the Backstage Service Catalog creates a centralized system for tracking all your software — making it easy for teams to manage 10 services and making it possible for a company to manage thousands of them.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eJumpstart productivity by standardizing software and tooling.\u003c/strong\u003e With software templates, engineers can spin up a new software project in minutes instead of hours. \u003ca rel=\"noreferrer noopener\" href=\"https://backstage.io/blog/2020/08/05/announcing-backstage-software-templates\" target=\"_blank\"\u003eBackstage Software Templates\u003c/a\u003e are like automated getting started guides. After an engineer chooses a template, Backstage takes care of the rest — automatically setting up the repo, deploying the first build, and providing a Hello World project, all ready to go — with your organization’s best practices built right in, right from the start. By reducing the number of low-variance choices a developer is forced to consider when starting a project, templates remove friction and allow developers to spend more cycles solving problems higher up in the stack. Standards can set engineers free.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eGet unstuck with great technical documentation made easy. \u003c/strong\u003eNo one can ever find documentation when they need it — and if they do, it might not be that helpful because it hasn’t been kept up to date. Backstage solves both ends of the problem. With \u003ca rel=\"noreferrer noopener\" href=\"https://backstage.io/blog/2020/09/08/announcing-tech-docs\" target=\"_blank\"\u003eour “docs like code” approach\u003c/a\u003e, engineers write their technical documentation in Markdown files right alongside their code. Whenever you create a new project in Backstage, a TechDocs site is automatically set up in the same repo — so you can update your code and your documentation with the same pull request. This integrated workflow and centralization makes great documentation easy. Easy to create and maintain. And easy to find and use.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCustomize and scale your infrastructure with a growing ecosystems of plugins.\u003c/strong\u003e Every company has their own, homegrown infrastructure — Backstage’s plugin architecture makes it simple to make Backstage a perfect fit for yours. Integrating your custom, proprietary tooling is as simple as building an internal plugin for your installation of Backstage. You can also build open source plugins to share with the community. The open source \u003ca rel=\"noreferrer noopener\" href=\"https://backstage.io/plugins\" target=\"_blank\"\u003eplugin marketplace\u003c/a\u003e for Backstage continues to grow, expanding Backstage’s functionality with each new plugin. It’s like an app store for your infrastructure.\u003c/li\u003e\u003c/ul\u003e\n\n\n\n\u003cp\u003eBackstage has already come a long way — and none of these features would be what they are today without contributions from the open source community.\u003c/p\u003e\n\n\n\n\u003ch2\u003eOur commitment to the open source community\u003c/h2\u003e\n\n\n\n\u003cp\u003eWe were excited by the reception Backstage received when we first released it. But we’ve been even more gratified by how the community of contributors has grown since then, as they’ve built new \u003ca href=\"https://backstage.io/plugins\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplugins\u003c/a\u003e and added new functionality to the core product. Over \u003ca href=\"https://github.com/spotify/backstage/graphs/contributors\" target=\"_blank\" rel=\"noreferrer noopener\"\u003e130 people\u003c/a\u003e have contributed to the project, and roughly 40% of pull requests are now coming from external, non-Spotify contributors.\u003c/p\u003e\n\n\n\n\u003cp\u003eAs Principal Product Manager \u003ca href=\"https://engineering.atspotify.com/2020/04/01/my-beat-stefan-alund/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eStefan Ålund\u003c/a\u003e writes on the \u003ca href=\"https://backstage.io/blog/2020/09/23/backstage-cncf-sandbox\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage blog\u003c/a\u003e:\u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cdiv\u003e\u003cp\u003eWe released the open source version of Backstage ‘early’. That was intentional. Because even though we’ve been using Backstage internally for years, we wanted the open source version to be developed with input and contributions from the community. And that’s exactly the product that’s going into the CNCF Sandbox today.\u003c/p\u003e\u003cp\u003eBackstage’s ability to simplify tooling and standardize engineering practices has attracted interest from other major tech companies, as well as airlines, auto manufacturers, investment firms, and global retailers. We know that Backstage solves a problem — infrastructure complexity — that’s common to a lot of large and growing companies today. But different companies work differently, use particular toolsets, and have unique use cases. By making Backstage open source, we can build it with people working inside a variety of engineering organizations all over the world. It makes for a better product that serves a wider group of users (beyond that of Spotify’s) and their needs.\u003c/p\u003e\u003c/div\u003e\u003c/blockquote\u003e\n\n\n\n\u003cp\u003eThank you to everyone who has already contributed to this project, inside and outside of Spotify. And if you’ve been curious about Backstage, now is the perfect time to dive in. Visit \u003ca href=\"https://backstage.io\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eBackstage.io\u003c/a\u003e to learn more and \u003ca href=\"https://mailchi.mp/spotify/backstage-community\" target=\"_blank\" rel=\"noreferrer noopener\"\u003esubscribe to our newsletter\u003c/a\u003e for updates. Check out open issues on \u003ca href=\"https://github.com/spotify/backstage/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eGitHub\u003c/a\u003e or get started building a \u003ca href=\"https://backstage.io/plugins\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eplugin\u003c/a\u003e for your favorite tool or service. We look forward to seeing the community grow, and can’t wait to see where open source takes us all next.\u003c/p\u003e\n\n\n\n\u003cp\u003eWe’ll give Remy DeCausemaker — Head of Spotify’s \u003ca href=\"https://thenewstack.io/does-your-organization-need-an-open-source-program-office/\" target=\"_blank\" rel=\"noreferrer noopener\"\u003eOpen Source Program Office\u003c/a\u003e (OSPO) — the last word: \u003c/p\u003e\n\n\n\n\u003cblockquote\u003e\u003cp\u003eWe’re excited to embark on this journey with the CNCF community. Backstage isn’t the first open source project Spotify has released, but it is the first one we felt was ready to dedicate to an upstream foundation, and we can’t wait to bring what we’ve learned to the next project. There’s so much great tech being built here, and it’s about time we share it to build even greater products, together.\u003c/p\u003e\u003c/blockquote\u003e\n        \u003cbr/\u003e\n\n        \n        \n\n        \n\n            \u003c/div\u003e\n    \n\n\n\u003c/article\u003e\n                \n\n            \n        \n    \u003c/section\u003e\u003c/div\u003e",
      "summary": "If you hear faint whooping in the background of your playlists today, it’s just us celebrating a new milestone for Spotify’s open source efforts: The Cloud Native Computing Foundation (CNCF) has accepted Backstage, our open source developer portal, as an early stage project in the CNCF Sandbox. It’s",
      "image": "https://storage.googleapis.com/rnd-atspotify/sites/2/2020/09/Cloud-native-computing-foundation-accepts-Backstage.png",
      "date_published": "2020-09-24T00:00:00Z",
      "author": {
        "name": "Published by Spotify Engineering"
      }
    }
  ]
}
