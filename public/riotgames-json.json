{
  "version": "https://jsonfeed.org/version/1",
  "title": "RiotGames",
  "home_page_url": "https://technology.riotgames.com/news/feed",
  "items": [
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/bug-blog-tft-bugs-and-patches",
      "title": "Bug Blog: TFT Bugs and Patches",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv property=\"content:encoded\"\u003e\u003cp dir=\"ltr\"\u003eHello! My name is Alex Sherrell, and I’m the quality owner for \u003cem\u003eTeamfight Tactics\u003c/em\u003e. I’m responsible for making sure \u003cem\u003eTFT \u003c/em\u003ereaches our quality standards, and I work closely with individual teams to establish quality bars, holding them accountable and helping smash bugs and improve the overall player experience. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eI’ve been part of \u003cem\u003eTFT \u003c/em\u003esince early days - I was one of the original 6 members of the team, and it was the first time Riot had really attempted this kind of idea, where a side project became a full game on a short timeline. We needed to have excellent communication and trust in each other to make it work at a quality we were willing to ship. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn this article, I’ll be describing how we handle patches across PC and mobile for \u003cem\u003eTFT\u003c/em\u003e, and how this relates to quality assurance. Later, I’ll tag in my engineering counterpart, Gavin Jenkins, to give a super techy point of view on patches, and we’ll dive into two use cases that demonstrate different types of patches and how we deploy them. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWelcome to the \u003cem\u003eTFT\u003c/em\u003e edition of the \u003ca href=\"https://technology.riotgames.com/news/404-tech-blog-not-found-welcome-bug-blog\" target=\"_blank\"\u003eBug Blog\u003c/a\u003e!\u003c/p\u003e\n\n\u003ch2 dir=\"ltr\"\u003eServer vs Client\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eTo understand patches in \u003cem\u003eTFT\u003c/em\u003e, we first have to explain how we handle game data and where it’s stored. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThere are two locations for information - server and client. \u003cstrong\u003eServer \u003c/strong\u003einformation is shared with players from Riot’s servers. \u003cstrong\u003eClient\u003c/strong\u003e information means the player is getting new data to store on their client (a PC or mobile device).\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eServer changes are significantly easier for players because they don’t affect their machines directly. But client changes require data changing in the local environment. A player’s computer needs to download the graphics files for the sick new skins (client change) but it’s easy to adjust damage ratios (server change). \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAnother important note - server data is stored in two places, Lua and our \u003ca href=\"https://technology.riotgames.com/news/content-efficiency-game-data-server\" target=\"_blank\"\u003eGame Data Server\u003c/a\u003e. Lua holds logic content, like how abilities happen and when to apply damage, while GDS holds raw numbers. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo illustrate the difference between these types of information even further - art assets are stored on a player’s \u003cstrong\u003eclient\u003c/strong\u003e, because otherwise downloading them during the game would be slow. And although a savvy user can manipulate their own client, changing client data like art assets shouldn’t negatively impact other players. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOn the other hand, game data isn’t manipulated by other players, so we store it on the \u003cstrong\u003eserver\u003c/strong\u003e. \u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eThe Different Kinds of Patches\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eNow that we have a general idea of the two kinds of changes - server and client - we can take a closer look at the patch options we have. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/tftbugs_1.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eFull patch: \u003c/strong\u003eThese are for \u003cem\u003eclient changes\u003c/em\u003e, the more extensive kind of download. We release at least one of these every 2 weeks to patch and balance the game. This is required for any kind of large change such as art assets or champion additions. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eMicropatch: \u003c/strong\u003eThese are for \u003cem\u003eserver changes\u003c/em\u003e, the easier kind of adjustment. These changes override existing data on the server. This is great for things like changing tooltips or any ability-related number (damage, health regen, mana, etc) which, as mentioned above, are based on data coming from the server.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eMobile Submissions\u003c/strong\u003e: If you had asked me 3 years ago, “How hard could mobile deploys be?” I’d be like… “There are what, 10 types of phones out there? Those are the ones I’ve seen ads for, anyway.” In today’s reality, though, there are hundreds of phones with wildly ranging specs, which can make a huge difference when it comes to app submissions, which we do through third party stores, like the Google Store or the Apple Store. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThese have their own requirements and submission processes that we need to keep in mind while building out patches for mobile. And because \u003cem\u003eTFT\u003c/em\u003e has \u003ca href=\"https://en.wikipedia.org/wiki/Cross-platform_play\" target=\"_blank\"\u003ecross-play\u003c/a\u003e, we need to consider how these different versions of applications all interact with each other, especially when we’re changing version or game data numbers with micropatches. Luckily, our mobile game does pull from the same servers as the PC versions, so micropatches - which are based on the server - \u003cem\u003edo impact mobile as well\u003c/em\u003e. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eUltimately, when we think about these kinds of deploys, we have to keep a global perspective to fully understand how we’re impacting our players. A full patch - which requires downloading new information - may be pretty simple for someone playing from the PC at home… but what about PC bangs? Or countries with slower download speeds? These kinds of considerations pile up, and heavily impact any bug-related decision making.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eFun fact - micropatches are actually a pretty recent feature that we’ve been able to leverage in the past, oh, four years or so. The ability to make small adjustments that affect any device with limited player impact - PC, Apple, Android, it doesn’t matter - is a total \u003cem\u003egame-changer\u003c/em\u003e.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBut micropatching in Summoner’s Rift can be pretty different from micropatching in \u003cem\u003eTFT \u003c/em\u003e- a simple port of the systems in our initial builds highlighted the ways these game modes process data differently. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo tell this story, I’d like to introduce you to Gavin Jenkins, \u003cem\u003eTFT\u003c/em\u003e engineering manager extraordinaire. I’ll see you again in a few paragraphs for bug-catching time.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch2 dir=\"ltr\"\u003eFrom SR To TFT\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eHey everyone, I’m Gavin Jenkins, and I’m an engineering manager on \u003cem\u003eTFT\u003c/em\u003e. The story of how we translated \u003cem\u003eLeague\u003c/em\u003e micropatches to \u003cem\u003eTFT \u003c/em\u003epatches is pretty interesting, from both a technical and social POV. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSo, remember, we can micropatch two things - Lua scripts and GDS data. Micropatches take that binary data and base 64 encode it, and create a long string which is put in a file on the server. When the game server runs, it loads that file, decodes the string, and checks to see if there’s a micropatch key for the property it’s loading. If so, it overrides the data from the original installation with the micropatched data. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eUltimately a pretty straightforward override system ported directly from \u003cem\u003eLeague\u003c/em\u003e. Or, it should have been, but here’s the catch - data within \u003cem\u003eTFT \u003c/em\u003einteracts with other data in \u003cem\u003eTFT \u003c/em\u003ein totally new ways. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFor example, a change to the carousel would impact multiple systems at the same time. When you upload micropatches for scripts, you have to upload the entire file, effectively locking out any other changes to that script file. Now consider the fact these systems were all originally created by a fleet of designers defining a web of interconnected game systems in script files to accomplish quick iteration to build out \u003cem\u003eTFT\u003c/em\u003e. That means that unlike \u003cem\u003eLeague\u003c/em\u003e, there are a large number of systems in script, so a change to the carousel - which would include multiple champions and items - ends up being a pretty huge set of script file changes that are now locked to any other changes. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/tftbugs_2.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eThe carousel from the Fates set. That’s a lot of items and champs!\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWith this kind of system, you have to base 64-encode the entire file for micropatches. If you wanted to make a \u003cem\u003esecond\u003c/em\u003e micropatch that involves a change to that file, you must again base 64-encode the entire file. But which base 64-encoded representation of that Lua file do you want the game to use as the override? The answer is, well, neither. In order to get both fixes you must base 64-encode the version of the file that has both fixes in it, and have the game use\u003cem\u003e that \u003c/em\u003eas the override.\u003c/p\u003e\n\u003cp\u003eIn other words, if there are two competing micropatches affecting the same file… who wins? What order is it loaded in? What is overwriting what?\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eBuilding Awareness\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eThis whole new world of data interactions meant we had to really investigate to figure out what was wrong. At first, it would look like new bugs would be fixed while also bringing back old bugs. We realized pretty quickly that we’d need to overhaul scripts by treating them more like how engineers structure code - using computer science principles like \u003ca href=\"https://www.geeksforgeeks.org/difference-between-abstraction-and-encapsulation-in-java-with-examples/#:~:text=Abstraction%20is%20the%20method%20of,to%20protect%20information%20from%20outside.\u0026amp;text=Whereas%20encapsulation%20can%20be%20implemented,i.e.%20private%2C%20protected%20and%20public.\" target=\"_blank\"\u003eabstraction and encapsulation\u003c/a\u003e, \u003ca href=\"https://en.wikipedia.org/wiki/Separation_of_concerns#:~:text=In%20computer%20science%2C%20separation%20of,section%20addresses%20a%20separate%20concern.\u0026amp;text=When%20concerns%20are%20well%2Dseparated,%2C%20reuse%2C%20and%20independent%20development\" target=\"_blank\"\u003edata separation\u003c/a\u003e, and \u003ca href=\"https://en.wikipedia.org/wiki/Separation_of_concerns#:~:text=In%20computer%20science%2C%20separation%20of,section%20addresses%20a%20separate%20concern.\u0026amp;text=When%20concerns%20are%20well%2Dseparated,%2C%20reuse%2C%20and%20independent%20development\" target=\"_blank\"\u003estaying DRY\u003c/a\u003e, to optimize the code structure and how we handle scripts to prevent bug regressions. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAnd equally important - we needed to socialize the problem among all designers and engineers on the team, because it would require a pretty drastic shift in how we saw patches and data structure in scripts.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis resulted in some pretty funny lunchtime conversations between \u003cem\u003eTFT\u003c/em\u003e and \u003cem\u003eLeague \u003c/em\u003eengineers, where \u003cem\u003eTFT \u003c/em\u003edevs would be discussing the complexity and problem with micropatches, and \u003cem\u003eLeague\u003c/em\u003e devs would be pretty confused, considering how simple and straightforward \u003cem\u003eLeague \u003c/em\u003emicropatches are. \u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eSustainable Structure\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eThere’s a computer science principle called \u003ca href=\"https://en.wikipedia.org/wiki/Separation_of_concerns#:~:text=In%20computer%20science%2C%20separation%20of,section%20addresses%20a%20separate%20concern.\u0026amp;text=When%20concerns%20are%20well%2Dseparated,%2C%20reuse%2C%20and%20independent%20development\" target=\"_blank\"\u003ethe separation of concerns\u003c/a\u003e which states the need to create distinct concern-based sections to avoid monolithic structural dilemmas. The well-defined core gameplay loop of SR means there’s already a pretty clear separation of concerns with champion scripting, which doesn’t exactly work for a system like the \u003cem\u003eTFT \u003c/em\u003ecarousel. So a critical step in our port of the micropatching tool included reframing what concerns we were basing our separation on. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOnce we reached this understanding, it was a pretty straightforward path to extracting bits that didn’t need to be monolithic and separating the Lua files for ease of micropatching. All of this led us to our current micropatching process, which lets us make informed decisions around different kinds of patches - and also improved maintainability of scripts and \u003cem\u003eTFT \u003c/em\u003egame systems in general. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003ePassing this back to you, Alex, to finish up with a deep dive into two bugs that demonstrate the power of patches… and the effectiveness of our micropatching tool.\u003c/p\u003e\n\u003chr/\u003e\n\n\u003cp dir=\"ltr\"\u003eAlex here! Now that we’ve got an understanding of some of the backend considerations and a bit of a history of our micropatching tool, let’s see these solutions in action with two recent bugs.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eBug 1: Micropatching Mor-evil-lonomicon\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eDecimals are the bane of all existence. When Mor-evil-lonomicon was patched recently, the burn damage wasn’t correctly tracking the ticks per second. Instead of dealing 4% damage every 1.0 second, it tried to deal 4% damage every 100.0 seconds. Battles never last that long, so the damage was just never applied.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/tftbugs_3.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eThe bug as described to players.\u003c/em\u003e\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eThe Solution\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eRemember earlier when we mentioned that game data like damage ratios are stored on the server? That means this bug is the perfect candidate for a micropatch. We used our sweet micropatching tool to quickly swap around decimals so the damage ratios would be correct. It only took about an hour for us to get that change going, so many players didn’t even encounter the issue.\u003c/p\u003e\n\u003ch4 dir=\"ltr\"\u003eChanging Things in Script\u003c/h4\u003e\n\u003cp dir=\"ltr\"\u003eBack in the day, this kind of mistake would require a ticker message explaining the broken item until a fix could be deployed. With our micropatching tool, a fix is a matter of hours, not days. \u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eBug 2: Mobile Art Assets\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eNow let’s take a look at something that can’t be micropatched, and some of the considerations we have to take into account for mobile deploys.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWith the new radiant items in 11.15, the armory is getting restocked with new items to replace the shadow thematic from the first half of the set. An art asset change ended up being applied to both our live patch and a future patch. This meant a double overlay of items, with both the shadow and radiant art assets layered on top of each other on mobile apps.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/tftbugs_4.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eWell, that’s not right.\u003c/em\u003e\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eThe Solution\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eIn this case, we do have to do a full patch because assets are stored on \u003cstrong\u003eplayer clients\u003c/strong\u003e and not in our servers. Because this issue only affected mobile, we only had to redeploy the patch for Apple and Android.\u003c/p\u003e\n\u003ch4 dir=\"ltr\"\u003eGame Versions with Mobile\u003c/h4\u003e\n\u003cp dir=\"ltr\"\u003eThe sneaky bit here is that we have to make sure that server and game clients can still communicate. In this case, mobile apps got a new version number, but PC didn’t. To ensure cross-play would still function, we did several tests to validate that the game clients could still communicate properly. By setting up a 1v1 in our testing environment, my testing partner and I were able to run a suite of tests to confirm cross-play was still possible.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eTFT\u003c/em\u003e’s\u003cem\u003e \u003c/em\u003edevelopment timeline required extensive communication, validation, and a testing mindset across all teams. We continue to think about how we can improve the player\u003cem\u003e \u003c/em\u003eexperience, especially in ways that the game mode diverges from \u003cem\u003eLeague\u003c/em\u003e loops.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003ePatching is something we take really seriously. It can make or break the game… literally. By focusing on customizing tools like the micropatch tool to increase usability and collaboration between designers, QA, engineering, and art, we can hit our quality standards without restricting creativity. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading! Post any comments or questions below.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/126/tftheader.png\" width=\"1600\" height=\"611\" alt=\"\" /\u003e\n\u003cp\u003eIn this article, I’ll be describing how we handle patches across PC and mobile for \u003cem\u003eTFT\u003c/em\u003e, and how this relates to quality assurance. Later, I’ll tag in my engineering counterpart, Gavin Jenkins, to give a super techy point of view on patches, and we’ll dive into two use cases that demonstrate different types of patches and how we deploy them.\u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/126/tftheader.png",
      "date_published": "2021-07-30T17:42:30Z",
      "author": {
        "name": "Alex Sherrell and Gavin Jenkins"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/leveling-networking-multi-game-future",
      "title": "Leveling Up Networking for A Multi-Game Future",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv property=\"content:encoded\"\u003e\u003cp dir=\"ltr\"\u003eHeyo! We’re Cody Haas and Ivan Vidal, and we’re engineers on the Riot Direct team. Our team is responsible for maintaining Riot’s global network - you may remember us from this \u003ca href=\"https://technology.riotgames.com/news/riot-direct-video\" target=\"_blank\"\u003eawesome video\u003c/a\u003e and this \u003ca href=\"https://technology.riotgames.com/news/fixing-internet-real-time-applications-part-i\" target=\"_blank\"\u003eseries of articles\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eRiot Direct exists because playing a game where someone has lag or high latency completely ruins the experience. Next to Wi-Fi, one of the biggest causes of lag and latency is the uncertainties of the internet. Riot Direct’s solution is to bring Riot’s network closer to our players by developing our own backbone network and collaborating with Internet Service Providers around the world. This has allowed us to limit these uncertainties and reduce lag and latency.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIt’s been a while since you’ve heard from our team. So in this article, we’re going to tell you a bit about what we’ve done to reinforce consistent and stable connections, reduce latency, and improve the overall player experience for our entire multi-game portfolio. \u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eWhen Riot Games was really just Riot Game, designing the network and everything inside it was less complicated.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWith a suddenly expanding set of games, we transitioned from a network designed for:\u003c/p\u003e\n\u003cul dir=\"ltr\"\u003e\n\u003cli\u003eOne game\u003c/li\u003e\n\u003cli\u003eGame servers located in a specific place per region and per shard\u003c/li\u003e\n\u003cli\u003ePrimarily one protocol\u003c/li\u003e\n\u003cli\u003eOne latency threshold\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003eTo:\u003c/p\u003e\n\u003cul dir=\"ltr\"\u003e\n\u003cli\u003eMultiple games\u003c/li\u003e\n\u003cli\u003eMultiple locations for game servers for each region\u003c/li\u003e\n\u003cli\u003eMultiple different protocols\u003c/li\u003e\n\u003cli\u003eMultiple latency thresholds\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003eMoving to a multi-game environment while still supporting\u003cem\u003e League of Legends\u003c/em\u003e meant we had to redesign the network while trying to avoid player pain.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eKeep in mind - each game is not just a game server, but the platform, services, and people required for both the game and the infrastructure supporting it. All of that translates into more capacity and new features.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eDoing all of this while dealing with daily operations was a major challenge.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eWhat We’re Working With\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eRiot Direct has been around since 2014, so at this point, we’re pretty robust. This is a great place to start - now we needed to take this \u003cem\u003eLeague\u003c/em\u003e-only network and \u003ca href=\"https://technology.riotgames.com/news/engineering-esports-tech-powers-worlds\" target=\"_blank\"\u003eits esports productions\u003c/a\u003e and adapt + extend it to properly support future games and potentially their esports.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eNot all games are the same when it comes to networking. They have different requirements for protocols, latency, and server location. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eProtocol: \u003c/strong\u003eFast-paced games like\u003cem\u003e League of Legends \u003c/em\u003eand \u003cem\u003eVALORANT \u003c/em\u003erely on the speed of UDP at the expense of reliability, while a slower-paced game like\u003cem\u003e Legends of Runeterra\u003c/em\u003e relies on the slower but more reliable TCP protocol.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eLatency:\u003c/strong\u003e  Even though we all want that sub-15 millisecond ping for \u003cem\u003eLeague of Legends,\u003c/em\u003e the game is still very playable at around 60ms latency on average. \u003cem\u003eLoR\u003c/em\u003e can be played at even higher levels of latency. If you tried to play \u003cem\u003eVALORANT\u003c/em\u003e with over 60ms latency, that would be pretty rough, so \u003cem\u003eVALORANT \u003c/em\u003eoptimized for a larger number of game servers.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eServer Location:\u003c/strong\u003e \u003cem\u003eVALORANT \u003c/em\u003egame servers are located all around the globe. \u003cem\u003eLeague of Legends \u003c/em\u003egame servers, however, are located in Chicago, Amsterdam, Tokyo, Seoul, São Paulo, Santiago, Istanbul, Miami, and Sydney.  \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/rd2021_1.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eVALORANT game server locations across the globe.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/rd20213.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eLeague game server locations across the globe.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003ca href=\"https://en.wikipedia.org/wiki/IPv4_address_exhaustion\" target=\"_blank\"\u003e\u003cstrong\u003eIPv4 Address Exhaustion\u003c/strong\u003e\u003c/a\u003e: IPv4 addresses are stored in a 32-bit unsigned integer, which means there are approximately 4,294,967,296 possible addresses. The number of available publicly routable addresses is even lower, because there are sets of rules that state which address blocks can be routed over the internet.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDenial of Service:\u003c/strong\u003e It feels pretty awful when games are compromised due to a \u003ca href=\"https://en.wikipedia.org/wiki/Denial-of-service_attack\" target=\"_blank\"\u003eDDoS\u003c/a\u003e. While building out our networking solutions, we always have to keep these attacks in mind, because as we add more games, we increase our exposure.  \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAll of these pieces combined meant that to support new games with individual server setups and network requirements, the original network designed for a centralized game server (like \u003cem\u003eLeague\u003c/em\u003e and \u003cem\u003eLoR\u003c/em\u003e) needed to also accommodate games with decentralized game servers (like \u003cem\u003eVALORANT\u003c/em\u003e). All during an unexpected, unprecedented global pandemic.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eCOVID-19: \u003c/strong\u003eThe COVID-19 pandemic meant we had to accomplish all of this without the ability to work and collaborate in an office. We also had to manage the effect on hardware deliveries, which impacted our Points of Presence (PoPs) - and more server locations meant more PoPs. \u003c/p\u003e\n\n\u003ch2 dir=\"ltr\"\u003eA Quick Overview\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eRiot Direct is basically a web of cables and routers specifically used for Riot’s packets, including packets for critical game-running processes. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo provide a little context, internet service providers (ISPs) default to \u003cstrong\u003ehot-potato routing\u003c/strong\u003e - this means they want to get customer traffic out of their network as soon as possible using the most cost-effective exit point. The way this impacts their customers (our players) varies widely based on the application they’re running and its overall bandwidth requirements. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe internet is a complicated place where we can’t just connect two points with a straight line. Each region is built based on individual geography, politics, and community policies, and whether via land or sea, we always need to adapt to their infrastructure. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBy having Riot Direct present around the globe in key locations, we’re close to other ISPs, so we can connect directly using \u003ca href=\"https://aws.amazon.com/blogs/architecture/internet-routing-and-traffic-engineering/\" target=\"_blank\"\u003ePrivate Network Interconnecting\u003c/a\u003e (PNIs) or via \u003ca href=\"https://en.wikipedia.org/wiki/Internet_exchange_point\" target=\"_blank\"\u003einternet exchanges\u003c/a\u003e. This way, ISPs can fully offload traffic going to our games close to the source, so we avoid the internet as much as possible. Each one of these locations is called a point of presence (PoP).\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOnce traffic is inside Riot Direct, we use \u003cstrong\u003ecold-potato routing\u003c/strong\u003e - we use the best possible path back to players by keeping traffic inside our backbone for as long as necessary. To achieve this, we need to create specific policies that take into consideration all the carriers we peer with as well as the geographical constraints of each available path. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eHere’s how it works. When someone decides to play one of Riot’s games, their client receives an address it needs to send packets to. The packets leave the player’s computer and travel onto their local network to an internet service provider’s (ISP) network. Ideally, the packets quickly leave the ISP’s network and enter Riot Direct’s network at one of our PoPs.\u003c/p\u003e\n\u003cp\u003eSo basically, the PoP says “Hey, if you’ve got traffic trying to get to these addresses, hand it all to me, because I know the way there.” Traffic can then quickly and easily travel through one of Riot Direct’s dedicated fibers to the correct game server. \u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eLeague of Legends Implementation\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eLeague of Legends \u003c/em\u003ewas Riot’s first game, so our Riot Direct network was originally specifically tailored to run \u003cem\u003eLeague\u003c/em\u003e games. Here’s a quick overview of how this works. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eLeague\u003c/em\u003e’s servers are all located in the same place for each region - in North America, for example, this is Chicago - so the original Riot Direct network was designed to route traffic to this single location. The game servers all have public IPv4 addresses - this impacts scalability due to IPv4 address exhaustion and increased financial cost.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAnother important piece is that \u003cem\u003eLeague\u003c/em\u003e’s return traffic will try to use the \u003cstrong\u003ebest path back, \u003c/strong\u003eand will succeed most of the time. This means traffic has several options. Sometimes it’ll travel Riot Direct’s fibers back to the original PoP, and sometimes it exits the game server’s location to ride the internet back. Network-wise, the best path is always calculated based on the point-of-view of the servers.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eReturn traffic is based on BGP best path selection. For us, the quality of the path is determined by how close we are. A PNI, for example, is always preferred over a transit (regular internet)  because we peer directly with them. Not just logically, but also physically - there’s an actual dedicated fiber between us, while a transit is an indirect connection over another ISP (which is different from home ISPs).\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eVALORANT Implementation\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eRiot’s first person shooter, \u003cem\u003eVALORANT\u003c/em\u003e, may look similar to \u003cem\u003eLeague\u003c/em\u003e from a competitive point of view, but it has a drastically different set of networking needs. The \u003cem\u003eVALORANT\u003c/em\u003e team’s early focus on high performance meant they deploy in multiple locations with many game servers to reduce lag, which can be absolutely game-changing in a first person shooter. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe worked closely with the \u003cem\u003eVALORANT\u003c/em\u003e team to support these goals, which meant creating a totally new strategy that could work with any future game with a similar network setup. Put simply, our solution was to bring our network and servers closer to players. We accomplished this with a variety of strategies, including leveraging anycast networking, and solving for the IPv4 exhaustion that we were already dealing with for \u003cem\u003eLeague\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eAnycast Overview\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWhen a player starts a \u003cem\u003eVALORANT\u003c/em\u003e game, the platform determines their location and assigns a relevant game server IP. Unlike with \u003cem\u003eLeague\u003c/em\u003e, this public IP does \u003cstrong\u003enot\u003c/strong\u003e originate from the game server, but from each PoP - these PoPs are distributed around a region. Each PoP within a region advertises the same IP address so players can reach the address from several locations, always ending up at the closest PoP - this networking strategy is called \u003ca href=\"https://en.wikipedia.org/wiki/Anycast\" target=\"_blank\"\u003eanycast\u003c/a\u003e. \u003c/p\u003e\n\u003ch4 dir=\"ltr\"\u003eTaking A Closer Look With Some Examples\u003c/h4\u003e\n\u003cp dir=\"ltr\"\u003eLet’s say we have a \u003cem\u003eVALORANT\u003c/em\u003e player located in Virginia, USA, and they receive an address of 192.207.0.1, entering Riot Direct’s network at our Virginia PoP. A week later, that player decides to take a trip to sunny Los Angeles, and logs on to \u003cem\u003eVALORANT\u003c/em\u003e from their LA hotel. That SoCal match will still get the IP 192.207.0.1, but this time it’ll enter into Riot Direct’s network from our Los Angeles PoP.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBy decoupling the subnet from the game server and moving the region selection to the platform, we can focus on optimal subnet use based on the internet topology in any given area. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFor \u003cem\u003eLeague of Legends\u003c/em\u003e, we had to use different subnets for the North America and Latin America North shards, which was a hard requirement on our end. But for \u003cem\u003eVALORANT\u003c/em\u003e, we use the same anycast subnet for North and Central Americas. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo compare, the Latin America South and Brazil shards had two different subnets for \u003cem\u003eLeague. \u003c/em\u003eWe decided to also use two for \u003cem\u003eVALORANT\u003c/em\u003e because if players were located in some regions in northern Argentina, the underlying internet infrastructure tended to route to Brazil instead of Santiago. So if we used the same anycast IP for both of these, it would be difficult from a routing perspective to determine what a player’s ISP would do. And since it’s the same IP, the platform has no control over where it’ll land. By having two anycast IPs we give the platform and players the flexibility to choose the best option.\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eBenefits of Anycast\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eWhile we have plenty of servers in strategic locations around the globe to ensure the lowest possible latency to all players, that doesn’t mean we have one server per PoP. Anycast origination is not tied to the game server itself, so internally we need to find the best solution for returning traffic between our edge and the game server. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eIncreasing Speed:\u003c/strong\u003e Anycast networking means we can leverage the internet to choose the closest ingress point into our network. Player traffic is always pinned to its ingress point on the way back.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eAvoiding IPv4 Exhaustion:\u003c/strong\u003e With our implementation of anycast networking, we added Network Address Translation (NAT) to our game flow, which allows game servers to use private IP addresses instead of public ones. Private IPs don’t need to be unique like public ones do. This means we can add more game servers closer to players, reducing latency even more. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eOptimizing Packet Return: \u003c/strong\u003eNATing also guarantees a return from the server via Riot Direct’s network. So when packets return from the server to the player, they always go back to the PoP they entered at via our dedicated cables. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis resolves a major challenge we have with \u003cem\u003eLeague \u003c/em\u003erouting - each routing decision between the edge and the game server is based on all available paths from the point of view of the server. With careful engineering, we can get it right most of the time, but due to the nature of the internet, sometimes it’s out of our hands.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eSoaking Up DoS Traffic:\u003c/strong\u003e When Denial of Service (DoS) or Distributed Denial of Service (DDoS) traffic is sent to the anycast address in any region, it’s distributed across each PoP that uses that specific location, soaking up the attack attempt. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/rd2021_2.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eRiot Direct’s edge soaking a 500Gb/s DDoS attack on VALORANT’s game servers.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eUltimately, lag and latency often depend on the highly variable public internet. By bringing more PoPs closer to players, we aim to reduce the uncertainty to ensure a consistent and excellent experience.\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eWhat About IPv6 Support?\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eSo… where’s the IPv6 support? IPv6 would solve our IPv4 address exhaustion problem, so we’re currently working on adding IPv6 anycast addresses. We’ll still keep the IPv4 anycast addresses, but adding IPv6 will give players with public IPv6 addresses native support, saving them from latency by avoiding ISP carrier grade NATs. The IPv4 and IPv6 anycast addresses will work together to ensure as many players as possible have native end-to-end support for their Internet Protocol versions.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eI know what you’re thinking. Wow, that\u003cem\u003e VALORANT \u003c/em\u003eimplementation seems pretty sweet - why not just do it for \u003cem\u003eLeague?\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWhile we’d love to just copy and paste our solutions over, it’s not quite that simple. \u003cem\u003eLeague \u003c/em\u003epredates Riot Direct - its systems were already in place when Riot Direct was first formed. \u003cem\u003eLeague\u003c/em\u003e is also an entirely different game, with its own client, backend, connections, and live player audience. We were able to collaborate with our friends over on \u003cem\u003eVALORANT\u003c/em\u003e to build this new network routing much earlier in their development process. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eRest assured, this is only the beginning. The changes to Riot Direct described in this article are just the foundations for our multi-game future. Riot Direct exists to help our game teams create and support the best possible player experience, and each new game enables us to learn more and create better tools for \u003cem\u003eall \u003c/em\u003eof our games. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading! If you have any questions, please post them in the section below. \u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/125/rdheader.png\" width=\"1600\" height=\"611\" alt=\"\" /\u003e\n\u003cp\u003eHeyo! We’re Cody Haas and Ivan Vidal, and we’re engineers on the Riot Direct team. It’s been a while since you’ve heard from our team. So in this article, we’re going to tell you a bit about what we’ve done to reinforce consistent and stable connections, reduce latency, and improve the overall player experience for our entire multi-game portfolio.\u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/125/rdheader.png",
      "date_published": "2021-06-30T01:06:51Z",
      "author": {
        "name": "Cody Haas and Ivan Vidal"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/legends-runeterra-cicd-pipeline",
      "title": "The Legends of Runeterra CI/CD Pipeline",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp dir=\"ltr\"\u003eOur workflow does result in a lot of branches and test environments, so we have an automated branch cleanup job. Each branch has an associated JIRA ticket, and when the JIRA ticket is closed, we delete the branch and any related test environments automatically.\u003c/p\u003e\u003cp dir=\"ltr\"\u003eUnfortunately, it can take over an hour for a change to progress through our full pipeline. This is something we definitely want to resolve, as it ends up taking our engineers out of their flow state while they either wait for the results, or switch context to work on a different problem. To make sure we\u0026#39;re being as efficient as possible, we\u0026#39;ve made some optimizations.\u003c/p\u003e\u003cp dir=\"ltr\"\u003e\u003cem\u003eOne of our CI/CD metrics dashboards in New Relic. We track detailed pipeline metrics to keep an eye on current and historical build health and pipeline performance.\u003c/em\u003e\u003c/p\u003e\u003cp dir=\"ltr\"\u003eThe simplest version of a build pipeline builds every artifact every time. However, some of our artifacts take a long time and a lot of resources to build. Many of the artifacts also don’t change from commit to commit or branch to branch - for example, if an artist changes some images, there’s no need to rebuild our audio files. To save time, at the start of the build, we compute hashes for the artifacts to see what has actually changed. The result is a JSON file that lists all of the artifacts in a given build, their hashes, and whether any of those hashes are dirty, indicating the artifact needs to be built. This is similar to \u003ca href=\"https://docs.bazel.build/versions/master/remote-caching.html\" target=\"_blank\"\u003eBazel’s Remote Caching\u003c/a\u003e (I sometimes dream about porting our pipeline to Bazel).\u003c/p\u003e\u003cdiv\u003e\n\u003ch3 dir=\"ltr\"\u003eIteration Builds vs Merge Readiness Builds\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eA full build, including all assets and game clients for three different platforms and potentially several game versions (the currently shipping content plus different sets of work-in-progress future content), game servers, validation steps, and deploys can consume a lot of build farm resources and take over an hour to run. When a developer just wants to iterate on a single feature and maybe do some playtests, they might only need Windows builds for just a single game version, and they might not care that much yet about breaking other features, especially if they’re just experimenting. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo make this quicker and less painful, we introduced a concept we call\u003cstrong\u003e Iteration builds\u003c/strong\u003e, where the pipeline just builds a single platform and skips all extra validation steps, and renamed our full build to Merge Readiness to indicate devs must run a \u003cstrong\u003eMerge Readiness build\u003c/strong\u003e prior to merge. Even if an Iteration build succeeds, the \u003ca href=\"https://docs.github.com/en/github/collaborating-with-issues-and-pull-requests/collaborating-on-repositories-with-code-quality-features/about-status-checks\" target=\"_blank\"\u003eGitHub commit status\u003c/a\u003e still gets set as a failure to encourage developers to make sure they run a Merge Readiness build. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline6.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eGitHub commit status for an Iteration build\u003c/em\u003e\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline_7.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eJenkins Blue Ocean view for an Iteration build\u003c/em\u003e\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline_8.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eJenkins Blue Ocean view for a Merge Readiness build on the same branch as above. Note the additional validation steps.\u003c/em\u003e\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eGit LFS Slow on Windows\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eA full clone of our game monorepo is about a hundred gigabytes and nearly a million files. Given a pre-existing workspace on a build node, a Git (plus LFS) sync takes one to two minutes (in Windows, for a t3.2xlarge AWS instance). This is pretty fast, but it could be even faster!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline9.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eA look at our monorepo using \u003c/em\u003e\u003cem\u003e\u003ca href=\"https://github.com/ariccio/altWinDirStat\" target=\"_blank\"\u003ealtWinDirStat\u003c/a\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWhy is our repo so big? Video games have a lot of assets - art, sound effects, etc. To simplify developer environment setup, we’ve also vendored most of our tools. For example, we’ve \u003ca href=\"https://stackoverflow.com/questions/26217488/what-is-vendoring\" target=\"_blank\"\u003evendored\u003c/a\u003e Python binaries for Windows, macOS, and Linux, along with all of our Python dependencies. Instead of needing to install a bunch of tools, devs just sync the repo and have everything immediately available.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOur pipeline contains many separate steps, each of which can run on a different build node. Each of these steps needs a workspace with our Git repo. We maintain persistent workspaces on long-lived build VMs that already have the repo checked out for speed (we’d like to eventually move to ephemeral build nodes that mount volumes that already contain our repo, but of course this is yet more complexity in an already complex system).\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eEven so, on Windows we were experiencing \u003ca href=\"https://github.com/git-lfs/git-lfs/issues/931\" target=\"_blank\"\u003eslow Git LFS syncs\u003c/a\u003e. To save time, at the start of each build, in parallel with computing our build plan, we sync the repo to a workspace and then upload the entire repo (minus the .git directory) to the same \u003ca href=\"https://technology.riotgames.com/news/supercharging-data-delivery-new-league-patcher\" target=\"_blank\"\u003echunking patcher\u003c/a\u003e we use for distributing our games. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline10.png\"/\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline10.png\"/\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorepipeline_10.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eRepo upload to the patcher\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn all subsequent steps of the pipeline, we use the patcher to sync our workspaces. We benefit from this in two ways. First, because we’re not doing any Git bookkeeping and we’re not copying the .git directory, we don’t get the performance hit from updating Git metadata. Second, the patcher is \u003cem\u003efast\u003c/em\u003e, and because it copies data from a \u003ca href=\"https://en.wikipedia.org/wiki/Content_delivery_network\" target=\"_blank\"\u003eCDN\u003c/a\u003e, we don’t need to worry about accidentally \u003ca href=\"https://en.wikipedia.org/wiki/Denial-of-service_attack\" target=\"_blank\"\u003eDDOSing\u003c/a\u003e our Artifactory LFS repo from running too many parallel builds.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eYou might wonder why we don’t just use \u003ca href=\"https://en.wikipedia.org/wiki/Rsync\" target=\"_blank\"\u003ersync\u003c/a\u003e or something similar for this instead of our patcher. There are a few reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eThe patcher is well-optimized on Windows.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003ersync might overload the source machine if we’re syncing to multiple machines at the same time, while the patcher syncs from a very scalable CDN.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eThe patcher has advanced features, like release metadata that lets us fetch particular versions of the repo.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003eUsing the patcher, our sync times for a pre-existing workspace drop to about ten to fifteen seconds, which is a nice improvement over the one to two minutes it takes with Git, especially when we’re doing up to a few dozen workspace syncs per run of the pipeline. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eNote that in the rare cases we need to sync a fresh empty workspace, even with the patcher it still takes quite a long time, because it’s simply a lot of data to move.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe’ve also started moving our vendored tools from Git LFS directly into the patcher. For some of our larger tools, like Unity itself, we upload the entire tool to the patcher, and then in the repo we have a dependency downloader script plus pointer files listing the latest patcher release ID for each tool. When I launch Unity on my work PC, it actually first runs the downloader script to see if there’s a newer version available. This is especially helpful for remote work - it’s much faster to pull down tools from a CDN than to try to download them over a VPN.\u003c/p\u003e\n\n\u003ch2 dir=\"ltr\"\u003eChallenge: \u003ca href=\"https://technology.riotgames.com/news/automated-testing-league-legends\" target=\"_blank\"\u003eVideo Game Testing\u003c/a\u003e is Difficult and Time-Consuming\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOur automated validation in CI includes:\u003c/p\u003e\n\u003cul dir=\"ltr\"\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/guykisel/inline-plz\" target=\"_blank\"\u003eStatic analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eRerunning Git \u003ca href=\"https://pre-commit.com/\" target=\"_blank\"\u003epre-commit\u003c/a\u003e hooks to make sure people didn’t skip them locally\u003c/li\u003e\n\u003cli\u003eAsset validation (for example all images must be square PNGs with power-of-two resolutions)\u003c/li\u003e\n\u003cli\u003eC# \u003ca href=\"https://xunit.net/\" target=\"_blank\"\u003eXunit\u003c/a\u003e tests\u003c/li\u003e\n\u003cli\u003eAutomated performance tests to ensure we don’t go over our mobile memory budget\u003c/li\u003e\n\u003cli\u003eAutomated functional tests (using pytest) that can test game servers on their own (\u003cem\u003eLoR\u003c/em\u003e is game-server authoritative) or clients + game servers (we usually run the clients \u003ca href=\"https://en.wikipedia.org/wiki/Headless_computer\" target=\"_blank\"\u003eheadless\u003c/a\u003e in functional tests for faster testing)​\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 dir=\"ltr\"\u003eSolution: Built-In QA Tooling\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eTo enable our automated tests, debug builds of the game include an HTTP server that provides direct control of the game, including a bunch of handy cheats and dev-only test cards. By controlling the game directly through HTTP calls, we avoid brittle UI-based testing. Instead of trying to write tests that click buttons, we can just tell the game to play a card, attack, concede, etc. We use \u003ca href=\"https://pytest.org/\" target=\"_blank\"\u003epytest\u003c/a\u003e to create test cases for much of our game logic. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFor our functional tests we normally run either just a game server on its own or a headless game client in a VM. For convenience, we launch the test game servers in the same docker swarm we use for running containerized build steps. For performance tests, we run headed on \u003ca href=\"https://en.wikipedia.org/wiki/Bare-metal_server\" target=\"_blank\"\u003ebare metal\u003c/a\u003e to ensure realistic results. To improve test speed, we reuse a single game server for each set of tests. The game server is designed to host many concurrent matches anyway, so hosting many concurrent tests is no problem, and this is much faster than launching a new game server for each test case. We can run several hundred functional tests in just a few minutes by running the game at 10x speed and parallelizing our tests. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline11.png\"/\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edef test_hecarim_level_one(server, clients):\n    \u0026#34;\u0026#34;\u0026#34;\n    test that hecarim summons 2 spectral riders on attack\n    \u0026#34;\u0026#34;\u0026#34;\n    player_one, player_two, *_ = clients\n\n    game_id = player_one.enter_game()\n    player_two.enter_game(game_id)\n\n    player_one.accept_hand()\n    player_two.accept_hand()\n\n    server.clear_all_cards(game_id)\n    server.set_turn_timer(game_id, False)\n    server.unlock_base_mana(game_id, 0)\n\n    hecarim = server.create_card_by_card_code(player_one, \u0026#34;01SI042\u0026#34;, RegionType.BackRow)[0]\n    player_one.attack(hecarim)\n    player_one.submit()\n    cards_in_attack = card_helper.get_cards_in_region(player_one, RegionType.Attack)\n    assert len(cards_in_attack) == 3\n    assert card_helper.check_card_code(player_one, cards_in_attack[1], \u0026#34;01SI024\u0026#34;)\n    assert card_helper.check_card_code(player_one, cards_in_attack[2], \u0026#34;01SI024\u0026#34;)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\n\u003cvideo controls=\"controls\" height=\"600\" id=\"video202142813720\" poster=\"\" width=\"800\"\u003e\u003csource src=\"https://technology.riotgames.com/sites/default/files/lorpipelinevideo.mp4\" type=\"video/mp4\"/\u003eYour browser doesn\u0026#39;t support video.\u003cbr/\u003e\nPlease download the file: \u003ca href=\"https://technology.riotgames.com/sites/default/files/lorpipelinevideo.mp4\"\u003evideo/mp4\u003c/a\u003e\u003c/video\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eA video demonstration of our tests running\u003c/em\u003e\u003c/p\u003e\n\n\u003ch2 dir=\"ltr\"\u003eChallenge: Non-Technical Folks Using Git\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eVersion control is hard. \u003ca href=\"https://github.com/k88hudson/git-flight-rules\" target=\"_blank\"\u003eGit is hard\u003c/a\u003e. I’ve been using Git for years as a software engineer and I still make mistakes all the time. For game designers, artists, and other folks who are probably only using Git because we’re forcing them to use it, Git can be intimidating, unfriendly, and unforgiving. We used to have people use various \u003ca href=\"https://git-scm.com/downloads/guis\" target=\"_blank\"\u003eGit GUI clients\u003c/a\u003e, since a GUI can be a bit more user friendly than the Git CLI, but even the most user friendly Git GUIs are meant for general purpose software development and have feature-rich user interfaces with a lot of buttons and options that can be overwhelming.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eSolution: Custom GUI Tool To Reduce User-Facing Complexity\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eTo improve our \u003cem\u003eLoR\u003c/em\u003e developer user experience, we built a custom GUI tool just for \u003cem\u003eLoR \u003c/em\u003e(inspired by a similar tool created for \u003cem\u003eLoL\u003c/em\u003e) that’s specialized for our workflow. Because our tool is narrowly scoped to just one project and one repo, we can make a lot of assumptions that reduce user-facing complexity. Additionally, we’ve integrated our Git workflow with our JIRA and Jenkins workflows. We call this tool LoRST (Legends of Runeterra Submit Tool). It’s built using Python with \u003ca href=\"https://pypi.org/project/PySide2/\" target=\"_blank\"\u003ePySide2\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFrom this one tool, \u003cem\u003eLoR\u003c/em\u003e devs can create new branches (complete with test environments, build jobs, JIRA tickets, and auto-merge configurations), commit and push changes, trigger Iteration builds and Merge Readiness builds, open pull requests, and resolve most merge conflicts.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline--12.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline--13.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline14.png\"/\u003e\u003c/p\u003e\n\n\u003ch2 dir=\"ltr\"\u003eChallenge: Complicated Pipelines Can Fail in Confusing Ways\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOur pipeline has dozens of steps, each of which could fail in a wide variety of ways. Understanding how and why a build failed can be difficult. The pipeline depends on about fifty different services, such as \u003ca href=\"https://sentry.io/\" target=\"_blank\"\u003eSentry\u003c/a\u003e, AWS, and various internal tools. If any of these fail, our pipeline could fail. Even if all these services work perfectly, a mistake by a developer could still cause a failure. Providing useful build failure feedback is critical to helping the team work efficiently.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline--15.png\"/\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eSolution: Build Failure Notifications\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWe’ve gone through several iterations of build failure notifications in an attempt to provide better failure feedback. Our first attempt was to redirect \u003ca href=\"https://en.wikipedia.org/wiki/Standard_streams#Standard_error_(stderr)\" target=\"_blank\"\u003estderr\u003c/a\u003e from our Python scripts to a temporary text file. If the script failed, we’d dump the stderr text file into the failure notification. This sometimes provided useful info, but only if the Python script had effective logging. Our next attempt was to use the \u003ca href=\"https://plugins.jenkins.io/build-failure-analyzer/\" target=\"_blank\"\u003eJenkins Build Failure Analyzer plugin\u003c/a\u003e, which uses \u003ca href=\"https://en.wikipedia.org/wiki/Regular_expression\" target=\"_blank\"\u003eregular expressions\u003c/a\u003e to match known failure causes against logs.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline16.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eOur older build notifications. A bit overwhelming.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe BFA plugin has some limitations - log scanning runs on the central Jenkins server, which can be a significant performance hit. The plugin also only scans the build’s console logs, but some of our tools output extremely verbose logs that we usually redirect to separate text files. I wrote a Python log scanner script that runs in our Jenkins Docker swarm to download all log files from a given build and scan them all. This solved the performance issue and the console log issue.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe Python log scanner still had a significant false positive rate which resulted in ambiguous or confusing failure notifications. It also stored all the failure causes as a large JSON file in a central location. Gabriel Preston, an engineering manager on the Riot Developer Experience: Continuous Service Delivery team, set up a conversation with \u003ca href=\"https://twitter.com/utsav_sha\" target=\"_blank\"\u003eUtsav Shah\u003c/a\u003e, one of his former teammates at Dropbox, where they had built their own sophisticated build failure analysis system. Utsav mentioned that for each failure, they created and saved a JSON file with metadata about the failure for later analysis. Inspired by this system, we redesigned our failure analysis to define failures right where they happen in our build scripts, which makes failures more maintainable and understandable and reduces the false positive rate. It also means we don’t usually need to do any log scanning. When something fails, we create a JSON file with failure metadata, and then later capture the failure JSON file to send telemetry to New Relic as well as a Slack notification.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline17.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline18.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline19.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eA few of our newer failure notifications. Much more concise.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline20.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eFailure handler usage examples from our readme\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/lorpipeline21.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eOur failure metrics dashboard in New Relic\u003c/em\u003e\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eNow that you’ve seen some of our past challenges and solutions, take a look at our next batch! Here are a few challenges we’re thinking about now. If any of these kinds of problems strike you as intriguing or exciting, take a look at our \u003ca href=\"https://www.riotgames.com/en/work-with-us#product=Legends%20of%20Runeterra\" target=\"_blank\"\u003eopen roles\u003c/a\u003e and shoot us over an application!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eA few items from our to-do list:\u003c/p\u003e\n\u003cul dir=\"ltr\"\u003e\n\u003cli\u003eIf we have ~50 dependencies, and each of those dependencies has 99% uptime, our pipeline has only .99 ^ 50 = 60% reliability even before code changes.\u003c/li\u003e\n\u003cli\u003eCould we switch to ephemeral build nodes? How would we bootstrap the workspace data? Could we switch to Linux-based Unity builds?\u003c/li\u003e\n\u003cli\u003eImproving iteration speed on the pipeline itself.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading! Feel free to post comments or questions below.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/124/lorpipelineheader.png\" width=\"1600\" height=\"611\" alt=\"\" /\u003e\n\u003cp dir=\"ltr\"\u003eHi, I’m Guy Kisel, and I’m a software engineer on \u003ca href=\"https://playruneterra.com/en-us/\" target=\"_blank\"\u003e\u003cem\u003eLegends of Runeterra\u003c/em\u003e\u003c/a\u003e’s \u003ca href=\"https://shopify.engineering/why-shopify-moved-to-the-production-engineering-model\" target=\"_blank\"\u003eProduction Engineering\u003c/a\u003e: Shared Tools, Automation, and Build team (PE:STAB for short). My team is responsible for solving cross-team shared client technology issues and increasing development efficiency. In this article I’m going to share some details about how we build, test, and deploy \u003ca href=\"https://www.youtube.com/watch?v=Mbq8lgzXCxQ\" target=\"_blank\"\u003e\u003cem\u003eLegends of Runeterra\u003c/em\u003e\u003c/a\u003e, a digital \u003ca href=\"https://en.wikipedia.org/wiki/Collectible_card_game\" target=\"_blank\"\u003ecollectible card game\u003c/a\u003e. \u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/124/lorpipelineheader.png",
      "date_published": "2021-05-31T17:26:34Z",
      "author": {
        "name": "Guy Kisel"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/strategies-working-uncertain-systems",
      "title": "Strategies for Working in Uncertain Systems",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv property=\"content:encoded\"\u003e\u003cp dir=\"ltr\"\u003eImagine yourself in this situation.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eYou sit down at your computer, ready to begin the work day. A steaming cup of coffee rests beside your keyboard and your next task is on the monitor in front of you. You start to write a program that must hit a third party URL over the internet every time you run it. The hours pass and you feel content with the progress you’ve made so far.  \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSuddenly things start breaking. You debug for an hour and you’ve collected some data: a dozen crashes, a handful of different HTTP error codes, several uncaught exceptions, and at least one 60 second timeout. Your requests only get through half the time and you’re getting responses back that don’t match the little documentation you could find. As you twirl the lock of hair you just yanked out of your head, you wonder: What happened?\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eMy name is Brian \u0026#34;Bizaym\u0026#34; Teschke. I\u0026#39;m a software engineer on a central technology team at Riot Games called Developer Connections and we’ve seen all kinds of system instability issues. My team is responsible for solving common problems across game teams so they can just focus on making awesome games. We often have to write programs that interface with several APIs and libraries - both internal and external - each with their own quirks and inconsistencies. This has allowed us to see patterns of both issues and resolutions, which I’m looking forward to outlining for you in this post. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eUltimately, I hope to answer this question:\u003c/p\u003e\n\u003cblockquote\u003e\u003cp dir=\"ltr\"\u003e\u003cem\u003eHow can I hope to write a system more reliable than the services that power it?\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp dir=\"ltr\"\u003eIn this article, I\u0026#39;ll explain some tips and tricks my team has used to overcome or avoid cases like the one above, and how we figure out why they happen in the first place. I\u0026#39;ll be taking the perspective of working with a third-party\u0026#39;s REST API for most of my examples, but these techniques can also be applied to a library package that interfaces with an online service. For each category of issue, I’ll provide a short example, a breakdown of common causes I’ve run into, and the solution for my example that applies these learnings.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/uncertainsystems_1.png\"/\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eExample: Slow Build Transfers\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eI once worked with a service we used for processing builds of our game binaries, the first step of which was to transfer the build (often large files of +3GB) - we would trigger workflows several times a day. For the most part, my files would transfer fine, but sometimes it would run 10 times slower, and I would frequently get errors, usually near the end of my workday. Luckily, it was always fast on Fridays so I could get off on time for the weekend. But why? \u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003ePattern Breakdown\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eIt’s very hard to keep a service running perfectly \u003cstrong\u003eforever\u003c/strong\u003e, and much easier to have one that only \u003cem\u003emostly\u003c/em\u003e works. You might see a timeout, an HTTP error code, or maybe a friendly “\u003cem\u003eTry again later\u003c/em\u003e” message. There are many causes of outages - some of them completely out of the developer’s control, such as a hardware failure or bandwidth saturation. Sometimes the changes are intentional and planned, like an update during a maintenance window or the rollout of a new version.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWhile still mid-development, it’s a good idea to save at least one good response from a dependent service and use those in case of an outage. Continue working on your program and use the static response to ensure that the code works until the real service is back up. \u003cem\u003ePro tip\u003c/em\u003e: put this in a folder called \u003cstrong\u003eUnit Tests\u003c/strong\u003e and run it \u003cem\u003econstantly\u003c/em\u003e. Your peers and future self will thank you.  \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis works when you’re still developing your program, but what if this happens in production?\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTypically the solution most often proposed for intermittent service errors is to simply retry the request. Maybe it’ll work next time, right? This works for short term failures... but what if it\u0026#39;s a longer outage? Hopefully you implemented an exponential backoff so you aren’t just hammering your third party while they try to come back online! Maybe there’s an underlying reason why this outage happens. Do some digging and see if you can find a pattern.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eSolution\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eIn the story above, I took my unreliable upload scripts and went looking for reasons why it was slow at specific times of the day. It turns out that the external service I was using had a lot of customers from Asia that would use their systems early in the morning which, with the time zone difference, is around late afternoon on the US Pacific Coast. My Friday would be their Saturday, so there wouldn’t be any peak traffic then! I scheduled all my upload tasks to run outside those peak times and error rates went way down.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/uncertainsystems_2.png\"/\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eExample: Datetime Struggles\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eMy team at Riot was working on a service that allowed game teams to run test scenarios on remote hardware. The third-party company hosting the remote hardware was also in the process of developing improvements to their solution to meet Riot\u0026#39;s unique needs. There wasn\u0026#39;t a lot of rigor around communicating API updates, so we would often test a feature in the morning that would hit breaking changes in the afternoon. I once experienced an odd crash when parsing what should have been a standard datetime string but instead got this interesting (but not necessarily incorrect) response:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e{\n  ...\n  \u0026#34;starttime\u0026#34;: \u0026#34;2021-02-10T00:28:58+00:00\u0026#34;\n  \u0026#34;endtime\u0026#34;: \u0026#34;Processing...\u0026#34;\n  ...\n}\u003c/code\u003e\u003c/pre\u003e\n\u003cp dir=\"ltr\"\u003eI supported many different date formats but that was not a response I was expecting.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003ePattern Breakdown\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWe’ve covered the case where a service goes down, but what about when a service changes unexpectedly? It could be that the endpoints are moved around on you, the request structure changes, or even the data types are now different. This happens most often while using a system still under active development, like a pre-release version. Take a look at the packages in your dependency tree that have a version less than 1.0. You might be surprised by how many there are!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSudden changes can happen often with APIs that already have a frontend offering (such as a website or app) controlling them. This is because the owner can release changes to both the frontend and backend to ensure breaking changes aren’t seen by the typical user.  Except maybe you’re not a typical user and you are hitting the backend directly and those breaking changes now affect you. So what can you do?\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe first step is to set up some tests that will show what’s breaking. Run them automatically during your off hours and you might have a list of \u003cem\u003eTo Do\u003c/em\u003es when you start work in the morning... instead of finding out while you’re in the zone, where they can derail your train of thought. These kinds of tests are especially important if you run a production service and have customers of your own you need to support. Finding the problems before they do and informing them with a warning message or banner can go a long way to improving the relationship and confidence your customers have in you and your systems.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eChange is inevitable, and if it’s frequently breaking your code, you might want to think about setting up some mechanisms to reduce its impact. The \u003ca href=\"https://en.wikipedia.org/wiki/Adapter_pattern\" target=\"_blank\"\u003e\u003cstrong\u003eadapter\u003c/strong\u003e\u003c/a\u003e pattern is a great fit. Write a single file that handles all the communication between this problematic third-party API and the rest of your system.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis has several benefits:\u003c/p\u003e\n\u003col\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eYou only have to fix bugs in one file if there are changes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eYou can capture any failures here so that they don’t bring down your entire system.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eIt keeps your system pristine, consistent, and isolated from the changes out of your control.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp dir=\"ltr\"\u003eKeep a clean separation by inspecting all information coming in from the external API; don’t make blind assumptions and pass around nested objects or you risk introducing bugs all over your code. Use the objects that make the most sense in your language and environment (i.e. a built-in timestamp class) and make the necessary translations to or from the third-party data (i.e. from a date formatted string) in your adapter if necessary.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eYou can get even more power by applying the \u003ca href=\"https://en.wikipedia.org/wiki/Strategy_pattern\" target=\"_blank\"\u003e\u003cstrong\u003estrategy\u003c/strong\u003e\u003c/a\u003e pattern on top of your adapters.  Ask yourself if there are other APIs out there that suit your needs. Write an adapter in front of those APIs too and switch between them when one isn’t working. You give your system the flexibility to stay operational while you prioritize the breaking updates with your other critical work. This opens the door for adding more potential benefits to your application, such as cross-referencing your data, managing requests around rate-limits, or replacing a paid service with multiple free offerings.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSometimes a breaking change is a feature you weren’t expecting. For example, if you suddenly get an array when you expect a single object, it could be an enhancement that allows the caller to query multiple records at once and the single object response is just the simplest case. Ensure your code can handle requests that could return one or more objects.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eSolution\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOnce in a while you just run into an edge case that the original author simply hadn\u0026#39;t thought about. In my example above, I discovered from the developer that there was a window between when the task completes and when their backend updates their database. Which is where my status call could sneak in and get a stale value (ever heard of something being \u003ca href=\"https://en.wikipedia.org/wiki/Eventual_consistency\" target=\"_blank\"\u003eeventually consistent\u003c/a\u003e?). This field was left as an unexpected default value when it was returned to me. I made my adapter functions more fault-tolerant. It now catches cases like this where data from an API is missing or malformed, but a default value (like \u003cstrong\u003enull\u003c/strong\u003e) will now be handled just fine in the rest of my program.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/uncertainsystems_3\"/\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eExample: Outdated Docs\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWhat about when it\u0026#39;s not the \u003cem\u003eservice\u003c/em\u003e changing that causes you frustration but that the user guide you\u0026#39;re following seems to conflict with reality? \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eI once had a colleague approach me about a library I owned that could parse a project file into a data structure that was easier to work with in a program. This colleague was particularly irritated at me for how the documentation seemed to make no sense and contradicted itself. Parts of it referenced features that didn\u0026#39;t seem to exist, arguments were incorrect, and even some examples were using a different programming language.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003ePattern Breakdown\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eDocumentation is often one of the last steps before releasing a service. During development, you don’t want to go back and update the docs every time you have to make changes. Unfortunately, this also means it’s often an area where corners are cut to meet deadlines, and it’s one of the last places to get updated as newer versions are rolled out. It could have errors in its details, completely undocumented features, or have been poorly translated from another language. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOn the opposite end of the spectrum, you could be looking at enormous amounts of text covering every change on every microversion of the platform. Your searches could yield millions of results over several years across a wide community and you\u0026#39;ll find it impossible to identify a solution to your problem that works in your specific set of circumstances.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTry to reach out to the author if possible. Submit a support ticket, open an issue on Github, join the company’s public Slack channel, or send a good old-fashioned email. Besides simply answering your questions, they could give you some insight into their project roadmap. You might be the only user of a feature they thought about dropping and now you can influence its priority. They could have newer, unpublished documentation or a beta version of the service they can give you access to use. In some cases you might even be able to help contribute to the project for the benefit of everyone.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAnd if that doesn’t work, and you’re stuck with the original documentation? See if you can do some exploratory investigation. Try a GET request on as many relevant nouns you can think of.  Maybe you’ll get lucky and find a crucial model’s data structure. Can\u0026#39;t figure out what fields a search query for\u003ccode\u003e/transactions\u003c/code\u003e expects? Try a GET on \u003ccode\u003e/item\u003c/code\u003e, \u003ccode\u003e/sku\u003c/code\u003e, \u003ccode\u003e/store\u003c/code\u003e, \u003ccode\u003e/employee\u003c/code\u003e\u0026gt; and see what fields are available on them.\u003c/p\u003e\n\u003cp\u003ePUT \u003ccode\u003e/person\u003c/code\u003e not working? Try a POST instead if other models are using that method.  Developers tend to be consistent, so follow the patterns in functionality. Other endpoints use plural nouns?  Try \u003ccode\u003e/persons\u003c/code\u003e or \u003ccode\u003e/people\u003c/code\u003e. There are a lot of words that mean the same thing that are not necessarily standard programming concepts, and this can be particularly confusing for an author whose first language is not English.  Instead of \u003ccode\u003eperson\u003c/code\u003e, it could be \u003ccode\u003euser\u003c/code\u003e, \u003ccode\u003ecustomer\u003c/code\u003e, \u003ccode\u003eclient\u003c/code\u003e, or even \u003ccode\u003eshopper\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eGreat developers often build solutions that cover a wider range of problems than asked for. The program’s requirements only ask for a single record, but we know it’s better to process things in bulk when we can, so maybe you’re passing in a single user where the service expects an array instead. Put yourself in the shoes of the author and imagine how you would write this feature. Search the internet for popular implementations for, as an example, a login endpoint. There’s a good chance the author did something similar and this can shed some light on how you need to pass your credentials.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eSolution\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eIt so happened that my project was based off an abandoned open source project that itself was also based off an older open source project from another language. I had also merged in some new features from other public forks of the old project, and in doing so, completely scrambled the documentation folder with bits from all contributors. I took some time to clean up the different parts that were merged together, updated the parts where I added new features, and translated the old parts into the new language.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eLet’s face it. None of us are perfect developers. As good as our intentions are to build the best solutions we can, we sometimes fall short of our own expectations. You may be thinking of some systems you yourself have written that you wish you had more time to go back and improve. It’s the reality of the world we live and work in.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eRemember that every challenge is an opportunity in disguise. I hope these tips from the kinds of problems I’ve seen over the years will help you crush those bugs before they even appear in your code, level up your craft, and boost your customer\u0026#39;s experience. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eLook towards long-term solutions instead of quick fixes and you end up saving more time.  Robust solutions often grow their own influence, by being the shining example for future features or being that solid foundation for yet another young developer to rely on for their next great project.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eKey strategies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eWrite \u003cstrong\u003eunit tests\u003c/strong\u003e using static responses to keep you working when systems are down.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eRely on the \u003cstrong\u003eadapter\u003c/strong\u003e pattern to keep external changes isolated.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eDon\u0026#39;t be afraid to\u003cstrong\u003e reach out\u003c/strong\u003e if the documentation seems iffy.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003eThe next time you run across an API or third-party plugin that frustrates you, remember that it was written by a programmer just like yourself. Forgive the quirks and inconsistencies of the author’s code and remember that you have techniques in your back pocket to help you get through this.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading! If you have any questions, feel free to post them below.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/123/uncertainsystemsheader.png\" width=\"1600\" height=\"611\" alt=\"\" /\u003e\n\u003cp\u003eMy name is Brian \"Bizaym\" Teschke. I'm a software engineer on a central technology team at Riot Games called Developer Connections and we’ve seen all kinds of system instability issues. My team is responsible for solving common problems across game teams so they can just focus on making awesome games. We often have to write programs that interface with several APIs and libraries - both internal and external - each with their own quirks and inconsistencies. This has allowed us to see patterns of both issues and resolutions, which I’m looking forward to outlining for you in this post. \u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/123/uncertainsystemsheader.png",
      "date_published": "2021-04-27T17:32:04Z",
      "author": {
        "name": "Brian Teschke"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/improving-developer-experience-operating-services",
      "title": "Improving the Developer Experience for Operating Services",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv property=\"content:encoded\"\u003e\u003cp dir=\"ltr\"\u003eHello! I’m James “WxWatch” Glenn and I’m a software engineer on the Riot Developer Experience: Operability (RDX:OP) team. My team focuses on providing tools for Riot engineers and operations teams that help them better understand the state of their live services across the globe. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSome of these tools include Riot’s service metrics, logging, and alerting pipelines. In this article, I’ll be talking about our one-stop-shop application for Rioters operating services - Console.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eBuilding Console has allowed us to deprecate and remove many of the standalone custom tools that we discussed in a \u003ca href=\"https://technology.riotgames.com/news/running-online-services-riot-part-v\" target=\"_blank\"\u003eprevious blog post\u003c/a\u003e. But before we get into the details of Console, let’s set the context of this problem space using an example of a troubleshooting experience an engineer would have had using these tools prior to the creation of Console.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFor this example, we’ll use a service my team owns called the “opmon.collector” -  this service is the primary interface that services at Riot use to send logs and metrics to our monitoring platform. The opmon.collector is deployed in datacenters across the globe.\u003c/p\u003e\n\u003cp\u003eIn this case, an alert triggers for opmon.collector and we need to figure out why. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console1.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eA map to guide you through this example.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo view the alert details, we’ll need to open up a browser, navigate to our alerting tool, and enter the service name and location to view the active alerts. From the active alerts, we see an alert saying we’re exceeding our allowed number of API timeouts, so let’s go check the logs to see if we can pinpoint the issue further. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo view the logs, we’ll log in to our monitoring platform log viewer and type in the service name and location, only to discover that there are no logs for this service instance in our monitoring platform! Since the service’s logs are not making it to our monitoring system, we\u0026#39;ll need to look at the container logs directly. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo do this, we turn to our container visualizer, Toolbox. Inside Toolbox, we once again drill down to the appropriate cluster, find the service and open it up. Looking at container logs, we’re able to see that our issue is that the service is unable to connect to a dependency service. To further diagnose this, we need to look into our service’s network rules.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eNavigating to our Network Viewer, we again have to search for the service. Once found we can open up the network rules and, upon inspection, discover that our service is missing a rule to allow it to communicate with a dependency. From here, we can add that rule and resolve the issue. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eGreat! We were able to use these tools to identify the issue. Each new tool we used, however, required us to reestablish the context of our search, which, in this case, was our service’s name and location. A more subtle inconvenience is that it required us to know the existence of (and have access to) all these tools in order to uncover the cause of our issue. Over time, this adds up to a significant amount of inconvenience, not only day-to-day as an engineer, but for one-time events like onboarding. \u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console_logo.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe built Console to solve these inconveniences. We took the core functionality of these bespoke tools (and many more) and bundled them into a single tool with a unified context and UI. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console2.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis means that you find your service once via the search bar and everything you view is within that context. In addition to removing many of the tools that were mentioned in the previous section, we’ve been able to include features that would be nearly impossible to manage across multiple tools (e.g. Console has Dark Mode).\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo illustrate this, let’s go through the same example as the section above, but this time we’ll use Console.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console3.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eTreasure obtained in a fraction of the time.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFirst, let’s check logs:\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console4.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAs before, we see there\u0026#39;s a network issue. Let’s check out the network rules, where we see that our service does not have the necessary network rule to other.service, as before.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console5.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThese are the same triage steps as before, but because of Console, we\u0026#39;re able to easily navigate between the features we needed and more quickly determine the cause of the problem.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eCombining all these tools into a common interface was not as straightforward as it initially seemed. To get the best experience, there were two main goals we needed to accomplish. First, we needed to distill all the useful features from every tool while leaving behind or rethinking the features that typically went unused. And second, we needed to provide a way for other teams to get their data and features into Console.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo accomplish this first task, we followed a “player experience first” mindset. My team’s audience - our version of “players” - are Riot engineers across the entire company, from game developers to infrastructure teams. If we can improve their experience by decreasing the amount of friction when using tooling, then we’re increasing the amount of time they have to work on features and games for players. To figure out everyone’s wants and needs, we just, well, asked them. We created design documents and wireframes and interviewed and surveyed engineers across Riot. This gave us a solid picture of what was (and wasn’t) important to developers.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console6.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eAn early Console wireframe\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eProviding a straightforward path for other teams to build features in Console boiled down to one major hurdle: Not all teams have dedicated front-end engineers, and teams don’t want to spend a lot of time designing and building a user interface. The manifestation of this hurdle in the past was the collection of pre-Console tools we talked about earlier - they were typically built using whichever JS framework (React, Angular, etc) and UI framework (Material, Bootstrap, etc) the team decided on at the time, meaning no two tools looked or felt the same. \u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eTechnology and Template Time\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eNow that we know what we wanted to accomplish with Console, let’s talk about how we did it. Console’s back-end is a Golang service that gathers data from services across Riot, caches it, and communicates it to the front-end via REST APIs. Console also provides a proxy that the front-end can use to communicate with other services directly, in the case where no additional processing is required on the back-end. This eliminates the need for an engineer to write boilerplate APIs simply to fetch data from services. Console’s front-end is a React application using Typescript for type checking (we initially used Flow but recently migrated to Typescript) and \u003ca href=\"https://ant.design\" target=\"_blank\"\u003eAnt Design\u003c/a\u003e for its UI components.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis architecture allows us to focus on having a consistent UI across the entire application. To help maintain consistency, we built a series of templates that teams can use when integrating their own features into Console. These templates give teams a framework to work with and allows engineers with less front-end experience to still be able to quickly build out good, consistent UIs within Console. It also lowers the barrier to entry, as it eliminates the need for engineers to come up with content from scratch.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eConsistency alone isn’t good enough though. We knew we needed to prioritize a good overall user experience so people would be motivated to use Console. It‘s a tool that engineers use every day, so any inconveniences - no matter how small - add up over time, generating a lot of pain and annoyance. Because of this, we focused on making sure Console not only has the right data, but also is easy to navigate and understand. For navigation, each feature in Console is scoped to specific service types and is only visible when viewing a service of that type, ensuring relevant features are easily accessible. Also, since Console collects data from different sources, we help the user understand the origin of the data they’re viewing by providing unobtrusive tooltips that display the data’s source.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eNow that we have all the data in one place, we can begin to correlate data that we weren’t able to previously. For example, since Console knows if a service is alerting, it can display that alert as a notification on the service summary page. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console7.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eConsole has many additional features that aren’t covered in this article:\u003c/p\u003e\n\u003cul\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eAbility to view service specification and deployment status/logs\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eConfiguration view, including when a configuration value was last changed\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eKill/Restart individual instances of a service\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eAbility to schedule service deployments\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eService health viewer\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/console8.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eDeployment logs for our service\u003c/em\u003e\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eTo ensure our investments have paid off, we look at analytics and metrics. When we first launched, Console only had a couple dozen users each month. Now, we’re up to well over 300 engineers per month!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/consolechart.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn addition to these metrics, we also conduct periodic surveys and interviews with users to gather direct feedback on the current state of Console. We engage with relevant engineers when we’re considering future features and improvements we have planned. My team wants to make sure we’re always working on the features that teams and engineers need most. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe two themes of Console moving forward will be integrating more teams’ features into Console and correlating the data that Console already has in useful ways.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eHere are a few concepts we’re interested in further exploring in the future:\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eEfficiency Tool\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eCurrently in beta, the Efficiency Tool measures how efficiently a service is using its resources. It uses a combination of CPU, memory, and other usage metrics to give an overall score (out of 100) to a service. This will help teams know if their services are requesting too many resources from the cluster or not. Metrics like these can also help with auto-scaling, load testing, and capacity planning.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003ePersonalization\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eConsole knows who users are (because they have to log in) and which services they’ve looked at recently (so they can quickly navigate back to where they’ve been) but doesn’t do anything else with that data. Personalization, however, could allow Console to immediately show you services that your team owns, any messages, alerts, or other issues that are present, and let a user favorite any services or other entities.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eDependency Correlation\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eEvery service at Riot has services that it depends on and, conversely, services that depend on it. With dependency correlation, Console, in the event of a service with an outage or other issue, could show users of other services within the afflicted service’s dependency chain that there is an active issue. This could assist engineers when triaging their own services, as well as allow operations teams to better understand the effects of issues on other services and products at Riot.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eAs you can see, Console has become a highly usable one-stop-shop for Riot engineers. Throughout its development we’ve prioritized feedback from engineers and teams across Riot, and as we look to the future, we continue to integrate input from the audience that will use our tools daily. As more teams add features, Console will continue to improve, and we’re invested in ensuring an excellent experience for developers across Riot so they can focus on what they do best.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading! If you have questions or comments, feel free to post them below.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/122/consoleheader.png\" width=\"1600\" height=\"611\" alt=\"\" /\u003e\n\u003cp dir=\"ltr\"\u003eHello! I’m James “WxWatch” Glenn and I’m a software engineer on the Riot Developer Experience: Operability (RDX:OP) team. My team focuses on providing tools for Riot engineers and operations teams that help them better understand the state of their live services across the globe. Some of these tools include Riot’s service metrics, logging, and alerting pipelines. In this article, I’ll be talking about our one-stop-shop application for Rioters operating services - Console.\u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/122/consoleheader.png",
      "date_published": "2021-03-30T17:47:57Z",
      "author": {
        "name": "James Glenn"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/keeping-legacy-software-alive-case-study",
      "title": "Keeping Legacy Software Alive: A Case Study",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv property=\"content:encoded\"\u003e\u003cp dir=\"ltr\"\u003eHi all, Brian \u0026#34;Penrif\u0026#34; Bossé here with a fresh batch of gory, nerdy details surrounding an outage for \u003cem\u003eLeague\u003c/em\u003e. Today we\u0026#39;ll be going through why the EU West shard was out to lunch for just over five hours on January 22, 2021. We don\u0026#39;t always write these things up - they take time to do and the reasons for outages aren\u0026#39;t always that interesting - but this one was particularly painful as it was quite long and on the heels of some other, unrelated outages so figured it\u0026#39;d be worth a dive. Hopefully by the end, you\u0026#39;ll have a better idea of what\u0026#39;s going on behind the scenes when \u003cem\u003eLeague\u003c/em\u003e is down and what problems we are tackling to reduce the frequency and severity of our outages.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eSo here\u0026#39;s a rough graph of the incident - the purple line is the number of players logged in to the client and the green is the number of players currently in a game. Clearly, something terrible happened at 5:25 and the data goes poof (red dotted line), but attempts a recovery just before 6:00. At that point, we took the shard down for a reboot, after which you can see a very sporadic amount of data coming in with clear trend lines of folks logging in, but not many actual games happening, which continues \u0026#39;til 8:15ish (yellow dotted line). After that, the data becomes more solid, but doesn\u0026#39;t look remotely healthy (green dotted line), and everything hits the floor again at 9:10, when we rebooted the shard a second time. A slow recovery period follows (pink dotted line), and we\u0026#39;re back at it by 11:00.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eNow that you have a general idea of what things looked like from our status monitors, we can take a look at both the technical breakdown and the human element of the incident response.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eIn short, a database primary serving some non-critical functions experienced a hardware failure. These things happen.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eWait, but like that\u0026#39;s normal, why didn\u0026#39;t it fail over?\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWe haven\u0026#39;t set up automatic failover to secondary for that database.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eUm, why not?!\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003ePriorities. This whole area of our backend is the old legacy tech - the Platform - that we\u0026#39;ve been spending a lot of effort in the past few years moving away from. A lot has been moved from there, but some things remain - some of them critical. But this database in particular wasn\u0026#39;t critical, and it was assumed a failure would not cause significant outage.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eWell.\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eYeah, about that. The primary reason we\u0026#39;re moving away from this old monolithic setup is because of how tightly coupled absolutely everything was inside of it is, so that irregularities in one corner of the system don\u0026#39;t blow up totally unrelated systems. Like some non-critical database blowing up, you know, the whole shard. Unspoken dependencies are a huge problem in software architecture, and this incident is rife with them. I don\u0026#39;t mean to rag on monolithic software here by default - it is completely feasible to manage a monolith in such a way that separation of concerns exists and errors don\u0026#39;t cause systemic problems. It\u0026#39;s just that \u003cstrong\u003eour\u003c/strong\u003e monolith wasn\u0026#39;t managed that way at all. It\u0026#39;s the tight, implicit coupling that\u0026#39;s the enemy here, so let\u0026#39;s dig into what those were.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFirst up - all database connections from the Platform process go through connection pooling. This is a great pattern that allows for separation between connections to different databases, so that problems with one don\u0026#39;t affect others. Sounds great, right? Well, the problem is, all of those connection pools utilize the same thread pool, and \u003cstrong\u003ethat\u003c/strong\u003e got starved by operations that couldn\u0026#39;t complete against the failed database. This is a sneaky problem; it feels like you\u0026#39;re practicing separations of concerns well, \u0026#39;cause you\u0026#39;re handling the problem of connection management separated from the rest of the business logic and in isolated pools. However, implicitly sharing the underlying threads gave a route for a single, unimportant database failure to cause an exhaustion of a shared resource needed by the entirety of the system to function. It\u0026#39;s like the classic \u003ca href=\"https://en.wikipedia.org/wiki/Dining_philosophers_problem\" target=\"_blank\"\u003eDining Philosophers synchronicity problem\u003c/a\u003e, except one philosopher just up and stole all the forks and went home. Nobody havin\u0026#39; pasta tonight.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eHere\u0026#39;s where the human element of incident response comes into play - nobody in the response team understood that old piece of software well enough that they could pinpoint that fact in the middle of a crisis triage. When your software\u0026#39;s down and folks are screaming to have it back up, you need a stone-cold understanding of exactly how all of your systems and the systems they interface with work. Without it, you start latching on to narratives that make sense in the moment based on intuition. With the complexity of everything that goes into making this game run, no single human can possibly have that amount of understanding over the whole ecosystem, so red herrings get chased. Common narratives become guilty until proven innocent, and they take precious time to prove false.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn this particular case, the red herrings took a \u003cstrong\u003elong\u003c/strong\u003e time to prove false. We had recently been dealing with \u003ca href=\"https://twitter.com/Riot_Penrif/status/1352376251239993345\" target=\"_blank\"\u003emalicious network attacks\u003c/a\u003e and there was some hardware maintenance going on that had caused minor impacts in other parts of the world, so the early incident response focused almost completely on determining if those factors were in play or not. Between that strong focus and an absolute flood of alerts from many services being degraded, the alert regarding the failed database was not noticed until about an hour into the outage. Unfortunately, that was \u003cstrong\u003eafter\u003c/strong\u003e a time-intensive soft restart of the platform software had been completed.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eFlying Blind\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eAt this point, we have a restarted platform that booted up on top of a failed database. Once identified, we were quick to get that database moved over to its secondary and healthy, but it was unclear whether the platform was in a functional state or not. Metrics were trickling in very slowly - you can see from the graph above we were getting reports from it very sporadically. Without that visibility it was exceedingly difficult to make confident decisions on how to proceed. This brings us to the second implicit coupling that plagued this incident - that all systems are running on the same JVM. As the soft-restarted software struggled to recover itself while taking on the load of players trying to log back in, garbage collection began taking significant chunks of time, essentially pausing the normal operations of the process for seconds at a time. Metrics reporting was not tolerant to those pauses, leaving giant holes in the data.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eUltimately, the only thing we could really trust was that games weren\u0026#39;t starting at the rate we would expect given how many people were logged in. Most of the component systems that exist along that critical path were reporting to be at least \u003cstrong\u003epartially\u003c/strong\u003e healthy, with maybe some nodes in a bad state - but nothing that should itself cause a system-wide failure.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAs time dragged on and we weren\u0026#39;t getting clear answers, we made the call to do a hard reboot of the cluster on the belief that the cascading effects of the failed database caused some form of systemic contagion that could not be recovered from. We suspect the core problem was a failure to isolate resources in our in-memory database cache solution, another source of implicit dependency - but lacked the data to be certain.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eTurn It Off And Back On Again\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/legacysoftwarealive_1_0.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eHey, it works. Be it your home router or the backend for a video game, when a complex system starts acting irrationally, nuking the state and starting from a known good is very effective. The downside to doing so as a large shard like EUW comes into peak is that you get a lot of synchronized actions as players rush to log in once it\u0026#39;s back up. What\u0026#39;s normally a steady stream of mixed traffic all of a sudden lines up as you return to service from a cold start. The only effective way to mitigate that is via a login queue, and we\u0026#39;ve got one of those!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eUnfortunately, it misbehaved, and didn\u0026#39;t really respect the limits we had on it which caused a lot more spikey of a flow than we were after:\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/legacysoftwarealive_2.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eEvery time we raised the limit, a rush of players would be let in, but then it would restrict it back down to a very small number. This gave us a false sense of security as we were watching the health of the cluster and seeing good things, assuming that the rate of players coming in was roughly the rate we had set. Ultimately, that resulted in false confidence which led to us effectively removing the limit in two large gulps, and nearly took us back down again. Thankfully, other than our metrics horking themselves some more, nothing went boom and the incident was fully resolved.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eClearly there\u0026#39;s some core problems we need to address here. At the root, our databases need to be more fault tolerant. Walking up the chain, our systems need to become more resilient and less reliant on implicitly shared dependencies. Finally, our incident response must become crisper and more capable of ruling out potential factors. That\u0026#39;s a tough problem to crack because you\u0026#39;re simultaneously dealing with an information management problem with high scope and the need for near-instant retrieval, and with figuring out how to predict what information will be useful in circumstances that were definitionally not foreseen.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eI\u0026#39;m quite pleased to be able to share with everyone that we\u0026#39;ve already got efforts well underway to address the root cause of this incident, by way of a BONUS INCIDENT REPORT!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/legacysoftwarealive_3.png\"/\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eBonus Incident Report\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eThat\u0026#39;s right, two for the price of one!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAt 4am PST, February 2, the hardware servicing EUNE\u0026#39;s core Platform database experienced a hardware failure. This database is central to many mission-critical functions performed by the legacy Platform software. Immediately, login events dropped, and it was clear the Platform was beginning to fail. Forty seconds into the incident, the automatic failover procedure had the database instance restarted, and the complete failover process was finished 68 seconds after the initial failure. Platform software almost completely recovered on its own, requiring intervention only to clear a minor non-impacting issue.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn addition to logins being unavailable for about a minute, approximately 5,000 players experienced a disconnection from the shard, but were able to immediately reconnect.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003ePlayers got to play, devs got to sleep - that\u0026#39;s what living in the future looks like.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eThanks for coming along for this ride with me, I hope it gave you some insight into the work that our behind the scenes teams are doing to increase the stability and reliability of \u003cem\u003eLeague\u003c/em\u003e. Our “to do” list is long and challenging, but we’re highly motivated by turning fragile systems capable of multi-hour outages into robust setups that blip at the worst. If the wins keep comin\u0026#39;, then we keep goin\u0026#39;.  \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIf that sort of work sounds interesting to you and the idea of replacing the engine on a plane while it\u0026#39;s at 30,000 ft frightens you in an exciting way, head over \u003ca href=\"https://www.riotgames.com/en/work-with-us#craft=engineering\" target=\"_blank\"\u003eto our jobs page\u003c/a\u003e and check out available positions.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/119/legacysoftwarealiveheader.jpg\" width=\"1999\" height=\"798\" alt=\"\" /\u003e\n\u003cp\u003eHi all, Brian \"Penrif\" Bossé here with a fresh batch of gory, nerdy details surrounding an outage for \u003cem\u003eLeague\u003c/em\u003e. Today we'll be going through why the EU West shard was out to lunch for just over five hours on January 22, 2021. We don't always write these things up - they take time to do and the reasons for outages aren't always that interesting - but this one was particularly painful as it was quite long and on the heels of some other, unrelated outages so figured it'd be worth a dive.\u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/119/legacysoftwarealiveheader.jpg",
      "date_published": "2021-02-23T19:21:37Z",
      "author": {
        "name": "Brian Bossé"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/engineering-tools-designers-legends-runeterra",
      "title": "Engineering Tools for Designers with Legends of Runeterra",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp dir=\"ltr\"\u003eThe main problem had to do with champion abilities. \u003cem\u003eLeague\u003c/em\u003e had defined really clear blocks for its needs - for example, abilities can be put into a subset of blocks like inflicting a status effect, causing a knock-up, or dealing damage. But for a card game, we found ourselves needing a looser set of rules for actions, because cards often intentionally break rules by doing things like shuffling decks or completing actions at future turns. \u003c/p\u003e\u003cdiv\u003e\n\u003cp dir=\"ltr\"\u003eBasically, designers would keep coming up with cool ideas, and then get bottlenecked because they needed an engineer to create a specific custom block in our visual scripting language. This turnaround became a growing limitation as the design team expanded and more designers wanted to experiment with new playstyles. \u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eLeveraging IronPython for Flexibility\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOur engineering team iterated on some ideas, and eventually landed on a scripting-based solution for designers, integrating with \u003ca href=\"https://ironpython.net/\" target=\"_blank\"\u003eIronPython\u003c/a\u003e. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eA huge benefit of IronPython is that we can access C# objects and call C# methods directly from a script. It’s the glue that connects our C# game engine to our Python card scripts. If a designer has a unique, new gameplay action they\u0026#39;d like to try out, they don\u0026#39;t need an engineer to build something first.\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eExample: Accessing Logic\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eWhen a buff is applied to a card, designers have access to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003ewhich card is getting the buff\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003ewhich card is applying the buff\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003ewhat is the buff being applied\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003ewhat is the entire state of the game when that buff is applied\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003eLet’s take a look at a specific example - the K/DA “Out of the Way” card. \u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/puffcap1.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn this case, a designer wants to create a card that makes all allied buffs permanent. With our Python implementation, they can see all the necessary objects directly. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSince they have all that data in their scripting language, they can listen for buffs being added, and directly change the duration with just two lines of code:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e## EventMutateEffectBeforeAdd\n\nbuffEffect.Duration = duration.Indefinite\u003c/code\u003e\u003c/pre\u003e\u003cp dir=\"ltr\"\u003eFor all you Python programmers out there, the \u0026#34;##\u0026#34; is how our engine understands that this card is listening for an event - in this case, EventMutateEffectBeforeAdd.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis allows us to keep that card\u0026#39;s \u0026#34;allied buffs are permanent\u0026#34; gameplay logic out of the C# game engine by storing it in a place where designers can tweak and balance.\u003c/p\u003e\n\u003cdiv\u003e\n\u003ch3 dir=\"ltr\"\u003eExample: Building Libraries\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eIn the past, if a designer wanted certain functionality in a library, they needed to wait for an engineer to have time to build it out in the game engine. With our Python solution, designers build code libraries that live entirely within the script, so they can create entire gameplay systems without ever needing to get another developer involved.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFor example, designers spun up the history system that tracks everything that happens over the course of a game. This history system is a Python script that tracks events and stores data that can be referenced later. It also exposes an API for designers to directly retrieve that data and use it in card scripts. This is important for complicated cards like champions, which often require keeping a count of values like “how many spells have been cast?” \u003c/p\u003e\n\u003cdiv\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/puffcap2.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eA card that relies on the “history” value.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe tradeoff here is that designers need to be more technical - at many studios, designers only use visual scripting systems and rarely need to dive into the code. We found it incredibly helpful to work with particularly technical designers, who ended up training other designers and onboarding new hires quickly.\u003c/p\u003e\n\u003cdiv\u003e\n\u003ch2 dir=\"ltr\"\u003eGeneral Architecture\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/puffcap3.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eEach card has its own small Python script associated with it. We’re able to call Python from C# and pass along C# object references. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe also have markup in the C# code, and a separate script that generates fake Python that we use for autocomplete when the Python script writers deal with these alien C# objects. Without this bit, they wouldn\u0026#39;t know what methods they were allowed to call on the C# objects without reading the code directly.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFinally, we wrote a plugin for Visual Studio Code that gives designers direct access to available methods. This gives them handy references for stuff like game events they might want to reference, and quick access to all other script files.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWithin each card script, designers call out events they’re interested in. For example, if a card needs to make you draw a card whenever anyone takes damage, then you can have the script listen for when damage is dealt and add modifiers. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis is a very simple 2-line script file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e## EventDoDamage\n\ngame.Draw()\u003c/code\u003e\u003c/pre\u003e\u003cp dir=\"ltr\"\u003eNote that “game” here is a reference to our C# game state that was passed to Python.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eEverything Is \u003cs\u003eA Minion\u003c/s\u003e On Cards\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eA foundational aspect of our scripting system is how we handle card storage. Designers store everything on cards - kind of like that joke about how \u003ca href=\"https://technology.riotgames.com/news/taxonomy-tech-debt\" target=\"_blank\"\u003eeverything in \u003cem\u003eLeague\u003c/em\u003e is a minion\u003c/a\u003e, everything in \u003cem\u003eLoR\u003c/em\u003e is stored on cards. For example, the Nexus is actually a card, and it stores the history tracking we mentioned earlier. When a unit is played, the Nexus has a script that listens and increments the count on its own storage, and those values can be referenced by other cards.  \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/puffcap4.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eYou’re saying those tiny things store WHAT?\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis is a hugely critical piece of how \u003cem\u003eLoR\u003c/em\u003e stores data. Designers can synthesize entirely new gameplay mechanics without requiring engineers to build anything new, because they can trigger off of any change in the game state, store any value in card storage, and have access to all of the basic blocks of game state manipulation that engineers do (moveCard, doDamage, attachScript, etc).\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eStory Time: Making the Transition \u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eSwitching an underlying game system is a massive undertaking. Our new scripting system definitely lived up to our expectations, but the transition had a couple bumps along the way. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOne story that really demonstrates the complexity of shifting an underlying system happened around three years ago during a big internal playtest. We had just wrapped up getting a build of the game working with Python, but those code paths were only taken when there were actually Python scripts to load. This was done so that it wouldn’t impact the playtest, which was still running on the original \u003cem\u003eLeague \u003c/em\u003eblock system. We kicked off a deploy of the block script build, and walked over to visit the playtest presentation in person, confident that the build was stable and the Python-based system was separate on our testing environments. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eImagine this - the presentation wraps up, right, and a crowd of Rioters are heading back to their desks to play, when someone says, “hey, uh… everything is broken.” Total nightmare material. This made no sense, because nobody had actually made any changes - we had just done a rebuild and redeploy of existing content we had tested already.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSo what happened? \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe have a set number of shared build nodes that we reuse, and try to only clean up what’s strictly necessary to avoid copying over duplicate assets every time. The Python builds had been running on a couple of them because we were iterating earlier, but once the Python build had run on the node, it effectively infected it, leaving the Python scripts behind. The block script-based rebuild/redeploy we did for the playtest happened to run on the nodes that had been polluted by the Python build. So some of the cards were loading Python scripts, and some of the cards were loading block scripts, and some did both so nothing would happen. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOnce we remoted in and realized there was both Python and block data, it just took a few hours to patch up and the playtest went on. But it was definitely a spooky event that demonstrated the complexity involved when transitioning over an underlying system in a live game. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOur biggest takeaway? While it can be an elegant solution to use the existence of new data to control which system is activated, \u003cstrong\u003esometimes just having a good ol’ toggle is an important safeguard when replacing a legacy system\u003c/strong\u003e. \u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eTeemo and his related Puffcap mushroom cards presented complex technical challenges that really highlighted the impact of our new scripting system. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis set of cards is based on a gameplay system that exists almost entirely in our Python scripting environment. Designers are able to quickly iterate on these kinds of ideas - like Puffcaps being objects placed on existing cards instead of individual cards placed in a deck. We built up a Python library of Puffcap functions which defined a really clear API for how cards could plant mushrooms into an opponent’s deck. The Puffcap’s representation in the game, how to add them, and what they do are \u003cem\u003eall defined within the Python scripting system\u003c/em\u003e.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/puffcap5.png\"/\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eIterating on Shrooms\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWe had a couple Puffcap iterations as Teemo’s shrooms became cooler and cooler over time. As we moved from spells in our block system to spells in Python, designers were increasingly able to resolve bugs themselves instead of requiring engineering time. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe Puffcap cards were originally planted in your deck by Teemo or his followers, and when you drew the cards, they’d cast a spell and you’d take damage. The Teemo level 2 card doubles the number of existing Puffcaps in a deck - which is simultaneously very exciting and \u003cem\u003every\u003c/em\u003e dangerous - so the Puffcap cards had the ability to crash entire games if enough were placed into a deck. With the new Python scripting language, we were able to port over the Puffcap system smoothly and simultaneously do some code cleanup, making it possible to switch the Puffcaps from individual cards to traps on existing cards.\u003c/p\u003e\n\u003cdiv\u003e\n\u003ch3 dir=\"ltr\"\u003eNot So Funsmith\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/puffcap6.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOne example of a problem with the original card iteration had to do with other cards that amplified damage. The card Funsmith from the original Foundations base set would increase any damage from spells or skills. But if you had a Funsmith and an enemy put a Puffcap in your deck, you\u0026#39;d be the one drawing the card - in other words, the source of the damage was technically you, so the damage would be amplified. Turning the Puffcaps into traps on specific cards gave us the chance to do some damage source redirection, which we use to clarify that this damage comes from the enemy and is not self-inflicted.\u003c/p\u003e\n\u003cdiv\u003e\n\u003ch3 dir=\"ltr\"\u003ePerformance\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eOnce we had switched the Puffcaps into traps on cards, we ran into another issue - performance. The original Puffcap system would plant traps into the enemy’s deck, and the game would go through each Puffcap one by one, randomly generating a number between 0 and the deck size, and placing the trap on that card specifically. This resulted in totally random distribution, which was great. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBut as we were preparing for the Friend Challenge, we realized that if two friends can play against each other, there will probably be players who want to work together to see how many Puffcaps can possibly be placed in a deck. This would cause the server to fall over due to the sheer amount of shrooms that needed to be assigned a number, so one game could feasibly crash all the other games running on that server. We tweaked the Puffcap insertion algorithm to instead go card by card in the deck and estimate how many mushrooms should be planted on each card. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAn easy way to calculate this would have been to just divide the number of shrooms by the number of cards and fudge the number a bit so it feels random, even though they’re evenly dispersed. We knew that would feel really bad and go against the intended sense of randomness that Puffcap cards promise, so we looked at statistical models to figure out what a more realistically random system would be. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo accomplish this, we generate a standard distribution for how many Puffcaps should be planted on each card, and then we randomly pick a point on that curve for each card. After that, we do some slight tweaking to make sure the number of Puffcaps we placed equals the total number of intended Puffcaps.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eFuture Proofing\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eAs we went through the transition to the new Python system, we were able to do a lot of code cleanup. For example, at one point, the traps themselves were a first class citizen in the C# game engine, as they were their own type of card. For just one mechanic that was unique to Teemo and his followers, that was pretty overkill. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWhile removing the Puffcap cards and turning them into traps, we thought a lot about future-proofing. If designers later want multiple trap-type cards that mimic the Puffcap mechanic of actions that live on other cards in a deck, they can easily iterate on their ideas by extending the “traps” Python library we’ve built out. \u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eOne of the best parts about working on\u003cem\u003e LoR\u003c/em\u003e is how integrated engineering and design are. The development environment is heavily collaborative, which allows engineers to get a better sense of what designers will need in the future. We can be more creative with our solutions; we’re not just handed a tech spec, we’re part of the conversation about the problem that needs to be solved.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe ratio of engineers vs designers on a card team really reflects this. Early on, we had two designers and four engineers. This shifted over time as we built out tools that could better support the designers - we now have around 15 designers and just three engineers.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eEngineers on \u003cem\u003eLoR\u003c/em\u003e are constantly measuring our progress against this goal - make designers’ lives easier. With early prototyping, it’s clear how many ways artists and designers rely on tooling tech to be able to do their jobs... and how this can slow them down. We need to build tools and processes to make things go smoothly, reducing engineering bottlenecks and encouraging creativity and experimentation. It’s their job to find the fun, and it’s our job to build enough highly usable tech that designers only have to talk to us when they want to.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading! If you have any questions or just want to tell us about your favorite cards, feel free to comment below.\u003c/p\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/118/puffcapheader.png\" width=\"1640\" height=\"624\" alt=\"\" /\u003e\n\u003cp\u003eWe’re Patrick Conaboy and Jeff Brock and we’re senior software engineers on the \u003cem\u003eLegends of Runeterra\u003c/em\u003e Card Design team. The engineers on our team are responsible for building tools, writing game logic, and supporting card designers. In this article, we’ll be covering how engineers on \u003cem\u003eLoR\u003c/em\u003e built out a new scripting system that enables designers to iterate and experiment with new card ideas easily. \u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/118/puffcapheader.png",
      "date_published": "2021-01-26T18:10:56Z",
      "author": {
        "name": "Jeff Brock and Patrick Conaboy"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/scalability-and-load-testing-valorant",
      "title": "Scalability and Load Testing for VALORANT",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv property=\"content:encoded\"\u003e\u003cp dir=\"ltr\"\u003eHi, I’m Keith Gunning, lead engineer on the Core Services team for \u003cem\u003eVALORANT\u003c/em\u003e. My team is responsible for making sure \u003cem\u003eVALORANT\u003c/em\u003e’s platform can scale to support our growing playerbase, with high uptime and sufficient monitoring to quickly detect and fix problems.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWhen \u003cem\u003eVALORANT\u003c/em\u003e was still early in development, we had high hopes that in the future we’d launch with high initial popularity. From the beginning, we prioritized scalability to make sure we could support the number of players we were hoping for. Once \u003cem\u003eVALORANT\u003c/em\u003e entered full production, we began working in earnest on a load test framework to prove out our tech. After many months of work, we successfully ran a load test of two million simulated players against one of our test shards, giving us the confidence we needed for a smooth launch. This article explains how we load tested our platform, and how we tackled the scaling challenges we encountered along the way. \u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eTwo key pieces of \u003cem\u003eVALORANT\u003c/em\u003e’s infrastructure allow us to support a large number of players - the\u003cstrong\u003e game server executable\u003c/strong\u003e and the \u003cstrong\u003eplatform\u003c/strong\u003e. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe game server executable controls the frame-to-frame gameplay of in-progress matches. To run a large number of games simultaneously, we need to run thousands of these game server executables, one for each running match. \u003ca href=\"https://technology.riotgames.com/news/valorants-128-tick-servers\" target=\"_blank\"\u003eThis Tech Blog post\u003c/a\u003e by Brent Randall from the \u003cem\u003eVALORANT\u003c/em\u003e Gameplay Integrity team explains the efforts made to hyper-optimize the game server executable, allowing it to run over three 128-tick games of \u003cem\u003eVALORANT\u003c/em\u003e per CPU core, minimizing the number of total CPUs necessary to host every game.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe second piece of infrastructure, which I’ll refer to as the \u003cem\u003eVALORANT\u003c/em\u003e platform, is a collection of microservices that handles every other part of \u003cem\u003eVALORANT\u003c/em\u003e besides moment-to-moment gameplay. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eHere\u0026#39;s a list of services the platform includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eparty membership\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003ematchmaking\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eAgent selection\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eprovisioning matches to datacenters based on ping\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eprocessing and storage of game results when games are finished\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eunlocking Agent contracts and Battlepasses\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003estore purchases\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eloadout selection\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003eThese platform services, together with the game provisioner pods, form a \u003cem\u003eVALORANT \u003c/em\u003eshard.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/loadtesting-1.png\"/\u003eEach shard\u0026#39;s platform manages a set of geographically distributed game server pods\u003c/em\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eA Closer Look At The \u003cem\u003eVALORANT\u003c/em\u003e Platform\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eEach piece of the platform’s functionality is encapsulated in a microservice, a small program responsible for only its own specific \u003cem\u003eVALORANT\u003c/em\u003e feature. For example, the personalization service stores the list of gun skins, sprays, and gun buddies players currently have equipped, and not much else. Some features require several microservices to communicate with each other. For example, when a party queues up for a ranked game, the parties service will ask the skill ratings service to verify that the players in the party are within the allowed rank spread to play a competitive match together. If so, it will then ask the matchmaking service to add the party to the competitive queue.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eVALORANT\u003c/em\u003e chose to create a platform composed of microservices based on what we had learned from \u003cem\u003eLeague of Legends\u003c/em\u003e. In the early days of \u003cem\u003eLeague\u003c/em\u003e, the platform was a monolithic service that could do everything. This made development and deployment simple, but came at a cost. A bug in one subsystem could bring down the whole platform. There was less flexibility to independently scale up individual subsystems, and individual subsystems couldn’t be built, tested, and deployed independently. \u003cem\u003eLeague\u003c/em\u003e has since upgraded to \u003ca href=\"https://technology.riotgames.com/news/running-online-services-riot-part-i\" target=\"_blank\"\u003ea more flexible microservices model\u003c/a\u003e to address these problems. Following in \u003cem\u003eLeague’s\u003c/em\u003e footsteps, \u003cem\u003eVALORANT\u003c/em\u003e avoided the pitfalls of a monolithic architecture by splitting functionality into smaller microservices from the start.\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eMicroservices\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eThe \u003cem\u003eVALORANT\u003c/em\u003e platform consists of over 30 microservices. Each shard needs a certain number of instances of each microservice to handle peak numbers of players. Some microservices are busier and require more instances than others to distribute the load. For example, the parties service does a lot of work, since players are constantly joining, leaving, and changing the state of their parties. We have many instances of the parties service running. The store’s microservice requires fewer instances, since players buy from the store much less frequently than they join parties.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo support the high load we were hoping for on launch day, we needed to answer several questions. How many instances of each microservice would be necessary? Did any microservice have a serious performance problem that couldn’t be solved by just deploying more instances of the service? Did any service have a bug that only manifested when handling large numbers of player requests at the same time? We also needed to make sure that we weren’t wasting money by overprovisioning too many instances of services that didn’t need it.\u003c/p\u003e\n\u003ch4 dir=\"ltr\"\u003eMicroservice Testing\u003c/h4\u003e\n\u003cp dir=\"ltr\"\u003eEach of our microservices has its own tests to validate the correctness of its functionality. But because player actions can require complex chains of calls between services, testing each service by itself in a vacuum isn’t enough to prove that the platform as a whole can handle the load generated by all the various types of player requests. The only way to know for sure that the platform can handle load from real players is to test it holistically with real player requests. And since it was infeasible to get millions of human players to test our still in-development game, we decided to do the next best thing: Simulate virtual players that would mimic the behaviors of actual players. \u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eWhen \u003cem\u003eVALORANT\u003c/em\u003e first entered production, we had only tested with about 100 players simultaneously. Based on stats from\u003cem\u003e League of Legends\u003c/em\u003e, optimistic estimates from our publishing team, and our future plans to expand into many regions, we decided that we wanted to make sure we could support at least two million concurrent players on each shard.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eRunning two million instances of the actual \u003cem\u003eVALORANT\u003c/em\u003e client that real players use wasn’t a feasible solution. Even if we disabled all graphical rendering, the overhead of creating this many processes was still excessive. We also wanted to avoid cluttering the client with all the extra code needed to run simulated automated tests. To maximize performance and ease of maintenance, we instead created a simulated mock client called the load test harness, specifically designed for simulating large numbers of players.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eLoad Test Harness\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eThe load test harness needed to make all the same requests to the \u003cem\u003eVALORANT\u003c/em\u003e platform that a real player would. But it didn’t need to simulate any of the game logic or rendering, since we already had the \u003ca href=\"https://technology.riotgames.com/news/valorants-128-tick-servers\" target=\"_blank\"\u003egame server load test\u003c/a\u003e to validate those pieces. The need to efficiently simulate large numbers of concurrent network requests was the key requirement that informed our technical decisions when designing the load test harness.\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eWriting in Go\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eWe chose to write the harness in \u003ca href=\"https://golang.org/\" target=\"_blank\"\u003eGo\u003c/a\u003e for a few reasons. First, its concurrency model allows for large numbers of routines to run concurrently and efficiently on a small number of threads, letting us simplify implementation of player behaviors by running parallel routines for every simulated player. Second, Go’s network I/O model that automatically handles blocking network calls using asynchronous ones under the hood allows us to easily run a large number of concurrent blocking http requests on a limited number of threads. And finally, Go is a \u003ca href=\"https://technology.riotgames.com/news/leveraging-golang-game-development-and-operations\" target=\"_blank\"\u003ewidely supported language at Riot\u003c/a\u003e that our team has expertise in. In fact, almost all of the services composing the \u003cem\u003eVALORANT\u003c/em\u003e platform are written in Go for these same reasons.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/loadtesting2.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eGo\u0026#39;s scheduler distributes lightweight routines across a smaller number of hardware threads to reduce overhead\u003c/em\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eHarness Components\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eThere are three main components to the load test harness: a simulated player, a scenario, and a player pool. Let’s take a look at how each of these work.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe first component of our harness is a \u003cstrong\u003esimulated player\u003c/strong\u003e. This simulated player aims to mimic the load of a single real \u003cem\u003eVALORANT\u003c/em\u003e client as closely as possible. The player runs the same background polling loops that a regular client would run. Some examples of this background polling include login token refreshing, dynamic configuration polling, and matchmaking queue configuration polling. The player also includes helper functions to run discrete operations that might be performed by a real player, such as buying from the store, equipping a gun skin, joining a party, queueing for a match, and selecting an Agent.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe second component of the harness is a \u003cstrong\u003escenario\u003c/strong\u003e. A scenario defines a set of actions commonly performed by players in sequence. For example, the MatchmakingScenario contains logic instructing a simulated player to queue for a match, join an Agent selection lobby when the queue pops, select an available Agent, lock in the Agent, and connect to the game server when it’s provisioned. The StorePurchase scenario fetches the store catalog and then purchases an available offer. The LoadoutEdit scenario equips some gun skins. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe third key component of the harness is the \u003cstrong\u003eplayer pool\u003c/strong\u003e. The pool creates new player objects at a configurable rate, meant to approximate the login rate of real players. Logged in players are added to a pool of idle players. Idle players are pulled out of the pool at a configurable rate and assigned to perform scenarios. Once a player finishes its assigned scenario, it returns itself to the idle pool. Before launch, we used data from \u003cem\u003eLeague of Legends\u003c/em\u003e to estimate how frequently we expected players to perform operations like queueing for matches and buying content, and we configured our player pool to assign these operations to idle players at that same rate.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/loadtesting-3.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eEach concurrently running scenario reserves a number of players from the pool depending on the scenario type and configuration\u003c/em\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003ePrepping for Tests\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOur first load tests involved just a single harness. We deployed one instance of the harness and ramped up the number of players to see how many we could simulate. After some careful optimizing and tweaking, we found that we could simulate 10,000 players on one harness process running on four physical CPUs. To simulate two million players, we would need 200 harness processes. We asked Riot’s Infrastructure Platform team to provision a compute cluster that could handle this for us. With \u003ca href=\"https://aws.amazon.com/\" target=\"_blank\"\u003eAmazon AWS\u003c/a\u003e, they deployed a new instance of the Riot Container Engine, a piece of Riot tech that uses \u003ca href=\"http://mesos.apache.org/\" target=\"_blank\"\u003eApache Mesos\u003c/a\u003e to schedule \u003ca href=\"https://www.docker.com/\" target=\"_blank\"\u003eDocker\u003c/a\u003e containers on top of heavy-duty \u003ca href=\"https://aws.amazon.com/ec2/\" target=\"_blank\"\u003eAmazon EC2\u003c/a\u003e compute nodes. Thanks to this cluster, we were able to provision 200 Docker containers of our harness, allowing us to test at the concurrency numbers we were looking for.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/loadtesting-4.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eOur load test environment uses Amazon AWS, Riot Container Engine, and Docker to deploy a large number of load test harness processes into the cloud\u003c/em\u003e\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eSimulating Game Servers\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eBefore we could run a full-scale load test with multiple harnesses, there was one final piece we needed to put in place - the game servers. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe had already set up a test environment including enough instances of our platform services to begin testing against. Our harnesses were ready to simulate two million clients. But actually running two million players’ worth of game servers would prove to be a challenge. Several factors made it impractical for us to use real game servers in our load testing environment. \u003c/p\u003e\n\u003cul\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eReimplementing gameplay logic and network protocols in the load test harness to interface with real game servers would be complicated and time consuming. \u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eSending and receiving all that additional network traffic would significantly increase the networking and computing resources needed by the harness, reducing the number of players we could simulate.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eDeploying that many real game servers would require many thousands of cloud CPUs, which we wanted to avoid having to provision if possible. \u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003eSo instead, we created a simple mock game server, written in Go, which could emulate the same provisioning process and end-of-game results recording that a real game server could, without any of the actual gameplay logic. Because these mock servers were so lightweight, we could simulate over 500 mock games on a single core. When a party of simulated players requested to start a game on the load test environment, they simply connected to the mock game server and did nothing for a configurable amount of time before ending the game.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn this way, we simulated every part of the game provisioning flow without relying on real game servers.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eBuilding Out \u0026amp; Running The Load Tests\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWith all these pieces in place, we were ready to run a load test. For our very first load test, we ran one thousand simulated players, recorded all the performance issues we ran into, and set out to address them. Once we resolved those issues and this test passed, we doubled the player count for subsequent tests, and repeated the process of identifying and fixing issues. After enough cycles of doubling our target player count, we eventually reached our two million player goal. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eReaching this goal was a huge accomplishment. It was also a huge relief, since platform scale validation was the final hurdle we had to clear before we could announce our launch. Watching real players get their hands on the game was incredibly exciting - it felt so good to see our work pay off. Our relatively smooth launch day was proof of the quality of the tech we had built.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSo how did we get there? Let’s take a look at some of the issues we encountered as we increased our scale targets, how we identified them, and how we addressed them.\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eExtensive Monitoring\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eBecause simulated players can’t talk or file bug reports, the first step to identifying our scale issues was to set up extensive monitoring, dashboarding, and alerting to let us monitor the health of our platform. Each of our microservices emit metrics events for successes and failures of all operations, runtimes for all operations, CPU and memory levels, and other information. We store all these metrics in \u003ca href=\"https://newrelic.com/\" target=\"_blank\"\u003eNew Relic\u003c/a\u003e, and use \u003ca href=\"https://grafana.com/\" target=\"_blank\"\u003eGrafana\u003c/a\u003e to create dashboards displaying the data. We also set up New Relic alerting rules that will send us Slack messages if any of our metrics go outside our expected thresholds. For each load test, we counted the test as a pass only if every metric stayed within bounds for the full duration.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/loadtesting-dashboard.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eA portion of our dashboards showing the results of a successful two million player, three hour load test\u003c/em\u003e\u003c/p\u003e\n\u003ch3 dir=\"ltr\"\u003eScalable Databases\u003c/h3\u003e\n\u003cp dir=\"ltr\"\u003eOne topic that came up early on as a potential performance concern was our need for scalable databases. \u003cem\u003eVALORANT\u003c/em\u003e needs to reliably store a lot of information about player accounts, like skill ratings, match histories, loadout selections, and Battlepass progress. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe easiest way to store all this data would be to put all player info into a single database. This would be easy to implement, very safe because updating all the info at once in an all-or-nothing manner makes it easy to avoid data corruption, and scalable up to a point by just upgrading to bigger and better database server hardware when available. Unfortunately, there’s a limit to how far this strategy can scale. Once you’ve bought the biggest hardware available and it’s still not enough, there are few options left. We decided not to go with a single database because we wanted to make sure we could scale horizontally, meaning we could increase our capacity by adding more smaller servers, rather than trying to further upgrade one big one. \u003c/p\u003e\n\u003ch4 dir=\"ltr\"\u003eLayers of Horizontal Scaling\u003c/h4\u003e\n\u003cp dir=\"ltr\"\u003eThe first part of our strategy for horizontal scaling is to split a player’s data such that each piece of data was owned by a single service. For example, the match history service saves the data for a player’s match history, and only that data. The loadouts for that player are stored separately in another database, owned by the loadout service. Spreading the data out like this gives each service the opportunity to scale up its database independently. It also reduces the impact of issues by isolating data owned by each service; an issue with the store database may slow down the store, but won’t affect match history performance. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eDoing things this way does come at a cost. Because data isn’t collocated, we can no longer rely on atomic transactions to update multiple pieces of data at once. Safely updating each of these multiple data sets in a single operation requires ledgers for in-progress operations, \u003ca href=\"https://en.wikipedia.org/wiki/Idempotence#:~:text=Idempotence%20(UK%3A%20%2F%CB%8C%C9%AA,result%20beyond%20the%20initial%20application.\" target=\"_blank\"\u003eidempotence \u003c/a\u003eto retry operations on failure, intermediate lock states, and other such complexities. But the added complexity is worth it to allow us to scale further.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/loadtesting5.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003ePlayer data is split across multiple tables in multiple databases to allow for horizontal scaling.\u003c/em\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eFor extremely high numbers of players, even splitting data by the service type that owns it isn’t enough to ensure horizontal scaling. When an individual service can’t performantly handle saving the data for all of its players in a single database, we split the data again, such that each individual database needs to handle only a segment of players. Each \u003cem\u003eVALORANT\u003c/em\u003e player has a unique ID, so in the simplest case, we can reduce our database load by half by storing odd-numbered players on one database, and even-numbered players on a second database. In practice, we ended up segmenting players into one of 64 tables by hashing each player’s ID and using the \u003ca href=\"https://en.wikipedia.org/wiki/Modulo_operation\" target=\"_blank\"\u003emodulo\u003c/a\u003e of that hash to determine their table index. We don’t need nearly 64 databases per service, but by segmenting players into 64 tables ahead of time, it’s very easy for us to horizontally scale by just adding more databases and moving tables from one database to another. For example, we can again halve our database load by switching from storing 32 tables on each of two databases, to 16 tables on each of four databases.\u003c/p\u003e\n\u003ch4 dir=\"ltr\"\u003eLoad Balancing Across Service Instances\u003c/h4\u003e\n\u003cp dir=\"ltr\"\u003eSplitting load by segmenting players by ID is a strategy we repeated again for load balancing across service instances. \u003cem\u003eVALORANT\u003c/em\u003e microservices are stateless, meaning that any instance of a service can handle a request from any player. This is a useful property, as it allows us to load balance incoming requests by assigning them to services round-robin. It also lets our services run without needing large amounts of memory. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOne challenge, though, is how to handle player processing that doesn’t originate from an explicit request from a client. As an example, our session service needs to change a player’s state to offline if that player suddenly stopped sending heartbeats without explicitly disconnecting, perhaps because they crashed or lost power. Which session service instance should measure the duration of non-responsiveness for each player and change their state to offline when that duration reaches a threshold? We answer this by assigning players into one of 256 shares based on the modulo of their player ID hash. We store a list of the 256 shares in a Redis shared memory cache. As services start up, they will check this list, and claim a portion of the shares by registering their own instance ID in the list. When new service instances are deployed to further spread out load, the new instances will steal shares from existing instances until each instance owns a roughly equal number of shares. Each service will then handle the background processing for all the players in the shares that it owns.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/loadtesting6.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eEach app reserves its own segment of players from a central list so that each player\u0026#39;s background processing happens exactly once\u003c/em\u003e\u003c/p\u003e\n\u003ch4 dir=\"ltr\"\u003eCaching\u003c/h4\u003e\n\u003cp dir=\"ltr\"\u003eAnother important technique we applied to improve our scalability was caching. We cache the result of certain computations in several places to avoid having to repeat them when we know the result will be the same each time. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThere were several places where we introduced caches. Each service that frequently returns the same results has a caching layer in its http response handler, which can save the result of calls and return the previously saved result instead of recomputing it. Additionally, services that call other services to fetch data include a cache on the calling end. If a service needed to query the same data twice, it checks its own cache of previously returned results and uses those instead if available. We can even cache at our network edge before requests enter our platform at all. Before reaching our platform, requests from clients first flow through \u003ca href=\"https://www.cloudflare.com/\" target=\"_blank\"\u003eCloudflare\u003c/a\u003e, and then through an \u003ca href=\"https://www.nginx.com/\" target=\"_blank\"\u003enginx\u003c/a\u003e server for load balancing. We can optionally enable caching at these layers to reduce the number of requests that reach our platform at all. In practice, we found few cases where we needed to rely on network edge caching. In most cases we were able to hit our performance goals with our own in-app caching alone, making the logic for cache eviction much simpler.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eIt took us many months to scale the platform up to our desired levels, and we cut it close on our deadline. We had less than two weeks to go until launch when we hit our first successful 2-million CCU loadtest. Nonetheless, the load test results gave us the confidence we needed to ship our game. We scaled up our live shards to the same presets that we had landed on in the load test environment and prepared for launch day. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSo, how did we do? It turns out, pretty well. Aside from a minor issue with parties and matchmaking that required a small code fix to address, \u003cem\u003eVALORANT\u003c/em\u003e’s launch day went smoothly. Our hard work paid off!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eMaking a scalable platform takes more than just one team. Reaching our scalability goals was only achieved thanks to every single feature team on \u003cem\u003eVALORANT\u003c/em\u003e contributing to making their microservices performant. The biggest value of our load tests and dashboards was that they empowered all our devs to make informed decisions about how to build their own services.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eI want to also thank all the central teams at Riot who maintain the technology and infrastructure that we use to deploy our services. \u003cem\u003eVALORANT\u003c/em\u003e couldn’t have launched without support from them.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading!\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/117/loadtestingheaderfinal.png\" width=\"1640\" height=\"624\" alt=\"\" /\u003e\n\u003cp\u003eWhen \u003cem\u003eVALORANT\u003c/em\u003e was still early in development, we had high hopes that in the future we’d launch with high initial popularity. From the beginning, we prioritized scalability to make sure we could support the number of players we were hoping for. Once \u003cem\u003eVALORANT\u003c/em\u003e entered full production, we began working in earnest on a load test framework to prove out our tech. After many months of work, we successfully ran a load test of two million simulated players against one of our test shards, giving us the confidence we needed for a smooth launch. This article explains how we load tested our platform, and how we tackled the scaling challenges we encountered along the way. \u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/117/loadtestingheaderfinal.png",
      "date_published": "2020-12-15T18:14:24Z",
      "author": {
        "name": "Keith Gunning"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/leveraging-golang-game-development-and-operations",
      "title": "Leveraging Golang for Game Development and Operations",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cp dir=\"ltr\"\u003eAt Riot, our two primary languages for services are Java and Go. As a result, both languages are viewed as first class citizens in terms of support - and because we deploy using containers, both are interoperable and relatively easy to package and deploy. We love Go at Riot for a number of reasons including:\u003c/p\u003e\u003cp dir=\"ltr\"\u003eThere’s also been a recent movement within the tech industry around Go, especially with regards to microservices, and it helps being able to tap into that interest and drive in the developer space. It’s also becoming increasingly popular in the system space for software like \u003ca href=\"https://etcd.io/\" target=\"_blank\"\u003eetcd\u003c/a\u003e, \u003ca href=\"https://www.docker.com/\" target=\"_blank\"\u003eDocker\u003c/a\u003e, \u003ca href=\"https://kubernetes.io/\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e, \u003ca href=\"https://prometheus.io/\" target=\"_blank\"\u003ePrometheus\u003c/a\u003e, and much more. There are excellent libraries for \u003ca href=\"https://github.com/sirupsen/logrus\" target=\"_blank\"\u003estructured logging\u003c/a\u003e, \u003ca href=\"https://github.com/hashicorp/raft\" target=\"_blank\"\u003econsensus algorithms\u003c/a\u003e, and \u003ca href=\"https://github.com/gorilla/websocket\" target=\"_blank\"\u003ewebsockets\u003c/a\u003e. Additionally, the standard library includes things like \u003ca href=\"https://golang.org/pkg/crypto/tls/\" target=\"_blank\"\u003eTLS\u003c/a\u003e and \u003ca href=\"https://golang.org/pkg/database/sql/\" target=\"_blank\"\u003eSQL\u003c/a\u003e support, so you can be very productive in Go very quickly.\u003c/p\u003e\u003cdiv\u003e\n\n\u003cp dir=\"ltr\"\u003eThe Service Lifecycle team’s primary project is our deployment tool, which is used to deploy and manage the lifecycle of services running in our Docker runtime. If you’ve read our earlier \u003ca href=\"https://technology.riotgames.com/news/running-online-services-riot-part-i\" target=\"_blank\"\u003e\u0026#34;Running Online Services\u0026#34; series\u003c/a\u003e you’ll get a better idea of the problem space we’re working in. Our deployment tool is written in Go because it enables us to quickly roll out updates, onboard new engineers to our tech stack, and quickly iterate from early development to production. It is backed by MySQL and a single instance can target multiple datacenter locations. There are a number of challenges that Go makes it easier for us to solve including:\u003c/p\u003e\n\u003cul\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eJSON/YAML support\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eHTTP client\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eNetwork connectivity\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eAPI integration\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 dir=\"ltr\"\u003eJSON/YAML support\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOur deployment tool operates on a custom YAML specification that describes what an app needs to run. There are several third party Go libraries which implement \u003ca href=\"https://json-schema.org/implementations.html#validator-go\" target=\"_blank\"\u003eJSONSchema\u003c/a\u003e for us. Go also provides native support for Marshaling and Unmarshaling Go structs into JSON as well as third party support for YAML.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg src=\"https://technology.riotgames.com/sites/default/files/golang1.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eA structured YAML schema that the deployment tool might consume.\u003c/em\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eHTTP Client\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOur tool connects with a number of other microservices for things like service discovery, logging, alerts, configuration management, provisioning databases, and more. The primary method of communication is HTTP requests. This means we often have to consider things such as the lifecycle of the request, internet blips, timeouts, and more. Fortunately, Go provides a very solid \u003ca href=\"https://golang.org/pkg/net/http/#Client\" target=\"_blank\"\u003eHTTP client\u003c/a\u003e with some defaults you’ll definitely want to tweak. For example, the client will never timeout by default.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg src=\"https://technology.riotgames.com/sites/default/files/golang2.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003ePerforming an HTTP request and printing the body of the response.\u003c/em\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eNetwork Connectivity\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOftentimes data centers can be isolated through additional layers of security, especially when working with partner regions. One very useful aspect of Go we’ve used for multiple projects is the Go \u003ca href=\"https://golang.org/pkg/net/http/httputil/#ReverseProxy\" target=\"_blank\"\u003ehttputil reverse proxy\u003c/a\u003e. This allows us to quickly proxy requests, add middleware for the lifecycle of requests to inject additional authentication or headers, and make everything relatively transparent to clients.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eAPI Libraries\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eAt Riot, we must interface with a variety of third party services including Hashicorp \u003ca href=\"https://www.vaultproject.io/\" target=\"_blank\"\u003eVault\u003c/a\u003e, \u003ca href=\"https://dcos.io/\" target=\"_blank\"\u003eDCOS\u003c/a\u003e, \u003ca href=\"https://aws.amazon.com/sdk-for-go/\" target=\"_blank\"\u003eAWS\u003c/a\u003e, and \u003ca href=\"https://kubernetes.io/\" target=\"_blank\"\u003eKubernetes\u003c/a\u003e. Most of these solutions provide native API client libraries for use by Go applications. Sometimes we use or fork third party libraries depending on our need as well. In all cases, we’ve been able to find adequate support for our needs.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAdditionally, during development, it’s easy for us to recompile and run a local version of our deployment tool for quick testing or \u003ca href=\"https://github.com/go-delve/delve\" target=\"_blank\"\u003edebugging\u003c/a\u003e. It also allows us to easily share code and libraries with other teams in our space.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eNow that we’ve taken a look at how my team uses Go for deployment, let’s take a look at two other examples.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eHi, I\u0026#39;m Chad Wyszynski from the RDX Operability team, and I’d like to show you how my team uses Go to minimize request latency in our operational monitoring pipeline. Most of Riot\u0026#39;s logs and metrics flow through my team\u0026#39;s monitoring service. It’s a constant, high volume of traffic that spikes higher when something goes wrong, so the service must maintain high throughput and low latency. Who wants to wait seconds to log an error? Go channels help us meet these requirements.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe operational monitoring service exists for one purpose: to forward logs and metrics to backend observability platforms, such as New Relic. The service first transforms request data into the format expected by the backend platform, then it forwards the transformed data to that platform. Both of these steps are time consuming. Instead of forcing clients to wait, the service places request data into a bounded channel for processing by another Goroutine. This allows the service to respond to the client almost immediately.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBut what happens when the bounded channel is full? By default, a Goroutine will block until the channel can accept data. We use Go\u0026#39;s time.After to bound this wait. If the channel can\u0026#39;t accept request data before the timeout, the service 503\u0026#39;s. Clients can retry the request later, hopefully after some exponential backoff.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/golang3.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThe real win with the channel-based design came when migrating from one observability backend to another. Riot recently \u003ca href=\"https://newrelic.com/resources/case-studies/riot-games\" target=\"_blank\"\u003emoved all metrics and logs from a hand-rolled pipeline to New Relic\u003c/a\u003e. The operational monitoring service had to forward data to both backends while teams configured dashboards and alerts on the new platform. Thanks to Go channels, dual-sending added essentially no latency to client requests. Our service just added request data to another bounded channel. The max server response time, then, was based on the time a Goroutine waited to put data onto a destination channel, not how long it took a destination server to respond.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/golang4.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eI was new to Go when I joined Riot, so I was excited to see a practical use case for channels and Goroutines. My colleague Ayse Gokmen designed the original workflow; I’m stoked to share our work.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eJustin O’Brien here from the Competitive team on Valorant! My team uses Go for all our backend services - as do all feature teams on Valorant. Our entire backend microservice architecture is built using Golang. This means that everything from spinning up and managing a game server process to purchasing items is all done using services written in Go. Though there have been many benefits to using Golang for all our services, I’m going to talk about three specific language features: concurrency primitives, implicit interfaces, and package modularity.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eConcurrency Primitives\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWe leverage Golang concurrency primitives in order to add back pressure when operations start slowing down, to parallelize independent operations, and to run background processes within our applications. One example of this is we often find ourselves in a chain of execution on a match but need to do something for each player, loading skin data for each player when starting a match for example. Our requirements for a shared function to accomplish this were to return once all subroutines were finished executing and return back a list of any errors that occurred.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efunc Execute(funcList []func() error) []error\u003c/code\u003e\u003c/pre\u003e\u003cp dir=\"ltr\"\u003eWe accomplished this by using two channels and a waitgroup. One channel was to capture the errors as each \u003ca href=\"https://en.wikipedia.org/wiki/Thunk\" target=\"_blank\"\u003ethunk\u003c/a\u003e executed, while the other was a finished channel that a Goroutine sent on when the waitgroup finished. The language features made this very common pattern straightforward to implement.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eImplicit Interfaces\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eAnother language feature we use extensively is implicit interfaces. We leverage them pretty heavily to test our code and as a tool to create modular code. For example, we set out early on that we would have a common datastore interface in all our services. This is an interface that every one of our services use in order to interact with a data source.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/golang5.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThis simple interface allowed us to implement many different backends in order to accomplish different things. We typically use an in-memory implementation for most of our tests and the small interface makes it very lightweight to implement inline in a test file for unique cases like access counts or to test our error handling. We also use a mixture of SQL and Redis for our services and have an implementation for both using this interface. This makes attaching a datastore to a new service particularly easy and also makes the ability to add more specific cases, like a write-through in memory cache backed by redis, also very possible.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003ePackage Modularity\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eLastly, something I would like to call out that isn’t necessarily a language feature is the wide selection of available third party packages that often can be used interchangeably with common builtin packages. This has helped us make changes that I would expect to be a larger refactor very small because of the modular nature of golang packages. For example, a few of our services were spending a lot of CPU cycles serializing and deserializing JSON. We used Golang’s out-of-the-box \u003ca href=\"https://golang.org/pkg/encoding/json/\" target=\"_blank\"\u003ejson package\u003c/a\u003e when first writing all our services. This works for 95% of use cases and typically JSON serialization does not show up on a flame graph (which now that I think of it golang’s built-in profiling tools are top notch as well). There were a few cases specifically around serializing large objects where a lot of a service’s time was spent in the json serializer. We set out to optimize and turns out there are many alternative third-party JSON packages that are compatible with the built in package. This made the change as easy as changing this line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport \u0026#34;json\u0026#34;\u003c/code\u003e\u003c/pre\u003e\u003cp dir=\"ltr\"\u003eto :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport \u0026#34;github.com/custom-json-library/json\u0026#34;\u003c/code\u003e\u003c/pre\u003e\u003cp dir=\"ltr\"\u003eAfterwards, any calls to the JSON library used the third-party library which made profiling and testing different packages easy.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eAaron back again! Now that we’ve taken a look at some Golang use cases across Riot, I’d like to show you how we’re all connected. The flexibility teams have when choosing tech stacks relies on the collaborative environment of Rioter technologists. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eRiot Games is a very social company, and our \u003ca href=\"https://www.riotgames.com/en/work-with-us/disciplines/engineering/riots-tech-community\" target=\"_blank\"\u003eTech department encourages Rioters\u003c/a\u003e to engage with learning and development communities. For example, our various Communities of Practice enable groups of Rioters with common interests to gather regularly to learn and share together. One of most active technical communities is the Go community, which I currently run. There’s a Slack channel to discuss new proposals, and we have a monthly meetup where members present either a topic they’re aware of or learning about, or Riot projects written in Go. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe also aspire to involve the community outside of Riot with talks from open source library maintainers. The CoP is also a place to coordinate changes that impact multiple teams such as discussions around security when the \u003ca href=\"https://blog.golang.org/module-mirror-launch\" target=\"_blank\"\u003emodule mirror\u003c/a\u003e launched. There are also discussions around bumping build containers, dealing with gotchas that we may encounter, or asking general questions about approach, tooling, or libraries to seek out individual expertise in another part of the org.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eI personally love having a channel consisting of Go enthusiasts across teams and disciplines to bounce ideas, discuss language changes, and share libraries we come across. This channel was the central point of discussion as we transitioned from old dependency solutions to Go modules and it’s a great way to meet engineers who are passionate about the language. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg src=\"https://technology.riotgames.com/sites/default/files/golang6.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eThe Go CoP’s flier.\u003c/em\u003e\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eAt Riot, a number of teams maintain services and tools written in the Go language. Go provides a robust standard library and great third party community support to help satisfy our development needs. \u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOur Community of Practice is a great way for developers to contribute to Go use at Riot and share their learnings and experiences. We’re excited about the future of Go at Riot, with the ability to stay flexible and highly communicative across the entire company.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading! Feel free to post any questions or comments below.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/116/golangheader.png\" width=\"1999\" height=\"758\" alt=\"\" /\u003e\n\u003cp\u003eHi, my name is Aaron Torres and I’m an engineering manager for the Riot Developer Experience team. We accelerate how game teams across Riot develop, deploy, and operate their backend microservices at scale - globally. I’ve been at the company for a little over 3 years and I’ve been writing Go code that entire time. In this article, we’ll be specifically looking at how a few different teams use Go. I’ll be tagging in two technologists - Chad Wyszynski from RDX Operability and Justin O’Brien from \u003cem\u003eVALORANT - \u003c/em\u003eto discuss how they use Go for their projects.\u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/116/golangheader.png",
      "date_published": "2020-11-06T18:09:11Z",
      "author": {
        "name": "Aaron Torres with Chad Wyszynski \u0026 Justin O’Brien"
      }
    },
    {
      "id": "",
      "url": "https://technology.riotgames.com/news/how-riot-games-uses-slack",
      "title": "How Riot Games Uses Slack",
      "content_html": "\u003cdiv id=\"readability-page-1\" class=\"page\"\u003e\u003cdiv property=\"content:encoded\"\u003e\u003cp dir=\"ltr\"\u003eHello!\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eI’m Byron Dover, engineering manager for information technology at Riot, and I lead the team responsible for developing enterprise software at Riot - or as we sometimes call it, Riot’s Operating System. I’m excited to share a look at how Riot integrates with Slack to support the game development lifecycle.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eI recently spoke at the Slack Frontiers conference on this topic - the video is \u003ca href=\"https://youtu.be/EXBpY5xTpbE?t=1163\" target=\"_blank\"\u003eavailable on YouTube\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eSlack adoption at Riot goes back several years, and started with small pockets of organic usage by Rioters in 2014. Interest quickly grew, and we officially adopted Slack as our primary real-time communications platform in 2015, replacing Atlassian HipChat and IRC at the time.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack1.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn 2017, we became one of Slack’s very first \u003ca href=\"https://slack.com/enterprise\" target=\"_blank\"\u003eEnterprise Grid\u003c/a\u003e customers, after an extensive early adopter alpha and beta testing period. Since then, our Slack usage has grown dramatically in both raw numbers and complexity, and today we’ll provide a little more insight into some of the ways we’re using Slack to accelerate game development at Riot.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eRiot supports more than 6,000 Slack users globally, and nearly 1,000 installed apps and integrations. The majority of Riot employees and external partners rely on Slack communication to do their jobs every day.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack2.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOur Slack app and integrations ecosystem is vast. We rely heavily on custom integrations which meet our strict information security requirements, along with a handful of heavily vetted marketplace apps (like \u003ca href=\"https://slack.com/apps/A6NL8MJ6Q-google-drive\" target=\"_blank\"\u003eGoogle Drive\u003c/a\u003e).\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSlack is also a key part of Riot’s enterprise strategy, in support of our player-focused company mission. It helps us streamline game development through a rich ecosystem of apps and integrations, and expedites incident response, triaging, and issue resolution.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eSlack plays an important role in engineering productivity at Riot. Investing in deeper and richer integrations has reduced mean time to resolution (MTTR) times and increased developer velocity.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eLet’s dive into how engineering teams at Riot use Slack to increase build pipeline speed and visibility.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eJENKINS PIPELINE INTEGRATIONS\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eOur first developer highlight comes from our \u003cem\u003eLegends of Runeterra\u003c/em\u003e team.\u003cem\u003e \u003c/em\u003e\u003ca href=\"https://playruneterra.com/\" target=\"_blank\"\u003e\u003cem\u003eLegends of Runeterra\u003c/em\u003e\u003c/a\u003e is a free-to-play digital collectible card game that supports PC, Android, and iOS cross-play. Internally we’ve developed a \u003ca href=\"https://technology.riotgames.com/news/revisiting-docker-and-jenkins\" target=\"_blank\"\u003esophisticated build and release pipeline\u003c/a\u003e for this game to ensure new versions are available to players simultaneously across all platforms.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBehind the scenes, \u003cem\u003eLegends of Runeterra\u003c/em\u003e build pipelines are powered by \u003ca href=\"https://technology.riotgames.com/news/thinking-inside-container\" target=\"_blank\"\u003eDocker\u003c/a\u003e and \u003ca href=\"https://slack.com/apps/A0F7VRFKN-jenkins-ci\" target=\"_blank\"\u003eJenkins\u003c/a\u003e. Riot engineers rely on custom Slack apps to provide insight and visibility into game builds and deploys in real time.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWhen a \u003cem\u003eLegends of Runeterra\u003c/em\u003e build starts, a new message is posted to a shared Slack channel. When a build fails, an alert is automatically posted to Slack which includes a detailed accounting of what went wrong. It even @ mentions the developer responsible for the latest code changes to begin immediate triage.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack3_0.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThese Slack applications and integrations can be highly interactive, with specific actions available to developers at every step in the form of buttons, dialogs, screens, and workflows.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eIn this way, Slack provides a single pane of glass into the build and deployment lifecycle for our engineers, reducing the need to context switch between a myriad of different continuous integration tools just to understand the status of game builds and releases.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eLEVERAGING DIRECT MESSAGES\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eNext up, a developer highlight from our \u003cem\u003eTeamfight Tactics\u003c/em\u003e team. \u003ca href=\"https://teamfighttactics.leagueoflegends.com/\" target=\"_blank\"\u003e\u003cem\u003eTeamfight Tactics\u003c/em\u003e\u003c/a\u003e is a free-to-play auto-battler game. Like \u003cem\u003eLegends of Runeterra\u003c/em\u003e, \u003cem\u003eTeamfight Tactics\u003c/em\u003e also supports PC, Android, and iOS cross-play, and benefits from sharing much of the same infrastructure and build/release technology as \u003ca href=\"https://leagueoflegends.com/\" target=\"_blank\"\u003e\u003cem\u003eLeague of Legends\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo reduce noise - and possibly some developer embarrassment - in public channels, we’ve developed a Slack app named Game Build and Deploy “GBaD” Bot, which sends engineers direct messages whenever they break a game build, and lets them know once they’ve fixed the issue.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack4.png\"/\u003e\u003c/p\u003e\n\u003cp\u003eThis implementation is a favorite among our R\u0026amp;D teams, who prefer to keep their build notifications out of public channels, and can still reap the benefits of automated build and deploy notifications and pipeline integrations via direct messages.\u003c/p\u003e\n\u003cp\u003eWe love the flexibility Slack provides when it comes to build and deploy pipeline integrations. Slack’s real-team communication platform allows Riot to mirror our organization in digital channels and workspaces, while the robust APIs and SDKs allow us to deeply integrate these channels with our source code repositories and work management tools to accelerate the game development lifecycle.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eJust as important as developing games is ensuring they’re up and running when players log on to play\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eLet’s take a look at how Riot engineers use Slack to stay on top of operations and on-call support.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eMONITORING \u0026amp; ALERTING\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eMany of our game teams integrate with \u003ca href=\"https://slack.com/apps/A011MFBJEUU-sentry\" target=\"_blank\"\u003eSentry\u003c/a\u003e for robust crash analytics and diagnostics. Sentry provides a feature-rich marketplace application which integrates directly with Riot’s game operations and monitoring channels.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack5.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSentry automatically captures an enormous amount of valuable information whenever a player experiences a game crash, including screenshots and the actions that occurred directly before a crash. A summary of these events is automatically posted to actively monitored Slack channels. On-call engineers can assign crashes to colleagues directly via Slack, or click through to learn more.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAnother Slack marketplace app we use heavily is \u003ca href=\"https://slack.com/apps/A1FKYAUUX-pagerduty\" target=\"_blank\"\u003ePagerDuty\u003c/a\u003e. PagerDuty provides excellent incident, service, and escalation policy management, which we ingest directly into Slack for central control and visibility.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack6.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe route PagerDuty alerts to dedicated notification channels created for each team at Riot. From there, teams can escalate as needed to a universal channel shared across all of our workspaces, which is actively monitored by Riot’s global Network Operations Center.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003ePINNED BUTTONS\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003ePinned Buttons is a custom Slack app we built to help Rioters surface information and request help from other teams. Pinned Buttons attach themselves just above the message bar, giving Rioters contextual links and references before they send a message.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack7.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe want to give support teams an easy way to highlight the most common information that Rioters need to do their jobs. Whether it’s a link to a FAQ, a runbook, or a way to submit a ticket, teams can configure Pinned Buttons to highlight the most useful contextual information for their channels.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack8_0.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003ePinned Buttons works by ensuring it’s always the latest message in the channel. The app listens to each channel in which it’s installed, and guarantees it’s always the bottom-most message by instantly posting itself again every time someone sends a new message (cleaning up its previous message each time).\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003ePinned Buttons is configurable per channel, with a custom banner and up to three buttons. Simply inviting the app into a channel will present users with a configuration model. After the initial setup, Rioters can use the /pinned_buttons slash command to update their channel configurations. Lastly, if Pinned Buttons are no longer of use or teams want to pause the functionality, they can simply kick the bot out the channel.\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eGOING CUSTOM\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWe’ve built hundreds of custom applications at Riot to augment everything from peer recognition to on-call support for engineering teams.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOne of our most popular custom apps is Riot’s dev team support app. This app works by listening for emoji reactions on messages in support channels, and then kicking off a triage workflow whenever the appropriate reactji is used.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack9.png\"/\u003e\u003c/p\u003e\n\u003cp\u003eReporters are prompted to categorize the issue based on their original message - information which the Slack app uses to automatically notify an on-call engineer from the appropriate team. The app @ mentions the on-call engineer in a threaded reply, and the conversation continues between the two Rioters in that thread as normal.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack10.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOn-call engineers don’t need to monitor or read every message in the support channel. They can trust the support app to @ mention them in any thread that requires their attention.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack11.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eOnce an issue is resolved, the app captures the entire threaded conversation in the appropriate incident management tool, and it even provides a link to the Slack thread for future reference.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eLet’s take a closer look at how we approach building custom Slack apps at Riot with PoroBot, one of our newest innovations in support request intake and management.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003ePoroBot is a custom-built Slack AI that can be trained to answer questions in Slack channels, triage and classify support tickets, and provide useful information based on previously answered questions by that channel\u0026#39;s on-call team.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack12-0.png\"/\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack12-0.gif\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eAdding PoroBot AI to a channel is self-service via /invite @porobot, and the app can be configured to create and manage tickets in either Jira and ServiceNow (via a pluggable work systems architecture). PoroBot uses extensible workflows behind the scenes, and leverages natural language processing (NLP) and machine learning (ML) models for pattern recognition and classification.\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack13_0.gif\"/\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003eDEVELOPER FRAMEWORK\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003ePoroBot uses \u003ca href=\"https://slack.dev/bolt-js/\" target=\"_blank\"\u003e@slack/bolt\u003c/a\u003e, which provides a great abstraction layer and SDK for quickly building Slack apps and integrations.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eBolt allows for quick bootstrapping while still leaving room for a high degree of customization and extensibility when we need to go deeper. The Bolt framework uses \u003ca href=\"https://expressjs.com/\" target=\"_blank\"\u003eExpress.js\u003c/a\u003e to abstract Slack events into API routes. In doing so, it also provides a heap of handy and familiar functions for interacting with Slack. We typically organize the code according to a consistent design pattern which abstracts away and separates the listener service and event workflows from the commands and business logic they invoke.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack14.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eUsing this approach with Bolt allows us to dive right into developing command and listener logic, without having to reinvent the wheel on event subscription mechanics or worrying about where we’re going to deploy the Slack app itself.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003elisteners.ts\u003c/em\u003e\u003c/p\u003e\n\u003cdiv dir=\"ltr\"\u003e\n\u003cpre\u003e\u003ccode\u003eimport { app } from \u0026#39;@core/app\u0026#39;;\n \nimport { exportMessagesListener } from \u0026#39;@commands/export_messages\u0026#39;;\nimport { notesListener } from \u0026#39;@commands/notes\u0026#39;;\nimport { wordcloudListener } from \u0026#39;@commands/wordcloud\u0026#39;;\nimport { authorizeListener, silentAckListener } from \u0026#39;@core/listeners\u0026#39;;\n \n// Register listeners\napp.command(\u0026#39;/export_messages\u0026#39;, authorizeListener, exportMessagesListener);\napp.command(\u0026#39;/notes\u0026#39;, silentAckListener, notesListener);\napp.command(\u0026#39;/wordcloud\u0026#39;, authorizeListener, wordcloudListener);\n \n// Export the completed app with listeners\nexport const server = app;\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e\n\u003ch2 dir=\"ltr\"\u003eCODE DEPLOYMENT\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eSlack’s Bolt framework provides us with the flexibility to deploy Slack apps in a number of locations and contexts, including serverless compute, depending on the use case. Here are a few examples of how we can extract and extend Bolt’s built-in Express.js server to suit a variety of different deployment environments.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cem\u003eLocal Development (Docker)\u003c/em\u003e\u003c/p\u003e\n\u003cdiv dir=\"ltr\"\u003e\n\u003cpre\u003e\u003ccode\u003e// Start the app\n(async () =\u0026gt; {\n  await initEnv();\n  const { server } = await import (\u0026#39;../listeners\u0026#39;);\n  await server.start(process.env.PORT || 3000);\n  console.log(\u0026#39;⚡️ Bolt app is running!\u0026#39;);\n})();\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cem\u003eAWS Lambda\u003c/em\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cpre\u003e\u003ccode\u003eimport { App, ExpressReceiver } from \u0026#39;@slack/bolt\u0026#39;\nimport { APIGatewayEvent, Context, Callback } from \u0026#39;aws-lambda\u0026#39;\nimport { createServer, proxy } from \u0026#39;aws-serverless-express\u0026#39;\n\nconst expressReceiver = new ExpressReceiver({\n  signingSecret: process.env[\u0026#39;SLACK_SIGNING_SECRET\u0026#39;]\n})\n\nexport const app = (event, context, callback) =\u0026gt; {\n  const server = createServer(expressReceiver.app)\n\n  context.succeed = (response) =\u0026gt; {\n    server.close()\n    callback(undefined, response)\n }\n\n  return proxy(server, event, context)\n}\u003c/code\u003e\u003c/pre\u003e\u003cp dir=\"ltr\"\u003e\u003cem\u003eGoogle Cloud Functions (Firebase)\u003c/em\u003e\u003c/p\u003e\n\u003cdiv dir=\"ltr\"\u003e\n\u003cpre\u003e\u003ccode\u003eimport { ExpressReceiver } from \u0026#39;@slack/bolt\u0026#39;\nimport { config, https } from \u0026#39;firebase-functions\u0026#39;\n\nconst expressReceiver = new ExpressReceiver({\n  signingSecret: config().slack_signing_secret\n})\n\nexport const slackapp = https.onRequest(expressReceiver.app)\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFor security and transparency, custom Slack apps deployed at Riot proxy their events through a universal Slack apps router similar to \u003ca href=\"https://eng.lyft.com/announcing-omnibot-a-slack-proxy-and-slack-bot-framework-d4e32dd85ee4\" target=\"_blank\"\u003eomnibot\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\n\u003ch2 dir=\"ltr\"\u003eMANAGING STATE\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eBolt also handy provides contextual hooks for managing state. We take advantage of this by injecting custom middleware functions into Bolt, which PoroBot uses to gather data about the requesting user and their internal account details.\u003c/p\u003e\n\u003cul\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eConversation Store:\u003c/strong\u003e We extend this to persist Slack conversation state beyond the original request. We do this with a Redis cache for short-lived workflows, and PostgreSQL for long-term storage.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eContext:\u003c/strong\u003e A shallow object which we can read/write to as needed between function calls.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp dir=\"ltr\"\u003ePoroBot hydrates state via middleware hooks, and provides that data as appropriate to the specific workflows being initialized. Workflows are built using a simple finite state machine triggered by Slack events (e.g. messages, button clicks, etc.) and containing metadata attached during middleware hydration (auto-response classifications, work system IDs, etc.).\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack15_0.gif\"/\u003e\u003c/p\u003e\n\u003ch2 dir=\"ltr\"\u003ePUSHING THE BOUNDARIES WITH MACHINE LEARNING\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eWhen a message enters a channel, PoroBot automatically attempts to classify it to determine whether or not to trigger an auto-response. Improving the classification model involves two distinct components: Patterns and Annotations.\u003c/p\u003e\n\u003col\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003ePatterns\u003c/strong\u003e: Analyze channel content and identify popular or common patterns of speech, then use this to create a list of source messages which are distilled into a generic syntax for the way questions are being asked. This syntax can be translated into patterns for deep learning with \u003ca href=\"https://spacy.io/\" target=\"_blank\"\u003eSpaCy\u003c/a\u003e \u003cem\u003ePatternMatcher\u003c/em\u003e.\u003cbr/\u003e\nEssentially, patterns are Slack messages that have been reduced to only the words that create meaning. We typically analyze and store 10–50 patterns per topic to make sure we have adequate coverage.\u003cbr/\u003e\nUsing patterns is adequate for most use cases.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003e\u003cstrong\u003eAnnotations\u003c/strong\u003e: Once patterns have been battle-tested, annotation tools help label the data which we then use to train a convolutional neural network (CNN) text classifier using SpaCy.\u003cbr/\u003e\nThis method is generally used when there are similar question classifications that aren’t easily targeted with patterns. We use \u003ca href=\"http://prodigy.ai/\" target=\"_blank\"\u003eProdigy\u003c/a\u003e as an aid in annotating the data, which allows us to train and correct the model in real time.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack16.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003ePoroBot and similar Slack AIs are still relatively new at Riot, and we’re excited to continue to push the envelope of what’s possible with Slack integrations to improve the game development lifecycle for Riot designers and engineers.\u003c/p\u003e\n\n\u003cp dir=\"ltr\"\u003eAs we’ve seen, Slack enables a rich ecosystem of integrations which allows Rioters to spend more time focused on delivering delightful player experiences.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eHere are our key learnings and takeaways from managing the Slack ecosystem at Riot:\u003c/p\u003e\n\u003col\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eAdopt best-in-class Slack apps like \u003ca href=\"https://slack.com/apps/A011MFBJEUU-sentry\" target=\"_blank\"\u003eSentry\u003c/a\u003e and \u003ca href=\"https://slack.com/apps/A1FKYAUUX-pagerduty\" target=\"_blank\"\u003ePagerDuty\u003c/a\u003e to reduce incident resolution time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eGive your developers a single pane of glass into their build pipelines to reduce context switching.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eAutomate support intake and on-call assignments so the rest of the dev team stays focused on feature delivery.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eLeverage buttons, dialogs and workflows to enable your developers take action directly from Slack.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli dir=\"ltr\"\u003e\n\u003cp dir=\"ltr\"\u003eBalance use of public channel and direct message integrations to improve focus and reduce noise.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 dir=\"ltr\"\u003eCOMMUNITY AND ENGAGEMENT\u003c/h2\u003e\n\u003cp dir=\"ltr\"\u003eDeveloping video games doesn’t just involve code, it requires humans working together on teams to design and create incredible player experiences. These human systems by nature involve a lot of complexity, which Slack helps us navigate by providing convenient and accessible locations to have the important conversations.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eWe have a discussion channel where our Diversity \u0026amp; Inclusion teams share insights into what they\u0026#39;re working on, and encourage open dialogue about diversity at Riot. Our GG Bot Slack app allows Rioters to recognize their peers, highlighting the specific Riot values exemplified by colleagues. We also have a vibrant social workspace where Rioters bond over thousands of different interests and hobbies - from pets and plants to boba and motorcycles, and everything in between.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eTo help keep our communication (and memes) strong, we also have a rich custom emoji culture. There are currently over 17,500 emojis at Riot! It’s a language all on its own, with reactions on messages being used in our development processes to initiate support requests and acknowledge when something is done. Fun fact: in the early days of Slack, we managed to accidentally DoS crash Slack’s emoji upload API while attempting a bulk emoji transfer between workspaces.\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eSlack has scaled up a lot since then, meanwhile our emoji game remains as strong as ever. \u003cimg src=\"https://technology.riotgames.com/sites/default/files/riotslack17.png\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003e\u003cimg alt=\"\" src=\"https://technology.riotgames.com/sites/default/files/riotslack18-0.gif\"/\u003e\u003c/p\u003e\n\u003cp dir=\"ltr\"\u003eThanks for reading! If you have any questions or comments, please post them below.\u003c/p\u003e\n\u003c/div\u003e\u003c/div\u003e",
      "summary": "\u003cimg typeof=\"foaf:Image\" src=\"https://technology.riotgames.com/sites/default/files/articles/115/slackriotheader2.png\" width=\"1640\" height=\"624\" alt=\"\" /\u003e\n\u003cp\u003eI’m Byron Dover, engineering manager for information technology at Riot, and I lead the team responsible for developing enterprise software at Riot - or as we sometimes call it, Riot’s Operating System. I’m excited to share a look at how Riot integrates with Slack to support the game development lifecycle.\u003c/p\u003e",
      "image": "https://technology.riotgames.com/sites/default/files/articles/115/slackriotheader2.png",
      "date_published": "2020-10-13T17:00:00Z",
      "author": {
        "name": "Byron Dover"
      }
    }
  ]
}
